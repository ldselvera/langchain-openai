{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147c91e7",
   "metadata": {},
   "source": [
    "# Installing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1266e61",
   "metadata": {},
   "source": [
    "- langchain\n",
    "- openai\n",
    "- tqdm: library to show the progress of an action (downloading, training, ...) \n",
    "- jq: lightweight and flexible JSON processor\n",
    "- unstructured: A library that prepares raw documents for downstream ML tasks\n",
    "- pypdf: A pure-python PDF library capable of splitting, merging, cropping, and transforming PDF files\n",
    "- tiktoken: a fast open-source tokenizer by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75736381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain openai tqdm unstructured pypdf tiktoken\n",
    "# %pip jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c725a3e0",
   "metadata": {},
   "source": [
    "# Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2832d6-4f98-4317-858e-1cfe14f67187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "import pandas as pd\n",
    "# https://www.kaggle.com/datasets/guillemservera/global-daily-climate-data?select=daily_weather.parquet\n",
    "df =  pd.read_parquet('data/daily_weather.parquet', engine='pyarrow')\n",
    " \n",
    "documents = []\n",
    "for i, row in df.head().iterrows():\n",
    "    string = \" \".join([\n",
    "         \"{}: {}\".format(k, v) \n",
    "         for k, v in row.to_dict().items()\n",
    "    ])\n",
    "    doc = Document(\n",
    "        page_content=string,\n",
    "        meta_data={'row': i}\n",
    "    )\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732819cf-5125-4883-b9e6-845488fa6ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='station_id: 41515 city_name: Asadabad date: 1957-07-01 00:00:00 season: Summer avg_temp_c: 27.0 min_temp_c: 21.1 max_temp_c: 35.6 precipitation_mm: 0.0 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan'),\n",
       " Document(page_content='station_id: 41515 city_name: Asadabad date: 1957-07-02 00:00:00 season: Summer avg_temp_c: 22.8 min_temp_c: 18.9 max_temp_c: 32.2 precipitation_mm: 0.0 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan'),\n",
       " Document(page_content='station_id: 41515 city_name: Asadabad date: 1957-07-03 00:00:00 season: Summer avg_temp_c: 24.3 min_temp_c: 16.7 max_temp_c: 35.6 precipitation_mm: 1.0 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan'),\n",
       " Document(page_content='station_id: 41515 city_name: Asadabad date: 1957-07-04 00:00:00 season: Summer avg_temp_c: 26.6 min_temp_c: 16.1 max_temp_c: 37.8 precipitation_mm: 4.1 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan'),\n",
       " Document(page_content='station_id: 41515 city_name: Asadabad date: 1957-07-05 00:00:00 season: Summer avg_temp_c: 30.8 min_temp_c: 20.0 max_temp_c: 41.7 precipitation_mm: 0.0 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3bbbda-341c-4103-ac3b-7b7401bb89ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='station_id: 41515 city_name: Asadabad date: 1957-07-01 00:00:00 season: Summer avg_temp_c: 27.0 min_temp_c: 21.1 max_temp_c: 35.6 precipitation_mm: 0.0 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84011167-9add-49a9-b238-19b8f8df75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import (\n",
    "    UnstructuredCSVLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredImageLoader,\n",
    "    PythonLoader,\n",
    "    PyPDFLoader,\n",
    "    JSONLoader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8648c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hastie.su.domains/Papers/ESLII.pdf\n",
    "file_path = 'mixed_data/element_of_SL.pdf'\n",
    "\n",
    "sl_loader = PyPDFLoader(file_path=file_path)\n",
    "sl_data = sl_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18a4cc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\\nnology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book‚Äôs coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting‚Äîthe first comprehensive treatment of thistopic in any book.\\nThis major new edition features many topics not covered in the original, including graphical\\nmodels, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for ‚Äúwide‚Äù data (p bigger than n), including multiple testing and false discovery rates.\\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\\nStanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\\n‚Ä∫springer.comSTATISTICS\\nisbn 978-0-387-84857-0Trevor Hastie ‚Ä¢ Robert Tibshirani ‚Ä¢ Jerome Friedman\\nThe Elements of Statictical Learning\\nHastie ‚Ä¢ Tibshirani ‚Ä¢ Friedman\\nSecond Edition', metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a131e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter\n",
    ")\n",
    "\n",
    "# split on \"\\n\\n\"\n",
    "splitter1 = CharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "# split [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "splitter2 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "\n",
    "sl_data1 = sl_loader.load_and_split(text_splitter=splitter1)\n",
    "sl_data2 = sl_loader.load_and_split(text_splitter=splitter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc0fc702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2173"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sl_data1[600].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27c185a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sl_data2[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac9933b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "folder_path = 'mixed_data/'\n",
    "\n",
    "mixed_loader = DirectoryLoader(\n",
    "    path=folder_path,\n",
    "    use_multithreading=True,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "mixed_data = mixed_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3930196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='DEEP LEARNING\\n\\nDS-GA 1008 ¬∑ SPRING 2021 ¬∑ NYU CENTER FOR DATA SCIENCE\\n\\nINSTRUCTORS Yann LeCun & Alfredo Canziani LECTURES Wednesday 9:30 ‚Äì 11:30, Zoom PRACTICA Tuesdays 9:30 ‚Äì 10:30, Zoom FORUM r/NYU_DeepLearning DISCORD NYU DL MATERIAL 2021 repo\\n\\n2021 edition disclaimer\\n\\nCheck the repo‚Äôs README.md and learn about:\\n\\nContent new organisation\\n\\nThe semester‚Äôs second half intellectual dilemma\\n\\nThis semester repository\\n\\nPrevious releases\\n\\nLectures\\n\\nMost of the lectures, labs, and notebooks are similar to the previous edition, nevertheless, some are brand new.\\nI will try to make clear which is which.\\n\\nLegend: üñ• slides, üìù notes, üìì Jupyter notebook, üé• YouTube video.\\n\\nTheme 1: Introduction\\n\\nHistory and resources üé• üñ•\\n\\nGradient descent and the backpropagation algorithm üé• üñ•\\n\\nNeural nets inference üé• üìì\\n\\nModules and architectures üé• üñ•\\n\\nNeural nets training üé• üñ• üìì üìì\\n\\nHomework 1: backprop\\n\\nTheme 2: Parameters sharing\\n\\nRecurrent and convolutional nets üé• üñ• üìù\\n\\nConvNets in practice üé• üñ• üìù\\n\\nNatural signals properties and the convolution üé• üñ• üìì\\n\\nRecurrent neural networks, vanilla and gated (LSTM) üé• üñ• üìì üìì\\n\\nHomework 2: RNN & CNN\\n\\nTheme 3: Energy based models, foundations\\n\\nEnergy based models (I) üé• üñ•\\n\\nInference for LV-EBMs üé• üñ•\\n\\nWhat are EBMs good for? üé•\\n\\nEnergy based models (II) üé• üñ• üìù\\n\\nTraining LV-EBMs üé• üñ•\\n\\nHomework 3: structured prediction\\n\\nTheme 4: Energy based models, advanced\\n\\nEnergy based models (III) üé• üñ•\\n\\nUnsup learning and autoencoders üé• üñ•\\n\\nEnergy based models (VI) üé• üñ•\\n\\nFrom LV-EBM to target prop to (any) autoencoder üé• üñ•\\n\\nEnergy based models (V) üé• üñ•\\n\\nAEs with PyTorch and GANs üé• üñ• üìì üìì\\n\\nJoint Embedding Methods (I) üé• üñ• üñ•\\n\\nJoint Embedding Methods (II) üé• üñ•\\n\\nTheme 5: Associative memories\\n\\nEnergy based models (V) üé• üñ•\\n\\nAttention & transformer üé• üñ• üìì\\n\\nTheme 6: Graphs\\n\\nGraph transformer nets [A][B] üé• üñ•\\n\\nGraph convolutional nets (I) [from last year] üé• üñ•\\n\\nGraph convolutional nets (II) üé• üñ• üìì\\n\\nTheme 7: Control\\n\\nPlanning and control üé• üñ•\\n\\nThe Truck Backer-Upper üé• üñ• üìì\\n\\nPrediction and Planning Under Uncertainty üé• üñ•\\n\\nTheme 8: Optimisation\\n\\nOptimisation (I) [from last year] üé• üñ•\\n\\nOptimisation (II) üé• üñ• üìù\\n\\nMiscellaneous\\n\\nSSL for vision [A][B] üé• üñ•\\n\\nLow resource machine translation [A][B] üé• üñ•\\n\\nLagrangian backprop, final project, and Q&A üé• üñ• üìù', metadata={'source': 'mixed_data\\\\DEEP LEARNING ¬∑ Deep Learning.html'}),\n",
       " Document(page_content=\"NYU Deep Learning Spring 2021 (NYU-DLSP21)\\n\\nüá¨üáß \\xa0 üá´üá∑\\n\\nContent new organisation\\n\\nThis semester we have reorganised the didactic material.\\nIn the first half of the semester we covered 3 topics, spanning two weeks, each followed by an assignment.\\nMoreover, each lecture had a corresponding practicum.\\n\\nHistory, backpropagation, and gradient descent\\n\\nParameter sharing: recurrent and convolutional networks\\n\\nLatent variable (LV) energy based models (EBMs)\\n\\nPay attention that we have redesigned the curriculum and lectures' content.\\nWe've treated LV-EBM as a basic module, which to build upon.\\n\\nEnters the semester's second half\\n\\nI thought I was going to repropose the same practica I've used during NYU-DLSP20, last year edition, just in different order.\\n\\nBut I couldn't.\\n\\nThis year's students have LV-EBMs on their side.\\nWe told them about the cake and now I cannot pretend it doesn't exist and teach as if they were unaware of the elephant in the room.\\nIt would have been intellectually dishonest.\\nHenceforth, I've redesigned my whole deck of slides.\\n\\nThis semester repository\\n\\nThat's why this repo has been created.\\nI'm not going to try to do the same insane work I've put up with last year, but I need a space where to post updated slides, notebooks, and host new transcriptions.\\nLast year material is still valid.\\nThis year you have a different take.\\nA more powerful one.\\n\\nPrevious releases\\n\\nBefore NYU-DLSP21 there were‚Ä¶\\n\\nNYU-DLSP20 (major release)\\n\\nNYU-DLSP19\\n\\nAIMS-DLFL19\\n\\nCoDaS-HEP18\\n\\nNYU-DLSP18\\n\\nPurdue-DLFL16\\n\\ntorch-Video-Tutorials\\n\\nMore info\\n\\nKeep reading on the class website.\", metadata={'source': 'mixed_data\\\\README.md'})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a6550",
   "metadata": {},
   "source": [
    "# Summarizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e05449",
   "metadata": {},
   "source": [
    "## The \"stuff\" chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c76dee1-35df-437b-9376-654bcd88fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d0db063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The \"Elements of Statistical Learning\" is a book that explores the field of statistical learning, focusing on concepts rather than mathematics. It covers topics such as data mining, machine learning, and bioinformatics, and includes a range of methods from supervised to unsupervised learning. The book is written by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, who are professors of statistics at Stanford University. The second edition includes new topics such as graphical models, random forests, and ensemble methods, making it a valuable resource for statisticians and those interested in data mining in science or industry.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='stuff'\n",
    ")\n",
    "\n",
    "chain.run(sl_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6196435b-66a5-4b9c-a9f3-fea4856b3864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The summary includes weather data for Asadabad from July 1st to July 5th, 1957 during the Summer season. The average temperatures ranged from 22.8¬∞C to 30.8¬∞C, with the highest temperature reaching 41.7¬∞C on July 5th. There was minimal precipitation on July 1st and July 5th, with a higher amount of 4.1mm on July 4th. Other weather parameters such as wind direction, speed, gust, sea level pressure, snow depth, and sunshine duration were not recorded.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa62cd8",
   "metadata": {},
   "source": [
    "## Custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64cc0c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e4a167e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'El libro \"The Elements of Statistical Learning\" es una obra que aborda el tema de la miner√≠a de datos, inferencia y predicci√≥n en el contexto del crecimiento de la tecnolog√≠a y la abundancia de datos en diversos campos. Escrito por Trevor Hastie, Robert Tibshirani y Jerome Friedman, abarca temas como aprendizaje supervisado y no supervisado, redes neuronales, m√°quinas de vectores de soporte, entre otros. La segunda edici√≥n incluye nuevos temas como modelos gr√°ficos, bosques aleatorios y m√©todos de conjunto. Los autores son reconocidos investigadores en el √°rea de la estad√≠stica en la Universidad de Stanford.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Write a concise summary of the following in spanish:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "CONCISE SUMMARY IN SPANISH:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt   \n",
    ")\n",
    "\n",
    "chain.run(sl_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e034ce43",
   "metadata": {},
   "source": [
    "## The Map-reduce chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d731eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Springer Series in Statistics\n",
      "Trevor Hastie\n",
      "Robert TibshiraniJerome FriedmanSpringer Series in Statistics\n",
      "The Elements of\n",
      "Statistical Learning\n",
      "Data Mining, Inference, and Prediction\n",
      "The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book‚Äôs coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting‚Äîthe first comprehensive treatment of thistopic in any book.\n",
      "This major new edition features many topics not covered in the original, including graphical\n",
      "models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for ‚Äúwide‚Äù data (p bigger than n), including multiple testing and false discovery rates.\n",
      "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\n",
      "Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
      "‚Ä∫springer.comSTATISTICS\n",
      "isbn 978-0-387-84857-0Trevor Hastie ‚Ä¢ Robert Tibshirani ‚Ä¢ Jerome Friedman\n",
      "The Elements of Statictical Learning\n",
      "Hastie ‚Ä¢ Tibshirani ‚Ä¢ Friedman\n",
      "Second Edition\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page v\n",
      "Printer: Opaque this\n",
      "To our parents:\n",
      "Valerie and Patrick Hastie\n",
      "Vera and Sami Tibshirani\n",
      "Florence and Harry Friedman\n",
      "and to our families:\n",
      "Samantha, Timothy, and Lynda\n",
      "Charlie, Ryan, Julie, and Cheryl\n",
      "Melanie, Dora, Monika, and Ildiko\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"vi\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page vii\n",
      "Printer: Opaque this\n",
      "Preface to the Second Edition\n",
      "In God we trust, all others bring data.\n",
      "‚ÄìWilliam Edwards Deming (1900-1993)1\n",
      "We have been gratiÔ¨Åed by the popularity of the Ô¨Årst edition of The\n",
      "Elements of Statistical Learning. This, along with the fast pace of research\n",
      "in the statistical learning Ô¨Åeld, motivated us to update our book with a\n",
      "second edition.\n",
      "We have added four new chapters and updated some of the existing\n",
      "chapters. Because many readers are familiar with the layout of the Ô¨Årst\n",
      "edition, we have tried to change it as little as possible. Here is a summary\n",
      "of the main changes:\n",
      "1On the Web, this quote has been widely attributed to both Demi ng and Robert W.\n",
      "Hayden; however Professor Hayden told us that he can claim no credit for this quote,\n",
      "and ironically we could Ô¨Ånd no ‚Äúdata‚Äù conÔ¨Årming that Deming a ctually said this.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"viii Preface to the Second Edition\n",
      "Chapter What‚Äôs new\n",
      "1.Introduction\n",
      "2.Overview of Supervised Learning\n",
      "3.Linear Methods for Regression LAR algorithm and generalizations\n",
      "of the lasso\n",
      "4.Linear Methods for ClassiÔ¨Åcation Lasso path for logistic regression\n",
      "5.Basis Expansions and Regulariza-\n",
      "tionAdditional illustrations of RKHS\n",
      "6.Kernel Smoothing Methods\n",
      "7.Model Assessment and Selection Strengths and pitfalls of cross-\n",
      "validation\n",
      "8.Model Inference and Averaging\n",
      "9.Additive Models, Trees, and\n",
      "Related Methods\n",
      "10.Boosting and Additive Trees New example from ecology; some\n",
      "material split oÔ¨Ä to Chapter 16.\n",
      "11.Neural Networks Bayesian neural nets and the NIPS\n",
      "2003 challenge\n",
      "12.Support Vector Machines and\n",
      "Flexible DiscriminantsPath algorithm for SVM classiÔ¨Åer\n",
      "13. Prototype Methods and\n",
      "Nearest-Neighbors\n",
      "14.Unsupervised Learning Spectral clustering, kernel PCA,\n",
      "sparse PCA, non-negative matrix\n",
      "factorization archetypal analysis,\n",
      "nonlinear dimension reduction,\n",
      "Google page rank algorithm, a\n",
      "direct approach to ICA\n",
      "15.Random Forests New\n",
      "16.Ensemble Learning New\n",
      "17.Undirected Graphical Models New\n",
      "18.High-Dimensional Problems New\n",
      "Some further notes:\n",
      "‚Ä¢Our Ô¨Årst edition was unfriendly to colorblind readers; in particular,\n",
      "we tended to favor red/green contrasts which are particularly trou-\n",
      "blesome. We have changed the color palette in this edition to a large\n",
      "extent, replacing the above with an orange /bluecontrast.\n",
      "‚Ä¢We have changed the name of Chapter 6 from ‚ÄúKernel Methods‚Äù to\n",
      "‚ÄúKernel Smoothing Methods‚Äù, to avoid confusion with the machine-\n",
      "learning kernel method that is discussed in the context of support vec-\n",
      "tor machines (Chapter 11) and more generally in Chapters 5 and 14.\n",
      "‚Ä¢In the Ô¨Årst edition, the discussion of error-rate estimation in Chap-\n",
      "ter 7 was sloppy, as we did not clearly diÔ¨Äerentiate the notions of\n",
      "conditional error rates (conditional on the training set) and uncondi-\n",
      "tional rates. We have Ô¨Åxed this in the new edition.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Preface to the Second Edition ix\n",
      "‚Ä¢Chapters 15 and 16 follow naturally from Chapter 10, and the chap-\n",
      "ters are probably best read in that order.\n",
      "‚Ä¢In Chapter 17, we have not attempted a comprehensive treatment\n",
      "of graphical models, and discuss only undirected models and some\n",
      "new methods for their estimation. Due to a lack of space, we have\n",
      "speciÔ¨Åcally omitted coverage of directed graphical models.\n",
      "‚Ä¢Chapter 18 explores the ‚Äú p‚â´N‚Äù problem, which is learning in high-\n",
      "dimensional feature spaces. These problems arise in many areas, in-\n",
      "cluding genomic and proteomic studies, and document classiÔ¨Åcation.\n",
      "We thank the many readers who have found the (too numerous) errors in\n",
      "the Ô¨Årst edition. We apologize for those and have done our best to avoid er-\n",
      "rors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry\n",
      "Wasserman for comments on some of the new chapters, and many Stanford\n",
      "graduate and post-doctoral students who oÔ¨Äered comments, in particular\n",
      "Mohammed AlQuraishi, John Boik, Holger HoeÔ¨Çing, Arian Maleki, Donal\n",
      "McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\n",
      "Hui Zou. We thank John Kimmel for his patience in guiding us through this\n",
      "new edition. RT dedicates this edition to the memory of Anna McPhee.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "August 2008\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"x Preface to the Second Edition\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page xi\n",
      "Printer: Opaque this\n",
      "Preface to the First Edition\n",
      "We are drowning in information and starving for knowledge.\n",
      "‚ÄìRutherford D. Roger\n",
      "The Ô¨Åeld of Statistics is constantly challenged by the problems that science\n",
      "and industry brings to its door. In the early days, these problems often came\n",
      "from agricultural and industrial experiments and were relatively small in\n",
      "scope. With the advent of computers and the information age, statistical\n",
      "problems have exploded both in size and complexity. Challenges in the\n",
      "areas of data storage, organization and searching have led to the new Ô¨Åeld\n",
      "of ‚Äúdata mining‚Äù; statistical and computational problems in biology and\n",
      "medicine have created ‚Äúbioinformatics.‚Äù Vast amounts of data are being\n",
      "generated in many Ô¨Åelds, and the statistician‚Äôs job is to make sense of it\n",
      "all: to extract important patterns and trends, and understand ‚Äúwhat the\n",
      "data says.‚Äù We call this learning from data .\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tistical sciences. Since computation plays such a key role, it is not surprising\n",
      "that much of this new development has been done by researchers in other\n",
      "Ô¨Åelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly categorized as\n",
      "either supervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "in unsupervised learning, there is no outcome measure, and the goal is to\n",
      "describe the associations and patterns among a set of input measures.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xii Preface to the First Edition\n",
      "This book is our attempt to bring together many of the important new\n",
      "ideas in learning, and explain them in a statistical framework. While some\n",
      "mathematical details are needed, we emphasize the methods and their con-\n",
      "ceptual underpinnings rather than their theoretical properties. As a result,\n",
      "we hope that this book will appeal not just to statisticians but also to\n",
      "researchers and practitioners in a wide variety of Ô¨Åelds.\n",
      "Just as we have learned a great deal from researchers outside of the Ô¨Åeld\n",
      "of statistics, our statistical viewpoint may help others to better understa nd\n",
      "diÔ¨Äerent aspects of learning:\n",
      "There is no true interpretation of anything; interpretatio n is a\n",
      "vehicle in the service of human comprehension. The value of\n",
      "interpretation is in enabling others to fruitfully think ab out an\n",
      "idea.\n",
      "‚ÄìAndreas Buja\n",
      "We would like to acknowledge the contribution of many people to the\n",
      "conception and completion of this book. David Andrews, Leo Breiman,\n",
      "Andreas Buja, John Chambers, Bradley Efron, GeoÔ¨Ärey Hinton, Werner\n",
      "Stuetzle, and John Tukey have greatly inÔ¨Çuenced our careers. Balasub-\n",
      "ramanian Narasimhan gave us advice and help on many computational\n",
      "problems, and maintained an excellent computing environment. Shin-Ho\n",
      "Bang helped in the production of a number of the Ô¨Ågures. Lee Wilkinson\n",
      "gave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Ma ya\n",
      "Gupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bog-\n",
      "dan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu\n",
      "Zhu, two reviewers and many students read parts of the manuscript and\n",
      "oÔ¨Äered helpful suggestions. John Kimmel was supportive, patient and help-\n",
      "ful at every phase; MaryAnn Brickner and Frank Ganz headed a superb\n",
      "production team at Springer. Trevor Hastie would like to thank the statis-\n",
      "tics department at the University of Cape Town for their hospitality during\n",
      "the Ô¨Ånal stages of this book. We gratefully acknowledge NSF and NIH for\n",
      "their support of this work. Finally, we would like to thank our families and\n",
      "our parents for their love and support.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "May 2001\n",
      "The quiet statisticians have changed our world; not by disco v-\n",
      "ering new facts or technical developments, but by changing t he\n",
      "ways that we reason, experiment and form our opinions ....\n",
      "‚ÄìIan Hacking\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page xiii\n",
      "Printer: Opaque this\n",
      "Contents\n",
      "Preface to the Second Edition vii\n",
      "Preface to the First Edition xi\n",
      "1 Introduction 1\n",
      "2 Overview of Supervised Learning 9\n",
      "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\n",
      "2.3 Two Simple Approaches to Prediction:\n",
      "Least Squares and Nearest Neighbors . . . . . . . . . . . 11\n",
      "2.3.1 Linear Models and Least Squares . . . . . . . . 11\n",
      "2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\n",
      "2.3.3 From Least Squares to Nearest Neighbors . . . . 16\n",
      "2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\n",
      "2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\n",
      "2.6 Statistical Models, Supervised Learning\n",
      "and Function Approximation . . . . . . . . . . . . . . . . 28\n",
      "2.6.1 A Statistical Model\n",
      "for the Joint Distribution Pr( X,Y) . . . . . . . 28\n",
      "2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\n",
      "2.6.3 Function Approximation . . . . . . . . . . . . . 29\n",
      "2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\n",
      "2.7.1 DiÔ¨Éculty of the Problem . . . . . . . . . . . . . 32\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xiv Contents\n",
      "2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\n",
      "2.8.1 Roughness Penalty and Bayesian Methods . . . 34\n",
      "2.8.2 Kernel Methods and Local Regression . . . . . . 34\n",
      "2.8.3 Basis Functions and Dictionary Methods . . . . 35\n",
      "2.9 Model Selection and the Bias‚ÄìVariance TradeoÔ¨Ä . . . . . 37\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "3 Linear Methods for Regression 43\n",
      "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\n",
      "3.2 Linear Regression Models and Least Squares . . . . . . . 44\n",
      "3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\n",
      "3.2.2 The Gauss‚ÄìMarkov Theorem . . . . . . . . . . . 51\n",
      "3.2.3 Multiple Regression\n",
      "from Simple Univariate Regression . . . . . . . . 52\n",
      "3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\n",
      "3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\n",
      "3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\n",
      "3.3.2 Forward- and Backward-Stepwise Selection . . . 58\n",
      "3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\n",
      "3.3.4 Prostate Cancer Data Example (Continued) . . 61\n",
      "3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\n",
      "3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\n",
      "3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\n",
      "3.4.3 Discussion: Subset Selection, Ridge Regression\n",
      "and the Lasso . . . . . . . . . . . . . . . . . . . 69\n",
      "3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\n",
      "3.5 Methods Using Derived Input Directions . . . . . . . . . 79\n",
      "3.5.1 Principal Components Regression . . . . . . . . 79\n",
      "3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\n",
      "3.6 Discussion: A Comparison of the Selection\n",
      "and Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\n",
      "3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\n",
      "3.8 More on the Lasso and Related Path Algorithms . . . . . 86\n",
      "3.8.1 Incremental Forward Stagewise Regression . . . 86\n",
      "3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\n",
      "3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\n",
      "3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\n",
      "3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\n",
      "3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\n",
      "3.9 Computational Considerations . . . . . . . . . . . . . . . 93\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xv\n",
      "4 Linear Methods for ClassiÔ¨Åcation 101\n",
      "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\n",
      "4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\n",
      "4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\n",
      "4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\n",
      "4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\n",
      "4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\n",
      "4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\n",
      "4.4.2 Example: South African Heart Disease . . . . . 122\n",
      "4.4.3 Quadratic Approximations and Inference . . . . 124\n",
      "4.4.4 L1Regularized Logistic Regression . . . . . . . . 125\n",
      "4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\n",
      "4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\n",
      "4.5.1 Rosenblatt‚Äôs Perceptron Learning Algorithm . . 130\n",
      "4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "5 Basis Expansions and Regularization 139\n",
      "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\n",
      "5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\n",
      "5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\n",
      "5.2.2 Example: South African Heart Disease (Continued)146\n",
      "5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\n",
      "5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\n",
      "5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\n",
      "5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\n",
      "5.5 Automatic Selection of the Smoothing Parameters . . . . 156\n",
      "5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\n",
      "5.5.2 The Bias‚ÄìVariance TradeoÔ¨Ä . . . . . . . . . . . . 158\n",
      "5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\n",
      "5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\n",
      "5.8.1 Spaces of Functions Generated by Kernels . . . 168\n",
      "5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\n",
      "5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\n",
      "5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\n",
      "5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Appendix: Computational Considerations for Splines . . . . . . 186\n",
      "Appendix: B-splines . . . . . . . . . . . . . . . . . . . . . 186\n",
      "Appendix: Computations for Smoothing Splines . . . . . 189\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xvi Contents\n",
      "6 Kernel Smoothing Methods 191\n",
      "6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\n",
      "6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\n",
      "6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\n",
      "6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\n",
      "6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\n",
      "6.4 Structured Local Regression Models in IRp. . . . . . . . 201\n",
      "6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\n",
      "6.4.2 Structured Regression Functions . . . . . . . . . 203\n",
      "6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\n",
      "6.6 Kernel Density Estimation and ClassiÔ¨Åcation . . . . . . . 208\n",
      "6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\n",
      "6.6.2 Kernel Density ClassiÔ¨Åcation . . . . . . . . . . . 210\n",
      "6.6.3 The Naive Bayes ClassiÔ¨Åer . . . . . . . . . . . . 210\n",
      "6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\n",
      "6.8 Mixture Models for Density Estimation and ClassiÔ¨Åcation 214\n",
      "6.9 Computational Considerations . . . . . . . . . . . . . . . 216\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "7 Model Assessment and Selection 219\n",
      "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\n",
      "7.3 The Bias‚ÄìVariance Decomposition . . . . . . . . . . . . . 223\n",
      "7.3.1 Example: Bias‚ÄìVariance TradeoÔ¨Ä . . . . . . . . 226\n",
      "7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\n",
      "7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\n",
      "7.6 The EÔ¨Äective Number of Parameters . . . . . . . . . . . . 232\n",
      "7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\n",
      "7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\n",
      "7.9 Vapnik‚ÄìChervonenkis Dimension . . . . . . . . . . . . . . 237\n",
      "7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\n",
      "7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\n",
      "7.10.1 K-Fold Cross-Validation . . . . . . . . . . . . . 241\n",
      "7.10.2 The Wrong and Right Way\n",
      "to Do Cross-validation . . . . . . . . . . . . . . . 245\n",
      "7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\n",
      "7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\n",
      "7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\n",
      "7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "8 Model Inference and Averaging 261\n",
      "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xvii\n",
      "8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\n",
      "8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\n",
      "8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\n",
      "8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\n",
      "8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\n",
      "8.4 Relationship Between the Bootstrap\n",
      "and Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\n",
      "8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\n",
      "8.5.1 Two-Component Mixture Model . . . . . . . . . 272\n",
      "8.5.2 The EM Algorithm in General . . . . . . . . . . 276\n",
      "8.5.3 EM as a Maximization‚ÄìMaximization Procedure 277\n",
      "8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\n",
      "8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n",
      "8.7.1 Example: Trees with Simulated Data . . . . . . 283\n",
      "8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\n",
      "8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n",
      "9 Additive Models, Trees, and Related Methods 295\n",
      "9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\n",
      "9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\n",
      "9.1.2 Example: Additive Logistic Regression . . . . . 299\n",
      "9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\n",
      "9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\n",
      "9.2.3 ClassiÔ¨Åcation Trees . . . . . . . . . . . . . . . . 308\n",
      "9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\n",
      "9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\n",
      "9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\n",
      "9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 321\n",
      "9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\n",
      "9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\n",
      "9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\n",
      "9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\n",
      "9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\n",
      "9.7 Computational Considerations . . . . . . . . . . . . . . . 334\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "10 Boosting and Additive Trees 337\n",
      "10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xviii Contents\n",
      "10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\n",
      "10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\n",
      "10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\n",
      "10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\n",
      "10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\n",
      "10.7 ‚ÄúOÔ¨Ä-the-Shelf‚Äù Procedures for Data Mining . . . . . . . . 350\n",
      "10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\n",
      "10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\n",
      "10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\n",
      "10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\n",
      "10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\n",
      "10.10.3 Implementations of Gradient Boosting . . . . . . 360\n",
      "10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\n",
      "10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\n",
      "10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\n",
      "10.13.1 Relative Importance of Predictor Variables . . . 367\n",
      "10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\n",
      "10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n",
      "10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\n",
      "10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\n",
      "10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n",
      "11 Neural Networks 389\n",
      "11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\n",
      "11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\n",
      "11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\n",
      "11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\n",
      "11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\n",
      "11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\n",
      "11.5.2 OverÔ¨Åtting . . . . . . . . . . . . . . . . . . . . . 398\n",
      "11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\n",
      "11.5.4 Number of Hidden Units and Layers . . . . . . . 400\n",
      "11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\n",
      "11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\n",
      "11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\n",
      "11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\n",
      "11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\n",
      "11.9.2 Performance Comparisons . . . . . . . . . . . . 412\n",
      "11.10 Computational Considerations . . . . . . . . . . . . . . . 414\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xix\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "12 Support Vector Machines and\n",
      "Flexible Discriminants 417\n",
      "12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\n",
      "12.2 The Support Vector ClassiÔ¨Åer . . . . . . . . . . . . . . . . 417\n",
      "12.2.1 Computing the Support Vector ClassiÔ¨Åer . . . . 420\n",
      "12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\n",
      "12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\n",
      "12.3.1 Computing the SVM for ClassiÔ¨Åcation . . . . . . 423\n",
      "12.3.2 The SVM as a Penalization Method . . . . . . . 426\n",
      "12.3.3 Function Estimation and Reproducing Kernels . 428\n",
      "12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\n",
      "12.3.5 A Path Algorithm for the SVM ClassiÔ¨Åer . . . . 432\n",
      "12.3.6 Support Vector Machines for Regression . . . . . 434\n",
      "12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\n",
      "12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\n",
      "12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 438\n",
      "12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\n",
      "12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\n",
      "12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\n",
      "12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\n",
      "12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "13 Prototype Methods and Nearest-Neighbors 459\n",
      "13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2.1 K-means Clustering . . . . . . . . . . . . . . . . 460\n",
      "13.2.2 Learning Vector Quantization . . . . . . . . . . 462\n",
      "13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\n",
      "13.3 k-Nearest-Neighbor ClassiÔ¨Åers . . . . . . . . . . . . . . . 463\n",
      "13.3.1 Example: A Comparative Study . . . . . . . . . 468\n",
      "13.3.2 Example: k-Nearest-Neighbors\n",
      "and Image Scene ClassiÔ¨Åcation . . . . . . . . . . 470\n",
      "13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\n",
      "13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\n",
      "13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\n",
      "13.4.2 Global Dimension Reduction\n",
      "for Nearest-Neighbors . . . . . . . . . . . . . . . 479\n",
      "13.5 Computational Considerations . . . . . . . . . . . . . . . 480\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xx Contents\n",
      "14 Unsupervised Learning 485\n",
      "14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\n",
      "14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\n",
      "14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\n",
      "14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\n",
      "14.2.3 Example: Market Basket Analysis . . . . . . . . 492\n",
      "14.2.4 Unsupervised as Supervised Learning . . . . . . 495\n",
      "14.2.5 Generalized Association Rules . . . . . . . . . . 497\n",
      "14.2.6 Choice of Supervised Learning Method . . . . . 499\n",
      "14.2.7 Example: Market Basket Analysis (Continued) . 499\n",
      "14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\n",
      "14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\n",
      "14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\n",
      "14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\n",
      "14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\n",
      "14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\n",
      "14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . . 509\n",
      "14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\n",
      "14.3.8 Example: Human Tumor Microarray Data . . . 512\n",
      "14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\n",
      "14.3.10 K-medoids . . . . . . . . . . . . . . . . . . . . . 515\n",
      "14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\n",
      "14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\n",
      "14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\n",
      "14.5 Principal Components, Curves and Surfaces . . . . . . . . 534\n",
      "14.5.1 Principal Components . . . . . . . . . . . . . . . 534\n",
      "14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\n",
      "14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\n",
      "14.5.4 Kernel Principal Components . . . . . . . . . . . 547\n",
      "14.5.5 Sparse Principal Components . . . . . . . . . . . 550\n",
      "14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\n",
      "14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\n",
      "14.7 Independent Component Analysis\n",
      "and Exploratory Projection Pursuit . . . . . . . . . . . . 557\n",
      "14.7.1 Latent Variables and Factor Analysis . . . . . . 558\n",
      "14.7.2 Independent Component Analysis . . . . . . . . 560\n",
      "14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\n",
      "14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\n",
      "14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\n",
      "14.9 Nonlinear Dimension Reduction\n",
      "and Local Multidimensional Scaling . . . . . . . . . . . . 572\n",
      "14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Contents xxi\n",
      "15 Random Forests 587\n",
      "15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\n",
      "15.2 DeÔ¨Ånition of Random Forests . . . . . . . . . . . . . . . . 587\n",
      "15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\n",
      "15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\n",
      "15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\n",
      "15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\n",
      "15.3.4 Random Forests and OverÔ¨Åtting . . . . . . . . . 596\n",
      "15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\n",
      "15.4.1 Variance and the De-Correlation EÔ¨Äect . . . . . 597\n",
      "15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\n",
      "15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\n",
      "16 Ensemble Learning 605\n",
      "16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\n",
      "16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\n",
      "16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\n",
      "16.2.2 The ‚ÄúBet on Sparsity‚Äù Principle . . . . . . . . . 610\n",
      "16.2.3 Regularization Paths, Over-Ô¨Åtting and Margins . 613\n",
      "16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\n",
      "16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\n",
      "16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n",
      "17 Undirected Graphical Models 625\n",
      "17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\n",
      "17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\n",
      "17.3 Undirected Graphical Models for Continuous Variables . 630\n",
      "17.3.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 631\n",
      "17.3.2 Estimation of the Graph Structure . . . . . . . . 635\n",
      "17.4 Undirected Graphical Models for Discrete Variables . . . 638\n",
      "17.4.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 639\n",
      "17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\n",
      "17.4.3 Estimation of the Graph Structure . . . . . . . . 642\n",
      "17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\n",
      "18 High-Dimensional Problems: p‚â´N 649\n",
      "18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"xxii Contents\n",
      "18.2 Diagonal Linear Discriminant Analysis\n",
      "and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\n",
      "18.3 Linear ClassiÔ¨Åers with Quadratic Regularization . . . . . 654\n",
      "18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\n",
      "18.3.2 Logistic Regression\n",
      "with Quadratic Regularization . . . . . . . . . . 657\n",
      "18.3.3 The Support Vector ClassiÔ¨Åer . . . . . . . . . . 657\n",
      "18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\n",
      "18.3.5 Computational Shortcuts When p‚â´N. . . . . 659\n",
      "18.4 Linear ClassiÔ¨Åers with L1Regularization . . . . . . . . . 661\n",
      "18.4.1 Application of Lasso\n",
      "to Protein Mass Spectroscopy . . . . . . . . . . 664\n",
      "18.4.2 The Fused Lasso for Functional Data . . . . . . 666\n",
      "18.5 ClassiÔ¨Åcation When Features are Unavailable . . . . . . . 668\n",
      "18.5.1 Example: String Kernels\n",
      "and Protein ClassiÔ¨Åcation . . . . . . . . . . . . . 668\n",
      "18.5.2 ClassiÔ¨Åcation and Other Models Using\n",
      "Inner-Product Kernels and Pairwise Distances . 670\n",
      "18.5.3 Example: Abstracts ClassiÔ¨Åcation . . . . . . . . 672\n",
      "18.6 High-Dimensional Regression:\n",
      "Supervised Principal Components . . . . . . . . . . . . . 674\n",
      "18.6.1 Connection to Latent-Variable Modeling . . . . 678\n",
      "18.6.2 Relationship with Partial Least Squares . . . . . 680\n",
      "18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\n",
      "18.7 Feature Assessment and the Multiple-Testing Problem . . 683\n",
      "18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\n",
      "18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\n",
      "18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\n",
      "18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n",
      "References 699\n",
      "Author Index 729\n",
      "Index 737\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, Ô¨Ånance and\n",
      "industry. Here are some examples of learning problems:\n",
      "‚Ä¢Predict whether a patient, hospitalized due to a heart attack, will\n",
      "have a second heart attack. The prediction is to be based on demo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "‚Ä¢Predict the price of a stock in 6 months from now, on the basis of\n",
      "company performance measures and economic data.\n",
      "‚Ä¢Identify the numbers in a handwritten ZIP code, from a digitized\n",
      "image.\n",
      "‚Ä¢Estimate the amount of glucose in the blood of a diabetic person,\n",
      "from the infrared absorption spectrum of that person‚Äôs blood.\n",
      "‚Ä¢Identify the risk factors for prostate cancer, based on clinical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the Ô¨Åelds of statistics, data\n",
      "mining and artiÔ¨Åcial intelligence, intersecting with areas of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a stock price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have a training set of data, in which we observe the outcome and feature\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"The book \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman explores the field of statistics in the context of data mining, machine learning, and bioinformatics. The book covers topics such as neural networks, support vector machines, and boosting, with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, and ensemble methods. The authors are renowned professors of statistics at Stanford University and have made significant contributions to the field.\n",
      "\n",
      "The dedication page of the book acknowledges the parents and families of the authors, including Valerie and Patrick Hastie, Vera and Sami Tibshirani, Florence and Harry Friedman, Samantha, Timothy, Lynda, Charlie, Ryan, Julie, Cheryl, Melanie, Dora, Monika, and Ildiko.\n",
      "\n",
      "\"vi\" is a text editor program used in Unix and Unix-like operating systems for creating and editing text files in a terminal interface.\n",
      "\n",
      "The second edition of \"The Elements of Statistical Learning\" has been updated with four new chapters and revisions to existing chapters. The popularity of the first edition and the fast pace of research in the statistical learning field prompted the authors to release a second edition. The layout has been kept similar to the first edition to maintain familiarity for readers.\n",
      "\n",
      "The second edition of the book has made several updates and changes, including new examples, additional illustrations, and adjustments to improve accessibility for colorblind readers. The chapter on Kernel Methods has been renamed to avoid confusion, and errors in error-rate estimation have been corrected. The book covers a wide range of topics in supervised and unsupervised learning, including linear methods, support vector machines, neural networks, ensemble learning, and high-dimensional problems.\n",
      "\n",
      "The preface to the second edition of the book discusses the organization of the chapters, including the recommendation to read Chapters 15 and 16 after Chapter 10. It also mentions that Chapter 17 focuses on undirected graphical models, omitting directed models due to space constraints. Chapter 18 explores the \"p‚â´N\" problem in high-dimensional feature spaces. The authors express gratitude for feedback on the first edition and acknowledge individuals who provided comments on the new chapters. The dedication of the edition is made in memory of Anna McPhee.\n",
      "\n",
      "The preface to the second edition discusses the author's reasons for updating the book and reflects on the impact of the first edition. It also outlines the changes made in the new edition and sets the tone for the rest of the book.\n",
      "\n",
      "The preface to the first edition of a statistics book discusses the challenges of the field in the modern information age, including the increase in size and complexity of statistical problems. It mentions the emergence of new fields like data mining and bioinformatics to address these challenges. The importance of learning from data and the revolution in statistical sciences, driven by advances in computation, are also highlighted. The book categorizes learning problems as supervised or unsupervised, with different goals for each approach.\n",
      "\n",
      "The book is a statistical framework that explains new ideas in learning, emphasizing methods and conceptual underpinnings. The authors acknowledge the influence of various individuals on their work and express gratitude to their families and funding sources. The preface also includes quotes from Andreas Buja and Ian Hacking on the importance of interpretation and the impact of statisticians on reasoning and opinions.\n",
      "\n",
      "The text provides an introduction to supervised learning, covering topics such as variable types, prediction methods like least squares and nearest neighbors, statistical decision theory, local methods in high dimensions, and structured regression models. It also discusses statistical models, supervised learning, and function approximation in the context of supervised learning.\n",
      "\n",
      "This section covers different classes of restricted estimators, model selection, and the bias-variance tradeoff in regression analysis. It also discusses linear regression models, subset selection, shrinkage methods, and methods using derived input directions. The focus is on techniques such as ridge regression, the Lasso, principal components regression, and partial least squares. Computational considerations and various path algorithms for the Lasso are also explored.\n",
      "\n",
      "This section of the text covers linear methods for classification, including linear regression, linear discriminant analysis, logistic regression, and separating hyperplanes. It also discusses basis expansions and regularization techniques such as piecewise polynomials, splines, filtering, feature extraction, smoothing splines, and wavelet smoothing. The appendix provides computational considerations for splines and B-splines.\n",
      "\n",
      "Chapter 6 discusses Kernel Smoothing Methods, including one-dimensional kernel smoothers, selecting the width of the kernel, local regression, structured local regression models, kernel density estimation and classification, radial basis functions, and mixture models. Chapter 7 focuses on model assessment and selection, covering bias, variance, model complexity, bias-variance decomposition, in-sample prediction error, effective number of parameters, cross-validation, and bootstrap methods. Chapter 8 discusses model inference and averaging.\n",
      "\n",
      "Chapter 8 of the text covers various statistical methods including the Bootstrap, Maximum Likelihood, Bayesian methods, EM Algorithm, MCMC, Bagging, Model Averaging, Stochastic Search, and more. Chapter 9 focuses on Additive Models, Trees, and related methods such as Generalized Additive Models, Tree-Based Methods, PRIM, MARS, Hierarchical Mixtures of Experts, and Missing Data. Chapter 10 delves into Boosting Methods.\n",
      "\n",
      "Chapter 10 discusses boosting, starting with fitting an additive model and forward stagewise additive modeling. It then covers exponential loss, AdaBoost, different loss functions, \"off-the-shelf\" procedures for data mining, boosting trees, numerical optimization via gradient boosting, right-sized trees for boosting, and regularization techniques. Interpretation methods and illustrations are also provided. Chapter 11 focuses on neural networks, including projection pursuit regression, fitting neural networks, training issues, examples with simulated and ZIP code data, Bayesian neural nets, and computational considerations.\n",
      "\n",
      "This section of the book covers Support Vector Machines, Flexible Discriminants, Prototype Methods, and Nearest-Neighbors. It discusses topics such as SVM for classification and regression, kernels, linear discriminant analysis, flexible discriminant analysis, penalized discriminant analysis, mixture discriminant analysis, prototype methods like K-means clustering and Gaussian mixtures, and k-nearest-neighbor classifiers. The section also includes exercises for practice.\n",
      "\n",
      "Chapter 14 of the contents covers Unsupervised Learning, including topics such as Association Rules, Cluster Analysis, Self-Organizing Maps, Principal Components, Non-negative Matrix Factorization, Independent Component Analysis, Multidimensional Scaling, Nonlinear Dimension Reduction, and the Google PageRank Algorithm. The chapter also includes Bibliographic Notes and Exercises.\n",
      "\n",
      "The text covers topics such as Random Forests, Ensemble Learning, and Undirected Graphical Models. It discusses the definition, details, analysis, and applications of Random Forests, as well as Boosting, Regularization Paths, and Learning Ensembles. It also explores Markov Graphs, Undirected Graphical Models for continuous and discrete variables, and high-dimensional problems where the number of features is much larger than the number of samples.\n",
      "\n",
      "This section of the text covers various topics related to linear classifiers, regularization, feature selection, and high-dimensional regression. It also discusses classification when features are unavailable, feature assessment, and the multiple-testing problem. Additionally, it includes bibliographic notes, exercises, references, author index, and index.\n",
      "\n",
      "This text introduces the concept of statistical learning, which is used in various fields such as science, finance, and industry. It provides examples of learning problems such as predicting a patient's risk of a second heart attack or estimating stock prices. The book focuses on learning from data to predict outcomes based on features in a training set.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"The book \"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman explores statistical concepts in data mining, machine learning, and bioinformatics. The second edition includes new topics and updates to improve accessibility, with a focus on supervised and unsupervised learning methods. The preface discusses the organization, changes, and reasons for updating the book, while the text covers topics such as linear methods, kernel smoothing, model assessment, boosting, neural networks, support vector machines, random forests, ensemble learning, and undirected graphical models. The book emphasizes learning from data to predict outcomes in various fields.\"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(sl_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f42beb",
   "metadata": {},
   "source": [
    "## Custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c315fda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7b463dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.combine_document_chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e203cdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Springer Series in Statistics\n",
      "Trevor Hastie\n",
      "Robert TibshiraniJerome FriedmanSpringer Series in Statistics\n",
      "The Elements of\n",
      "Statistical Learning\n",
      "Data Mining, Inference, and Prediction\n",
      "The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book‚Äôs coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting‚Äîthe first comprehensive treatment of thistopic in any book.\n",
      "This major new edition features many topics not covered in the original, including graphical\n",
      "models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for ‚Äúwide‚Äù data (p bigger than n), including multiple testing and false discovery rates.\n",
      "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\n",
      "Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
      "‚Ä∫springer.comSTATISTICS\n",
      "isbn 978-0-387-84857-0Trevor Hastie ‚Ä¢ Robert Tibshirani ‚Ä¢ Jerome Friedman\n",
      "The Elements of Statictical Learning\n",
      "Hastie ‚Ä¢ Tibshirani ‚Ä¢ Friedman\n",
      "Second Edition\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "This is page v\n",
      "Printer: Opaque this\n",
      "To our parents:\n",
      "Valerie and Patrick Hastie\n",
      "Vera and Sami Tibshirani\n",
      "Florence and Harry Friedman\n",
      "and to our families:\n",
      "Samantha, Timothy, and Lynda\n",
      "Charlie, Ryan, Julie, and Cheryl\n",
      "Melanie, Dora, Monika, and Ildiko\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "vi\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "This is page vii\n",
      "Printer: Opaque this\n",
      "Preface to the Second Edition\n",
      "In God we trust, all others bring data.\n",
      "‚ÄìWilliam Edwards Deming (1900-1993)1\n",
      "We have been gratiÔ¨Åed by the popularity of the Ô¨Årst edition of The\n",
      "Elements of Statistical Learning. This, along with the fast pace of research\n",
      "in the statistical learning Ô¨Åeld, motivated us to update our book with a\n",
      "second edition.\n",
      "We have added four new chapters and updated some of the existing\n",
      "chapters. Because many readers are familiar with the layout of the Ô¨Årst\n",
      "edition, we have tried to change it as little as possible. Here is a summary\n",
      "of the main changes:\n",
      "1On the Web, this quote has been widely attributed to both Demi ng and Robert W.\n",
      "Hayden; however Professor Hayden told us that he can claim no credit for this quote,\n",
      "and ironically we could Ô¨Ånd no ‚Äúdata‚Äù conÔ¨Årming that Deming a ctually said this.\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "viii Preface to the Second Edition\n",
      "Chapter What‚Äôs new\n",
      "1.Introduction\n",
      "2.Overview of Supervised Learning\n",
      "3.Linear Methods for Regression LAR algorithm and generalizations\n",
      "of the lasso\n",
      "4.Linear Methods for ClassiÔ¨Åcation Lasso path for logistic regression\n",
      "5.Basis Expansions and Regulariza-\n",
      "tionAdditional illustrations of RKHS\n",
      "6.Kernel Smoothing Methods\n",
      "7.Model Assessment and Selection Strengths and pitfalls of cross-\n",
      "validation\n",
      "8.Model Inference and Averaging\n",
      "9.Additive Models, Trees, and\n",
      "Related Methods\n",
      "10.Boosting and Additive Trees New example from ecology; some\n",
      "material split oÔ¨Ä to Chapter 16.\n",
      "11.Neural Networks Bayesian neural nets and the NIPS\n",
      "2003 challenge\n",
      "12.Support Vector Machines and\n",
      "Flexible DiscriminantsPath algorithm for SVM classiÔ¨Åer\n",
      "13. Prototype Methods and\n",
      "Nearest-Neighbors\n",
      "14.Unsupervised Learning Spectral clustering, kernel PCA,\n",
      "sparse PCA, non-negative matrix\n",
      "factorization archetypal analysis,\n",
      "nonlinear dimension reduction,\n",
      "Google page rank algorithm, a\n",
      "direct approach to ICA\n",
      "15.Random Forests New\n",
      "16.Ensemble Learning New\n",
      "17.Undirected Graphical Models New\n",
      "18.High-Dimensional Problems New\n",
      "Some further notes:\n",
      "‚Ä¢Our Ô¨Årst edition was unfriendly to colorblind readers; in particular,\n",
      "we tended to favor red/green contrasts which are particularly trou-\n",
      "blesome. We have changed the color palette in this edition to a large\n",
      "extent, replacing the above with an orange /bluecontrast.\n",
      "‚Ä¢We have changed the name of Chapter 6 from ‚ÄúKernel Methods‚Äù to\n",
      "‚ÄúKernel Smoothing Methods‚Äù, to avoid confusion with the machine-\n",
      "learning kernel method that is discussed in the context of support vec-\n",
      "tor machines (Chapter 11) and more generally in Chapters 5 and 14.\n",
      "‚Ä¢In the Ô¨Årst edition, the discussion of error-rate estimation in Chap-\n",
      "ter 7 was sloppy, as we did not clearly diÔ¨Äerentiate the notions of\n",
      "conditional error rates (conditional on the training set) and uncondi-\n",
      "tional rates. We have Ô¨Åxed this in the new edition.\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Preface to the Second Edition ix\n",
      "‚Ä¢Chapters 15 and 16 follow naturally from Chapter 10, and the chap-\n",
      "ters are probably best read in that order.\n",
      "‚Ä¢In Chapter 17, we have not attempted a comprehensive treatment\n",
      "of graphical models, and discuss only undirected models and some\n",
      "new methods for their estimation. Due to a lack of space, we have\n",
      "speciÔ¨Åcally omitted coverage of directed graphical models.\n",
      "‚Ä¢Chapter 18 explores the ‚Äú p‚â´N‚Äù problem, which is learning in high-\n",
      "dimensional feature spaces. These problems arise in many areas, in-\n",
      "cluding genomic and proteomic studies, and document classiÔ¨Åcation.\n",
      "We thank the many readers who have found the (too numerous) errors in\n",
      "the Ô¨Årst edition. We apologize for those and have done our best to avoid er-\n",
      "rors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry\n",
      "Wasserman for comments on some of the new chapters, and many Stanford\n",
      "graduate and post-doctoral students who oÔ¨Äered comments, in particular\n",
      "Mohammed AlQuraishi, John Boik, Holger HoeÔ¨Çing, Arian Maleki, Donal\n",
      "McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\n",
      "Hui Zou. We thank John Kimmel for his patience in guiding us through this\n",
      "new edition. RT dedicates this edition to the memory of Anna McPhee.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "August 2008\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "x Preface to the Second Edition\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "This is page xi\n",
      "Printer: Opaque this\n",
      "Preface to the First Edition\n",
      "We are drowning in information and starving for knowledge.\n",
      "‚ÄìRutherford D. Roger\n",
      "The Ô¨Åeld of Statistics is constantly challenged by the problems that science\n",
      "and industry brings to its door. In the early days, these problems often came\n",
      "from agricultural and industrial experiments and were relatively small in\n",
      "scope. With the advent of computers and the information age, statistical\n",
      "problems have exploded both in size and complexity. Challenges in the\n",
      "areas of data storage, organization and searching have led to the new Ô¨Åeld\n",
      "of ‚Äúdata mining‚Äù; statistical and computational problems in biology and\n",
      "medicine have created ‚Äúbioinformatics.‚Äù Vast amounts of data are being\n",
      "generated in many Ô¨Åelds, and the statistician‚Äôs job is to make sense of it\n",
      "all: to extract important patterns and trends, and understand ‚Äúwhat the\n",
      "data says.‚Äù We call this learning from data .\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tistical sciences. Since computation plays such a key role, it is not surprising\n",
      "that much of this new development has been done by researchers in other\n",
      "Ô¨Åelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly categorized as\n",
      "either supervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "in unsupervised learning, there is no outcome measure, and the goal is to\n",
      "describe the associations and patterns among a set of input measures.\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xii Preface to the First Edition\n",
      "This book is our attempt to bring together many of the important new\n",
      "ideas in learning, and explain them in a statistical framework. While some\n",
      "mathematical details are needed, we emphasize the methods and their con-\n",
      "ceptual underpinnings rather than their theoretical properties. As a result,\n",
      "we hope that this book will appeal not just to statisticians but also to\n",
      "researchers and practitioners in a wide variety of Ô¨Åelds.\n",
      "Just as we have learned a great deal from researchers outside of the Ô¨Åeld\n",
      "of statistics, our statistical viewpoint may help others to better understa nd\n",
      "diÔ¨Äerent aspects of learning:\n",
      "There is no true interpretation of anything; interpretatio n is a\n",
      "vehicle in the service of human comprehension. The value of\n",
      "interpretation is in enabling others to fruitfully think ab out an\n",
      "idea.\n",
      "‚ÄìAndreas Buja\n",
      "We would like to acknowledge the contribution of many people to the\n",
      "conception and completion of this book. David Andrews, Leo Breiman,\n",
      "Andreas Buja, John Chambers, Bradley Efron, GeoÔ¨Ärey Hinton, Werner\n",
      "Stuetzle, and John Tukey have greatly inÔ¨Çuenced our careers. Balasub-\n",
      "ramanian Narasimhan gave us advice and help on many computational\n",
      "problems, and maintained an excellent computing environment. Shin-Ho\n",
      "Bang helped in the production of a number of the Ô¨Ågures. Lee Wilkinson\n",
      "gave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Ma ya\n",
      "Gupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bog-\n",
      "dan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu\n",
      "Zhu, two reviewers and many students read parts of the manuscript and\n",
      "oÔ¨Äered helpful suggestions. John Kimmel was supportive, patient and help-\n",
      "ful at every phase; MaryAnn Brickner and Frank Ganz headed a superb\n",
      "production team at Springer. Trevor Hastie would like to thank the statis-\n",
      "tics department at the University of Cape Town for their hospitality during\n",
      "the Ô¨Ånal stages of this book. We gratefully acknowledge NSF and NIH for\n",
      "their support of this work. Finally, we would like to thank our families and\n",
      "our parents for their love and support.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "May 2001\n",
      "The quiet statisticians have changed our world; not by disco v-\n",
      "ering new facts or technical developments, but by changing t he\n",
      "ways that we reason, experiment and form our opinions ....\n",
      "‚ÄìIan Hacking\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "This is page xiii\n",
      "Printer: Opaque this\n",
      "Contents\n",
      "Preface to the Second Edition vii\n",
      "Preface to the First Edition xi\n",
      "1 Introduction 1\n",
      "2 Overview of Supervised Learning 9\n",
      "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\n",
      "2.3 Two Simple Approaches to Prediction:\n",
      "Least Squares and Nearest Neighbors . . . . . . . . . . . 11\n",
      "2.3.1 Linear Models and Least Squares . . . . . . . . 11\n",
      "2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\n",
      "2.3.3 From Least Squares to Nearest Neighbors . . . . 16\n",
      "2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\n",
      "2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\n",
      "2.6 Statistical Models, Supervised Learning\n",
      "and Function Approximation . . . . . . . . . . . . . . . . 28\n",
      "2.6.1 A Statistical Model\n",
      "for the Joint Distribution Pr( X,Y) . . . . . . . 28\n",
      "2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\n",
      "2.6.3 Function Approximation . . . . . . . . . . . . . 29\n",
      "2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\n",
      "2.7.1 DiÔ¨Éculty of the Problem . . . . . . . . . . . . . 32\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xiv Contents\n",
      "2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\n",
      "2.8.1 Roughness Penalty and Bayesian Methods . . . 34\n",
      "2.8.2 Kernel Methods and Local Regression . . . . . . 34\n",
      "2.8.3 Basis Functions and Dictionary Methods . . . . 35\n",
      "2.9 Model Selection and the Bias‚ÄìVariance TradeoÔ¨Ä . . . . . 37\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "3 Linear Methods for Regression 43\n",
      "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\n",
      "3.2 Linear Regression Models and Least Squares . . . . . . . 44\n",
      "3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\n",
      "3.2.2 The Gauss‚ÄìMarkov Theorem . . . . . . . . . . . 51\n",
      "3.2.3 Multiple Regression\n",
      "from Simple Univariate Regression . . . . . . . . 52\n",
      "3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\n",
      "3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\n",
      "3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\n",
      "3.3.2 Forward- and Backward-Stepwise Selection . . . 58\n",
      "3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\n",
      "3.3.4 Prostate Cancer Data Example (Continued) . . 61\n",
      "3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\n",
      "3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\n",
      "3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\n",
      "3.4.3 Discussion: Subset Selection, Ridge Regression\n",
      "and the Lasso . . . . . . . . . . . . . . . . . . . 69\n",
      "3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\n",
      "3.5 Methods Using Derived Input Directions . . . . . . . . . 79\n",
      "3.5.1 Principal Components Regression . . . . . . . . 79\n",
      "3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\n",
      "3.6 Discussion: A Comparison of the Selection\n",
      "and Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\n",
      "3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\n",
      "3.8 More on the Lasso and Related Path Algorithms . . . . . 86\n",
      "3.8.1 Incremental Forward Stagewise Regression . . . 86\n",
      "3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\n",
      "3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\n",
      "3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\n",
      "3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\n",
      "3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\n",
      "3.9 Computational Considerations . . . . . . . . . . . . . . . 93\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Contents xv\n",
      "4 Linear Methods for ClassiÔ¨Åcation 101\n",
      "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\n",
      "4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\n",
      "4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\n",
      "4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\n",
      "4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\n",
      "4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\n",
      "4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\n",
      "4.4.2 Example: South African Heart Disease . . . . . 122\n",
      "4.4.3 Quadratic Approximations and Inference . . . . 124\n",
      "4.4.4 L1Regularized Logistic Regression . . . . . . . . 125\n",
      "4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\n",
      "4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\n",
      "4.5.1 Rosenblatt‚Äôs Perceptron Learning Algorithm . . 130\n",
      "4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "5 Basis Expansions and Regularization 139\n",
      "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\n",
      "5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\n",
      "5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\n",
      "5.2.2 Example: South African Heart Disease (Continued)146\n",
      "5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\n",
      "5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\n",
      "5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\n",
      "5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\n",
      "5.5 Automatic Selection of the Smoothing Parameters . . . . 156\n",
      "5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\n",
      "5.5.2 The Bias‚ÄìVariance TradeoÔ¨Ä . . . . . . . . . . . . 158\n",
      "5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\n",
      "5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\n",
      "5.8.1 Spaces of Functions Generated by Kernels . . . 168\n",
      "5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\n",
      "5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\n",
      "5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\n",
      "5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Appendix: Computational Considerations for Splines . . . . . . 186\n",
      "Appendix: B-splines . . . . . . . . . . . . . . . . . . . . . 186\n",
      "Appendix: Computations for Smoothing Splines . . . . . 189\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xvi Contents\n",
      "6 Kernel Smoothing Methods 191\n",
      "6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\n",
      "6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\n",
      "6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\n",
      "6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\n",
      "6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\n",
      "6.4 Structured Local Regression Models in IRp. . . . . . . . 201\n",
      "6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\n",
      "6.4.2 Structured Regression Functions . . . . . . . . . 203\n",
      "6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\n",
      "6.6 Kernel Density Estimation and ClassiÔ¨Åcation . . . . . . . 208\n",
      "6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\n",
      "6.6.2 Kernel Density ClassiÔ¨Åcation . . . . . . . . . . . 210\n",
      "6.6.3 The Naive Bayes ClassiÔ¨Åer . . . . . . . . . . . . 210\n",
      "6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\n",
      "6.8 Mixture Models for Density Estimation and ClassiÔ¨Åcation 214\n",
      "6.9 Computational Considerations . . . . . . . . . . . . . . . 216\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "7 Model Assessment and Selection 219\n",
      "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\n",
      "7.3 The Bias‚ÄìVariance Decomposition . . . . . . . . . . . . . 223\n",
      "7.3.1 Example: Bias‚ÄìVariance TradeoÔ¨Ä . . . . . . . . 226\n",
      "7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\n",
      "7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\n",
      "7.6 The EÔ¨Äective Number of Parameters . . . . . . . . . . . . 232\n",
      "7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\n",
      "7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\n",
      "7.9 Vapnik‚ÄìChervonenkis Dimension . . . . . . . . . . . . . . 237\n",
      "7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\n",
      "7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\n",
      "7.10.1 K-Fold Cross-Validation . . . . . . . . . . . . . 241\n",
      "7.10.2 The Wrong and Right Way\n",
      "to Do Cross-validation . . . . . . . . . . . . . . . 245\n",
      "7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\n",
      "7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\n",
      "7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\n",
      "7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "8 Model Inference and Averaging 261\n",
      "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Contents xvii\n",
      "8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\n",
      "8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\n",
      "8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\n",
      "8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\n",
      "8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\n",
      "8.4 Relationship Between the Bootstrap\n",
      "and Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\n",
      "8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\n",
      "8.5.1 Two-Component Mixture Model . . . . . . . . . 272\n",
      "8.5.2 The EM Algorithm in General . . . . . . . . . . 276\n",
      "8.5.3 EM as a Maximization‚ÄìMaximization Procedure 277\n",
      "8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\n",
      "8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n",
      "8.7.1 Example: Trees with Simulated Data . . . . . . 283\n",
      "8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\n",
      "8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n",
      "9 Additive Models, Trees, and Related Methods 295\n",
      "9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\n",
      "9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\n",
      "9.1.2 Example: Additive Logistic Regression . . . . . 299\n",
      "9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\n",
      "9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\n",
      "9.2.3 ClassiÔ¨Åcation Trees . . . . . . . . . . . . . . . . 308\n",
      "9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\n",
      "9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\n",
      "9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\n",
      "9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 321\n",
      "9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\n",
      "9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\n",
      "9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\n",
      "9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\n",
      "9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\n",
      "9.7 Computational Considerations . . . . . . . . . . . . . . . 334\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "10 Boosting and Additive Trees 337\n",
      "10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xviii Contents\n",
      "10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\n",
      "10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\n",
      "10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\n",
      "10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\n",
      "10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\n",
      "10.7 ‚ÄúOÔ¨Ä-the-Shelf‚Äù Procedures for Data Mining . . . . . . . . 350\n",
      "10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\n",
      "10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\n",
      "10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\n",
      "10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\n",
      "10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\n",
      "10.10.3 Implementations of Gradient Boosting . . . . . . 360\n",
      "10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\n",
      "10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\n",
      "10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\n",
      "10.13.1 Relative Importance of Predictor Variables . . . 367\n",
      "10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\n",
      "10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n",
      "10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\n",
      "10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\n",
      "10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n",
      "11 Neural Networks 389\n",
      "11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\n",
      "11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\n",
      "11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\n",
      "11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\n",
      "11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\n",
      "11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\n",
      "11.5.2 OverÔ¨Åtting . . . . . . . . . . . . . . . . . . . . . 398\n",
      "11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\n",
      "11.5.4 Number of Hidden Units and Layers . . . . . . . 400\n",
      "11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\n",
      "11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\n",
      "11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\n",
      "11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\n",
      "11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\n",
      "11.9.2 Performance Comparisons . . . . . . . . . . . . 412\n",
      "11.10 Computational Considerations . . . . . . . . . . . . . . . 414\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Contents xix\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "12 Support Vector Machines and\n",
      "Flexible Discriminants 417\n",
      "12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\n",
      "12.2 The Support Vector ClassiÔ¨Åer . . . . . . . . . . . . . . . . 417\n",
      "12.2.1 Computing the Support Vector ClassiÔ¨Åer . . . . 420\n",
      "12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\n",
      "12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\n",
      "12.3.1 Computing the SVM for ClassiÔ¨Åcation . . . . . . 423\n",
      "12.3.2 The SVM as a Penalization Method . . . . . . . 426\n",
      "12.3.3 Function Estimation and Reproducing Kernels . 428\n",
      "12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\n",
      "12.3.5 A Path Algorithm for the SVM ClassiÔ¨Åer . . . . 432\n",
      "12.3.6 Support Vector Machines for Regression . . . . . 434\n",
      "12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\n",
      "12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\n",
      "12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 438\n",
      "12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\n",
      "12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\n",
      "12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\n",
      "12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\n",
      "12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "13 Prototype Methods and Nearest-Neighbors 459\n",
      "13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2.1 K-means Clustering . . . . . . . . . . . . . . . . 460\n",
      "13.2.2 Learning Vector Quantization . . . . . . . . . . 462\n",
      "13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\n",
      "13.3 k-Nearest-Neighbor ClassiÔ¨Åers . . . . . . . . . . . . . . . 463\n",
      "13.3.1 Example: A Comparative Study . . . . . . . . . 468\n",
      "13.3.2 Example: k-Nearest-Neighbors\n",
      "and Image Scene ClassiÔ¨Åcation . . . . . . . . . . 470\n",
      "13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\n",
      "13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\n",
      "13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\n",
      "13.4.2 Global Dimension Reduction\n",
      "for Nearest-Neighbors . . . . . . . . . . . . . . . 479\n",
      "13.5 Computational Considerations . . . . . . . . . . . . . . . 480\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xx Contents\n",
      "14 Unsupervised Learning 485\n",
      "14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\n",
      "14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\n",
      "14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\n",
      "14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\n",
      "14.2.3 Example: Market Basket Analysis . . . . . . . . 492\n",
      "14.2.4 Unsupervised as Supervised Learning . . . . . . 495\n",
      "14.2.5 Generalized Association Rules . . . . . . . . . . 497\n",
      "14.2.6 Choice of Supervised Learning Method . . . . . 499\n",
      "14.2.7 Example: Market Basket Analysis (Continued) . 499\n",
      "14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\n",
      "14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\n",
      "14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\n",
      "14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\n",
      "14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\n",
      "14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\n",
      "14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . . 509\n",
      "14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\n",
      "14.3.8 Example: Human Tumor Microarray Data . . . 512\n",
      "14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\n",
      "14.3.10 K-medoids . . . . . . . . . . . . . . . . . . . . . 515\n",
      "14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\n",
      "14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\n",
      "14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\n",
      "14.5 Principal Components, Curves and Surfaces . . . . . . . . 534\n",
      "14.5.1 Principal Components . . . . . . . . . . . . . . . 534\n",
      "14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\n",
      "14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\n",
      "14.5.4 Kernel Principal Components . . . . . . . . . . . 547\n",
      "14.5.5 Sparse Principal Components . . . . . . . . . . . 550\n",
      "14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\n",
      "14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\n",
      "14.7 Independent Component Analysis\n",
      "and Exploratory Projection Pursuit . . . . . . . . . . . . 557\n",
      "14.7.1 Latent Variables and Factor Analysis . . . . . . 558\n",
      "14.7.2 Independent Component Analysis . . . . . . . . 560\n",
      "14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\n",
      "14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\n",
      "14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\n",
      "14.9 Nonlinear Dimension Reduction\n",
      "and Local Multidimensional Scaling . . . . . . . . . . . . 572\n",
      "14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "Contents xxi\n",
      "15 Random Forests 587\n",
      "15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\n",
      "15.2 DeÔ¨Ånition of Random Forests . . . . . . . . . . . . . . . . 587\n",
      "15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\n",
      "15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\n",
      "15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\n",
      "15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\n",
      "15.3.4 Random Forests and OverÔ¨Åtting . . . . . . . . . 596\n",
      "15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\n",
      "15.4.1 Variance and the De-Correlation EÔ¨Äect . . . . . 597\n",
      "15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\n",
      "15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\n",
      "16 Ensemble Learning 605\n",
      "16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\n",
      "16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\n",
      "16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\n",
      "16.2.2 The ‚ÄúBet on Sparsity‚Äù Principle . . . . . . . . . 610\n",
      "16.2.3 Regularization Paths, Over-Ô¨Åtting and Margins . 613\n",
      "16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\n",
      "16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\n",
      "16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n",
      "17 Undirected Graphical Models 625\n",
      "17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\n",
      "17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\n",
      "17.3 Undirected Graphical Models for Continuous Variables . 630\n",
      "17.3.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 631\n",
      "17.3.2 Estimation of the Graph Structure . . . . . . . . 635\n",
      "17.4 Undirected Graphical Models for Discrete Variables . . . 638\n",
      "17.4.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 639\n",
      "17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\n",
      "17.4.3 Estimation of the Graph Structure . . . . . . . . 642\n",
      "17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\n",
      "18 High-Dimensional Problems: p‚â´N 649\n",
      "18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "xxii Contents\n",
      "18.2 Diagonal Linear Discriminant Analysis\n",
      "and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\n",
      "18.3 Linear ClassiÔ¨Åers with Quadratic Regularization . . . . . 654\n",
      "18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\n",
      "18.3.2 Logistic Regression\n",
      "with Quadratic Regularization . . . . . . . . . . 657\n",
      "18.3.3 The Support Vector ClassiÔ¨Åer . . . . . . . . . . 657\n",
      "18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\n",
      "18.3.5 Computational Shortcuts When p‚â´N. . . . . 659\n",
      "18.4 Linear ClassiÔ¨Åers with L1Regularization . . . . . . . . . 661\n",
      "18.4.1 Application of Lasso\n",
      "to Protein Mass Spectroscopy . . . . . . . . . . 664\n",
      "18.4.2 The Fused Lasso for Functional Data . . . . . . 666\n",
      "18.5 ClassiÔ¨Åcation When Features are Unavailable . . . . . . . 668\n",
      "18.5.1 Example: String Kernels\n",
      "and Protein ClassiÔ¨Åcation . . . . . . . . . . . . . 668\n",
      "18.5.2 ClassiÔ¨Åcation and Other Models Using\n",
      "Inner-Product Kernels and Pairwise Distances . 670\n",
      "18.5.3 Example: Abstracts ClassiÔ¨Åcation . . . . . . . . 672\n",
      "18.6 High-Dimensional Regression:\n",
      "Supervised Principal Components . . . . . . . . . . . . . 674\n",
      "18.6.1 Connection to Latent-Variable Modeling . . . . 678\n",
      "18.6.2 Relationship with Partial Least Squares . . . . . 680\n",
      "18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\n",
      "18.7 Feature Assessment and the Multiple-Testing Problem . . 683\n",
      "18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\n",
      "18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\n",
      "18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\n",
      "18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n",
      "References 699\n",
      "Author Index 729\n",
      "Index 737\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of documents\n",
      "\n",
      "This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, Ô¨Ånance and\n",
      "industry. Here are some examples of learning problems:\n",
      "‚Ä¢Predict whether a patient, hospitalized due to a heart attack, will\n",
      "have a second heart attack. The prediction is to be based on demo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "‚Ä¢Predict the price of a stock in 6 months from now, on the basis of\n",
      "company performance measures and economic data.\n",
      "‚Ä¢Identify the numbers in a handwritten ZIP code, from a digitized\n",
      "image.\n",
      "‚Ä¢Estimate the amount of glucose in the blood of a diabetic person,\n",
      "from the infrared absorption spectrum of that person‚Äôs blood.\n",
      "‚Ä¢Identify the risk factors for prostate cancer, based on clinical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the Ô¨Åelds of statistics, data\n",
      "mining and artiÔ¨Åcial intelligence, intersecting with areas of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a stock price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have a training set of data, in which we observe the outcome and feature\n",
      "\n",
      "Based on this list of docs, please identify the main themes \n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a set of summaries:\n",
      "\n",
      "1. Data mining, inference, and prediction\n",
      "2. Statistical learning\n",
      "3. Machine learning\n",
      "4. Bioinformatics\n",
      "5. Supervised learning\n",
      "6. Unsupervised learning\n",
      "7. Neural networks\n",
      "8. Support vector machines\n",
      "9. Classification trees\n",
      "10. Boosting\n",
      "11. Graphical models\n",
      "12. Random forests\n",
      "13. Ensemble methods\n",
      "14. Lasso\n",
      "15. Non-negative matrix factorization\n",
      "16. Spectral clustering\n",
      "17. Methods for \"wide\" data\n",
      "18. Multiple testing\n",
      "19. False discovery rates\n",
      "\n",
      "1. Acknowledgment and gratitude towards parents and families\n",
      "2. Family relationships and connections\n",
      "3. Sense of unity and support within the family\n",
      "4. Celebration of family bonds and connections.\n",
      "\n",
      "The main themes that can be identified from this list of documents are likely related to computer science and technology. The presence of documents such as \"vi\" suggests a focus on programming and software development. Other themes may include education, research, and possibly documentation or tutorials.\n",
      "\n",
      "1. The popularity of the first edition of \"The Elements of Statistical Learning\" and the fast pace of research in the statistical learning field have motivated the authors to release a second edition.\n",
      "2. The second edition includes four new chapters and updates to existing chapters.\n",
      "3. The layout of the second edition has been kept similar to the first edition for the benefit of familiar readers.\n",
      "4. The preface includes a quote by William Edwards Deming emphasizing the importance of data in decision-making.\n",
      "\n",
      "The main themes of the documents listed are:\n",
      "1. Overview of supervised learning\n",
      "2. Linear methods for regression and classification\n",
      "3. Basis expansions and regularization\n",
      "4. Kernel smoothing methods\n",
      "5. Model assessment and selection\n",
      "6. Model inference and averaging\n",
      "7. Additive models, trees, and related methods\n",
      "8. Boosting and additive trees\n",
      "9. Neural networks\n",
      "10. Support vector machines and flexible discriminants\n",
      "11. Prototype methods and nearest-neighbors\n",
      "12. Unsupervised learning\n",
      "13. Random forests\n",
      "14. Ensemble learning\n",
      "15. Undirected graphical models\n",
      "16. High-dimensional problems\n",
      "17. Colorblind accessibility improvements in the second edition.\n",
      "\n",
      "1. The main themes of the documents include:\n",
      "- The structure of the chapters and their recommended order for reading.\n",
      "- Focus on graphical models, specifically undirected models and methods for their estimation.\n",
      "- Exploration of the \"p‚â´N\" problem, which involves learning in high-dimensional feature spaces.\n",
      "- Acknowledgement of errors in the first edition and efforts to avoid them in the second edition.\n",
      "- Thanks to individuals who provided comments and feedback on the new chapters.\n",
      "- Dedication of the edition to the memory of Anna McPhee.\n",
      "\n",
      "1. Preface to the Second Edition: This document likely discusses the reasons for creating a second edition of a book or document, any changes or updates that have been made, and the overall goals for the new edition. It may also touch on the reception of the first edition and how feedback was incorporated into the new edition. \n",
      "\n",
      "Main themes: Revision, updates, feedback, goals.\n",
      "\n",
      "1. The challenges and complexities of dealing with vast amounts of data in the field of Statistics\n",
      "2. The evolution of statistical sciences due to advancements in technology and the information age\n",
      "3. The importance of learning from data and extracting patterns and trends\n",
      "4. The revolution in statistical sciences driven by computation and interdisciplinary collaboration\n",
      "5. The categorization of learning problems as supervised or unsupervised in terms of predicting outcomes and describing associations and patterns.\n",
      "\n",
      "1. Statistical framework in learning\n",
      "2. Importance of interpretation in enabling comprehension\n",
      "3. Acknowledgement of contributions from various individuals\n",
      "4. Influence of statisticians in changing reasoning and opinions\n",
      "\n",
      "The main themes in the documents listed are supervised learning, variable types and terminology, prediction methods (least squares and nearest neighbors), statistical decision theory, local methods in high dimensions, statistical models for function approximation, and structured regression models.\n",
      "\n",
      "1. Estimation methods in regression analysis, including restricted estimators, roughness penalty, Bayesian methods, kernel methods, local regression, basis functions, and dictionary methods.\n",
      "\n",
      "2. Model selection and the bias-variance tradeoff in regression analysis.\n",
      "\n",
      "3. Linear methods for regression, such as linear regression models, least squares, subset selection, shrinkage methods (ridge regression, Lasso), methods using derived input directions (principal components regression, partial least squares), multiple outcome shrinkage and selection, and computational considerations.\n",
      "\n",
      "4. Specific algorithms and techniques related to the Lasso method, including incremental forward stagewise regression, piecewise-linear path algorithms, the Dantzig Selector, the Grouped Lasso, further properties of the Lasso, and pathwise coordinate optimization.\n",
      "\n",
      "1. Linear methods for classification, including linear regression, linear discriminant analysis, logistic regression, and separating hyperplanes.\n",
      "2. Basis expansions and regularization techniques, such as piecewise polynomials, splines, filtering, feature extraction, smoothing splines, nonparametric logistic regression, multidimensional splines, regularization in reproducing kernel Hilbert spaces, and wavelet smoothing.\n",
      "3. Computational considerations for splines, including B-splines and computations for smoothing splines.\n",
      "\n",
      "Based on the list of documents provided, the main themes appear to be:\n",
      "1. Kernel Smoothing Methods\n",
      "2. Local regression and structured local regression models\n",
      "3. Kernel density estimation and classification\n",
      "4. Radial basis functions and kernels\n",
      "5. Mixture models for density estimation and classification\n",
      "6. Model assessment and selection, including bias, variance, model complexity, and various methods for assessing model performance\n",
      "7. Model inference and averaging.\n",
      "\n",
      "1. Statistical Inference Methods: This includes topics such as the Bootstrap, Maximum Likelihood Methods, Bayesian Methods, and the EM Algorithm.\n",
      "2. Machine Learning Techniques: This includes topics such as Bagging, Model Averaging, Stochastic Search, and Boosting Methods.\n",
      "3. Tree-Based Methods: This includes topics such as Generalized Additive Models, Tree-Based Methods, PRIM, MARS, and Hierarchical Mixtures of Experts.\n",
      "4. Data Analysis Techniques: This includes topics such as Missing Data, Computational Considerations, and Sampling from the Posterior using MCMC.\n",
      "5. Exercises and Bibliographic Notes: These sections likely provide practice problems and additional resources for further learning.\n",
      "\n",
      "Based on the list of documents provided, the main themes appear to be:\n",
      "\n",
      "1. Boosting: including additive modeling, forward stagewise modeling, exponential loss, AdaBoost, numerical optimization via gradient boosting, regularization, interpretation, and illustrations.\n",
      "2. Neural Networks: including projection pursuit regression, fitting neural networks, training issues, examples with simulated data and ZIP code data, Bayesian neural nets, performance comparisons, and computational considerations.\n",
      "\n",
      "1. Support Vector Machines and Flexible Discriminants: This section covers the concepts of Support Vector Machines (SVM) and Flexible Discriminants, including topics such as SVM for classification and regression, kernels, function estimation, reproducing kernels, curse of dimensionality, path algorithms, and penalized discriminant analysis.\n",
      "\n",
      "2. Prototype Methods and Nearest-Neighbors: This section discusses prototype methods such as K-means clustering, Learning Vector Quantization, and Gaussian Mixtures, as well as k-Nearest-Neighbor classifiers. It also covers topics like adaptive nearest-neighbor methods, global dimension reduction, and computational considerations related to nearest-neighbor methods.\n",
      "\n",
      "1. Unsupervised Learning\n",
      "2. Association Rules and Market Basket Analysis\n",
      "3. Cluster Analysis and Clustering Algorithms\n",
      "4. Self-Organizing Maps\n",
      "5. Principal Components, Curves, and Surfaces\n",
      "6. Non-negative Matrix Factorization\n",
      "7. Independent Component Analysis and Exploratory Projection Pursuit\n",
      "8. Multidimensional Scaling\n",
      "9. Nonlinear Dimension Reduction and Local Multidimensional Scaling\n",
      "10. The Google PageRank Algorithm\n",
      "\n",
      "1. Random Forests (Chapter 15): This section covers the concept of random forests, including details, analysis, and applications such as variable importance and proximity plots.\n",
      "\n",
      "2. Ensemble Learning (Chapter 16): Focuses on boosting and regularization paths, learning ensembles, and the \"Bet on Sparsity\" principle in machine learning.\n",
      "\n",
      "3. Undirected Graphical Models (Chapter 17): Discusses Markov graphs, undirected graphical models for continuous and discrete variables, parameter estimation, and graph structure estimation.\n",
      "\n",
      "4. High-Dimensional Problems (Chapter 18): Addresses challenges and solutions when dealing with high-dimensional data where the number of features is much larger than the number of samples.\n",
      "\n",
      "1. Linear Classifiers and Regularization Techniques: This theme includes topics such as Linear Discriminant Analysis, Nearest Shrunken Centroids, Logistic Regression, Support Vector Classifier, Feature Selection, L1 Regularization, and the application of Lasso in different domains.\n",
      "\n",
      "2. High-Dimensional Regression and Feature Selection: This theme covers topics related to supervised Principal Components, Latent-Variable Modeling, Partial Least Squares, and pre-conditioning for feature selection in high-dimensional regression problems.\n",
      "\n",
      "3. Classification without Available Features: This theme discusses methods for classification when features are unavailable, such as using string kernels, inner-product kernels, pairwise distances, and examples like protein classification and abstracts classification.\n",
      "\n",
      "4. Feature Assessment and Multiple-Testing Problem: This theme focuses on feature assessment techniques, the False Discovery Rate, asymmetric cutpoints, the SAM procedure, and a Bayesian interpretation of the False Discovery Rate.\n",
      "\n",
      "5. Bibliographic Notes and Exercises: These sections likely provide additional resources, further reading, and exercises for readers to test their understanding of the topics discussed in the document.\n",
      "\n",
      "1. Statistical learning in various fields such as science, finance, and industry\n",
      "2. Examples of learning problems such as predicting outcomes based on features\n",
      "3. The intersection of statistics, data mining, and artificial intelligence\n",
      "4. Learning from data and predicting outcomes based on training data.\n",
      "\n",
      "Take these and distill it into a final, consolidated list of the main themes. \n",
      "Return that list as a comma separated list. \n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Data mining, statistical learning, machine learning, bioinformatics, supervised learning, unsupervised learning, neural networks, support vector machines, classification trees, boosting, graphical models, random forests, ensemble methods, Lasso, non-negative matrix factorization, spectral clustering, methods for \"wide\" data, multiple testing, false discovery rates, linear methods, model assessment and selection, high-dimensional problems, kernel smoothing methods, model inference and averaging, estimation methods, model selection and bias-variance tradeoff, specific algorithms, computational considerations, prototype methods, neural networks, support vector machines, flexible discriminants, boosting, random forests, ensemble learning, undirected graphical models, high-dimensional problems, linear classifiers, regularization techniques, high-dimensional regression, feature selection, classification without available features, feature assessment, multiple-testing problem, statistical learning applications, learning problems, intersection of statistics, data mining, and artificial intelligence.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_template = \"\"\"The following is a set of documents\n",
    "\n",
    "{text}\n",
    "\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "combine_template = \"\"\"The following is a set of summaries:\n",
    "\n",
    "{text}\n",
    "\n",
    "Take these and distill it into a final, consolidated list of the main themes. \n",
    "Return that list as a comma separated list. \n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "combine_prompt = PromptTemplate.from_template(combine_template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(sl_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edf78077-ca13-4b53-a867-c9219c06c599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"station_id: 41515 city_name: Asadabad date: 1957-07-01 00:00:00 season: Summer avg_temp_c: 27.0 min_temp_c: 21.1 max_temp_c: 35.6 precipitation_mm: 0.0 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"station_id: 41515 city_name: Asadabad date: 1957-07-02 00:00:00 season: Summer avg_temp_c: 22.8 min_temp_c: 18.9 max_temp_c: 32.2 precipitation_mm: 0.0 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"station_id: 41515 city_name: Asadabad date: 1957-07-03 00:00:00 season: Summer avg_temp_c: 24.3 min_temp_c: 16.7 max_temp_c: 35.6 precipitation_mm: 1.0 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"station_id: 41515 city_name: Asadabad date: 1957-07-04 00:00:00 season: Summer avg_temp_c: 26.6 min_temp_c: 16.1 max_temp_c: 37.8 precipitation_mm: 4.1 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"station_id: 41515 city_name: Asadabad date: 1957-07-05 00:00:00 season: Summer avg_temp_c: 30.8 min_temp_c: 20.0 max_temp_c: 41.7 precipitation_mm: 0.0 snow_depth_mm: nan avg_wind_dir_deg: nan avg_wind_speed_kmh: nan peak_wind_gust_kmh: nan avg_sea_level_pres_hpa: nan sunshine_total_min: nan\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"In Asadabad, station_id 41515 recorded summer weather on July 1, 1957 with an average temperature of 27.0¬∞C, minimum temperature of 21.1¬∞C, and maximum temperature of 35.6¬∞C. There was no precipitation, and other weather data such as wind speed, sea level pressure, and sunshine duration were not available.\n",
      "\n",
      "The weather data for Asadabad on July 2, 1957, during the summer season, shows an average temperature of 22.8¬∞C, with a minimum of 18.9¬∞C and a maximum of 32.2¬∞C. There was no precipitation recorded, and other details such as wind direction, speed, gust, sea level pressure, and sunshine duration are not available.\n",
      "\n",
      "On July 3, 1957, in Asadabad, station ID 41515 recorded summer weather with an average temperature of 24.3¬∞C, a minimum of 16.7¬∞C, and a maximum of 35.6¬∞C. There was 1.0mm of precipitation, and other weather data such as wind direction, speed, gust, pressure, and sunshine were not available.\n",
      "\n",
      "On July 4, 1957, in Asadabad, station_id 41515 recorded a summer day with an average temperature of 26.6¬∞C, a minimum temperature of 16.1¬∞C, and a maximum temperature of 37.8¬∞C. There was 4.1mm of precipitation, but no data available for snow depth, wind direction, wind speed, wind gust, sea level pressure, or sunshine duration.\n",
      "\n",
      "The data collected from station_id 41515 in Asadabad on July 5, 1957, during the summer season showed an average temperature of 30.8¬∞C with a minimum of 20.0¬∞C and a maximum of 41.7¬∞C. There was no precipitation recorded, and other weather parameters such as wind direction, wind speed, peak wind gust, sea level pressure, and sunshine duration were not available.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In Asadabad, station_id 41515 recorded summer weather data from July 1 to July 5, 1957. Temperatures ranged from 22.8¬∞C to 30.8¬∞C, with precipitation on July 3 and July 4. Other weather details such as wind speed, sea level pressure, and sunshine duration were not available for any of the days.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(documents[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4543a694",
   "metadata": {},
   "source": [
    "## The Refine chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6286e000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Springer Series in Statistics\n",
      "Trevor Hastie\n",
      "Robert TibshiraniJerome FriedmanSpringer Series in Statistics\n",
      "The Elements of\n",
      "Statistical Learning\n",
      "Data Mining, Inference, and Prediction\n",
      "The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book‚Äôs coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting‚Äîthe first comprehensive treatment of thistopic in any book.\n",
      "This major new edition features many topics not covered in the original, including graphical\n",
      "models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for ‚Äúwide‚Äù data (p bigger than n), including multiple testing and false discovery rates.\n",
      "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\n",
      "Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
      "‚Ä∫springer.comSTATISTICS\n",
      "isbn 978-0-387-84857-0Trevor Hastie ‚Ä¢ Robert Tibshirani ‚Ä¢ Jerome Friedman\n",
      "The Elements of Statictical Learning\n",
      "Hastie ‚Ä¢ Tibshirani ‚Ä¢ Friedman\n",
      "Second Edition\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book that covers important concepts in data mining, inference, and prediction. The book discusses various topics in statistics, machine learning, and bioinformatics, with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors are prominent researchers in statistics and have made significant contributions to the field.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "This is page v\n",
      "Printer: Opaque this\n",
      "To our parents:\n",
      "Valerie and Patrick Hastie\n",
      "Vera and Sami Tibshirani\n",
      "Florence and Harry Friedman\n",
      "and to our families:\n",
      "Samantha, Timothy, and Lynda\n",
      "Charlie, Ryan, Julie, and Cheryl\n",
      "Melanie, Dora, Monika, and Ildiko\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book that covers important concepts in data mining, inference, and prediction. The book discusses various topics in statistics, machine learning, and bioinformatics, with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors are prominent researchers in statistics and have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "vi\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book that covers important concepts in data mining, inference, and prediction. The book discusses various topics in statistics, machine learning, and bioinformatics, with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors are prominent researchers in statistics and have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "This is page vii\n",
      "Printer: Opaque this\n",
      "Preface to the Second Edition\n",
      "In God we trust, all others bring data.\n",
      "‚ÄìWilliam Edwards Deming (1900-1993)1\n",
      "We have been gratiÔ¨Åed by the popularity of the Ô¨Årst edition of The\n",
      "Elements of Statistical Learning. This, along with the fast pace of research\n",
      "in the statistical learning Ô¨Åeld, motivated us to update our book with a\n",
      "second edition.\n",
      "We have added four new chapters and updated some of the existing\n",
      "chapters. Because many readers are familiar with the layout of the Ô¨Årst\n",
      "edition, we have tried to change it as little as possible. Here is a summary\n",
      "of the main changes:\n",
      "1On the Web, this quote has been widely attributed to both Demi ng and Robert W.\n",
      "Hayden; however Professor Hayden told us that he can claim no credit for this quote,\n",
      "and ironically we could Ô¨Ånd no ‚Äúdata‚Äù conÔ¨Årming that Deming a ctually said this.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book that covers important concepts in data mining, inference, and prediction. The book discusses various topics in statistics, machine learning, and bioinformatics, with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors are prominent researchers in statistics and have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support. The second edition was motivated by the popularity of the first edition and the fast pace of research in the statistical learning field, with four new chapters added and existing chapters updated while maintaining a familiar layout.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "viii Preface to the Second Edition\n",
      "Chapter What‚Äôs new\n",
      "1.Introduction\n",
      "2.Overview of Supervised Learning\n",
      "3.Linear Methods for Regression LAR algorithm and generalizations\n",
      "of the lasso\n",
      "4.Linear Methods for ClassiÔ¨Åcation Lasso path for logistic regression\n",
      "5.Basis Expansions and Regulariza-\n",
      "tionAdditional illustrations of RKHS\n",
      "6.Kernel Smoothing Methods\n",
      "7.Model Assessment and Selection Strengths and pitfalls of cross-\n",
      "validation\n",
      "8.Model Inference and Averaging\n",
      "9.Additive Models, Trees, and\n",
      "Related Methods\n",
      "10.Boosting and Additive Trees New example from ecology; some\n",
      "material split oÔ¨Ä to Chapter 16.\n",
      "11.Neural Networks Bayesian neural nets and the NIPS\n",
      "2003 challenge\n",
      "12.Support Vector Machines and\n",
      "Flexible DiscriminantsPath algorithm for SVM classiÔ¨Åer\n",
      "13. Prototype Methods and\n",
      "Nearest-Neighbors\n",
      "14.Unsupervised Learning Spectral clustering, kernel PCA,\n",
      "sparse PCA, non-negative matrix\n",
      "factorization archetypal analysis,\n",
      "nonlinear dimension reduction,\n",
      "Google page rank algorithm, a\n",
      "direct approach to ICA\n",
      "15.Random Forests New\n",
      "16.Ensemble Learning New\n",
      "17.Undirected Graphical Models New\n",
      "18.High-Dimensional Problems New\n",
      "Some further notes:\n",
      "‚Ä¢Our Ô¨Årst edition was unfriendly to colorblind readers; in particular,\n",
      "we tended to favor red/green contrasts which are particularly trou-\n",
      "blesome. We have changed the color palette in this edition to a large\n",
      "extent, replacing the above with an orange /bluecontrast.\n",
      "‚Ä¢We have changed the name of Chapter 6 from ‚ÄúKernel Methods‚Äù to\n",
      "‚ÄúKernel Smoothing Methods‚Äù, to avoid confusion with the machine-\n",
      "learning kernel method that is discussed in the context of support vec-\n",
      "tor machines (Chapter 11) and more generally in Chapters 5 and 14.\n",
      "‚Ä¢In the Ô¨Årst edition, the discussion of error-rate estimation in Chap-\n",
      "ter 7 was sloppy, as we did not clearly diÔ¨Äerentiate the notions of\n",
      "conditional error rates (conditional on the training set) and uncondi-\n",
      "tional rates. We have Ô¨Åxed this in the new edition.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book that covers important concepts in data mining, inference, and prediction. The book discusses various topics in statistics, machine learning, and bioinformatics, with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors are prominent researchers in statistics and have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support. The second edition was motivated by the popularity of the first edition and the fast pace of research in the statistical learning field, with four new chapters added and existing chapters updated while maintaining a familiar layout. The second edition also addresses issues from the first edition, such as being colorblind-friendly, clarifying error-rate estimation, and renaming chapters for better clarity.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Preface to the Second Edition ix\n",
      "‚Ä¢Chapters 15 and 16 follow naturally from Chapter 10, and the chap-\n",
      "ters are probably best read in that order.\n",
      "‚Ä¢In Chapter 17, we have not attempted a comprehensive treatment\n",
      "of graphical models, and discuss only undirected models and some\n",
      "new methods for their estimation. Due to a lack of space, we have\n",
      "speciÔ¨Åcally omitted coverage of directed graphical models.\n",
      "‚Ä¢Chapter 18 explores the ‚Äú p‚â´N‚Äù problem, which is learning in high-\n",
      "dimensional feature spaces. These problems arise in many areas, in-\n",
      "cluding genomic and proteomic studies, and document classiÔ¨Åcation.\n",
      "We thank the many readers who have found the (too numerous) errors in\n",
      "the Ô¨Årst edition. We apologize for those and have done our best to avoid er-\n",
      "rors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry\n",
      "Wasserman for comments on some of the new chapters, and many Stanford\n",
      "graduate and post-doctoral students who oÔ¨Äered comments, in particular\n",
      "Mohammed AlQuraishi, John Boik, Holger HoeÔ¨Çing, Arian Maleki, Donal\n",
      "McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\n",
      "Hui Zou. We thank John Kimmel for his patience in guiding us through this\n",
      "new edition. RT dedicates this edition to the memory of Anna McPhee.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "August 2008\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book covering important concepts in data mining, inference, and prediction. The authors discuss various topics in statistics, machine learning, and bioinformatics with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support. The second edition was motivated by the popularity of the first edition and the fast pace of research in the statistical learning field, with four new chapters added and existing chapters updated while maintaining a familiar layout. The second edition also addresses issues from the first edition, such as being colorblind-friendly, clarifying error-rate estimation, and renaming chapters for better clarity. The preface to the second edition provides additional guidance on the order in which to read certain chapters, as well as insights into the new topics covered, including undirected graphical models and high-dimensional feature spaces. The authors express gratitude to readers who identified errors in the first edition and have made efforts to avoid them in the new edition. They also thank individuals who provided feedback on the new chapters, as well as John Kimmel for his guidance. Robert Tibshirani dedicates this edition to the memory of Anna McPhee.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "x Preface to the Second Edition\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book covering important concepts in data mining, inference, and prediction. The authors discuss various topics in statistics, machine learning, and bioinformatics with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support. The second edition was motivated by the popularity of the first edition and the fast pace of research in the statistical learning field, with four new chapters added and existing chapters updated while maintaining a familiar layout. The second edition also addresses issues from the first edition, such as being colorblind-friendly, clarifying error-rate estimation, and renaming chapters for better clarity. The preface to the second edition provides additional guidance on the order in which to read certain chapters, as well as insights into the new topics covered, including undirected graphical models and high-dimensional feature spaces. The authors express gratitude to readers who identified errors in the first edition and have made efforts to avoid them in the new edition. They also thank individuals who provided feedback on the new chapters, as well as John Kimmel for his guidance. Robert Tibshirani dedicates this edition to the memory of Anna McPhee.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "This is page xi\n",
      "Printer: Opaque this\n",
      "Preface to the First Edition\n",
      "We are drowning in information and starving for knowledge.\n",
      "‚ÄìRutherford D. Roger\n",
      "The Ô¨Åeld of Statistics is constantly challenged by the problems that science\n",
      "and industry brings to its door. In the early days, these problems often came\n",
      "from agricultural and industrial experiments and were relatively small in\n",
      "scope. With the advent of computers and the information age, statistical\n",
      "problems have exploded both in size and complexity. Challenges in the\n",
      "areas of data storage, organization and searching have led to the new Ô¨Åeld\n",
      "of ‚Äúdata mining‚Äù; statistical and computational problems in biology and\n",
      "medicine have created ‚Äúbioinformatics.‚Äù Vast amounts of data are being\n",
      "generated in many Ô¨Åelds, and the statistician‚Äôs job is to make sense of it\n",
      "all: to extract important patterns and trends, and understand ‚Äúwhat the\n",
      "data says.‚Äù We call this learning from data .\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tistical sciences. Since computation plays such a key role, it is not surprising\n",
      "that much of this new development has been done by researchers in other\n",
      "Ô¨Åelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly categorized as\n",
      "either supervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "in unsupervised learning, there is no outcome measure, and the goal is to\n",
      "describe the associations and patterns among a set of input measures.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book covering important concepts in data mining, inference, and prediction. The authors discuss various topics in statistics, machine learning, and bioinformatics with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support. The second edition was motivated by the popularity of the first edition and the fast pace of research in the statistical learning field, with four new chapters added and existing chapters updated while maintaining a familiar layout. The second edition also addresses issues from the first edition, such as being colorblind-friendly, clarifying error-rate estimation, and renaming chapters for better clarity. The preface to the second edition provides additional guidance on the order in which to read certain chapters, as well as insights into the new topics covered, including undirected graphical models and high-dimensional feature spaces. The authors express gratitude to readers who identified errors in the first edition and have made efforts to avoid them in the new edition. They also thank individuals who provided feedback on the new chapters, as well as John Kimmel for his guidance. Robert Tibshirani dedicates this edition to the memory of Anna McPhee. The book addresses the challenges in learning from data, which has led to a revolution in the statistical sciences with a focus on supervised and unsupervised learning problems.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "xii Preface to the First Edition\n",
      "This book is our attempt to bring together many of the important new\n",
      "ideas in learning, and explain them in a statistical framework. While some\n",
      "mathematical details are needed, we emphasize the methods and their con-\n",
      "ceptual underpinnings rather than their theoretical properties. As a result,\n",
      "we hope that this book will appeal not just to statisticians but also to\n",
      "researchers and practitioners in a wide variety of Ô¨Åelds.\n",
      "Just as we have learned a great deal from researchers outside of the Ô¨Åeld\n",
      "of statistics, our statistical viewpoint may help others to better understa nd\n",
      "diÔ¨Äerent aspects of learning:\n",
      "There is no true interpretation of anything; interpretatio n is a\n",
      "vehicle in the service of human comprehension. The value of\n",
      "interpretation is in enabling others to fruitfully think ab out an\n",
      "idea.\n",
      "‚ÄìAndreas Buja\n",
      "We would like to acknowledge the contribution of many people to the\n",
      "conception and completion of this book. David Andrews, Leo Breiman,\n",
      "Andreas Buja, John Chambers, Bradley Efron, GeoÔ¨Ärey Hinton, Werner\n",
      "Stuetzle, and John Tukey have greatly inÔ¨Çuenced our careers. Balasub-\n",
      "ramanian Narasimhan gave us advice and help on many computational\n",
      "problems, and maintained an excellent computing environment. Shin-Ho\n",
      "Bang helped in the production of a number of the Ô¨Ågures. Lee Wilkinson\n",
      "gave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Ma ya\n",
      "Gupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bog-\n",
      "dan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu\n",
      "Zhu, two reviewers and many students read parts of the manuscript and\n",
      "oÔ¨Äered helpful suggestions. John Kimmel was supportive, patient and help-\n",
      "ful at every phase; MaryAnn Brickner and Frank Ganz headed a superb\n",
      "production team at Springer. Trevor Hastie would like to thank the statis-\n",
      "tics department at the University of Cape Town for their hospitality during\n",
      "the Ô¨Ånal stages of this book. We gratefully acknowledge NSF and NIH for\n",
      "their support of this work. Finally, we would like to thank our families and\n",
      "our parents for their love and support.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "May 2001\n",
      "The quiet statisticians have changed our world; not by disco v-\n",
      "ering new facts or technical developments, but by changing t he\n",
      "ways that we reason, experiment and form our opinions ....\n",
      "‚ÄìIan Hacking\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book covering important concepts in data mining, inference, and prediction. The authors discuss various topics in statistics, machine learning, and bioinformatics with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support. The second edition was motivated by the popularity of the first edition and the fast pace of research in the statistical learning field, with four new chapters added and existing chapters updated while maintaining a familiar layout. The second edition also addresses issues from the first edition, such as being colorblind-friendly, clarifying error-rate estimation, and renaming chapters for better clarity. The preface to the second edition provides additional guidance on the order in which to read certain chapters, as well as insights into the new topics covered, including undirected graphical models and high-dimensional feature spaces. The authors express gratitude to readers who identified errors in the first edition and have made efforts to avoid them in the new edition. They also thank individuals who provided feedback on the new chapters, as well as John Kimmel for his guidance. Robert Tibshirani dedicates this edition to the memory of Anna McPhee. The book addresses the challenges in learning from data, which has led to a revolution in the statistical sciences with a focus on supervised and unsupervised learning problems. The authors also acknowledge the contributions of various individuals to the conception and completion of the book, as well as the support from organizations like NSF and NIH.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "This is page xiii\n",
      "Printer: Opaque this\n",
      "Contents\n",
      "Preface to the Second Edition vii\n",
      "Preface to the First Edition xi\n",
      "1 Introduction 1\n",
      "2 Overview of Supervised Learning 9\n",
      "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\n",
      "2.3 Two Simple Approaches to Prediction:\n",
      "Least Squares and Nearest Neighbors . . . . . . . . . . . 11\n",
      "2.3.1 Linear Models and Least Squares . . . . . . . . 11\n",
      "2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\n",
      "2.3.3 From Least Squares to Nearest Neighbors . . . . 16\n",
      "2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\n",
      "2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\n",
      "2.6 Statistical Models, Supervised Learning\n",
      "and Function Approximation . . . . . . . . . . . . . . . . 28\n",
      "2.6.1 A Statistical Model\n",
      "for the Joint Distribution Pr( X,Y) . . . . . . . 28\n",
      "2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\n",
      "2.6.3 Function Approximation . . . . . . . . . . . . . 29\n",
      "2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\n",
      "2.7.1 DiÔ¨Éculty of the Problem . . . . . . . . . . . . . 32\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book covering important concepts in data mining, inference, and prediction. The authors discuss various topics in statistics, machine learning, and bioinformatics with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support. The second edition was motivated by the popularity of the first edition and the fast pace of research in the statistical learning field, with four new chapters added and existing chapters updated while maintaining a familiar layout. The second edition also addresses issues from the first edition, such as being colorblind-friendly, clarifying error-rate estimation, and renaming chapters for better clarity. The preface to the second edition provides additional guidance on the order in which to read certain chapters, as well as insights into the new topics covered, including undirected graphical models and high-dimensional feature spaces. The authors express gratitude to readers who identified errors in the first edition and have made efforts to avoid them in the new edition. They also thank individuals who provided feedback on the new chapters, as well as John Kimmel for his guidance. Robert Tibshirani dedicates this edition to the memory of Anna McPhee. The book addresses the challenges in learning from data, which has led to a revolution in the statistical sciences with a focus on supervised and unsupervised learning problems. The authors also acknowledge the contributions of various individuals to the conception and completion of the book, as well as the support from organizations like NSF and NIH. The content of the book includes an introduction, an overview of supervised learning, variable types and terminology, approaches to prediction, statistical decision theory, local methods in high dimensions, statistical models, function approximation, and structured regression models.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "xiv Contents\n",
      "2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\n",
      "2.8.1 Roughness Penalty and Bayesian Methods . . . 34\n",
      "2.8.2 Kernel Methods and Local Regression . . . . . . 34\n",
      "2.8.3 Basis Functions and Dictionary Methods . . . . 35\n",
      "2.9 Model Selection and the Bias‚ÄìVariance TradeoÔ¨Ä . . . . . 37\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "3 Linear Methods for Regression 43\n",
      "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\n",
      "3.2 Linear Regression Models and Least Squares . . . . . . . 44\n",
      "3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\n",
      "3.2.2 The Gauss‚ÄìMarkov Theorem . . . . . . . . . . . 51\n",
      "3.2.3 Multiple Regression\n",
      "from Simple Univariate Regression . . . . . . . . 52\n",
      "3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\n",
      "3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\n",
      "3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\n",
      "3.3.2 Forward- and Backward-Stepwise Selection . . . 58\n",
      "3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\n",
      "3.3.4 Prostate Cancer Data Example (Continued) . . 61\n",
      "3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\n",
      "3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\n",
      "3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\n",
      "3.4.3 Discussion: Subset Selection, Ridge Regression\n",
      "and the Lasso . . . . . . . . . . . . . . . . . . . 69\n",
      "3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\n",
      "3.5 Methods Using Derived Input Directions . . . . . . . . . 79\n",
      "3.5.1 Principal Components Regression . . . . . . . . 79\n",
      "3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\n",
      "3.6 Discussion: A Comparison of the Selection\n",
      "and Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\n",
      "3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\n",
      "3.8 More on the Lasso and Related Path Algorithms . . . . . 86\n",
      "3.8.1 Incremental Forward Stagewise Regression . . . 86\n",
      "3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\n",
      "3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\n",
      "3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\n",
      "3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\n",
      "3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\n",
      "3.9 Computational Considerations . . . . . . . . . . . . . . . 93\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book covering important concepts in data mining, inference, and prediction. The authors discuss various topics in statistics, machine learning, and bioinformatics with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support. The second edition was motivated by the popularity of the first edition and the fast pace of research in the statistical learning field, with four new chapters added and existing chapters updated while maintaining a familiar layout. The second edition also addresses issues from the first edition, such as being colorblind-friendly, clarifying error-rate estimation, and renaming chapters for better clarity. The preface to the second edition provides additional guidance on the order in which to read certain chapters, as well as insights into the new topics covered, including undirected graphical models and high-dimensional feature spaces. The authors express gratitude to readers who identified errors in the first edition and have made efforts to avoid them in the new edition. They also thank individuals who provided feedback on the new chapters, as well as John Kimmel for his guidance. Robert Tibshirani dedicates this edition to the memory of Anna McPhee. The book addresses the challenges in learning from data, which has led to a revolution in the statistical sciences with a focus on supervised and unsupervised learning problems. The content of the book includes an introduction, an overview of supervised learning, variable types and terminology, approaches to prediction, statistical decision theory, local methods in high dimensions, statistical models, function approximation, and structured regression models. The book also delves into linear methods for regression, covering topics such as linear regression models, subset selection, shrinkage methods, methods using derived input directions, and computational considerations.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Contents xv\n",
      "4 Linear Methods for ClassiÔ¨Åcation 101\n",
      "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\n",
      "4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\n",
      "4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\n",
      "4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\n",
      "4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\n",
      "4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\n",
      "4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\n",
      "4.4.2 Example: South African Heart Disease . . . . . 122\n",
      "4.4.3 Quadratic Approximations and Inference . . . . 124\n",
      "4.4.4 L1Regularized Logistic Regression . . . . . . . . 125\n",
      "4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\n",
      "4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\n",
      "4.5.1 Rosenblatt‚Äôs Perceptron Learning Algorithm . . 130\n",
      "4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "5 Basis Expansions and Regularization 139\n",
      "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\n",
      "5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\n",
      "5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\n",
      "5.2.2 Example: South African Heart Disease (Continued)146\n",
      "5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\n",
      "5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\n",
      "5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\n",
      "5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\n",
      "5.5 Automatic Selection of the Smoothing Parameters . . . . 156\n",
      "5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\n",
      "5.5.2 The Bias‚ÄìVariance TradeoÔ¨Ä . . . . . . . . . . . . 158\n",
      "5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\n",
      "5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\n",
      "5.8.1 Spaces of Functions Generated by Kernels . . . 168\n",
      "5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\n",
      "5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\n",
      "5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\n",
      "5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Appendix: Computational Considerations for Splines . . . . . . 186\n",
      "Appendix: B-splines . . . . . . . . . . . . . . . . . . . . . 186\n",
      "Appendix: Computations for Smoothing Splines . . . . . 189\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book covering important concepts in data mining, inference, and prediction. The authors discuss various topics in statistics, machine learning, and bioinformatics with a focus on concepts rather than mathematics. The second edition includes new topics such as graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, have made significant contributions to the field. The dedication page acknowledges the authors' gratitude towards their parents and families for their support. The second edition was motivated by the popularity of the first edition and the fast pace of research in the statistical learning field, with four new chapters added and existing chapters updated while maintaining a familiar layout. The second edition also addresses issues from the first edition, such as being colorblind-friendly, clarifying error-rate estimation, and renaming chapters for better clarity. The preface to the second edition provides additional guidance on the order in which to read certain chapters, as well as insights into the new topics covered, including undirected graphical models and high-dimensional feature spaces. The authors express gratitude to readers who identified errors in the first edition and have made efforts to avoid them in the new edition. They also thank individuals who provided feedback on the new chapters, as well as John Kimmel for his guidance. Robert Tibshirani dedicates this edition to the memory of Anna McPhee. The book addresses the challenges in learning from data, which has led to a revolution in the statistical sciences with a focus on supervised and unsupervised learning problems. The content of the book includes an introduction, an overview of supervised learning, variable types and terminology, approaches to prediction, statistical decision theory, local methods in high dimensions, statistical models, function approximation, and structured regression models. The book also delves into linear methods for regression, covering topics such as linear regression models, subset selection, shrinkage methods, methods using derived input directions, and computational considerations. Additionally, the book explores linear methods for classification, including linear discriminant analysis, logistic regression, and separating hyperplanes. It also discusses basis expansions, regularization techniques, and various types of splines for data smoothing and feature extraction.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "xvi Contents\n",
      "6 Kernel Smoothing Methods 191\n",
      "6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\n",
      "6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\n",
      "6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\n",
      "6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\n",
      "6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\n",
      "6.4 Structured Local Regression Models in IRp. . . . . . . . 201\n",
      "6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\n",
      "6.4.2 Structured Regression Functions . . . . . . . . . 203\n",
      "6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\n",
      "6.6 Kernel Density Estimation and ClassiÔ¨Åcation . . . . . . . 208\n",
      "6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\n",
      "6.6.2 Kernel Density ClassiÔ¨Åcation . . . . . . . . . . . 210\n",
      "6.6.3 The Naive Bayes ClassiÔ¨Åer . . . . . . . . . . . . 210\n",
      "6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\n",
      "6.8 Mixture Models for Density Estimation and ClassiÔ¨Åcation 214\n",
      "6.9 Computational Considerations . . . . . . . . . . . . . . . 216\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "7 Model Assessment and Selection 219\n",
      "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\n",
      "7.3 The Bias‚ÄìVariance Decomposition . . . . . . . . . . . . . 223\n",
      "7.3.1 Example: Bias‚ÄìVariance TradeoÔ¨Ä . . . . . . . . 226\n",
      "7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\n",
      "7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\n",
      "7.6 The EÔ¨Äective Number of Parameters . . . . . . . . . . . . 232\n",
      "7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\n",
      "7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\n",
      "7.9 Vapnik‚ÄìChervonenkis Dimension . . . . . . . . . . . . . . 237\n",
      "7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\n",
      "7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\n",
      "7.10.1 K-Fold Cross-Validation . . . . . . . . . . . . . 241\n",
      "7.10.2 The Wrong and Right Way\n",
      "to Do Cross-validation . . . . . . . . . . . . . . . 245\n",
      "7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\n",
      "7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\n",
      "7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\n",
      "7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "8 Model Inference and Averaging 261\n",
      "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman is a comprehensive book that covers important concepts in data mining, inference, and prediction. The authors focus on various topics in statistics, machine learning, and bioinformatics, emphasizing concepts over mathematics. The second edition of the book includes new topics like graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, who are prominent researchers in statistics, have made significant contributions to the field. The second edition was motivated by the success of the first edition and the rapid pace of research in statistical learning, with four new chapters added and existing chapters updated while retaining a familiar layout. The authors also made efforts to address issues from the first edition, such as improving colorblind accessibility, clarifying error-rate estimation, and renaming chapters for better clarity. The preface to the second edition offers guidance on the sequence in which to read specific chapters and provides insights into new topics covered, including undirected graphical models and high-dimensional feature spaces. The authors express appreciation for readers who helped identify errors in the first edition and for those who provided feedback on the new chapters. Additionally, Robert Tibshirani dedicates this edition to the memory of Anna McPhee. The book delves into the challenges of learning from data, which has revolutionized the statistical sciences, focusing on supervised and unsupervised learning problems. The content covers an introduction, supervised learning overview, variable types, prediction approaches, statistical decision theory, local methods in high dimensions, statistical models, function approximation, and structured regression models. The book also explores linear methods for regression and classification, including topics like linear regression models, subset selection, shrinkage methods, logistic regression, and basis expansions for data smoothing and feature extraction.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Contents xvii\n",
      "8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\n",
      "8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\n",
      "8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\n",
      "8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\n",
      "8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\n",
      "8.4 Relationship Between the Bootstrap\n",
      "and Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\n",
      "8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\n",
      "8.5.1 Two-Component Mixture Model . . . . . . . . . 272\n",
      "8.5.2 The EM Algorithm in General . . . . . . . . . . 276\n",
      "8.5.3 EM as a Maximization‚ÄìMaximization Procedure 277\n",
      "8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\n",
      "8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n",
      "8.7.1 Example: Trees with Simulated Data . . . . . . 283\n",
      "8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\n",
      "8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n",
      "9 Additive Models, Trees, and Related Methods 295\n",
      "9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\n",
      "9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\n",
      "9.1.2 Example: Additive Logistic Regression . . . . . 299\n",
      "9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\n",
      "9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\n",
      "9.2.3 ClassiÔ¨Åcation Trees . . . . . . . . . . . . . . . . 308\n",
      "9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\n",
      "9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\n",
      "9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\n",
      "9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 321\n",
      "9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\n",
      "9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\n",
      "9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\n",
      "9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\n",
      "9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\n",
      "9.7 Computational Considerations . . . . . . . . . . . . . . . 334\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "10 Boosting and Additive Trees 337\n",
      "10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The existing summary provides an overview of \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The book covers important concepts in data mining, inference, and prediction, with a focus on statistics, machine learning, and bioinformatics. The second edition includes new topics like graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, made significant contributions to the field and updated the second edition to reflect the rapid pace of research in statistical learning. The book explores challenges in learning from data, including supervised and unsupervised learning problems, and covers topics such as linear methods for regression and classification. The new context provided delves into additional topics covered in the book, such as the bootstrap, Bayesian methods, the EM algorithm, MCMC for sampling from the posterior, bagging, model averaging, stacking, and boosting methods. The book also explores additive models, trees, and related methods like generalized additive models, tree-based methods, PRIM, MARS, hierarchical mixtures of experts, and computational considerations.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "xviii Contents\n",
      "10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\n",
      "10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\n",
      "10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\n",
      "10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\n",
      "10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\n",
      "10.7 ‚ÄúOÔ¨Ä-the-Shelf‚Äù Procedures for Data Mining . . . . . . . . 350\n",
      "10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\n",
      "10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\n",
      "10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\n",
      "10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\n",
      "10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\n",
      "10.10.3 Implementations of Gradient Boosting . . . . . . 360\n",
      "10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\n",
      "10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\n",
      "10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\n",
      "10.13.1 Relative Importance of Predictor Variables . . . 367\n",
      "10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\n",
      "10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n",
      "10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\n",
      "10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\n",
      "10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n",
      "11 Neural Networks 389\n",
      "11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\n",
      "11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\n",
      "11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\n",
      "11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\n",
      "11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\n",
      "11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\n",
      "11.5.2 OverÔ¨Åtting . . . . . . . . . . . . . . . . . . . . . 398\n",
      "11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\n",
      "11.5.4 Number of Hidden Units and Layers . . . . . . . 400\n",
      "11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\n",
      "11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\n",
      "11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\n",
      "11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\n",
      "11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\n",
      "11.9.2 Performance Comparisons . . . . . . . . . . . . 412\n",
      "11.10 Computational Considerations . . . . . . . . . . . . . . . 414\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The existing summary of \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman covers important concepts in data mining, inference, and prediction, with a focus on statistics, machine learning, and bioinformatics. The second edition includes new topics like graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, made significant contributions to the field and updated the second edition to reflect the rapid pace of research in statistical learning. The book explores challenges in learning from data, including supervised and unsupervised learning problems, and covers topics such as linear methods for regression and classification. The new context provided delves into additional topics covered in the book, such as boosting fits, forward stagewise additive modeling, exponential loss, AdaBoost, loss functions and robustness, off-the-shelf procedures for data mining, boosting trees, numerical optimization via gradient boosting, right-sized trees for boosting, regularization, interpretation, and illustrations of various datasets. Additionally, the book covers neural networks, projection pursuit regression, fitting neural networks, issues in training neural networks, examples of simulated and ZIP code data, Bayesian neural nets, performance comparisons, and computational considerations.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Contents xix\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "12 Support Vector Machines and\n",
      "Flexible Discriminants 417\n",
      "12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\n",
      "12.2 The Support Vector ClassiÔ¨Åer . . . . . . . . . . . . . . . . 417\n",
      "12.2.1 Computing the Support Vector ClassiÔ¨Åer . . . . 420\n",
      "12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\n",
      "12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\n",
      "12.3.1 Computing the SVM for ClassiÔ¨Åcation . . . . . . 423\n",
      "12.3.2 The SVM as a Penalization Method . . . . . . . 426\n",
      "12.3.3 Function Estimation and Reproducing Kernels . 428\n",
      "12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\n",
      "12.3.5 A Path Algorithm for the SVM ClassiÔ¨Åer . . . . 432\n",
      "12.3.6 Support Vector Machines for Regression . . . . . 434\n",
      "12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\n",
      "12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\n",
      "12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 438\n",
      "12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\n",
      "12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\n",
      "12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\n",
      "12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\n",
      "12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "13 Prototype Methods and Nearest-Neighbors 459\n",
      "13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2.1 K-means Clustering . . . . . . . . . . . . . . . . 460\n",
      "13.2.2 Learning Vector Quantization . . . . . . . . . . 462\n",
      "13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\n",
      "13.3 k-Nearest-Neighbor ClassiÔ¨Åers . . . . . . . . . . . . . . . 463\n",
      "13.3.1 Example: A Comparative Study . . . . . . . . . 468\n",
      "13.3.2 Example: k-Nearest-Neighbors\n",
      "and Image Scene ClassiÔ¨Åcation . . . . . . . . . . 470\n",
      "13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\n",
      "13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\n",
      "13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\n",
      "13.4.2 Global Dimension Reduction\n",
      "for Nearest-Neighbors . . . . . . . . . . . . . . . 479\n",
      "13.5 Computational Considerations . . . . . . . . . . . . . . . 480\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The existing summary of \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman covers important concepts in data mining, inference, and prediction, with a focus on statistics, machine learning, and bioinformatics. The second edition includes new topics like graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, made significant contributions to the field and updated the second edition to reflect the rapid pace of research in statistical learning. The book explores challenges in learning from data, including supervised and unsupervised learning problems, and covers topics such as linear methods for regression and classification. The new context provided delves into additional topics covered in the book, such as boosting fits, forward stagewise additive modeling, exponential loss, AdaBoost, loss functions and robustness, off-the-shelf procedures for data mining, boosting trees, numerical optimization via gradient boosting, right-sized trees for boosting, regularization, interpretation, and illustrations of various datasets. Additionally, the book covers neural networks, projection pursuit regression, fitting neural networks, issues in training neural networks, examples of simulated and ZIP code data, Bayesian neural nets, performance comparisons, and computational considerations. Support Vector Machines, Flexible Discriminants, Prototype Methods, and Nearest-Neighbors are also discussed in detail, providing a comprehensive overview of key concepts in statistical learning and data analysis.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "xx Contents\n",
      "14 Unsupervised Learning 485\n",
      "14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\n",
      "14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\n",
      "14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\n",
      "14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\n",
      "14.2.3 Example: Market Basket Analysis . . . . . . . . 492\n",
      "14.2.4 Unsupervised as Supervised Learning . . . . . . 495\n",
      "14.2.5 Generalized Association Rules . . . . . . . . . . 497\n",
      "14.2.6 Choice of Supervised Learning Method . . . . . 499\n",
      "14.2.7 Example: Market Basket Analysis (Continued) . 499\n",
      "14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\n",
      "14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\n",
      "14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\n",
      "14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\n",
      "14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\n",
      "14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\n",
      "14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . . 509\n",
      "14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\n",
      "14.3.8 Example: Human Tumor Microarray Data . . . 512\n",
      "14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\n",
      "14.3.10 K-medoids . . . . . . . . . . . . . . . . . . . . . 515\n",
      "14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\n",
      "14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\n",
      "14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\n",
      "14.5 Principal Components, Curves and Surfaces . . . . . . . . 534\n",
      "14.5.1 Principal Components . . . . . . . . . . . . . . . 534\n",
      "14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\n",
      "14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\n",
      "14.5.4 Kernel Principal Components . . . . . . . . . . . 547\n",
      "14.5.5 Sparse Principal Components . . . . . . . . . . . 550\n",
      "14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\n",
      "14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\n",
      "14.7 Independent Component Analysis\n",
      "and Exploratory Projection Pursuit . . . . . . . . . . . . 557\n",
      "14.7.1 Latent Variables and Factor Analysis . . . . . . 558\n",
      "14.7.2 Independent Component Analysis . . . . . . . . 560\n",
      "14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\n",
      "14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\n",
      "14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\n",
      "14.9 Nonlinear Dimension Reduction\n",
      "and Local Multidimensional Scaling . . . . . . . . . . . . 572\n",
      "14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The existing summary of \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman provides a comprehensive overview of important concepts in data mining, inference, and prediction, focusing on statistics, machine learning, and bioinformatics. The second edition includes new topics like graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, updated the second edition to reflect the rapid pace of research in statistical learning. The book explores challenges in learning from data, covering topics such as linear methods for regression and classification, boosting fits, forward stagewise additive modeling, exponential loss, AdaBoost, loss functions, robustness, off-the-shelf procedures for data mining, boosting trees, numerical optimization via gradient boosting, right-sized trees for boosting, regularization, interpretation, and illustrations of various datasets. Additionally, the book delves into neural networks, projection pursuit regression, fitting neural networks, issues in training neural networks, examples of simulated and ZIP code data, Bayesian neural nets, performance comparisons, computational considerations, Support Vector Machines, Flexible Discriminants, Prototype Methods, and Nearest-Neighbors. The new context introduces further topics such as unsupervised learning, association rules, market basket analysis, cluster analysis, self-organizing maps, principal components, curves and surfaces, non-negative matrix factorization, independent component analysis, multidimensional scaling, nonlinear dimension reduction, local multidimensional scaling, and the Google PageRank Algorithm. This expansion enhances the understanding of key concepts in statistical learning and data analysis covered in the book.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Contents xxi\n",
      "15 Random Forests 587\n",
      "15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\n",
      "15.2 DeÔ¨Ånition of Random Forests . . . . . . . . . . . . . . . . 587\n",
      "15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\n",
      "15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\n",
      "15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\n",
      "15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\n",
      "15.3.4 Random Forests and OverÔ¨Åtting . . . . . . . . . 596\n",
      "15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\n",
      "15.4.1 Variance and the De-Correlation EÔ¨Äect . . . . . 597\n",
      "15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\n",
      "15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\n",
      "16 Ensemble Learning 605\n",
      "16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\n",
      "16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\n",
      "16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\n",
      "16.2.2 The ‚ÄúBet on Sparsity‚Äù Principle . . . . . . . . . 610\n",
      "16.2.3 Regularization Paths, Over-Ô¨Åtting and Margins . 613\n",
      "16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\n",
      "16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\n",
      "16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n",
      "17 Undirected Graphical Models 625\n",
      "17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\n",
      "17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\n",
      "17.3 Undirected Graphical Models for Continuous Variables . 630\n",
      "17.3.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 631\n",
      "17.3.2 Estimation of the Graph Structure . . . . . . . . 635\n",
      "17.4 Undirected Graphical Models for Discrete Variables . . . 638\n",
      "17.4.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 639\n",
      "17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\n",
      "17.4.3 Estimation of the Graph Structure . . . . . . . . 642\n",
      "17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\n",
      "18 High-Dimensional Problems: p‚â´N 649\n",
      "18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The existing summary of \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman provides a comprehensive overview of important concepts in data mining, inference, and prediction, focusing on statistics, machine learning, and bioinformatics. The second edition includes new topics like graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, updated the second edition to reflect the rapid pace of research in statistical learning. The book explores challenges in learning from data, covering topics such as linear methods for regression and classification, boosting fits, forward stagewise additive modeling, exponential loss, AdaBoost, loss functions, robustness, off-the-shelf procedures for data mining, boosting trees, numerical optimization via gradient boosting, right-sized trees for boosting, regularization, interpretation, and illustrations of various datasets. Additionally, the book delves into neural networks, projection pursuit regression, fitting neural networks, issues in training neural networks, examples of simulated and ZIP code data, Bayesian neural nets, performance comparisons, computational considerations, Support Vector Machines, Flexible Discriminants, Prototype Methods, and Nearest-Neighbors. The new context introduces further topics such as random forests, ensemble learning, undirected graphical models, and high-dimensional problems, enhancing the understanding of key concepts in statistical learning and data analysis covered in the book.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "xxii Contents\n",
      "18.2 Diagonal Linear Discriminant Analysis\n",
      "and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\n",
      "18.3 Linear ClassiÔ¨Åers with Quadratic Regularization . . . . . 654\n",
      "18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\n",
      "18.3.2 Logistic Regression\n",
      "with Quadratic Regularization . . . . . . . . . . 657\n",
      "18.3.3 The Support Vector ClassiÔ¨Åer . . . . . . . . . . 657\n",
      "18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\n",
      "18.3.5 Computational Shortcuts When p‚â´N. . . . . 659\n",
      "18.4 Linear ClassiÔ¨Åers with L1Regularization . . . . . . . . . 661\n",
      "18.4.1 Application of Lasso\n",
      "to Protein Mass Spectroscopy . . . . . . . . . . 664\n",
      "18.4.2 The Fused Lasso for Functional Data . . . . . . 666\n",
      "18.5 ClassiÔ¨Åcation When Features are Unavailable . . . . . . . 668\n",
      "18.5.1 Example: String Kernels\n",
      "and Protein ClassiÔ¨Åcation . . . . . . . . . . . . . 668\n",
      "18.5.2 ClassiÔ¨Åcation and Other Models Using\n",
      "Inner-Product Kernels and Pairwise Distances . 670\n",
      "18.5.3 Example: Abstracts ClassiÔ¨Åcation . . . . . . . . 672\n",
      "18.6 High-Dimensional Regression:\n",
      "Supervised Principal Components . . . . . . . . . . . . . 674\n",
      "18.6.1 Connection to Latent-Variable Modeling . . . . 678\n",
      "18.6.2 Relationship with Partial Least Squares . . . . . 680\n",
      "18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\n",
      "18.7 Feature Assessment and the Multiple-Testing Problem . . 683\n",
      "18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\n",
      "18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\n",
      "18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\n",
      "18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n",
      "References 699\n",
      "Author Index 729\n",
      "Index 737\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The existing summary of \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman provides a comprehensive overview of important concepts in data mining, inference, and prediction, focusing on statistics, machine learning, and bioinformatics. The second edition includes new topics like graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, updated the second edition to reflect the rapid pace of research in statistical learning. The book explores challenges in learning from data, covering topics such as linear methods for regression and classification, boosting fits, forward stagewise additive modeling, exponential loss, AdaBoost, loss functions, robustness, off-the-shelf procedures for data mining, boosting trees, numerical optimization via gradient boosting, right-sized trees for boosting, regularization, interpretation, and illustrations of various datasets. Additionally, the book delves into neural networks, projection pursuit regression, fitting neural networks, issues in training neural networks, examples of simulated and ZIP code data, Bayesian neural nets, performance comparisons, computational considerations, Support Vector Machines, Flexible Discriminants, Prototype Methods, and Nearest-Neighbors. The new context introduces further topics such as random forests, ensemble learning, undirected graphical models, high-dimensional problems, linear discriminant analysis, quadratic regularization, L1 regularization, feature selection, classification when features are unavailable, high-dimensional regression, supervised principal components, feature assessment, the multiple-testing problem, false discovery rate, asymmetric cutpoints, the SAM procedure, and a Bayesian interpretation of the FDR, enhancing the understanding of key concepts in statistical learning and data analysis covered in the book.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, Ô¨Ånance and\n",
      "industry. Here are some examples of learning problems:\n",
      "‚Ä¢Predict whether a patient, hospitalized due to a heart attack, will\n",
      "have a second heart attack. The prediction is to be based on demo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "‚Ä¢Predict the price of a stock in 6 months from now, on the basis of\n",
      "company performance measures and economic data.\n",
      "‚Ä¢Identify the numbers in a handwritten ZIP code, from a digitized\n",
      "image.\n",
      "‚Ä¢Estimate the amount of glucose in the blood of a diabetic person,\n",
      "from the infrared absorption spectrum of that person‚Äôs blood.\n",
      "‚Ä¢Identify the risk factors for prostate cancer, based on clinical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the Ô¨Åelds of statistics, data\n",
      "mining and artiÔ¨Åcial intelligence, intersecting with areas of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a stock price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have a training set of data, in which we observe the outcome and feature\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The existing summary of \"The Elements of Statistical Learning\" by Trevor Hastie, Robert Tibshirani, and Jerome Friedman provides a comprehensive overview of important concepts in data mining, inference, and prediction, focusing on statistics, machine learning, and bioinformatics. The second edition includes new topics like graphical models, random forests, ensemble methods, and methods for \"wide\" data. The authors, prominent researchers in statistics, updated the second edition to reflect the rapid pace of research in statistical learning. The book explores challenges in learning from data, covering topics such as linear methods for regression and classification, boosting fits, forward stagewise additive modeling, exponential loss, AdaBoost, loss functions, robustness, off-the-shelf procedures for data mining, boosting trees, numerical optimization via gradient boosting, right-sized trees for boosting, regularization, interpretation, and illustrations of various datasets. Additionally, the book delves into neural networks, projection pursuit regression, fitting neural networks, issues in training neural networks, examples of simulated and ZIP code data, Bayesian neural nets, performance comparisons, computational considerations, Support Vector Machines, Flexible Discriminants, Prototype Methods, and Nearest-Neighbors. The new context introduces further topics such as undirected graphical models, high-dimensional problems, linear discriminant analysis, quadratic regularization, L1 regularization, feature selection, classification when features are unavailable, high-dimensional regression, supervised principal components, feature assessment, the multiple-testing problem, false discovery rate, asymmetric cutpoints, the SAM procedure, and a Bayesian interpretation of the FDR, enhancing the understanding of key concepts in statistical learning and data analysis covered in the book. The book also emphasizes the importance of learning from data in various fields such as science, finance, and industry, providing examples of different learning problems and highlighting the intersection of statistics, data mining, artificial intelligence, and engineering.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: The book covers a comprehensive range of topics in machine learning, including supervised learning, regression, classification, kernel smoothing, model assessment and selection, additive models, trees, boosting, neural networks, support vector machines, prototype methods, unsupervised learning, random forests, ensemble learning, undirected graphical models, high-dimensional problems, and more. The second edition also includes chapters on undirected graphical models, learning in high-dimensional feature spaces, support vector machines, flexible discriminants, prototype methods, and nearest-neighbors. The authors have made improvements to address colorblind readers and clarify concepts related to error-rate estimation. The book also covers variable types, statistical decision theory, local methods in high dimensions, structured regression models, function approximation, restricted estimators, model selection, and the bias-variance tradeoff. It includes various linear methods for regression and classification, as well as topics such as filtering and feature extraction, smoothing splines, nonparametric logistic regression, regularization, wavelet smoothing, and more. The book also delves into the bootstrap and maximum likelihood methods, Bayesian methods, the EM algorithm, MCMC for sampling from the posterior, bagging, model averaging, stochastic search, additive models, tree-based methods, bump hunting, multivariate adaptive regression splines, hierarchical mixtures of experts, missing data, boosting, and additive trees. Additionally, the book includes chapters on boosting fits an additive model, exponential loss and AdaBoost, boosting trees, numerical optimization via gradient boosting, regularization, interpretation, and illustrations. It also covers neural networks, projection pursuit regression, fitting neural networks, issues in training neural networks, example simulations and ZIP code data, Bayesian neural nets, computational considerations, support vector machines, flexible discriminants, prototype methods, nearest-neighbor methods, unsupervised learning, association rules, cluster analysis, self-organizing maps, principal components, curves and surfaces, non-negative matrix factorization, independent component analysis, exploratory projection pursuit, multidimensional scaling, nonlinear dimension reduction, local multidimensional scaling, and the Google PageRank algorithm. The new context also includes chapters on random forests, ensemble learning, undirected graphical models, and high-dimensional problems.\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "xxii Contents\n",
      "18.2 Diagonal Linear Discriminant Analysis\n",
      "and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\n",
      "18.3 Linear ClassiÔ¨Åers with Quadratic Regularization . . . . . 654\n",
      "18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\n",
      "18.3.2 Logistic Regression\n",
      "with Quadratic Regularization . . . . . . . . . . 657\n",
      "18.3.3 The Support Vector ClassiÔ¨Åer . . . . . . . . . . 657\n",
      "18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\n",
      "18.3.5 Computational Shortcuts When p‚â´N. . . . . 659\n",
      "18.4 Linear ClassiÔ¨Åers with L1Regularization . . . . . . . . . 661\n",
      "18.4.1 Application of Lasso\n",
      "to Protein Mass Spectroscopy . . . . . . . . . . 664\n",
      "18.4.2 The Fused Lasso for Functional Data . . . . . . 666\n",
      "18.5 ClassiÔ¨Åcation When Features are Unavailable . . . . . . . 6 68\n",
      "18.5.1 Example: String Kernels\n",
      "and Protein ClassiÔ¨Åcation . . . . . . . . . . . . . 668\n",
      "18.5.2 ClassiÔ¨Åcation and Other Models Using\n",
      "Inner-Product Kernels and Pairwise Distances . 670\n",
      "18.5.3 Example: Abstracts ClassiÔ¨Åcation . . . . . . . . 672\n",
      "18.6 High-Dimensional Regression:\n",
      "Supervised Principal Components . . . . . . . . . . . . . 674\n",
      "18.6.1 Connection to Latent-Variable Modeling . . . . 678\n",
      "18.6.2 Relationship with Partial Least Squares . . . . . 680\n",
      "18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\n",
      "18.7 Feature Assessment and the Multiple-Testing Problem . . 683\n",
      "18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\n",
      "18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\n",
      "18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\n",
      "18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n",
      "References 699\n",
      "Author Index 729\n",
      "Index 737\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary\n",
      "We have provided an existing summary up to a certain point: The new context includes a chapter on linear classifiers with quadratic regularization, including regularized discriminant analysis, logistic regression with quadratic regularization, the support vector classifier, feature selection, and computational shortcuts. It also covers linear classifiers with L1 regularization, such as the application of Lasso to protein mass spectroscopy and the fused Lasso for functional data. The chapter also discusses classification when features are unavailable, high-dimensional regression using supervised principal components, feature assessment and the multiple-testing problem, and provides bibliographic notes.\n",
      "We have the opportunity to refine the existing summary(only if needed) with some more context below.\n",
      "------------\n",
      "This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, Ô¨Ånance and\n",
      "industry. Here are some examples of learning problems:\n",
      "‚Ä¢Predict whether a patient, hospitalized due to a heart attac k, will\n",
      "have a second heart attack. The prediction is to be based on de mo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "‚Ä¢Predict the price of a stock in 6 months from now, on the basis o f\n",
      "company performance measures and economic data.\n",
      "‚Ä¢Identify the numbers in a handwritten ZIP code, from a digiti zed\n",
      "image.\n",
      "‚Ä¢Estimate the amount of glucose in the blood of a diabetic pers on,\n",
      "from the infrared absorption spectrum of that person‚Äôs bloo d.\n",
      "‚Ä¢Identify the risk factors for prostate cancer, based on clin ical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the Ô¨Åelds of statis tics, data\n",
      "mining and artiÔ¨Åcial intelligence, intersecting with area s of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a sto ck price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have atraining set of data, in which we observe the outcome and feature\n",
      "------------\n",
      "Given the new context, refine the original summary\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The existing summary is already comprehensive and does not require any refinement.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(sl_data[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f0c7b",
   "metadata": {},
   "source": [
    "## Custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc3725f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a concise summary of the following:\n",
      "\n",
      "\n",
      "\"{text}\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\n"
     ]
    }
   ],
   "source": [
    "print(chain.initial_llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86b62ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: {existing_answer}\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "{text}\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\n"
     ]
    }
   ],
   "source": [
    "print(chain.refine_llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0487c42c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Extract the most relevant themes from the following:\n",
      "\n",
      "\n",
      "\"Springer Series in Statistics\n",
      "Trevor Hastie\n",
      "Robert TibshiraniJerome FriedmanSpringer Series in Statistics\n",
      "The Elements of\n",
      "Statistical Learning\n",
      "Data Mining, Inference, and Prediction\n",
      "The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book‚Äôs coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting‚Äîthe first comprehensive treatment of thistopic in any book.\n",
      "This major new edition features many topics not covered in the original, including graphical\n",
      "models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for ‚Äúwide‚Äù data (p bigger than n), including multiple testing and false discovery rates.\n",
      "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\n",
      "Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
      "‚Ä∫springer.comSTATISTICS\n",
      "isbn 978-0-387-84857-0Trevor Hastie ‚Ä¢ Robert Tibshirani ‚Ä¢ Jerome Friedman\n",
      "The Elements of Statictical Learning\n",
      "Hastie ‚Ä¢ Tibshirani ‚Ä¢ Friedman\n",
      "Second Edition\"\n",
      "\n",
      "\n",
      "THEMES:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: 1. Data mining and machine learning\n",
      "2. Statistical modeling and inference\n",
      "3. Use of computational and information technology\n",
      "4. Application of statistics in various fields such as medicine, biology, finance, and marketing\n",
      "5. Supervised and unsupervised learning methods\n",
      "6. Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting\n",
      "7. Prominent researchers and their contributions in the field of statistics and data mining.\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "This is page v\n",
      "Printer: Opaque this\n",
      "To our parents:\n",
      "Valerie and Patrick Hastie\n",
      "Vera and Sami Tibshirani\n",
      "Florence and Harry Friedman\n",
      "and to our families:\n",
      "Samantha, Timothy, and Lynda\n",
      "Charlie, Ryan, Julie, and Cheryl\n",
      "Melanie, Dora, Monika, and Ildiko\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: 1. Data mining and machine learning, 2. Statistical modeling and inference, 3. Use of computational and information technology, 4. Application of statistics in various fields such as medicine, biology, finance, and marketing, 5. Supervised and unsupervised learning methods, 6. Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting, 7. Prominent researchers and their contributions in the field of statistics and data mining\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "vi\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: 1. Data mining and machine learning, 2. Statistical modeling and inference, 3. Use of computational and information technology, 4. Application of statistics in various fields such as medicine, biology, finance, and marketing, 5. Supervised and unsupervised learning methods, 6. Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting, 7. Prominent researchers and their contributions in the field of statistics and data mining\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "This is page vii\n",
      "Printer: Opaque this\n",
      "Preface to the Second Edition\n",
      "In God we trust, all others bring data.\n",
      "‚ÄìWilliam Edwards Deming (1900-1993)1\n",
      "We have been gratiÔ¨Åed by the popularity of the Ô¨Årst edition of The\n",
      "Elements of Statistical Learning. This, along with the fast pace of research\n",
      "in the statistical learning Ô¨Åeld, motivated us to update our book with a\n",
      "second edition.\n",
      "We have added four new chapters and updated some of the existing\n",
      "chapters. Because many readers are familiar with the layout of the Ô¨Årst\n",
      "edition, we have tried to change it as little as possible. Here is a summary\n",
      "of the main changes:\n",
      "1On the Web, this quote has been widely attributed to both Demi ng and Robert W.\n",
      "Hayden; however Professor Hayden told us that he can claim no credit for this quote,\n",
      "and ironically we could Ô¨Ånd no ‚Äúdata‚Äù conÔ¨Årming that Deming a ctually said this.\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: 1. Data mining and machine learning, 2. Statistical modeling and inference, 3. Use of computational and information technology, 4. Application of statistics in various fields such as medicine, biology, finance, and marketing, 5. Supervised and unsupervised learning methods, 6. Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting, 7. Prominent researchers and their contributions in the field of statistics and data mining\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "viii Preface to the Second Edition\n",
      "Chapter What‚Äôs new\n",
      "1.Introduction\n",
      "2.Overview of Supervised Learning\n",
      "3.Linear Methods for Regression LAR algorithm and generalizations\n",
      "of the lasso\n",
      "4.Linear Methods for ClassiÔ¨Åcation Lasso path for logistic regression\n",
      "5.Basis Expansions and Regulariza-\n",
      "tionAdditional illustrations of RKHS\n",
      "6.Kernel Smoothing Methods\n",
      "7.Model Assessment and Selection Strengths and pitfalls of cross-\n",
      "validation\n",
      "8.Model Inference and Averaging\n",
      "9.Additive Models, Trees, and\n",
      "Related Methods\n",
      "10.Boosting and Additive Trees New example from ecology; some\n",
      "material split oÔ¨Ä to Chapter 16.\n",
      "11.Neural Networks Bayesian neural nets and the NIPS\n",
      "2003 challenge\n",
      "12.Support Vector Machines and\n",
      "Flexible DiscriminantsPath algorithm for SVM classiÔ¨Åer\n",
      "13. Prototype Methods and\n",
      "Nearest-Neighbors\n",
      "14.Unsupervised Learning Spectral clustering, kernel PCA,\n",
      "sparse PCA, non-negative matrix\n",
      "factorization archetypal analysis,\n",
      "nonlinear dimension reduction,\n",
      "Google page rank algorithm, a\n",
      "direct approach to ICA\n",
      "15.Random Forests New\n",
      "16.Ensemble Learning New\n",
      "17.Undirected Graphical Models New\n",
      "18.High-Dimensional Problems New\n",
      "Some further notes:\n",
      "‚Ä¢Our Ô¨Årst edition was unfriendly to colorblind readers; in particular,\n",
      "we tended to favor red/green contrasts which are particularly trou-\n",
      "blesome. We have changed the color palette in this edition to a large\n",
      "extent, replacing the above with an orange /bluecontrast.\n",
      "‚Ä¢We have changed the name of Chapter 6 from ‚ÄúKernel Methods‚Äù to\n",
      "‚ÄúKernel Smoothing Methods‚Äù, to avoid confusion with the machine-\n",
      "learning kernel method that is discussed in the context of support vec-\n",
      "tor machines (Chapter 11) and more generally in Chapters 5 and 14.\n",
      "‚Ä¢In the Ô¨Årst edition, the discussion of error-rate estimation in Chap-\n",
      "ter 7 was sloppy, as we did not clearly diÔ¨Äerentiate the notions of\n",
      "conditional error rates (conditional on the training set) and uncondi-\n",
      "tional rates. We have Ô¨Åxed this in the new edition.\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: 1. Data mining and machine learning, 2. Statistical modeling and inference, 3. Use of computational and information technology, 4. Application of statistics in various fields such as medicine, biology, finance, and marketing, 5. Supervised and unsupervised learning methods, 6. Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting, 7. Prominent researchers and their contributions in the field of statistics and data mining\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "Preface to the Second Edition ix\n",
      "‚Ä¢Chapters 15 and 16 follow naturally from Chapter 10, and the chap-\n",
      "ters are probably best read in that order.\n",
      "‚Ä¢In Chapter 17, we have not attempted a comprehensive treatment\n",
      "of graphical models, and discuss only undirected models and some\n",
      "new methods for their estimation. Due to a lack of space, we have\n",
      "speciÔ¨Åcally omitted coverage of directed graphical models.\n",
      "‚Ä¢Chapter 18 explores the ‚Äú p‚â´N‚Äù problem, which is learning in high-\n",
      "dimensional feature spaces. These problems arise in many areas, in-\n",
      "cluding genomic and proteomic studies, and document classiÔ¨Åcation.\n",
      "We thank the many readers who have found the (too numerous) errors in\n",
      "the Ô¨Årst edition. We apologize for those and have done our best to avoid er-\n",
      "rors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry\n",
      "Wasserman for comments on some of the new chapters, and many Stanford\n",
      "graduate and post-doctoral students who oÔ¨Äered comments, in particular\n",
      "Mohammed AlQuraishi, John Boik, Holger HoeÔ¨Çing, Arian Maleki, Donal\n",
      "McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\n",
      "Hui Zou. We thank John Kimmel for his patience in guiding us through this\n",
      "new edition. RT dedicates this edition to the memory of Anna McPhee.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "August 2008\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: 1. Data mining and machine learning, 2. Statistical modeling and inference, 3. Use of computational and information technology, 4. Application of statistics in various fields such as medicine, biology, finance, and marketing, 5. Supervised and unsupervised learning methods, 6. Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting, 7. Prominent researchers and their contributions in the field of statistics and data mining\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "x Preface to the Second Edition\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: 1. Data mining and machine learning, 2. Statistical modeling and inference, 3. Use of computational and information technology, 4. Application of statistics in various fields such as medicine, biology, finance, and marketing, 5. Supervised and unsupervised learning methods, 6. Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting, 7. Prominent researchers and their contributions in the field of statistics and data mining\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "This is page xi\n",
      "Printer: Opaque this\n",
      "Preface to the First Edition\n",
      "We are drowning in information and starving for knowledge.\n",
      "‚ÄìRutherford D. Roger\n",
      "The Ô¨Åeld of Statistics is constantly challenged by the problems that science\n",
      "and industry brings to its door. In the early days, these problems often came\n",
      "from agricultural and industrial experiments and were relatively small in\n",
      "scope. With the advent of computers and the information age, statistical\n",
      "problems have exploded both in size and complexity. Challenges in the\n",
      "areas of data storage, organization and searching have led to the new Ô¨Åeld\n",
      "of ‚Äúdata mining‚Äù; statistical and computational problems in biology and\n",
      "medicine have created ‚Äúbioinformatics.‚Äù Vast amounts of data are being\n",
      "generated in many Ô¨Åelds, and the statistician‚Äôs job is to make sense of it\n",
      "all: to extract important patterns and trends, and understand ‚Äúwhat the\n",
      "data says.‚Äù We call this learning from data .\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tistical sciences. Since computation plays such a key role, it is not surprising\n",
      "that much of this new development has been done by researchers in other\n",
      "Ô¨Åelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly categorized as\n",
      "either supervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "in unsupervised learning, there is no outcome measure, and the goal is to\n",
      "describe the associations and patterns among a set of input measures.\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Data mining and machine learning, Statistical modeling and inference, Use of computational and information technology, Application of statistics in various fields such as medicine, biology, finance, and marketing, Supervised and unsupervised learning methods, Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting, Prominent researchers and their contributions in the field of statistics and data mining\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "xii Preface to the First Edition\n",
      "This book is our attempt to bring together many of the important new\n",
      "ideas in learning, and explain them in a statistical framework. While some\n",
      "mathematical details are needed, we emphasize the methods and their con-\n",
      "ceptual underpinnings rather than their theoretical properties. As a result,\n",
      "we hope that this book will appeal not just to statisticians but also to\n",
      "researchers and practitioners in a wide variety of Ô¨Åelds.\n",
      "Just as we have learned a great deal from researchers outside of the Ô¨Åeld\n",
      "of statistics, our statistical viewpoint may help others to better understa nd\n",
      "diÔ¨Äerent aspects of learning:\n",
      "There is no true interpretation of anything; interpretatio n is a\n",
      "vehicle in the service of human comprehension. The value of\n",
      "interpretation is in enabling others to fruitfully think ab out an\n",
      "idea.\n",
      "‚ÄìAndreas Buja\n",
      "We would like to acknowledge the contribution of many people to the\n",
      "conception and completion of this book. David Andrews, Leo Breiman,\n",
      "Andreas Buja, John Chambers, Bradley Efron, GeoÔ¨Ärey Hinton, Werner\n",
      "Stuetzle, and John Tukey have greatly inÔ¨Çuenced our careers. Balasub-\n",
      "ramanian Narasimhan gave us advice and help on many computational\n",
      "problems, and maintained an excellent computing environment. Shin-Ho\n",
      "Bang helped in the production of a number of the Ô¨Ågures. Lee Wilkinson\n",
      "gave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Ma ya\n",
      "Gupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bog-\n",
      "dan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu\n",
      "Zhu, two reviewers and many students read parts of the manuscript and\n",
      "oÔ¨Äered helpful suggestions. John Kimmel was supportive, patient and help-\n",
      "ful at every phase; MaryAnn Brickner and Frank Ganz headed a superb\n",
      "production team at Springer. Trevor Hastie would like to thank the statis-\n",
      "tics department at the University of Cape Town for their hospitality during\n",
      "the Ô¨Ånal stages of this book. We gratefully acknowledge NSF and NIH for\n",
      "their support of this work. Finally, we would like to thank our families and\n",
      "our parents for their love and support.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "May 2001\n",
      "The quiet statisticians have changed our world; not by disco v-\n",
      "ering new facts or technical developments, but by changing t he\n",
      "ways that we reason, experiment and form our opinions ....\n",
      "‚ÄìIan Hacking\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Data mining and machine learning, Statistical modeling and inference, Use of computational and information technology, Application of statistics in various fields such as medicine, biology, finance, and marketing, Supervised and unsupervised learning methods, Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting, Prominent researchers and their contributions in the field of statistics and data mining\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "This is page xiii\n",
      "Printer: Opaque this\n",
      "Contents\n",
      "Preface to the Second Edition vii\n",
      "Preface to the First Edition xi\n",
      "1 Introduction 1\n",
      "2 Overview of Supervised Learning 9\n",
      "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\n",
      "2.3 Two Simple Approaches to Prediction:\n",
      "Least Squares and Nearest Neighbors . . . . . . . . . . . 11\n",
      "2.3.1 Linear Models and Least Squares . . . . . . . . 11\n",
      "2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\n",
      "2.3.3 From Least Squares to Nearest Neighbors . . . . 16\n",
      "2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\n",
      "2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\n",
      "2.6 Statistical Models, Supervised Learning\n",
      "and Function Approximation . . . . . . . . . . . . . . . . 28\n",
      "2.6.1 A Statistical Model\n",
      "for the Joint Distribution Pr( X,Y) . . . . . . . 28\n",
      "2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\n",
      "2.6.3 Function Approximation . . . . . . . . . . . . . 29\n",
      "2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\n",
      "2.7.1 DiÔ¨Éculty of the Problem . . . . . . . . . . . . . 32\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Data mining and machine learning, Statistical modeling and inference, Use of computational and information technology, Application of statistics in various fields such as medicine, biology, finance, and marketing, Supervised and unsupervised learning methods, Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting, Prominent researchers and their contributions in the field of statistics and data mining\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "xiv Contents\n",
      "2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\n",
      "2.8.1 Roughness Penalty and Bayesian Methods . . . 34\n",
      "2.8.2 Kernel Methods and Local Regression . . . . . . 34\n",
      "2.8.3 Basis Functions and Dictionary Methods . . . . 35\n",
      "2.9 Model Selection and the Bias‚ÄìVariance TradeoÔ¨Ä . . . . . 37\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "3 Linear Methods for Regression 43\n",
      "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\n",
      "3.2 Linear Regression Models and Least Squares . . . . . . . 44\n",
      "3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\n",
      "3.2.2 The Gauss‚ÄìMarkov Theorem . . . . . . . . . . . 51\n",
      "3.2.3 Multiple Regression\n",
      "from Simple Univariate Regression . . . . . . . . 52\n",
      "3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\n",
      "3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\n",
      "3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\n",
      "3.3.2 Forward- and Backward-Stepwise Selection . . . 58\n",
      "3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\n",
      "3.3.4 Prostate Cancer Data Example (Continued) . . 61\n",
      "3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\n",
      "3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\n",
      "3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\n",
      "3.4.3 Discussion: Subset Selection, Ridge Regression\n",
      "and the Lasso . . . . . . . . . . . . . . . . . . . 69\n",
      "3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\n",
      "3.5 Methods Using Derived Input Directions . . . . . . . . . 79\n",
      "3.5.1 Principal Components Regression . . . . . . . . 79\n",
      "3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\n",
      "3.6 Discussion: A Comparison of the Selection\n",
      "and Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\n",
      "3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\n",
      "3.8 More on the Lasso and Related Path Algorithms . . . . . 86\n",
      "3.8.1 Incremental Forward Stagewise Regression . . . 86\n",
      "3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\n",
      "3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\n",
      "3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\n",
      "3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\n",
      "3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\n",
      "3.9 Computational Considerations . . . . . . . . . . . . . . . 93\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Data mining and machine learning, Statistical modeling and inference, Use of computational and information technology, Application of statistics in various fields such as medicine, biology, finance, and marketing, Supervised and unsupervised learning methods, Specific methods and algorithms in data analysis such as neural networks, support vector machines, and boosting, Prominent researchers and their contributions in the field of statistics and data mining\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "Contents xv\n",
      "4 Linear Methods for ClassiÔ¨Åcation 101\n",
      "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\n",
      "4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\n",
      "4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\n",
      "4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\n",
      "4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\n",
      "4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\n",
      "4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\n",
      "4.4.2 Example: South African Heart Disease . . . . . 122\n",
      "4.4.3 Quadratic Approximations and Inference . . . . 124\n",
      "4.4.4 L1Regularized Logistic Regression . . . . . . . . 125\n",
      "4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\n",
      "4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\n",
      "4.5.1 Rosenblatt‚Äôs Perceptron Learning Algorithm . . 130\n",
      "4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "5 Basis Expansions and Regularization 139\n",
      "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\n",
      "5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\n",
      "5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\n",
      "5.2.2 Example: South African Heart Disease (Continued)146\n",
      "5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\n",
      "5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\n",
      "5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\n",
      "5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\n",
      "5.5 Automatic Selection of the Smoothing Parameters . . . . 156\n",
      "5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\n",
      "5.5.2 The Bias‚ÄìVariance TradeoÔ¨Ä . . . . . . . . . . . . 158\n",
      "5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\n",
      "5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\n",
      "5.8.1 Spaces of Functions Generated by Kernels . . . 168\n",
      "5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\n",
      "5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\n",
      "5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\n",
      "5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Appendix: Computational Considerations for Splines . . . . . . 186\n",
      "Appendix: B-splines . . . . . . . . . . . . . . . . . . . . . 186\n",
      "Appendix: Computations for Smoothing Splines . . . . . 189\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Linear Methods for Classification, Basis Expansions and Regularization\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "xvi Contents\n",
      "6 Kernel Smoothing Methods 191\n",
      "6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\n",
      "6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\n",
      "6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\n",
      "6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\n",
      "6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\n",
      "6.4 Structured Local Regression Models in IRp. . . . . . . . 201\n",
      "6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\n",
      "6.4.2 Structured Regression Functions . . . . . . . . . 203\n",
      "6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\n",
      "6.6 Kernel Density Estimation and ClassiÔ¨Åcation . . . . . . . 208\n",
      "6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\n",
      "6.6.2 Kernel Density ClassiÔ¨Åcation . . . . . . . . . . . 210\n",
      "6.6.3 The Naive Bayes ClassiÔ¨Åer . . . . . . . . . . . . 210\n",
      "6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\n",
      "6.8 Mixture Models for Density Estimation and ClassiÔ¨Åcation 214\n",
      "6.9 Computational Considerations . . . . . . . . . . . . . . . 216\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "7 Model Assessment and Selection 219\n",
      "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\n",
      "7.3 The Bias‚ÄìVariance Decomposition . . . . . . . . . . . . . 223\n",
      "7.3.1 Example: Bias‚ÄìVariance TradeoÔ¨Ä . . . . . . . . 226\n",
      "7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\n",
      "7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\n",
      "7.6 The EÔ¨Äective Number of Parameters . . . . . . . . . . . . 232\n",
      "7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\n",
      "7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\n",
      "7.9 Vapnik‚ÄìChervonenkis Dimension . . . . . . . . . . . . . . 237\n",
      "7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\n",
      "7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\n",
      "7.10.1 K-Fold Cross-Validation . . . . . . . . . . . . . 241\n",
      "7.10.2 The Wrong and Right Way\n",
      "to Do Cross-validation . . . . . . . . . . . . . . . 245\n",
      "7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\n",
      "7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\n",
      "7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\n",
      "7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "8 Model Inference and Averaging 261\n",
      "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Linear Methods for Classification, Basis Expansions and Regularization, Kernel Smoothing Methods, Model Assessment and Selection, Model Inference and Averaging\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "Contents xvii\n",
      "8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\n",
      "8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\n",
      "8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\n",
      "8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\n",
      "8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\n",
      "8.4 Relationship Between the Bootstrap\n",
      "and Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\n",
      "8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\n",
      "8.5.1 Two-Component Mixture Model . . . . . . . . . 272\n",
      "8.5.2 The EM Algorithm in General . . . . . . . . . . 276\n",
      "8.5.3 EM as a Maximization‚ÄìMaximization Procedure 277\n",
      "8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\n",
      "8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n",
      "8.7.1 Example: Trees with Simulated Data . . . . . . 283\n",
      "8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\n",
      "8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n",
      "9 Additive Models, Trees, and Related Methods 295\n",
      "9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\n",
      "9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\n",
      "9.1.2 Example: Additive Logistic Regression . . . . . 299\n",
      "9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\n",
      "9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\n",
      "9.2.3 ClassiÔ¨Åcation Trees . . . . . . . . . . . . . . . . 308\n",
      "9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\n",
      "9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\n",
      "9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\n",
      "9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 321\n",
      "9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\n",
      "9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\n",
      "9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\n",
      "9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\n",
      "9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\n",
      "9.7 Computational Considerations . . . . . . . . . . . . . . . 334\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "10 Boosting and Additive Trees 337\n",
      "10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Linear Methods for Classification, Basis Expansions and Regularization, Kernel Smoothing Methods, Model Assessment and Selection, Model Inference and Averaging, Bootstrap and Maximum Likelihood Methods, Bayesian Methods, Relationship Between the Bootstrap and Bayesian Inference, The EM Algorithm, MCMC for Sampling from the Posterior, Bagging, Model Averaging and Stacking, Stochastic Search: Bumping, Additive Models, Trees, and Related Methods, Generalized Additive Models, Tree-Based Methods, PRIM: Bump Hunting, MARS: Multivariate Adaptive Regression Splines, Hierarchical Mixtures of Experts, Missing Data, Computational Considerations, Boosting and Additive Trees\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "xviii Contents\n",
      "10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\n",
      "10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\n",
      "10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\n",
      "10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\n",
      "10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\n",
      "10.7 ‚ÄúOÔ¨Ä-the-Shelf‚Äù Procedures for Data Mining . . . . . . . . 350\n",
      "10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\n",
      "10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\n",
      "10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\n",
      "10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\n",
      "10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\n",
      "10.10.3 Implementations of Gradient Boosting . . . . . . 360\n",
      "10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\n",
      "10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\n",
      "10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\n",
      "10.13.1 Relative Importance of Predictor Variables . . . 367\n",
      "10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\n",
      "10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n",
      "10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\n",
      "10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\n",
      "10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n",
      "11 Neural Networks 389\n",
      "11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\n",
      "11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\n",
      "11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\n",
      "11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\n",
      "11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\n",
      "11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\n",
      "11.5.2 OverÔ¨Åtting . . . . . . . . . . . . . . . . . . . . . 398\n",
      "11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\n",
      "11.5.4 Number of Hidden Units and Layers . . . . . . . 400\n",
      "11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\n",
      "11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\n",
      "11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\n",
      "11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\n",
      "11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\n",
      "11.9.2 Performance Comparisons . . . . . . . . . . . . 412\n",
      "11.10 Computational Considerations . . . . . . . . . . . . . . . 414\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Boosting and Additive Trees, Neural Networks, Projection Pursuit Regression, Fitting Neural Networks, Some Issues in Training Neural Networks, Example: Simulated Data, Example: ZIP Code Data, Discussion, Bayesian Neural Nets and the NIPS 2003 Challenge, Computational Considerations, Bayes, Boosting and Bagging, Performance Comparisons\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "Contents xix\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "12 Support Vector Machines and\n",
      "Flexible Discriminants 417\n",
      "12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\n",
      "12.2 The Support Vector ClassiÔ¨Åer . . . . . . . . . . . . . . . . 417\n",
      "12.2.1 Computing the Support Vector ClassiÔ¨Åer . . . . 420\n",
      "12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\n",
      "12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\n",
      "12.3.1 Computing the SVM for ClassiÔ¨Åcation . . . . . . 423\n",
      "12.3.2 The SVM as a Penalization Method . . . . . . . 426\n",
      "12.3.3 Function Estimation and Reproducing Kernels . 428\n",
      "12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\n",
      "12.3.5 A Path Algorithm for the SVM ClassiÔ¨Åer . . . . 432\n",
      "12.3.6 Support Vector Machines for Regression . . . . . 434\n",
      "12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\n",
      "12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\n",
      "12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 438\n",
      "12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\n",
      "12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\n",
      "12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\n",
      "12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\n",
      "12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "13 Prototype Methods and Nearest-Neighbors 459\n",
      "13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2.1 K-means Clustering . . . . . . . . . . . . . . . . 460\n",
      "13.2.2 Learning Vector Quantization . . . . . . . . . . 462\n",
      "13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\n",
      "13.3 k-Nearest-Neighbor ClassiÔ¨Åers . . . . . . . . . . . . . . . 463\n",
      "13.3.1 Example: A Comparative Study . . . . . . . . . 468\n",
      "13.3.2 Example: k-Nearest-Neighbors\n",
      "and Image Scene ClassiÔ¨Åcation . . . . . . . . . . 470\n",
      "13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\n",
      "13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\n",
      "13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\n",
      "13.4.2 Global Dimension Reduction\n",
      "for Nearest-Neighbors . . . . . . . . . . . . . . . 479\n",
      "13.5 Computational Considerations . . . . . . . . . . . . . . . 480\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Boosting and Additive Trees, Neural Networks, Projection Pursuit Regression, Fitting Neural Networks, Some Issues in Training Neural Networks, Example: Simulated Data, Example: ZIP Code Data, Discussion, Bayesian Neural Nets and the NIPS 2003 Challenge, Computational Considerations, Bayes, Boosting and Bagging, Performance Comparisons\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "xx Contents\n",
      "14 Unsupervised Learning 485\n",
      "14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\n",
      "14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\n",
      "14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\n",
      "14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\n",
      "14.2.3 Example: Market Basket Analysis . . . . . . . . 492\n",
      "14.2.4 Unsupervised as Supervised Learning . . . . . . 495\n",
      "14.2.5 Generalized Association Rules . . . . . . . . . . 497\n",
      "14.2.6 Choice of Supervised Learning Method . . . . . 499\n",
      "14.2.7 Example: Market Basket Analysis (Continued) . 499\n",
      "14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\n",
      "14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\n",
      "14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\n",
      "14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\n",
      "14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\n",
      "14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\n",
      "14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . . 509\n",
      "14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\n",
      "14.3.8 Example: Human Tumor Microarray Data . . . 512\n",
      "14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\n",
      "14.3.10 K-medoids . . . . . . . . . . . . . . . . . . . . . 515\n",
      "14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\n",
      "14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\n",
      "14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\n",
      "14.5 Principal Components, Curves and Surfaces . . . . . . . . 534\n",
      "14.5.1 Principal Components . . . . . . . . . . . . . . . 534\n",
      "14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\n",
      "14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\n",
      "14.5.4 Kernel Principal Components . . . . . . . . . . . 547\n",
      "14.5.5 Sparse Principal Components . . . . . . . . . . . 550\n",
      "14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\n",
      "14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\n",
      "14.7 Independent Component Analysis\n",
      "and Exploratory Projection Pursuit . . . . . . . . . . . . 557\n",
      "14.7.1 Latent Variables and Factor Analysis . . . . . . 558\n",
      "14.7.2 Independent Component Analysis . . . . . . . . 560\n",
      "14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\n",
      "14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\n",
      "14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\n",
      "14.9 Nonlinear Dimension Reduction\n",
      "and Local Multidimensional Scaling . . . . . . . . . . . . 572\n",
      "14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Boosting and Additive Trees, Neural Networks, Projection Pursuit Regression, Fitting Neural Networks, Some Issues in Training Neural Networks, Example: Simulated Data, Example: ZIP Code Data, Discussion, Bayesian Neural Nets and the NIPS 2003 Challenge, Computational Considerations, Bayes, Boosting and Bagging, Performance Comparisons\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "Contents xxi\n",
      "15 Random Forests 587\n",
      "15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\n",
      "15.2 DeÔ¨Ånition of Random Forests . . . . . . . . . . . . . . . . 587\n",
      "15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\n",
      "15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\n",
      "15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\n",
      "15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\n",
      "15.3.4 Random Forests and OverÔ¨Åtting . . . . . . . . . 596\n",
      "15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\n",
      "15.4.1 Variance and the De-Correlation EÔ¨Äect . . . . . 597\n",
      "15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\n",
      "15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\n",
      "16 Ensemble Learning 605\n",
      "16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\n",
      "16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\n",
      "16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\n",
      "16.2.2 The ‚ÄúBet on Sparsity‚Äù Principle . . . . . . . . . 610\n",
      "16.2.3 Regularization Paths, Over-Ô¨Åtting and Margins . 613\n",
      "16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\n",
      "16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\n",
      "16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n",
      "17 Undirected Graphical Models 625\n",
      "17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\n",
      "17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\n",
      "17.3 Undirected Graphical Models for Continuous Variables . 630\n",
      "17.3.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 631\n",
      "17.3.2 Estimation of the Graph Structure . . . . . . . . 635\n",
      "17.4 Undirected Graphical Models for Discrete Variables . . . 638\n",
      "17.4.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 639\n",
      "17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\n",
      "17.4.3 Estimation of the Graph Structure . . . . . . . . 642\n",
      "17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\n",
      "18 High-Dimensional Problems: p‚â´N 649\n",
      "18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Boosting and Additive Trees, Neural Networks, Projection Pursuit Regression, Fitting Neural Networks, Some Issues in Training Neural Networks, Example: Simulated Data, Example: ZIP Code Data, Discussion, Bayesian Neural Nets and the NIPS 2003 Challenge, Computational Considerations, Bayes, Boosting and Bagging, Performance Comparisons, Random Forests, Ensemble Learning, Undirected Graphical Models, High-Dimensional Problems: p‚â´N\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "xxii Contents\n",
      "18.2 Diagonal Linear Discriminant Analysis\n",
      "and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\n",
      "18.3 Linear ClassiÔ¨Åers with Quadratic Regularization . . . . . 654\n",
      "18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\n",
      "18.3.2 Logistic Regression\n",
      "with Quadratic Regularization . . . . . . . . . . 657\n",
      "18.3.3 The Support Vector ClassiÔ¨Åer . . . . . . . . . . 657\n",
      "18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\n",
      "18.3.5 Computational Shortcuts When p‚â´N. . . . . 659\n",
      "18.4 Linear ClassiÔ¨Åers with L1Regularization . . . . . . . . . 661\n",
      "18.4.1 Application of Lasso\n",
      "to Protein Mass Spectroscopy . . . . . . . . . . 664\n",
      "18.4.2 The Fused Lasso for Functional Data . . . . . . 666\n",
      "18.5 ClassiÔ¨Åcation When Features are Unavailable . . . . . . . 668\n",
      "18.5.1 Example: String Kernels\n",
      "and Protein ClassiÔ¨Åcation . . . . . . . . . . . . . 668\n",
      "18.5.2 ClassiÔ¨Åcation and Other Models Using\n",
      "Inner-Product Kernels and Pairwise Distances . 670\n",
      "18.5.3 Example: Abstracts ClassiÔ¨Åcation . . . . . . . . 672\n",
      "18.6 High-Dimensional Regression:\n",
      "Supervised Principal Components . . . . . . . . . . . . . 674\n",
      "18.6.1 Connection to Latent-Variable Modeling . . . . 678\n",
      "18.6.2 Relationship with Partial Least Squares . . . . . 680\n",
      "18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\n",
      "18.7 Feature Assessment and the Multiple-Testing Problem . . 683\n",
      "18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\n",
      "18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\n",
      "18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\n",
      "18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n",
      "References 699\n",
      "Author Index 729\n",
      "Index 737\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Your job is to extract the most relevant themes\n",
      "We have provided an existing list of themes up to a certain point: Boosting and Additive Trees, Neural Networks, Projection Pursuit Regression, Fitting Neural Networks, Some Issues in Training Neural Networks, Example: Simulated Data, Example: ZIP Code Data, Discussion, Bayesian Neural Nets and the NIPS 2003 Challenge, Computational Considerations, Bayes, Boosting and Bagging, Performance Comparisons, Random Forests, Ensemble Learning, Undirected Graphical Models, High-Dimensional Problems: p‚â´N, Diagonal Linear Discriminant Analysis, Nearest Shrunken Centroids, Linear Classifiers with Quadratic Regularization, Regularized Discriminant Analysis, Logistic Regression with Quadratic Regularization, The Support Vector Classifier, Feature Selection, Computational Shortcuts When p‚â´N, Linear Classifiers with L1 Regularization, Application of Lasso to Protein Mass Spectroscopy, The Fused Lasso for Functional Data, Classification When Features are Unavailable, Example: String Kernels and Protein Classification, Classification and Other Models Using Inner-Product Kernels and Pairwise Distances, Example: Abstracts Classification, High-Dimensional Regression: Supervised Principal Components, Connection to Latent-Variable Modeling, Relationship with Partial Least Squares, Pre-Conditioning for Feature Selection, Feature Assessment and the Multiple-Testing Problem, The False Discovery Rate, Asymmetric Cutpoints and the SAM Procedure, A Bayesian Interpretation of the FDR, Bibliographic Notes, Exercises, References, Author Index, Index\n",
      "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
      "------------\n",
      "This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, Ô¨Ånance and\n",
      "industry. Here are some examples of learning problems:\n",
      "‚Ä¢Predict whether a patient, hospitalized due to a heart attack, will\n",
      "have a second heart attack. The prediction is to be based on demo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "‚Ä¢Predict the price of a stock in 6 months from now, on the basis of\n",
      "company performance measures and economic data.\n",
      "‚Ä¢Identify the numbers in a handwritten ZIP code, from a digitized\n",
      "image.\n",
      "‚Ä¢Estimate the amount of glucose in the blood of a diabetic person,\n",
      "from the infrared absorption spectrum of that person‚Äôs blood.\n",
      "‚Ä¢Identify the risk factors for prostate cancer, based on clinical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the Ô¨Åelds of statistics, data\n",
      "mining and artiÔ¨Åcial intelligence, intersecting with areas of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a stock price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have a training set of data, in which we observe the outcome and feature\n",
      "------------\n",
      "Given the new context, refine the original list\n",
      "If the context isn't useful, return the original list and ONLY the original list.\n",
      "Return that list as a comma separated list.\n",
      "\n",
      "LIST:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Boosting and Additive Trees, Neural Networks, Projection Pursuit Regression, Fitting Neural Networks, Some Issues in Training Neural Networks, Example: Simulated Data, Example: ZIP Code Data, Bayesian Neural Nets and the NIPS 2003 Challenge, Computational Considerations, Bayes, Boosting and Bagging, Performance Comparisons, Random Forests, Ensemble Learning, Undirected Graphical Models, High-Dimensional Problems: p‚â´N, Diagonal Linear Discriminant Analysis, Nearest Shrunken Centroids, Linear Classifiers with Quadratic Regularization, Regularized Discriminant Analysis, Logistic Regression with Quadratic Regularization, The Support Vector Classifier, Feature Selection, Computational Shortcuts When p‚â´N, Linear Classifiers with L1 Regularization, Application of Lasso to Protein Mass Spectroscopy, The Fused Lasso for Functional Data, Classification When Features are Unavailable, Example: String Kernels and Protein Classification, Classification and Other Models Using Inner-Product Kernels and Pairwise Distances, Example: Abstracts Classification, High-Dimensional Regression: Supervised Principal Components, Connection to Latent-Variable Modeling, Relationship with Partial Least Squares, Pre-Conditioning for Feature Selection, Feature Assessment and the Multiple-Testing Problem, The False Discovery Rate, Asymmetric Cutpoints and the SAM Procedure, A Bayesian Interpretation of the FDR, Bibliographic Notes, Exercises, References, Author Index, Index'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Explosion in computation and information technology, Vast amounts of data in various fields, Tools and techniques in statistics, data mining, machine learning, and bioinformatics, Common conceptual framework for understanding these areas, Emphasis on concepts rather than mathematics, Examples and color graphics provided, Broad coverage, including supervised and unsupervised learning, Introduction of new topics in the second edition, Prominent researchers and authors in the field, Statistical modeling software and environment (R/S-PLUS), Introduction of various data mining tools and techniques'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_template = \"\"\"\n",
    "Extract the most relevant themes from the following:\n",
    "\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "\n",
    "THEMES:\"\"\"\n",
    "\n",
    "refine_template = \"\"\"\n",
    "Your job is to extract the most relevant themes\n",
    "We have provided an existing list of themes up to a certain point: {existing_answer}\n",
    "We have the opportunity to refine the existing list(only if needed) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "Given the new context, refine the original list\n",
    "If the context isn't useful, return the original list and ONLY the original list.\n",
    "Return that list as a comma separated list.\n",
    "\n",
    "LIST:\"\"\"\n",
    "\n",
    "initial_prompt = PromptTemplate.from_template(initial_template)\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    question_prompt=initial_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(sl_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73bba574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {foo}\")\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd8a0b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Why did the math book look sad during the test? \\nBecause it had too many problems!')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'foo':'test'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbd68c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
