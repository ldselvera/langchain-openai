{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "470bfced-d9b9-4543-9955-721ad8cb8e9a",
   "metadata": {},
   "source": [
    "# Indexing a GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9a386a-e7f4-4178-bf6e-a7a61b22b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install GitPython\n",
    "# %pip install langchain openai\n",
    "# %pip install -U langchain-community\n",
    "# %pip install tiktoken\n",
    "# %pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67db38dc-5609-4667-9f0d-c2d920741165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GitLoader\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "    repo_path=\"./data/repo/\",\n",
    "    file_filter=lambda file_path: file_path.endswith(\".py\"),\n",
    "    branch=\"master\",\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "133ae40f-9718-4f38-872f-41dae0dc5c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "python scripts/release_branch.py anthropic bagatur\n",
      "\"\"\"\n",
      "\n",
      "import glob\n",
      "import tomllib\n",
      "import toml\n",
      "import subprocess\n",
      "import sys\n",
      "\n",
      "\n",
      "def main(*args):\n",
      "    pkg = args[1]\n",
      "    if len(args) >= 2:\n",
      "        user = args[2]\n",
      "    else:\n",
      "        user = \"auto\"\n",
      "    for path in glob.glob(\"./libs/**/pyproject.toml\", recursive=True):\n",
      "        if pkg in path:\n",
      "            break\n",
      "\n",
      "    with open(path, \"rb\") as f:\n",
      "        pyproject = tomllib.load(f)\n",
      "    major, minor, patch = pyproject[\"tool\"][\"poetry\"][\"version\"].split(\".\")\n",
      "    patch = str(int(patch) + 1)\n",
      "    bumped = \".\".join((major, minor, patch))\n",
      "    pyproject[\"tool\"][\"poetry\"][\"version\"] = bumped\n",
      "    with open(path, \"w\") as f:\n",
      "        toml.dump(pyproject, f)\n",
      "\n",
      "    branch = f\"{user}/{pkg}_{bumped.replace('.', '_')}\"\n",
      "    print(\n",
      "        subprocess.run(\n",
      "            f\"git checkout -b {branch}; git commit -am '{pkg}[patch]: Release {bumped}'; git push -u origin {branch}\",\n",
      "            shell=True,\n",
      "            capture_output=True,\n",
      "            text=True,\n",
      "        )\n",
      "    )\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main(*sys.argv)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a8f4e81-aa47-4c9c-acc7-f8df4a3bcf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4931"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f80ed1b-2b26-4aa3-99ae-7d9e41d35180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, \n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "documents = python_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44912e52-46e5-48dd-87f4-0ead2b299290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'scripts\\\\release_branch.py', 'file_path': 'scripts\\\\release_branch.py', 'file_name': 'release_branch.py', 'file_type': '.py'}, page_content='\"\"\"\\npython scripts/release_branch.py anthropic bagatur\\n\"\"\"\\n\\nimport glob\\nimport tomllib\\nimport toml\\nimport subprocess\\nimport sys')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8d65ff-6623-4358-b9d7-45b7cd94d150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27820"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36517f77-a86c-474d-bb50-06864fb01cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(show_progress_bar=True, disallowed_special=())\n",
    "llm = ChatOpenAI()\n",
    "handler = StdOutCallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b36d3db-b189-4e5d-9399-ced9e0c0a639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28/28 [03:17<00:00,  7.07s/it]\n",
      "C:\\Users\\Owner\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "chain = prompt | fake_llm\n",
      "\n",
      "def _load_stuff_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    prompt: BasePromptTemplate = stuff_prompt.PROMPT,\n",
      "    document_prompt: BasePromptTemplate = stuff_prompt.EXAMPLE_PROMPT,\n",
      "    document_variable_name: str = \"summaries\",\n",
      "    verbose: Optional[bool] = None,\n",
      "    **kwargs: Any,\n",
      ") -> StuffDocumentsChain:\n",
      "    llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)  # type: ignore[arg-type]\n",
      "    return StuffDocumentsChain(\n",
      "        llm_chain=llm_chain,\n",
      "        document_variable_name=document_variable_name,\n",
      "        document_prompt=document_prompt,\n",
      "        verbose=verbose,  # type: ignore[arg-type]\n",
      "        **kwargs,\n",
      "    )\n",
      "\n",
      "return StuffDocumentsChain(\n",
      "        llm_chain=llm_chain, document_prompt=document_prompt, **config\n",
      "    )\n",
      "\n",
      "def _load_stuff_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    prompt: Optional[BasePromptTemplate] = None,\n",
      "    document_variable_name: str = \"context\",\n",
      "    verbose: Optional[bool] = None,\n",
      "    callback_manager: Optional[BaseCallbackManager] = None,\n",
      "    callbacks: Callbacks = None,\n",
      "    **kwargs: Any,\n",
      ") -> StuffDocumentsChain:\n",
      "    _prompt = prompt or stuff_prompt.PROMPT_SELECTOR.get_prompt(llm)\n",
      "    llm_chain = LLMChain(\n",
      "        llm=llm,\n",
      "        prompt=_prompt,\n",
      "        verbose=verbose,  # type: ignore[arg-type]\n",
      "        callback_manager=callback_manager,\n",
      "        callbacks=callbacks,\n",
      "    )\n",
      "    # TODO: document prompt\n",
      "    return StuffDocumentsChain(\n",
      "        llm_chain=llm_chain,\n",
      "        document_variable_name=document_variable_name,\n",
      "        verbose=verbose,  # type: ignore[arg-type]\n",
      "        callback_manager=callback_manager,\n",
      "        callbacks=callbacks,\n",
      "        **kwargs,\n",
      "    )\n",
      "Human: What is a stuff chain?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A stuff chain is a concept within the code that involves creating a chain of processes related to handling and processing documents or information. It likely involves using a language model to generate or interact with text in some way. The specifics of what a stuff chain does would depend on the implementation details within the codebase.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "index = FAISS.from_documents(documents, embeddings)\n",
    "retriever = index.as_retriever()\n",
    "\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "query = \"What is a stuff chain?\"\n",
    "\n",
    "qa.run(query, callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02425ee6-81aa-404b-b92e-2fe40237ac4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "class MapReduceChain(Chain):\n",
      "    \"\"\"Map-reduce chain.\"\"\"\n",
      "\n",
      "    combine_documents_chain: BaseCombineDocumentsChain\n",
      "    \"\"\"Chain to use to combine documents.\"\"\"\n",
      "    text_splitter: TextSplitter\n",
      "    \"\"\"Text splitter to use.\"\"\"\n",
      "    input_key: str = \"input_text\"  #: :meta private:\n",
      "    output_key: str = \"output_text\"  #: :meta private:\n",
      "\n",
      "class MapReduceDocumentsChain(BaseCombineDocumentsChain):\n",
      "    \"\"\"Combining documents by mapping a chain over them, then combining results.\n",
      "\n",
      "    We first call `llm_chain` on each document individually, passing in the\n",
      "    `page_content` and any other kwargs. This is the `map` step.\n",
      "\n",
      "    We then process the results of that `map` step in a `reduce` step. This should\n",
      "    likely be a ReduceDocumentsChain.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain.chains import (\n",
      "                StuffDocumentsChain,\n",
      "                LLMChain,\n",
      "                ReduceDocumentsChain,\n",
      "                MapReduceDocumentsChain,\n",
      "            )\n",
      "            from langchain_core.prompts import PromptTemplate\n",
      "            from langchain_community.llms import OpenAI\n",
      "\n",
      "chain = prompt | fake_llm\n",
      "\n",
      "verbose=verbose,  # type: ignore[arg-type]\n",
      "            callback_manager=callback_manager,\n",
      "        )\n",
      "    reduce_documents_chain = ReduceDocumentsChain(  # type: ignore[misc]\n",
      "        combine_documents_chain=combine_documents_chain,\n",
      "        collapse_documents_chain=collapse_chain,\n",
      "        token_max=token_max,\n",
      "        verbose=verbose,\n",
      "    )\n",
      "    return MapReduceDocumentsChain(\n",
      "        llm_chain=map_chain,\n",
      "        document_variable_name=map_reduce_document_variable_name,\n",
      "        reduce_documents_chain=reduce_documents_chain,\n",
      "        verbose=verbose,  # type: ignore[arg-type]\n",
      "        callback_manager=callback_manager,\n",
      "        callbacks=callbacks,\n",
      "        **kwargs,\n",
      "    )\n",
      "\n",
      "from langchain.chains import ReduceDocumentsChain\n",
      "from langchain.chains.base import Chain\n",
      "from langchain.chains.combine_documents.base import BaseCombineDocumentsChain\n",
      "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
      "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
      "from langchain.chains.llm import LLMChain\n",
      "from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain\n",
      "from langchain.chains.qa_with_sources.map_reduce_prompt import (\n",
      "    COMBINE_PROMPT,\n",
      "    EXAMPLE_PROMPT,\n",
      "    QUESTION_PROMPT,\n",
      ")\n",
      "\n",
      "chain: Runnable = input_map | router\n",
      "    if sys.version_info >= (3, 9):\n",
      "        assert dumps(chain, pretty=True) == snapshot\n",
      "\n",
      "    result = chain.invoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n",
      "    assert result == \"4\"\n",
      "\n",
      "    result2 = chain.batch(\n",
      "        [{\"key\": \"math\", \"question\": \"2 + 2\"}, {\"key\": \"english\", \"question\": \"2 + 2\"}]\n",
      "    )\n",
      "    assert result2 == [\"4\", \"2\"]\n",
      "\n",
      "    result = await chain.ainvoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n",
      "    assert result == \"4\"\n",
      "\n",
      "    result2 = await chain.abatch(\n",
      "        [{\"key\": \"math\", \"question\": \"2 + 2\"}, {\"key\": \"english\", \"question\": \"2 + 2\"}]\n",
      "    )\n",
      "    assert result2 == [\"4\", \"2\"]\n",
      "\n",
      "from rag_fusion.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    original_query = \"impact of climate change\"\n",
      "    print(chain.invoke(original_query))\n",
      "\n",
      ")\n",
      "    else:\n",
      "        _collapse_llm = collapse_llm or llm\n",
      "        collapse_chain = StuffDocumentsChain(\n",
      "            llm_chain=LLMChain(\n",
      "                llm=_collapse_llm,\n",
      "                prompt=collapse_prompt,\n",
      "                verbose=verbose,  # type: ignore[arg-type]\n",
      "                callbacks=callbacks,\n",
      "            ),\n",
      "            document_variable_name=combine_document_variable_name,\n",
      "        )\n",
      "    reduce_documents_chain = ReduceDocumentsChain(\n",
      "        combine_documents_chain=combine_documents_chain,\n",
      "        collapse_documents_chain=collapse_chain,\n",
      "        token_max=token_max,\n",
      "        verbose=verbose,  # type: ignore[arg-type]\n",
      "        callbacks=callbacks,\n",
      "        collapse_max_retries=collapse_max_retries,\n",
      "    )\n",
      "    return MapReduceDocumentsChain(\n",
      "        llm_chain=map_chain,\n",
      "        reduce_documents_chain=reduce_documents_chain,\n",
      "        document_variable_name=map_reduce_document_variable_name,\n",
      "        verbose=verbose,  # type: ignore[arg-type]\n",
      "        callbacks=callbacks,\n",
      "\n",
      "def _load_map_reduce_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    map_prompt: BasePromptTemplate = map_reduce_prompt.PROMPT,\n",
      "    combine_prompt: BasePromptTemplate = map_reduce_prompt.PROMPT,\n",
      "    combine_document_variable_name: str = \"text\",\n",
      "    map_reduce_document_variable_name: str = \"text\",\n",
      "    collapse_prompt: Optional[BasePromptTemplate] = None,\n",
      "    reduce_llm: Optional[BaseLanguageModel] = None,\n",
      "    collapse_llm: Optional[BaseLanguageModel] = None,\n",
      "    verbose: Optional[bool] = None,\n",
      "    token_max: int = 3000,\n",
      "    callbacks: Callbacks = None,\n",
      "    *,\n",
      "    collapse_max_retries: Optional[int] = None,\n",
      "    **kwargs: Any,\n",
      ") -> MapReduceDocumentsChain:\n",
      "    map_chain = LLMChain(\n",
      "        llm=llm,\n",
      "        prompt=map_prompt,\n",
      "        verbose=verbose,  # type: ignore[arg-type]\n",
      "        callbacks=callbacks,  # type: ignore[arg-type]\n",
      "    )\n",
      "    _reduce_llm = reduce_llm or llm\n",
      "    reduce_chain = LLMChain(\n",
      "        llm=_reduce_llm,\n",
      "        prompt=combine_prompt,\n",
      "\n",
      "def _load_map_reduce_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    question_prompt: BasePromptTemplate = map_reduce_prompt.QUESTION_PROMPT,\n",
      "    combine_prompt: BasePromptTemplate = map_reduce_prompt.COMBINE_PROMPT,\n",
      "    document_prompt: BasePromptTemplate = map_reduce_prompt.EXAMPLE_PROMPT,\n",
      "    combine_document_variable_name: str = \"summaries\",\n",
      "    map_reduce_document_variable_name: str = \"context\",\n",
      "    collapse_prompt: Optional[BasePromptTemplate] = None,\n",
      "    reduce_llm: Optional[BaseLanguageModel] = None,\n",
      "    collapse_llm: Optional[BaseLanguageModel] = None,\n",
      "    verbose: Optional[bool] = None,\n",
      "    token_max: int = 3000,\n",
      "    **kwargs: Any,\n",
      ") -> MapReduceDocumentsChain:\n",
      "    map_chain = LLMChain(llm=llm, prompt=question_prompt, verbose=verbose)  # type: ignore[arg-type]\n",
      "    _reduce_llm = reduce_llm or llm\n",
      "    reduce_chain = LLMChain(llm=_reduce_llm, prompt=combine_prompt, verbose=verbose)  # type: ignore[arg-type]\n",
      "    combine_documents_chain = StuffDocumentsChain(\n",
      "\n",
      "def _load_map_reduce_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    question_prompt: Optional[BasePromptTemplate] = None,\n",
      "    combine_prompt: Optional[BasePromptTemplate] = None,\n",
      "    combine_document_variable_name: str = \"summaries\",\n",
      "    map_reduce_document_variable_name: str = \"context\",\n",
      "    collapse_prompt: Optional[BasePromptTemplate] = None,\n",
      "    reduce_llm: Optional[BaseLanguageModel] = None,\n",
      "    collapse_llm: Optional[BaseLanguageModel] = None,\n",
      "    verbose: Optional[bool] = None,\n",
      "    callback_manager: Optional[BaseCallbackManager] = None,\n",
      "    callbacks: Callbacks = None,\n",
      "    token_max: int = 3000,\n",
      "    **kwargs: Any,\n",
      ") -> MapReduceDocumentsChain:\n",
      "    _question_prompt = (\n",
      "        question_prompt or map_reduce_prompt.QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
      "    )\n",
      "    _combine_prompt = (\n",
      "        combine_prompt or map_reduce_prompt.COMBINE_PROMPT_SELECTOR.get_prompt(llm)\n",
      "    )\n",
      "    map_chain = LLMChain(\n",
      "        llm=llm,\n",
      "        prompt=_question_prompt,\n",
      "\n",
      "def _load_map_reduce_documents_chain(\n",
      "    config: dict, **kwargs: Any\n",
      ") -> MapReduceDocumentsChain:\n",
      "    if \"llm_chain\" in config:\n",
      "        llm_chain_config = config.pop(\"llm_chain\")\n",
      "        llm_chain = load_chain_from_config(llm_chain_config, **kwargs)\n",
      "    elif \"llm_chain_path\" in config:\n",
      "        llm_chain = load_chain(config.pop(\"llm_chain_path\"), **kwargs)\n",
      "    else:\n",
      "        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\n",
      "\n",
      "    if not isinstance(llm_chain, LLMChain):\n",
      "        raise ValueError(f\"Expected LLMChain, got {llm_chain}\")\n",
      "\n",
      "    if \"reduce_documents_chain\" in config:\n",
      "        reduce_documents_chain = load_chain_from_config(\n",
      "            config.pop(\"reduce_documents_chain\"), **kwargs\n",
      "        )\n",
      "    elif \"reduce_documents_chain_path\" in config:\n",
      "        reduce_documents_chain = load_chain(\n",
      "            config.pop(\"reduce_documents_chain_path\"), **kwargs\n",
      "        )\n",
      "    else:\n",
      "        reduce_documents_chain = _load_reduce_documents_chain(config, **kwargs)\n",
      "\n",
      "@property\n",
      "    def _chain_type(self) -> str:\n",
      "        return \"map_reduce_documents_chain\"\n",
      "\n",
      "from langchain import chains\n",
      "\n",
      ")\n",
      "            chain = ReduceDocumentsChain(\n",
      "                combine_documents_chain=combine_documents_chain,\n",
      "                collapse_documents_chain=collapse_documents_chain,\n",
      "            )\n",
      "    \"\"\"\n",
      "\n",
      "from rag_aws_bedrock.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    query = \"What is this data about?\"\n",
      "\n",
      "    print(chain.invoke(query))\n",
      "\n",
      "from neo4j_advanced_rag.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    original_query = \"What is the plot of the Dune?\"\n",
      "    print(\n",
      "        chain.invoke(\n",
      "            {\"question\": original_query},\n",
      "            {\"configurable\": {\"strategy\": \"parent_strategy\"}},\n",
      "        )\n",
      "    )\n",
      "\n",
      "reduce_prompt = PromptTemplate.from_template(\n",
      "                \"Combine these summaries: {context}\"\n",
      "            )\n",
      "            reduce_llm_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
      "            combine_documents_chain = StuffDocumentsChain(\n",
      "                llm_chain=reduce_llm_chain,\n",
      "                document_prompt=document_prompt,\n",
      "                document_variable_name=document_variable_name\n",
      "            )\n",
      "            reduce_documents_chain = ReduceDocumentsChain(\n",
      "                combine_documents_chain=combine_documents_chain,\n",
      "            )\n",
      "            chain = MapReduceDocumentsChain(\n",
      "                llm_chain=llm_chain,\n",
      "                reduce_documents_chain=reduce_documents_chain,\n",
      "            )\n",
      "            # If we wanted to, we could also pass in collapse_documents_chain\n",
      "            # which is specifically aimed at collapsing documents BEFORE\n",
      "            # the final call.\n",
      "            prompt = PromptTemplate.from_template(\n",
      "\n",
      "combine_documents_chain=combine_results_chain\n",
      "        )\n",
      "        combine_documents_chain = MapReduceDocumentsChain(\n",
      "            llm_chain=llm_question_chain,\n",
      "            reduce_documents_chain=reduce_documents_chain,\n",
      "            document_variable_name=\"context\",\n",
      "        )\n",
      "        return cls(\n",
      "            combine_documents_chain=combine_documents_chain,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "from rewrite_retrieve_read.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    chain.invoke(\"man that sam bankman fried trial was crazy! what is langchain?\")\n",
      "Human: When should I use a map reduce chain?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You should use a map reduce chain when you want to combine documents by mapping a chain over them and then combining the results. This is useful when you need to process a large number of documents individually, map a chain over them to get intermediate results, and then combine those results using a reduce step.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs[\"distance_metric\"] = \"cos\"\n",
    "retriever.search_kwargs['fetch_k'] = 200\n",
    "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
    "retriever.search_kwargs['k'] = 20\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "query = \"When should I use a map reduce chain?\"\n",
    "\n",
    "qa.run(query, callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0485d47c-f888-4cb7-b63e-5ef567cb9e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "chain = prompt | fake_llm\n",
      "\n",
      "def _load_map_rerank_chain(\n",
      "    llm: BaseLanguageModel,\n",
      "    prompt: BasePromptTemplate = MAP_RERANK_PROMPT,\n",
      "    verbose: bool = False,\n",
      "    document_variable_name: str = \"context\",\n",
      "    rank_key: str = \"score\",\n",
      "    answer_key: str = \"answer\",\n",
      "    callback_manager: Optional[BaseCallbackManager] = None,\n",
      "    callbacks: Callbacks = None,\n",
      "    **kwargs: Any,\n",
      ") -> MapRerankDocumentsChain:\n",
      "    llm_chain = LLMChain(\n",
      "        llm=llm,\n",
      "        prompt=prompt,\n",
      "        verbose=verbose,\n",
      "        callback_manager=callback_manager,\n",
      "        callbacks=callbacks,\n",
      "    )\n",
      "    return MapRerankDocumentsChain(\n",
      "        llm_chain=llm_chain,\n",
      "        rank_key=rank_key,\n",
      "        answer_key=answer_key,\n",
      "        document_variable_name=document_variable_name,\n",
      "        verbose=verbose,\n",
      "        callback_manager=callback_manager,\n",
      "        **kwargs,\n",
      "    )\n",
      "\n",
      "def _load_map_rerank_documents_chain(\n",
      "    config: dict, **kwargs: Any\n",
      ") -> MapRerankDocumentsChain:\n",
      "    if \"llm_chain\" in config:\n",
      "        llm_chain_config = config.pop(\"llm_chain\")\n",
      "        llm_chain = load_chain_from_config(llm_chain_config, **kwargs)\n",
      "    elif \"llm_chain_path\" in config:\n",
      "        llm_chain = load_chain(config.pop(\"llm_chain_path\"), **kwargs)\n",
      "    else:\n",
      "        raise ValueError(\"One of `llm_chain` or `llm_chain_path` must be present.\")\n",
      "    return MapRerankDocumentsChain(llm_chain=llm_chain, **config)  # type: ignore[arg-type]\n",
      "\n",
      "output_parser=output_parser,\n",
      "            )\n",
      "            llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
      "            chain = MapRerankDocumentsChain(\n",
      "                llm_chain=llm_chain,\n",
      "                document_variable_name=document_variable_name,\n",
      "                rank_key=\"score\",\n",
      "                answer_key=\"answer\",\n",
      "            )\n",
      "    \"\"\"\n",
      "\n",
      "class MapRerankDocumentsChain(BaseCombineDocumentsChain):\n",
      "    \"\"\"Combining documents by mapping a chain over them, then reranking results.\n",
      "\n",
      "    This algorithm calls an LLMChain on each input document. The LLMChain is expected\n",
      "    to have an OutputParser that parses the result into both an answer (`answer_key`)\n",
      "    and a score (`rank_key`). The answer with the highest score is then returned.\n",
      "\n",
      "    Example:\n",
      "        .. code-block:: python\n",
      "\n",
      "            from langchain.chains import StuffDocumentsChain, LLMChain\n",
      "            from langchain_core.prompts import PromptTemplate\n",
      "            from langchain_community.llms import OpenAI\n",
      "            from langchain.output_parsers.regex import RegexParser\n",
      "\n",
      "from langchain import chains\n",
      "\n",
      "\"\"\"Tests for correct functioning of chains.\"\"\"\n",
      "\n",
      "from rag_aws_bedrock.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    query = \"What is this data about?\"\n",
      "\n",
      "    print(chain.invoke(query))\n",
      "\n",
      "from langchain import tools\n",
      "\n",
      "from neo4j_advanced_rag.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    original_query = \"What is the plot of the Dune?\"\n",
      "    print(\n",
      "        chain.invoke(\n",
      "            {\"question\": original_query},\n",
      "            {\"configurable\": {\"strategy\": \"parent_strategy\"}},\n",
      "        )\n",
      "    )\n",
      "\n",
      "class MapReduceChain(Chain):\n",
      "    \"\"\"Map-reduce chain.\"\"\"\n",
      "\n",
      "    combine_documents_chain: BaseCombineDocumentsChain\n",
      "    \"\"\"Chain to use to combine documents.\"\"\"\n",
      "    text_splitter: TextSplitter\n",
      "    \"\"\"Text splitter to use.\"\"\"\n",
      "    input_key: str = \"input_text\"  #: :meta private:\n",
      "    output_key: str = \"output_text\"  #: :meta private:\n",
      "\n",
      "from rag_pinecone_rerank.chain import chain\n",
      "\n",
      "__all__ = [\"chain\"]\n",
      "\n",
      "from rag_google_cloud_vertexai_search.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    query = \"Who is the CEO of Google Cloud?\"\n",
      "    print(chain.invoke(query))\n",
      "\n",
      "from neo4j_parent.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    original_query = \"What is the plot of the Dune?\"\n",
      "    print(chain.invoke(original_query))\n",
      "\n",
      "- Grading the accuracy of a response against ground truth answers: :class:`QAEvalChain <langchain.evaluation.qa.eval_chain.QAEvalChain>`\n",
      "- Comparing the output of two models: :class:`PairwiseStringEvalChain <langchain.evaluation.comparison.eval_chain.PairwiseStringEvalChain>` or :class:`LabeledPairwiseStringEvalChain <langchain.evaluation.comparison.eval_chain.LabeledPairwiseStringEvalChain>` when there is additionally a reference label.\n",
      "- Judging the efficacy of an agent's tool usage: :class:`TrajectoryEvalChain <langchain.evaluation.agents.trajectory_eval_chain.TrajectoryEvalChain>`\n",
      "- Checking whether an output complies with a set of criteria: :class:`CriteriaEvalChain <langchain.evaluation.criteria.eval_chain.CriteriaEvalChain>` or :class:`LabeledCriteriaEvalChain <langchain.evaluation.criteria.eval_chain.LabeledCriteriaEvalChain>` when there is additionally a reference label.\n",
      "\n",
      "\"Cogniswitch Tools\"\n",
      "\n",
      "chain: Runnable = input_map | router\n",
      "    if sys.version_info >= (3, 9):\n",
      "        assert dumps(chain, pretty=True) == snapshot\n",
      "\n",
      "    result = chain.invoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n",
      "    assert result == \"4\"\n",
      "\n",
      "    result2 = chain.batch(\n",
      "        [{\"key\": \"math\", \"question\": \"2 + 2\"}, {\"key\": \"english\", \"question\": \"2 + 2\"}]\n",
      "    )\n",
      "    assert result2 == [\"4\", \"2\"]\n",
      "\n",
      "    result = await chain.ainvoke({\"key\": \"math\", \"question\": \"2 + 2\"})\n",
      "    assert result == \"4\"\n",
      "\n",
      "    result2 = await chain.abatch(\n",
      "        [{\"key\": \"math\", \"question\": \"2 + 2\"}, {\"key\": \"english\", \"question\": \"2 + 2\"}]\n",
      "    )\n",
      "    assert result2 == [\"4\", \"2\"]\n",
      "\n",
      "from rewrite_retrieve_read.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    chain.invoke(\"man that sam bankman fried trial was crazy! what is langchain?\")\n",
      "\n",
      "from rag_fusion.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    original_query = \"impact of climate change\"\n",
      "    print(chain.invoke(original_query))\n",
      "\n",
      "from rag_aws_kendra.chain import chain\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    query = \"Does Kendra support table extraction?\"\n",
      "\n",
      "    print(chain.invoke(query))\n",
      "Human: When should I use a map rank chain?\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You should use a Map Rank Chain when you want to combine documents by mapping a chain over them and then reranking the results. This algorithm involves calling an LLMChain on each input document, where the LLMChain is expected to parse the result into an answer and a score. The answer with the highest score is then returned.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"When should I use a map rank chain?\"\n",
    "\n",
    "qa.run(query, callbacks=[handler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2de45-3bab-4ac8-aaac-afc36797aa7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
