{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0053445c-3375-496a-b6fd-4be1ca6ef1cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install pypdf\n",
    "# %pip install chromadb\n",
    "# %pip install --upgrade chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36ff6d87-d6f3-42a5-bac7-eaf6d7f53ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 0}, page_content='Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\\nnology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\\nThis major new edition features many topics not covered in the original, including graphical\\nmodels, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\\nStanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\\n›springer.comSTATISTICS\\nisbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\\nThe Elements of Statictical Learning\\nHastie • Tibshirani • Friedman\\nSecond Edition'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 1}, page_content='This is page v\\nPrinter: Opaque this\\nTo our parents:\\nValerie and Patrick Hastie\\nVera and Sami Tibshirani\\nFlorence and Harry Friedman\\nand to our families:\\nSamantha, Timothy, and Lynda\\nCharlie, Ryan, Julie, and Cheryl\\nMelanie, Dora, Monika, and Ildiko'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 2}, page_content='vi'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 3}, page_content='This is page vii\\nPrinter: Opaque this\\nPreface to the Second Edition\\nIn God we trust, all others bring data.\\n–William Edwards Deming (1900-1993)1\\nWe have been gratiﬁed by the popularity of the ﬁrst edition of The\\nElements of Statistical Learning. This, along with the fast pace of research\\nin the statistical learning ﬁeld, motivated us to update our book with a\\nsecond edition.\\nWe have added four new chapters and updated some of the existing\\nchapters. Because many readers are familiar with the layout of the ﬁrst\\nedition, we have tried to change it as little as possible. Here is a summary\\nof the main changes:\\n1On the Web, this quote has been widely attributed to both Demi ng and Robert W.\\nHayden; however Professor Hayden told us that he can claim no credit for this quote,\\nand ironically we could ﬁnd no “data” conﬁrming that Deming a ctually said this.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 4}, page_content='viii Preface to the Second Edition\\nChapter What’s new\\n1.Introduction\\n2.Overview of Supervised Learning\\n3.Linear Methods for Regression LAR algorithm and generalizations\\nof the lasso\\n4.Linear Methods for Classiﬁcation Lasso path for logistic regression\\n5.Basis Expansions and Regulariza-\\ntionAdditional illustrations of RKHS\\n6.Kernel Smoothing Methods\\n7.Model Assessment and Selection Strengths and pitfalls of cross-\\nvalidation\\n8.Model Inference and Averaging\\n9.Additive Models, Trees, and\\nRelated Methods\\n10.Boosting and Additive Trees New example from ecology; some\\nmaterial split oﬀ to Chapter 16.\\n11.Neural Networks Bayesian neural nets and the NIPS\\n2003 challenge\\n12.Support Vector Machines and\\nFlexible DiscriminantsPath algorithm for SVM classiﬁer\\n13. Prototype Methods and\\nNearest-Neighbors\\n14.Unsupervised Learning Spectral clustering, kernel PCA,\\nsparse PCA, non-negative matrix\\nfactorization archetypal analysis,\\nnonlinear dimension reduction,\\nGoogle page rank algorithm, a\\ndirect approach to ICA\\n15.Random Forests New\\n16.Ensemble Learning New\\n17.Undirected Graphical Models New\\n18.High-Dimensional Problems New\\nSome further notes:\\n•Our ﬁrst edition was unfriendly to colorblind readers; in particular,\\nwe tended to favor red/green contrasts which are particularly trou-\\nblesome. We have changed the color palette in this edition to a large\\nextent, replacing the above with an orange /bluecontrast.\\n•We have changed the name of Chapter 6 from “Kernel Methods” to\\n“Kernel Smoothing Methods”, to avoid confusion with the machine-\\nlearning kernel method that is discussed in the context of support vec-\\ntor machines (Chapter 11) and more generally in Chapters 5 and 14.\\n•In the ﬁrst edition, the discussion of error-rate estimation in Chap-\\nter 7 was sloppy, as we did not clearly diﬀerentiate the notions of\\nconditional error rates (conditional on the training set) and uncondi-\\ntional rates. We have ﬁxed this in the new edition.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 5}, page_content='Preface to the Second Edition ix\\n•Chapters 15 and 16 follow naturally from Chapter 10, and the chap-\\nters are probably best read in that order.\\n•In Chapter 17, we have not attempted a comprehensive treatment\\nof graphical models, and discuss only undirected models and some\\nnew methods for their estimation. Due to a lack of space, we have\\nspeciﬁcally omitted coverage of directed graphical models.\\n•Chapter 18 explores the “ p≫N” problem, which is learning in high-\\ndimensional feature spaces. These problems arise in many areas, in-\\ncluding genomic and proteomic studies, and document classiﬁcation.\\nWe thank the many readers who have found the (too numerous) errors in\\nthe ﬁrst edition. We apologize for those and have done our best to avoid er-\\nrors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry\\nWasserman for comments on some of the new chapters, and many Stanford\\ngraduate and post-doctoral students who oﬀered comments, in particular\\nMohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Maleki, Donal\\nMcMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\\nHui Zou. We thank John Kimmel for his patience in guiding us through this\\nnew edition. RT dedicates this edition to the memory of Anna McPhee.\\nTrevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nStanford, California\\nAugust 2008'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 6}, page_content='x Preface to the Second Edition'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 7}, page_content='This is page xi\\nPrinter: Opaque this\\nPreface to the First Edition\\nWe are drowning in information and starving for knowledge.\\n–Rutherford D. Roger\\nThe ﬁeld of Statistics is constantly challenged by the problems that science\\nand industry brings to its door. In the early days, these problems often came\\nfrom agricultural and industrial experiments and were relatively small in\\nscope. With the advent of computers and the information age, statistical\\nproblems have exploded both in size and complexity. Challenges in the\\nareas of data storage, organization and searching have led to the new ﬁeld\\nof “data mining”; statistical and computational problems in biology and\\nmedicine have created “bioinformatics.” Vast amounts of data are being\\ngenerated in many ﬁelds, and the statistician’s job is to make sense of it\\nall: to extract important patterns and trends, and understand “what the\\ndata says.” We call this learning from data .\\nThe challenges in learning from data have led to a revolution in the sta-\\ntistical sciences. Since computation plays such a key role, it is not surprising\\nthat much of this new development has been done by researchers in other\\nﬁelds such as computer science and engineering.\\nThe learning problems that we consider can be roughly categorized as\\neither supervised orunsupervised . In supervised learning, the goal is to pre-\\ndict the value of an outcome measure based on a number of input measures;\\nin unsupervised learning, there is no outcome measure, and the goal is to\\ndescribe the associations and patterns among a set of input measures.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 8}, page_content='xii Preface to the First Edition\\nThis book is our attempt to bring together many of the important new\\nideas in learning, and explain them in a statistical framework. While some\\nmathematical details are needed, we emphasize the methods and their con-\\nceptual underpinnings rather than their theoretical properties. As a result,\\nwe hope that this book will appeal not just to statisticians but also to\\nresearchers and practitioners in a wide variety of ﬁelds.\\nJust as we have learned a great deal from researchers outside of the ﬁeld\\nof statistics, our statistical viewpoint may help others to better understa nd\\ndiﬀerent aspects of learning:\\nThere is no true interpretation of anything; interpretatio n is a\\nvehicle in the service of human comprehension. The value of\\ninterpretation is in enabling others to fruitfully think ab out an\\nidea.\\n–Andreas Buja\\nWe would like to acknowledge the contribution of many people to the\\nconception and completion of this book. David Andrews, Leo Breiman,\\nAndreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton, Werner\\nStuetzle, and John Tukey have greatly inﬂuenced our careers. Balasub-\\nramanian Narasimhan gave us advice and help on many computational\\nproblems, and maintained an excellent computing environment. Shin-Ho\\nBang helped in the production of a number of the ﬁgures. Lee Wilkinson\\ngave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Ma ya\\nGupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bog-\\ndan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu\\nZhu, two reviewers and many students read parts of the manuscript and\\noﬀered helpful suggestions. John Kimmel was supportive, patient and help-\\nful at every phase; MaryAnn Brickner and Frank Ganz headed a superb\\nproduction team at Springer. Trevor Hastie would like to thank the statis-\\ntics department at the University of Cape Town for their hospitality during\\nthe ﬁnal stages of this book. We gratefully acknowledge NSF and NIH for\\ntheir support of this work. Finally, we would like to thank our families and\\nour parents for their love and support.\\nTrevor Hastie\\nRobert Tibshirani\\nJerome Friedman\\nStanford, California\\nMay 2001\\nThe quiet statisticians have changed our world; not by disco v-\\nering new facts or technical developments, but by changing t he\\nways that we reason, experiment and form our opinions ....\\n–Ian Hacking'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 9}, page_content='This is page xiii\\nPrinter: Opaque this\\nContents\\nPreface to the Second Edition vii\\nPreface to the First Edition xi\\n1 Introduction 1\\n2 Overview of Supervised Learning 9\\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\\n2.3 Two Simple Approaches to Prediction:\\nLeast Squares and Nearest Neighbors . . . . . . . . . . . 11\\n2.3.1 Linear Models and Least Squares . . . . . . . . 11\\n2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\\n2.3.3 From Least Squares to Nearest Neighbors . . . . 16\\n2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\\n2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\\n2.6 Statistical Models, Supervised Learning\\nand Function Approximation . . . . . . . . . . . . . . . . 28\\n2.6.1 A Statistical Model\\nfor the Joint Distribution Pr( X,Y) . . . . . . . 28\\n2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\\n2.6.3 Function Approximation . . . . . . . . . . . . . 29\\n2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\\n2.7.1 Diﬃculty of the Problem . . . . . . . . . . . . . 32'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 10}, page_content='xiv Contents\\n2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\\n2.8.1 Roughness Penalty and Bayesian Methods . . . 34\\n2.8.2 Kernel Methods and Local Regression . . . . . . 34\\n2.8.3 Basis Functions and Dictionary Methods . . . . 35\\n2.9 Model Selection and the Bias–Variance Tradeoﬀ . . . . . 37\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\\n3 Linear Methods for Regression 43\\n3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\\n3.2 Linear Regression Models and Least Squares . . . . . . . 44\\n3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\\n3.2.2 The Gauss–Markov Theorem . . . . . . . . . . . 51\\n3.2.3 Multiple Regression\\nfrom Simple Univariate Regression . . . . . . . . 52\\n3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\\n3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\\n3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\\n3.3.2 Forward- and Backward-Stepwise Selection . . . 58\\n3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\\n3.3.4 Prostate Cancer Data Example (Continued) . . 61\\n3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\\n3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\\n3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\\n3.4.3 Discussion: Subset Selection, Ridge Regression\\nand the Lasso . . . . . . . . . . . . . . . . . . . 69\\n3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\\n3.5 Methods Using Derived Input Directions . . . . . . . . . 79\\n3.5.1 Principal Components Regression . . . . . . . . 79\\n3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\\n3.6 Discussion: A Comparison of the Selection\\nand Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\\n3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\\n3.8 More on the Lasso and Related Path Algorithms . . . . . 86\\n3.8.1 Incremental Forward Stagewise Regression . . . 86\\n3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\\n3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\\n3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\\n3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\\n3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\\n3.9 Computational Considerations . . . . . . . . . . . . . . . 93\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 11}, page_content='Contents xv\\n4 Linear Methods for Classiﬁcation 101\\n4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\\n4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\\n4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\\n4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\\n4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\\n4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\\n4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\\n4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\\n4.4.2 Example: South African Heart Disease . . . . . 122\\n4.4.3 Quadratic Approximations and Inference . . . . 124\\n4.4.4 L1Regularized Logistic Regression . . . . . . . . 125\\n4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\\n4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\\n4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130\\n4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\\n5 Basis Expansions and Regularization 139\\n5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\\n5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\\n5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\\n5.2.2 Example: South African Heart Disease (Continued)146\\n5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\\n5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\\n5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\\n5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\\n5.5 Automatic Selection of the Smoothing Parameters . . . . 156\\n5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\\n5.5.2 The Bias–Variance Tradeoﬀ . . . . . . . . . . . . 158\\n5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\\n5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\\n5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\\n5.8.1 Spaces of Functions Generated by Kernels . . . 168\\n5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\\n5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\\n5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\\n5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\\nAppendix: Computational Considerations for Splines . . . . . . 186\\nAppendix: B-splines . . . . . . . . . . . . . . . . . . . . . 186\\nAppendix: Computations for Smoothing Splines . . . . . 189'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 12}, page_content='xvi Contents\\n6 Kernel Smoothing Methods 191\\n6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\\n6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\\n6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\\n6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\\n6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\\n6.4 Structured Local Regression Models in IRp. . . . . . . . 201\\n6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\\n6.4.2 Structured Regression Functions . . . . . . . . . 203\\n6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\\n6.6 Kernel Density Estimation and Classiﬁcation . . . . . . . 208\\n6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\\n6.6.2 Kernel Density Classiﬁcation . . . . . . . . . . . 210\\n6.6.3 The Naive Bayes Classiﬁer . . . . . . . . . . . . 210\\n6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\\n6.8 Mixture Models for Density Estimation and Classiﬁcation 214\\n6.9 Computational Considerations . . . . . . . . . . . . . . . 216\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\\n7 Model Assessment and Selection 219\\n7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\\n7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\\n7.3 The Bias–Variance Decomposition . . . . . . . . . . . . . 223\\n7.3.1 Example: Bias–Variance Tradeoﬀ . . . . . . . . 226\\n7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\\n7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\\n7.6 The Eﬀective Number of Parameters . . . . . . . . . . . . 232\\n7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\\n7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\\n7.9 Vapnik–Chervonenkis Dimension . . . . . . . . . . . . . . 237\\n7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\\n7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\\n7.10.1 K-Fold Cross-Validation . . . . . . . . . . . . . 241\\n7.10.2 The Wrong and Right Way\\nto Do Cross-validation . . . . . . . . . . . . . . . 245\\n7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\\n7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\\n7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\\n7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\\n8 Model Inference and Averaging 261\\n8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 13}, page_content='Contents xvii\\n8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\\n8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\\n8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\\n8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\\n8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\\n8.4 Relationship Between the Bootstrap\\nand Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\\n8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\\n8.5.1 Two-Component Mixture Model . . . . . . . . . 272\\n8.5.2 The EM Algorithm in General . . . . . . . . . . 276\\n8.5.3 EM as a Maximization–Maximization Procedure 277\\n8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\\n8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\\n8.7.1 Example: Trees with Simulated Data . . . . . . 283\\n8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\\n8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\\n9 Additive Models, Trees, and Related Methods 295\\n9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\\n9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\\n9.1.2 Example: Additive Logistic Regression . . . . . 299\\n9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\\n9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\\n9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\\n9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\\n9.2.3 Classiﬁcation Trees . . . . . . . . . . . . . . . . 308\\n9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\\n9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\\n9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\\n9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\\n9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 321\\n9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\\n9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\\n9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\\n9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\\n9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\\n9.7 Computational Considerations . . . . . . . . . . . . . . . 334\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\\n10 Boosting and Additive Trees 337\\n10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\\n10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 14}, page_content='xviii Contents\\n10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\\n10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\\n10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\\n10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\\n10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\\n10.7 “Oﬀ-the-Shelf” Procedures for Data Mining . . . . . . . . 350\\n10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\\n10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\\n10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\\n10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\\n10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\\n10.10.3 Implementations of Gradient Boosting . . . . . . 360\\n10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\\n10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\\n10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\\n10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\\n10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\\n10.13.1 Relative Importance of Predictor Variables . . . 367\\n10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\\n10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\\n10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\\n10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\\n10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\\n11 Neural Networks 389\\n11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\\n11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\\n11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\\n11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\\n11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\\n11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\\n11.5.2 Overﬁtting . . . . . . . . . . . . . . . . . . . . . 398\\n11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\\n11.5.4 Number of Hidden Units and Layers . . . . . . . 400\\n11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\\n11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\\n11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\\n11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\\n11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\\n11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\\n11.9.2 Performance Comparisons . . . . . . . . . . . . 412\\n11.10 Computational Considerations . . . . . . . . . . . . . . . 414\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 15}, page_content='Contents xix\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\\n12 Support Vector Machines and\\nFlexible Discriminants 417\\n12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\\n12.2 The Support Vector Classiﬁer . . . . . . . . . . . . . . . . 417\\n12.2.1 Computing the Support Vector Classiﬁer . . . . 420\\n12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\\n12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\\n12.3.1 Computing the SVM for Classiﬁcation . . . . . . 423\\n12.3.2 The SVM as a Penalization Method . . . . . . . 426\\n12.3.3 Function Estimation and Reproducing Kernels . 428\\n12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\\n12.3.5 A Path Algorithm for the SVM Classiﬁer . . . . 432\\n12.3.6 Support Vector Machines for Regression . . . . . 434\\n12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\\n12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\\n12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 438\\n12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\\n12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\\n12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\\n12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\\n12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\\n13 Prototype Methods and Nearest-Neighbors 459\\n13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\\n13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\\n13.2.1 K-means Clustering . . . . . . . . . . . . . . . . 460\\n13.2.2 Learning Vector Quantization . . . . . . . . . . 462\\n13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\\n13.3 k-Nearest-Neighbor Classiﬁers . . . . . . . . . . . . . . . 463\\n13.3.1 Example: A Comparative Study . . . . . . . . . 468\\n13.3.2 Example: k-Nearest-Neighbors\\nand Image Scene Classiﬁcation . . . . . . . . . . 470\\n13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\\n13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\\n13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\\n13.4.2 Global Dimension Reduction\\nfor Nearest-Neighbors . . . . . . . . . . . . . . . 479\\n13.5 Computational Considerations . . . . . . . . . . . . . . . 480\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 16}, page_content='xx Contents\\n14 Unsupervised Learning 485\\n14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\\n14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\\n14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\\n14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\\n14.2.3 Example: Market Basket Analysis . . . . . . . . 492\\n14.2.4 Unsupervised as Supervised Learning . . . . . . 495\\n14.2.5 Generalized Association Rules . . . . . . . . . . 497\\n14.2.6 Choice of Supervised Learning Method . . . . . 499\\n14.2.7 Example: Market Basket Analysis (Continued) . 499\\n14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\\n14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\\n14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\\n14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\\n14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\\n14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\\n14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . . 509\\n14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\\n14.3.8 Example: Human Tumor Microarray Data . . . 512\\n14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\\n14.3.10 K-medoids . . . . . . . . . . . . . . . . . . . . . 515\\n14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\\n14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\\n14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\\n14.5 Principal Components, Curves and Surfaces . . . . . . . . 534\\n14.5.1 Principal Components . . . . . . . . . . . . . . . 534\\n14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\\n14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\\n14.5.4 Kernel Principal Components . . . . . . . . . . . 547\\n14.5.5 Sparse Principal Components . . . . . . . . . . . 550\\n14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\\n14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\\n14.7 Independent Component Analysis\\nand Exploratory Projection Pursuit . . . . . . . . . . . . 557\\n14.7.1 Latent Variables and Factor Analysis . . . . . . 558\\n14.7.2 Independent Component Analysis . . . . . . . . 560\\n14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\\n14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\\n14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\\n14.9 Nonlinear Dimension Reduction\\nand Local Multidimensional Scaling . . . . . . . . . . . . 572\\n14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 17}, page_content='Contents xxi\\n15 Random Forests 587\\n15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\\n15.2 Deﬁnition of Random Forests . . . . . . . . . . . . . . . . 587\\n15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\\n15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\\n15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\\n15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\\n15.3.4 Random Forests and Overﬁtting . . . . . . . . . 596\\n15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\\n15.4.1 Variance and the De-Correlation Eﬀect . . . . . 597\\n15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\\n15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\\n16 Ensemble Learning 605\\n16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\\n16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\\n16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\\n16.2.2 The “Bet on Sparsity” Principle . . . . . . . . . 610\\n16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613\\n16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\\n16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\\n16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\\nBibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\\n17 Undirected Graphical Models 625\\n17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\\n17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\\n17.3 Undirected Graphical Models for Continuous Variables . 630\\n17.3.1 Estimation of the Parameters\\nwhen the Graph Structure is Known . . . . . . . 631\\n17.3.2 Estimation of the Graph Structure . . . . . . . . 635\\n17.4 Undirected Graphical Models for Discrete Variables . . . 638\\n17.4.1 Estimation of the Parameters\\nwhen the Graph Structure is Known . . . . . . . 639\\n17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\\n17.4.3 Estimation of the Graph Structure . . . . . . . . 642\\n17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\\n18 High-Dimensional Problems: p≫N 649\\n18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 18}, page_content='xxii Contents\\n18.2 Diagonal Linear Discriminant Analysis\\nand Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\\n18.3 Linear Classiﬁers with Quadratic Regularization . . . . . 654\\n18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\\n18.3.2 Logistic Regression\\nwith Quadratic Regularization . . . . . . . . . . 657\\n18.3.3 The Support Vector Classiﬁer . . . . . . . . . . 657\\n18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\\n18.3.5 Computational Shortcuts When p≫N. . . . . 659\\n18.4 Linear Classiﬁers with L1Regularization . . . . . . . . . 661\\n18.4.1 Application of Lasso\\nto Protein Mass Spectroscopy . . . . . . . . . . 664\\n18.4.2 The Fused Lasso for Functional Data . . . . . . 666\\n18.5 Classiﬁcation When Features are Unavailable . . . . . . . 668\\n18.5.1 Example: String Kernels\\nand Protein Classiﬁcation . . . . . . . . . . . . . 668\\n18.5.2 Classiﬁcation and Other Models Using\\nInner-Product Kernels and Pairwise Distances . 670\\n18.5.3 Example: Abstracts Classiﬁcation . . . . . . . . 672\\n18.6 High-Dimensional Regression:\\nSupervised Principal Components . . . . . . . . . . . . . 674\\n18.6.1 Connection to Latent-Variable Modeling . . . . 678\\n18.6.2 Relationship with Partial Least Squares . . . . . 680\\n18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\\n18.7 Feature Assessment and the Multiple-Testing Problem . . 683\\n18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\\n18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\\n18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\\n18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\\nReferences 699\\nAuthor Index 729\\nIndex 737'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 19}, page_content='This is page 1\\nPrinter: Opaque this\\n1\\nIntroduction\\nStatistical learning plays a key role in many areas of science, ﬁnance and\\nindustry. Here are some examples of learning problems:\\n•Predict whether a patient, hospitalized due to a heart attack, will\\nhave a second heart attack. The prediction is to be based on demo-\\ngraphic, diet and clinical measurements for that patient.\\n•Predict the price of a stock in 6 months from now, on the basis of\\ncompany performance measures and economic data.\\n•Identify the numbers in a handwritten ZIP code, from a digitized\\nimage.\\n•Estimate the amount of glucose in the blood of a diabetic person,\\nfrom the infrared absorption spectrum of that person’s blood.\\n•Identify the risk factors for prostate cancer, based on clinical and\\ndemographic variables.\\nThe science of learning plays a key role in the ﬁelds of statistics, data\\nmining and artiﬁcial intelligence, intersecting with areas of engineering and\\nother disciplines.\\nThis book is about learning from data. In a typical scenario, we have\\nan outcome measurement, usually quantitative (such as a stock price) or\\ncategorical (such as heart attack/no heart attack), that we wish to predict\\nbased on a set of features (such as diet and clinical measurements). We\\nhave a training set of data, in which we observe the outcome and feature'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 20}, page_content='2 1. Introduction\\nTABLE 1.1. Average percentage of words or characters in an email message\\nequal to the indicated word or character. We have chosen the wo rds and characters\\nshowing the largest diﬀerence between spamandemail.\\ngeorge you your hp free hpl ! our re edu remove\\nspam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28\\nemail 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01\\nmeasurements for a set of objects (such as people). Using this data we build\\na prediction model, or learner , which will enable us to predict the outcome\\nfor new unseen objects. A good learner is one that accurately predicts such\\nan outcome.\\nThe examples above describe what is called the supervised learning prob-\\nlem. It is called “supervised” because of the presence of the outcome vari-\\nable to guide the learning process. In the unsupervised learning problem ,\\nwe observe only the features and have no measurements of the outcome.\\nOur task is rather to describe how the data are organized or clustered. We\\ndevote most of this book to supervised learning; the unsupervised problem\\nis less developed in the literature, and is the focus of Chapter 14.\\nHere are some examples of real learning problems that are discussed in\\nthis book.\\nExample 1: Email Spam\\nThe data for this example consists of information from 4601 email mes-\\nsages, in a study to try to predict whether the email was junk email, or\\n“spam.” The objective was to design an automatic spam detector that\\ncould ﬁlter out spam before clogging the users’ mailboxes. For all 4601\\nemail messages, the true outcome (email type) email orspamis available,\\nalong with the relative frequencies of 57 of the most commonly occurring\\nwords and punctuation marks in the email message. This is a supervised\\nlearning problem, with the outcome the class variable email/spam. It is also\\ncalled a classiﬁcation problem.\\nTable 1.1 lists the words and characters showing the largest average\\ndiﬀerence between spamandemail.\\nOur learning method has to decide which features to use and how: for\\nexample, we might use a rule such as\\nif (%george <0.6) & (%you>1.5) then spam\\nelseemail.\\nAnother form of a rule might be:\\nif (0.2≤%you−0.3≤%george )>0 then spam\\nelseemail.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 21}, page_content='1. Introduction 3\\nlpsa−1 1 2 3 4\\noooooo ooo ooo o oooo o oooo oooooooooo oooooooooooooooooooooooooooooo oo oooo ooooooooooooooooooooooooooooo\\nooo ooo ooooo oooooooooooooooooooooooooo o ooooooooo oo ooooooo oooooooooooo ooooooooooooooooooooooooooooo40 50 60 70 80\\noo oooo ooo ooo oooooo o ooooooooooooo oooooooooooooooo o o oooooo oo oooo oooooooooooooooooooo ooooooooooooooo\\noo o ooo o o o oo ooooo o oo o o o ooooo ooo ooo oo o oo oo oo o o ooo o o oo ooo o o o ooooo o o o ooo o o o o oooo oo oo ooooo o o oo o o oooooo0.0 0.4 0.8\\noo o ooo ooo oo o oooooooooooooooooooo oooooo o ooooooo o oo oooooooooooo o o o oooo oo o o oooo oo o ooo o oo o o ooo o oooooo\\noo o ooo ooo oo o ooooooooo o o o oooooo oo oooo o o oooooooo o oo ooooo ooooooo ooo ooooooo ooooooo o ooooooo oooo ooooooo6.0 7.0 8.0 9.0\\noo oooo ooo oo o oooooooooooooooooooo oooooooo o ooooo o oo ooooooooooooo ooooooooooo o ooooooooo o ooooooooooooo\\n0 1 2 3 4 5oo oooo ooo oo o ooooooooooo o oo o o o ooo oooooooo o ooooo o oo o o o oooooooooo oo oo o ooo ooo o ooo o oooooooo oooo oooooo o−1 1 2 3 4ooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nlcavol\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\noo oo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\nooo o\\noo\\no oo\\nooooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\no oooo\\noo oo\\noooo\\noo\\nooo\\noo\\nooo\\no oo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\no oooo\\noo o o\\noo o o\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\nooo o\\noo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\no o\\nooo\\nooo\\nooooo\\noo o o\\noooo\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\no oo\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooo\\nooo o\\noo\\nooo\\noo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noo\\noooo\\noo\\noo\\no\\nooo\\nooo\\noooo\\noo\\no oo\\nooooo\\noo\\no oo\\nooo\\noo\\nooo\\nooo\\no o\\noo o\\nooo\\nooooo\\noo oo\\noooo\\noo\\nooo\\noo\\nooo\\no oo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\nooooooo\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooolweight\\noo\\nooooooo\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\noooo\\no ooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\noo o\\nooo\\noo\\noooooo o\\noo o\\nooo\\nooo\\noo\\no oooo\\nooo\\nooo\\no\\noooo\\no\\noo\\noooo o\\noooo\\no ooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooo oooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\no oo oo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\nooo\\nooo\\noo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooooo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\noo\\nooo oooo\\noo\\noo\\nooo\\nooo\\n2.5 3.5 4.5oo\\noooooo o\\nooo\\nooo\\nooo\\noo\\nooo oo\\nooo\\nooo\\no\\noooo\\no\\noo\\nooooo\\nooo o\\noooo\\noo\\noo\\nooo\\noo\\nooo\\noo oooo\\no\\noo\\noo\\noo o\\noo\\nooooooo\\noo\\noo\\nooo\\nooo40 50 60 70 80ooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noo oooooo ooo oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooooo\\nooo\\no\\noo\\noooooooooo ooo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\nage\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\no o oooooooo o oo\\noo oo\\noo\\nooo\\no oo\\noo\\no\\nooo\\nooo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\no o oooooo oo ooo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 21}, page_content='oooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\no o oooooo oo ooo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\nooo\\noo\\noo\\no\\noooo oo\\nooo\\no\\noo\\noooooooo oo o oo\\noooo\\noo\\nooo\\nooo\\noo\\no\\nooo\\no oo\\noo\\nooo\\noo\\noo\\nooooooo\\noo\\no\\nooooo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo o\\noo o ooooo\\no ooo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\nooo\\noo\\noooo\\no\\noooo oooo\\no o oo\\no oooo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no ooo\\no\\no oo ooooo\\noo oo\\no o ooo\\nooo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no ooo\\no\\no o oo oooo\\no ooo\\no oo oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\noooo\\nooo\\noo\\nooo\\noo\\no o oo\\nolbph\\no o o o o ooo\\no o oo\\no o o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\no o o o o ooo\\no o oo\\noo ooo\\nooo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no ooo\\no\\no o oo o ooo\\no o oo\\no o ooo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\no ooo\\nooo\\noo\\no oo\\noo\\no o oo\\no\\n−1 0 1 2o o oo o ooo\\no o oo\\noo o oo\\no oo\\noo\\nooo\\no\\noo\\no\\nooo\\no\\no oo\\no\\noo\\no\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noo\\noooo\\noooo\\nooo\\noo\\no oo\\noo\\noooo\\no0.0 0.4 0.8oo o ooo o o o oo o o o o o o o o o o o o o o o o o o o o o o o o o o oo\\no o o o o o oo\\no o o o o o o o o o o o o oo\\noo\\no o o o o oo\\noo oo o\\no oo\\no o oo\\no oo\\nooo o\\noooooo o\\noooo oo o oo o oo o oooo oo o o oo oo ooo o oo o oo o o ooo\\no o oo oooo\\no oo o oo o oooo o ooo\\noo\\noo ooo oo\\noo o o o\\no oo\\noo oo\\nooo\\noo oo\\nooo oo o o\\no oo ooo o o oo o oo o oo ooo oooo ooo ooo o oooo o oo oo\\noo oo oo oo\\noo o o oo oo oo o oo oo\\noo\\noo oo o oo\\nooo oo\\no oo\\noo oo\\nooo\\noo oo\\nooo oo oo\\no o oo oo ooo ooo o oo o oo o oo o o o o ooo oo o o oo o o ooo\\no o oo o ooo\\no o o oo o oo ooo oo oo\\noo\\noo o oooo\\nooo o o\\no oo\\noooo\\nooo\\nooo o\\noo oo o o o\\no o o o o o o oo o o oo o o o oo o oo oo o ooo ooo o oo o o oooo\\no o o oo o oo\\no o oo o oo o oo oo o oo\\noo\\no o o oo oo\\noo oo o\\nooo\\no o oo\\no oo\\noo o o\\noo o o o oo\\nsvi\\no o o o o o o o o o o o oo oo o oo o o oo oo o oo o ooo o o oo ooo\\noo o o oooo\\noo oo o o oo o oo o o oo\\noo\\no o ooo oo\\noo o o o\\nooo\\no o oo\\nooo\\noo oo\\noo o oo o o\\no o oo o o o o o o o o o o oo oo o o o oo oo o o o oo o o o o o o ooo\\no ooo o o oo\\noo o oo o o o o oo o ooo\\noo\\no o o oo oo\\noo oo o\\no oo\\no o oo\\no oo\\noo o o\\noo o o o o o\\no o oo o o o o o o o o oo o o oo o o o oo oo o oo oo o o o o oo o oo\\no oooo o oo\\noo o oo ooo oo o o ooo\\noo\\no o ooooo\\noo oo o\\no oo\\noo oo\\nooo\\noo o o\\noo ooo oo\\noo o ooo o o o oo oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\noooo oo o oo o ooo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no ooo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\nooo\\no o ooooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\nooooo\\noo\\no oo ooo o o oo o oo\\noo\\nooo\\no ooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no oo oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\nooooo\\noo\\no o oo oo ooo oooo\\noo\\nooo\\no ooo\\noo\\no oo\\nooo\\no\\no ooo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\noooo\\no oo\\no oo oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\nooooo\\noo\\no o o o o o o oo o o oo\\noo\\nooo\\no ooo\\noo\\nooo\\nooo\\no\\noo oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\noo o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\no oo\\noo\\noo\\nooo\\no\\no oooo\\noo\\no o o o o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\nlcp\\no o oo o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\noooo\\no oo\\no o ooooo\\nooo\\no\\nooo\\nooooo\\no\\noo'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 21}, page_content='oo\\no ooo\\no oo\\no o o oooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\nlcp\\no o oo o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\noooo\\no oo\\no o ooooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo\\n−1 0 1 2 3o o oo o o o o o o o oo\\noo\\nooo\\no o oo\\noo\\no oo\\nooo\\no\\no o oo\\noo\\noo\\no\\no ooo\\no\\noo\\no\\noo\\no ooo\\no oo\\no o ooooo\\nooo\\no\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\nooo\\no\\no oooo\\noo6.0 7.0 8.0 9.0ooo\\nooo o o o oo oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo o o\\noo o o oo\\no o o o o o o o oo\\no o\\nooo o\\noooooo o\\nooo\\no oo o oo o ooo oo\\noo\\no o o oo\\noo\\no ooo o\\noo o oo ooo\\no ooo\\no\\nooooo\\no\\nooo\\noo o ooo\\noo o\\nooo\\no\\noo oo\\noo oo oo\\no oo o o oo o oo\\no o\\noo oo\\nooo oo o o\\no oo\\nooo o o oo o oo o o\\noo\\noo ooo\\noo\\nooooo\\no oooo ooo\\no ooo\\no\\nooo oo\\no\\no oo\\noo oo oo\\nooo\\nooo\\no\\noo oo\\noooo oo\\noo o oo oo o oo\\noo\\noo oo\\nooo oo oo\\no oo\\no oo ooo oooo oo\\noo\\no o ooo\\noo\\no ooo o\\no o o oo ooo\\no ooo\\no\\noo ooo\\no\\no oo\\noo oo oo\\nooo\\nooo\\no\\noo o o\\nooo ooo\\no o o ooooo oo\\no o\\nooo o\\noo oo o o o\\no oo\\no o o o oo o o oo o o\\noo\\no o ooo\\noo\\nooo oo\\no o oo o ooo\\no o oo\\no\\noo o oo\\no\\no oo\\nooo o oo\\noo o\\nooo\\no\\noo o o\\noo o ooo\\no o ooo o o ooo\\no o\\noo o o\\noo o o o oo\\no oo\\no o o o o o o o oo o o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no ooo\\no\\noo o oo\\no\\no oo\\noo o o o o\\noo o\\nooo\\no\\noo o o\\noo oo oo\\no o o o oo o o oo\\no o\\noo o o\\noo o o o o o\\no oo\\no o o o o o o o ooo o\\noo\\noo o oo\\noo\\no ooo o\\no oo o o ooo\\no ooo\\no\\nooooo\\no\\no oo\\noo oo o o\\noo o\\nooo\\no\\noo oo\\noo oo oo\\no ooo oo o ooo\\no o\\noo oo\\noo o oo o ogleason\\no oo\\no o o o o o o o ooo o\\noo\\no o o oo\\noo\\no oo o o\\no o o o o ooo\\no ooo\\no\\noo o oo\\no\\no oo\\noooo oo\\noo o\\nooo\\no\\noo oo\\noo oo oo\\no o oo ooo ooo\\no o\\noo o o\\noo ooo oo\\n0 1 2 3 4 5ooo\\nooo o o o oo oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\nooo\\no oo o oo o ooo\\noooo\\no o o oo\\noo\\no oo\\noo\\noo o oo ooooo\\noo\\no\\nooooo\\no\\nooo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n2.5 3.5 4.5o oo\\nooo o o oo o oo\\no ooo\\noo ooo\\noo\\nooo\\noo\\no oooo ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no oo ooo oooo\\noooo\\no o ooo\\noo\\no oo\\noo\\no o o oo ooooo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n−1 0 1 2o oo\\no o o o oo o o oo\\no ooo\\no o ooo\\noo\\nooo\\noo\\no o oo o ooooo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n−1 0 1 2 3o oo\\no o o o o o o o oo\\no ooo\\noo o oo\\noo\\no oo\\noo\\no oo o o ooo oo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\no oo\\no o o o o o o o oo\\no ooo\\no o o oo\\noo\\no oo\\noo\\no o o o o ooooo\\noo\\no\\nooooo\\no\\no oo\\noo\\no\\noo\\noooo\\nooo\\no\\nooo\\no\\nooo\\nooo\\nooo\\noo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\noo\\no\\n0 20 60 100\\n0 20 60 100pgg45\\nFIGURE 1.1. Scatterplot matrix of the prostate cancer data. The ﬁrst row sho ws\\nthe response against each of the predictors in turn. Two of the pr edictors, sviand\\ngleason , are categorical.\\nFor this problem not all errors are equal; we want to avoid ﬁltering out\\ngood email, while letting spam get through is not desirable but less serious\\nin its consequences. We discuss a number of diﬀerent methods for tackling\\nthis learning problem in the book.\\nExample 2: Prostate Cancer\\nThe data for this example, displayed in Figure 1.11, come from a study\\nby Stamey et al. (1989) that examined the correlation between the level of\\n1There was an error in these data in the ﬁrst edition of this boo k. Subject 32 had\\na value of 6.1 for lweight , which translates to a 449 gm prostate! The correct value is\\n44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 22}, page_content='4 1. Introduction\\nFIGURE 1.2. Examples of handwritten digits from U.S. postal envelopes.\\nprostate speciﬁc antigen (PSA) and a number of clinical measures, in 97\\nmen who were about to receive a radical prostatectomy.\\nThe goal is to predict the log of PSA ( lpsa) from a number of measure-\\nments including log cancer volume ( lcavol ), log prostate weight lweight ,\\nage, log of benign prostatic hyperplasia amount lbph, seminal vesicle in-\\nvasionsvi, log of capsular penetration lcp, Gleason score gleason , and\\npercent of Gleason scores 4 or 5 pgg45. Figure 1.1 is a scatterplot matrix\\nof the variables. Some correlations with lpsaare evident, but a good pre-\\ndictive model is diﬃcult to construct by eye.\\nThis is a supervised learning problem, known as a regression problem ,\\nbecause the outcome measurement is quantitative.\\nExample 3: Handwritten Digit Recognition\\nThe data from this example come from the handwritten ZIP codes on\\nenvelopes from U.S. postal mail. Each image is a segment from a ﬁve digi t\\nZIP code, isolating a single digit. The images are 16 ×16 eight-bit grayscale\\nmaps, with each pixel ranging in intensity from 0 to 255. Some sample\\nimages are shown in Figure 1.2.\\nThe images have been normalized to have approximately the same size\\nand orientation. The task is to predict, from the 16 ×16 matrix of pixel\\nintensities, the identity of each image (0 ,1,... ,9) quickly and accurately. If\\nit is accurate enough, the resulting algorithm would be used as part of an\\nautomatic sorting procedure for envelopes. This is a classiﬁcation problem\\nfor which the error rate needs to be kept very low to avoid misdirection of'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 23}, page_content='1. Introduction 5\\nmail. In order to achieve this low error rate, some objects can be assigned\\nto a “don’t know” category, and sorted instead by hand.\\nExample 4: DNA Expression Microarrays\\nDNA stands for deoxyribonucleic acid, and is the basic material that makes\\nup human chromosomes. DNA microarrays measure the expression of a\\ngene in a cell by measuring the amount of mRNA (messenger ribonucleic\\nacid) present for that gene. Microarrays are considered a breakthrough\\ntechnology in biology, facilitating the quantitative study of thousands of\\ngenes simultaneously from a single sample of cells.\\nHere is how a DNA microarray works. The nucleotide sequences for a few\\nthousand genes are printed on a glass slide. A target sample and a reference\\nsample are labeled with red and green dyes, and each are hybridized with\\nthe DNA on the slide. Through ﬂuoroscopy, the log (red/green) intensities\\nof RNA hybridizing at each site is measured. The result is a few thousand\\nnumbers, typically ranging from say −6 to 6, measuring the expression level\\nof each gene in the target relative to the reference sample. Positive values\\nindicate higher expression in the target versus the reference, and vice versa\\nfor negative values.\\nA gene expression dataset collects together the expression values from a\\nseries of DNA microarray experiments, with each column representing an\\nexperiment. There are therefore several thousand rows representing individ-\\nual genes, and tens of columns representing samples: in the particular ex-\\nample of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns),\\nalthough for clarity only a random sample of 100 rows are shown. The ﬁg-\\nure displays the data set as a heat map, ranging from green (negative) to\\nred (positive). The samples are 64 cancer tumors from diﬀerent patients.\\nThe challenge here is to understand how the genes and samples are or-\\nganized. Typical questions include the following:\\n(a) which samples are most similar to each other, in terms of their expres-\\nsion proﬁles across genes?\\n(b) which genes are most similar to each other, in terms of their expression\\nproﬁles across samples?\\n(c) do certain genes show very high (or low) expression for certain cancer\\nsamples?\\nWe could view this task as a regression problem, with two categorical\\npredictor variables—genes and samples—with the response variable being\\nthe level of expression. However, it is probably more useful to view it as\\nunsupervised learning problem. For example, for question (a) above, we\\nthink of the samples as points in 6830–dimensional space, which we want\\ntocluster together in some way.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 24}, page_content='6 1. Introduction\\nSID42354SID31984SID301902SIDW128368SID375990SID360097SIDW325120ESTsChr.10SIDW365099SID377133SID381508SIDW308182SID380265SIDW321925ESTsChr.15SIDW362471SIDW417270SIDW298052SID381079SIDW428642TUPLE1TUP1ERLUMENSIDW416621SID43609ESTsSID52979SIDW357197SIDW366311ESTsSMALLNUCSIDW486740ESTsSID297905SID485148SID284853ESTsChr.15SID200394SIDW322806ESTsChr.2SIDW257915SID46536SIDW488221ESTsChr.5SID280066SIDW376394ESTsChr.15SIDW321854WASWiskottHYPOTHETICALSIDW376776SIDW205716SID239012SIDW203464HLACLASSISIDW510534SIDW279664SIDW201620SID297117SID377419SID114241ESTsCh31SIDW376928SIDW310141SIDW298203PTPRCSID289414SID127504ESTsChr.3SID305167SID488017SIDW296310ESTsChr.6SID47116MITOCHONDRIAL60ChrSIDW376586HomosapiensSIDW487261SIDW470459SID167117SIDW31489SID375812DNAPOLYMERSID377451ESTsChr.1MYBPROTOSID471915ESTsSIDW469884HumanmRNASIDW377402ESTsSID207172RASGTPASESID325394H.sapiensmRNAGNALSID73161SIDW380102SIDW299104BREAST\\nRENAL\\nMELANOMAMELANOMA\\nMCF7D-repro\\nCOLONCOLON\\nK562B-repro\\nCOLON\\nNSCLC\\nLEUKEMIA\\nRENAL\\nMELANOMA\\nBREAST\\nCNSCNS\\nRENAL\\nMCF7A-repro\\nNSCLC\\nK562A-repro\\nCOLON\\nCNS\\nNSCLCNSCLC\\nLEUKEMIA\\nCNS\\nOVARIAN\\nBREAST\\nLEUKEMIA\\nMELANOMAMELANOMA\\nOVARIANOVARIAN\\nNSCLC\\nRENAL\\nBREAST\\nMELANOMA\\nOVARIANOVARIAN\\nNSCLC\\nRENAL\\nBREAST\\nMELANOMA\\nLEUKEMIA\\nCOLON\\nBREAST\\nLEUKEMIA\\nCOLON\\nCNS\\nMELANOMA\\nNSCLC\\nPROSTATE\\nNSCLC\\nRENALRENAL\\nNSCLC\\nRENAL\\nLEUKEMIA\\nOVARIAN\\nPROSTATE\\nCOLON\\nBREAST\\nRENAL\\nUNKNOWNFIGURE 1.3. DNA microarray data: expression matrix of 6830genes (rows)\\nand64samples (columns), for the human tumor data. Only a random sampl e\\nof100rows are shown. The display is a heat map, ranging from bright gre en\\n(negative, under expressed) to bright red (positive, over expre ssed). Missing values\\nare gray. The rows and columns are displayed in a randomly chosen order.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 25}, page_content='1. Introduction 7\\nWho Should Read this Book\\nThis book is designed for researchers and students in a broad variety of\\nﬁelds: statistics, artiﬁcial intelligence, engineering, ﬁnance and others. We\\nexpect that the reader will have had at least one elementary course in\\nstatistics, covering basic topics including linear regression.\\nWe have not attempted to write a comprehensive catalog of learning\\nmethods, but rather to describe some of the most important techniques.\\nEqually notable, we describe the underlying concepts and considerations\\nby which a researcher can judge a learning method. We have tried to write\\nthis book in an intuitive fashion, emphasizing concepts rather than math-\\nematical details.\\nAs statisticians, our exposition will naturally reﬂect our backgrounds and\\nareas of expertise. However in the past eight years we have been attending\\nconferences in neural networks, data mining and machine learning, and our\\nthinking has been heavily inﬂuenced by these exciting ﬁelds. This inﬂuence\\nis evident in our current research, and in this book.\\nHow This Book is Organized\\nOur view is that one must understand simple methods before trying to\\ngrasp more complex ones. Hence, after giving an overview of the supervis-\\ning learning problem in Chapter 2 , we discuss linear methods for regression\\nand classiﬁcation in Chapters 3 and4. InChapter 5 we describe splines,\\nwavelets and regularization/penalization methods for a single predictor,\\nwhile Chapter 6 covers kernel methods and local regression. Both of these\\nsets of methods are important building blocks for high-dimensional learn-\\ning techniques. Model assessment and selection is the topic of Chapter 7 ,\\ncovering the concepts of bias and variance, overﬁtting and methods such as\\ncross-validation for choosing models. Chapter 8 discusses model inference\\nand averaging, including an overview of maximum likelihood, Bayesian in-\\nference and the bootstrap, the EM algorithm, Gibbs sampling and bagging,\\nA related procedure called boosting is the focus of Chapter 10 .\\nInChapters 9–13 we describe a series of structured methods for su-\\npervised learning, with Chapters 9 and 11 covering regression and Chap-\\nters 12 and 13 focusing on classiﬁcation. Chapter 14 describes methods for\\nunsupervised learning. Two recently proposed techniques, random forests\\nand ensemble learning, are discussed in Chapters 15 and 16 . We describe\\nundirected graphical models in Chapter 17 and ﬁnally we study high-\\ndimensional problems in Chapter 18 .\\nAt the end of each chapter we discuss computational considerations im-\\nportant for data mining applications, including how the computations scale\\nwith the number of observations and predictors. Each chapter ends with\\nBibliographic Notes giving background references for the material.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 26}, page_content='8 1. Introduction\\nWe recommend that Chapters 1–4 be ﬁrst read in sequence. Chapter 7\\nshould also be considered mandatory, as it covers central concepts that\\npertain to all learning methods. With this in mind, the rest of the book\\ncan be read sequentially, or sampled, depending on the reader’s interest.\\nThe symbol\\n indicates a technically diﬃcult section, one that can\\nbe skipped without interrupting the ﬂow of the discussion.\\nBook Website\\nThe website for this book is located at\\nhttp://www-stat.stanford.edu/ElemStatLearn\\nIt contains a number of resources, including many of the datasets used in\\nthis book.\\nNote for Instructors\\nWe have successively used the ﬁrst edition of this book as the basis for a\\ntwo-quarter course, and with the additional materials in this second edition,\\nit could even be used for a three-quarter sequence. Exercises are provided at\\nthe end of each chapter. It is important for students to have access to good\\nsoftware tools for these topics. We used the R and S-PLUS programming\\nlanguages in our courses.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 27}, page_content='This is page 9\\nPrinter: Opaque this\\n2\\nOverview of Supervised Learning\\n2.1 Introduction\\nThe ﬁrst three examples described in Chapter 1 have several components\\nin common. For each there is a set of variables that might be denoted as\\ninputs , which are measured or preset. These have some inﬂuence on one or\\nmoreoutputs . For each example the goal is to use the inputs to predict the\\nvalues of the outputs. This exercise is called supervised learning .\\nWe have used the more modern language of machine learning. In the\\nstatistical literature the inputs are often called the predictors , a term we\\nwill use interchangeably with inputs, and more classically the independent\\nvariables . In the pattern recognition literature the term features is preferred,\\nwhich we use as well. The outputs are called the responses , or classically\\nthedependent variables .\\n2.2 Variable Types and Terminology\\nThe outputs vary in nature among the examples. In the glucose prediction\\nexample, the output is a quantitative measurement, where some measure-\\nments are bigger than others, and measurements close in value are close\\nin nature. In the famous Iris discrimination example due to R. A. Fisher,\\nthe output is qualitative (species of Iris) and assumes values in a ﬁnite set\\nG={Virginica ,Setosa andVersicolor }. In the handwritten digit example\\nthe output is one of 10 diﬀerent digit classes :G={0,1,... ,9}. In both of'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 28}, page_content='10 2. Overview of Supervised Learning\\nthese there is no explicit ordering in the classes, and in fact often descrip-\\ntive labels rather than numbers are used to denote the classes. Qualitative\\nvariables are also referred to as categorical ordiscrete variables as well as\\nfactors .\\nFor both types of outputs it makes sense to think of using the inputs to\\npredict the output. Given some speciﬁc atmospheric measurements today\\nand yesterday, we want to predict the ozone level tomorrow. Given the\\ngrayscale values for the pixels of the digitized image of the handwritten\\ndigit, we want to predict its class label.\\nThis distinction in output type has led to a naming convention for the\\nprediction tasks: regression when we predict quantitative outputs, and clas-\\nsiﬁcation when we predict qualitative outputs. We will see that these two\\ntasks have a lot in common, and in particular both can be viewed as a task\\nin function approximation.\\nInputs also vary in measurement type; we can have some of each of qual-\\nitative and quantitative input variables. These have also led to distinctio ns\\nin the types of methods that are used for prediction: some methods are\\ndeﬁned most naturally for quantitative inputs, some most naturally for\\nqualitative and some for both.\\nA third variable type is ordered categorical , such as small, medium and\\nlarge, where there is an ordering between the values, but no metric notion\\nis appropriate (the diﬀerence between medium and small need not be the\\nsame as that between large and medium). These are discussed further in\\nChapter 4.\\nQualitative variables are typically represented numerically by codes. The\\neasiest case is when there are only two classes or categories, such as “suc-\\ncess” or “failure,” “survived” or “died.” These are often represented by a\\nsingle binary digit or bit as 0 or 1, or else by −1 and 1. For reasons that will\\nbecome apparent, such numeric codes are sometimes referred to as targets.\\nWhen there are more than two categories, several alternatives are available.\\nThe most useful and commonly used coding is via dummy variables . Here a\\nK-level qualitative variable is represented by a vector of Kbinary variables\\nor bits, only one of which is “on” at a time. Although more compact coding\\nschemes are possible, dummy variables are symmetric in the levels of the\\nfactor.\\nWe will typically denote an input variable by the symbol X. IfXis\\na vector, its components can be accessed by subscripts Xj. Quantitative\\noutputs will be denoted by Y, and qualitative outputs by G(for group).\\nWe use uppercase letters such as X,YorGwhen referring to the generic\\naspects of a variable. Observed values are written in lowercase; hence the\\nith observed value of Xis written as xi(where xiis again a scalar or\\nvector). Matrices are represented by bold uppercase letters; for example, a\\nset of Ninput p-vectors xi, i= 1,... ,N would be represented by the N×p\\nmatrix X. In general, vectors will not be bold, except when they have N\\ncomponents; this convention distinguishes a p-vector of inputs xifor the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 29}, page_content='2.3 Least Squares and Nearest Neighbors 11\\nith observation from the N-vector xjconsisting of all the observations on\\nvariable Xj. Since all vectors are assumed to be column vectors, the ith\\nrow of XisxT\\ni, the vector transpose of xi.\\nFor the moment we can loosely state the learning task as follows: given\\nthe value of an input vector X, make a good prediction of the output Y,\\ndenoted by ˆY(pronounced “y-hat”). If Ytakes values in IR then so should\\nˆY; likewise for categorical outputs, ˆGshould take values in the same set G\\nassociated with G.\\nFor a two-class G, one approach is to denote the binary coded target\\nasY, and then treat it as a quantitative output. The predictions ˆYwill\\ntypically lie in [0 ,1], and we can assign to ˆGthe class label according to\\nwhether ˆ y >0.5. This approach generalizes to K-level qualitative outputs\\nas well.\\nWe need data to construct prediction rules, often a lot of it. We thus\\nsuppose we have available a set of measurements ( xi,yi) or ( xi,gi), i=\\n1,... ,N , known as the training data , with which to construct our prediction\\nrule.\\n2.3 Two Simple Approaches to Prediction: Least\\nSquares and Nearest Neighbors\\nIn this section we develop two simple but powerful prediction methods: the\\nlinear model ﬁt by least squares and the k-nearest-neighbor prediction rule.\\nThe linear model makes huge assumptions about structure and yields stable\\nbut possibly inaccurate predictions. The method of k-nearest neighbors\\nmakes very mild structural assumptions: its predictions are often accurate\\nbut can be unstable.\\n2.3.1 Linear Models and Least Squares\\nThe linear model has been a mainstay of statistics for the past 30 years\\nand remains one of our most important tools. Given a vector of inputs\\nXT= (X1,X2,... ,X p), we predict the output Yvia the model\\nˆY=ˆβ0+p∑\\nj=1Xjˆβj. (2.1)\\nThe term ˆβ0is the intercept, also known as the biasin machine learning.\\nOften it is convenient to include the constant variable 1 in X, include ˆβ0in\\nthe vector of coeﬃcients ˆβ, and then write the linear model in vector form\\nas an inner product\\nˆY=XTˆβ, (2.2)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 30}, page_content='12 2. Overview of Supervised Learning\\nwhere XTdenotes vector or matrix transpose ( Xbeing a column vector).\\nHere we are modeling a single output, so ˆYis a scalar; in general ˆYcan be\\naK–vector, in which case βwould be a p×Kmatrix of coeﬃcients. In the\\n(p+ 1)-dimensional input–output space, ( X,ˆY) represents a hyperplane.\\nIf the constant is included in X, then the hyperplane includes the origin\\nand is a subspace; if not, it is an aﬃne set cutting the Y-axis at the point\\n(0,ˆβ0). From now on we assume that the intercept is included in ˆβ.\\nViewed as a function over the p-dimensional input space, f(X) =XTβ\\nis linear, and the gradient f′(X) =βis a vector in input space that points\\nin the steepest uphill direction.\\nHow do we ﬁt the linear model to a set of training data? There are\\nmany diﬀerent methods, but by far the most popular is the method of\\nleast squares . In this approach, we pick the coeﬃcients βto minimize the\\nresidual sum of squares\\nRSS(β) =N∑\\ni=1(yi−xT\\niβ)2. (2.3)\\nRSS(β) is a quadratic function of the parameters, and hence its minimum\\nalways exists, but may not be unique. The solution is easiest to characterize\\nin matrix notation. We can write\\nRSS(β) = (y−Xβ)T(y−Xβ), (2.4)\\nwhere Xis an N×pmatrix with each row an input vector, and yis an\\nN-vector of the outputs in the training set. Diﬀerentiating w.r.t. βwe get\\nthenormal equations\\nXT(y−Xβ) = 0. (2.5)\\nIfXTXis nonsingular, then the unique solution is given by\\nˆβ= (XTX)−1XTy, (2.6)\\nand the ﬁtted value at the ith input xiis ˆyi= ˆy(xi) =xT\\niˆβ. At an arbi-\\ntrary input x0the prediction is ˆ y(x0) =xT\\n0ˆβ. The entire ﬁtted surface is\\ncharacterized by the pparameters ˆβ. Intuitively, it seems that we do not\\nneed a very large data set to ﬁt such a model.\\nLet’s look at an example of the linear model in a classiﬁcation context.\\nFigure 2.1 shows a scatterplot of training data on a pair of inputs X1and\\nX2. The data are simulated, and for the present the simulation model is\\nnot important. The output class variable Ghas the values BLUEorORANGE ,\\nand is represented as such in the scatterplot. There are 100 points in each\\nof the two classes. The linear regression model was ﬁt to these data, with\\nthe response Ycoded as 0 for BLUEand 1 for ORANGE . The ﬁtted values ˆY\\nare converted to a ﬁtted class variable ˆGaccording to the rule\\nˆG={\\nORANGE ifˆY >0.5,\\nBLUE ifˆY≤0.5.(2.7)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 31}, page_content='2.3 Least Squares and Nearest Neighbors 13\\nLinear Regression of 0/1 Response'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 31}, page_content='.. . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 31}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 31}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 31}, page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 31}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.1. A classiﬁcation example in two dimensions. The classes are coded\\nas a binary variable ( BLUE= 0,ORANGE = 1), and then ﬁt by linear regression.\\nThe line is the decision boundary deﬁned by xTˆβ= 0.5. The orange shaded region\\ndenotes that part of input space classiﬁed as ORANGE , while the blue region is\\nclassiﬁed as BLUE.\\nThe set of points in IR2classiﬁed as ORANGE corresponds to {x:xTˆβ >0.5},\\nindicated in Figure 2.1, and the two predicted classes are separated by the\\ndecision boundary {x:xTˆβ= 0.5}, which is linear in this case. We see\\nthat for these data there are several misclassiﬁcations on both sides of the\\ndecision boundary. Perhaps our linear model is too rigid— or are such errors\\nunavoidable? Remember that these are errors on the training data itself,\\nand we have not said where the constructed data came from. Consider the\\ntwo possible scenarios:\\nScenario 1: The training data in each class were generated from bivariate\\nGaussian distributions with uncorrelated components and diﬀerent\\nmeans.\\nScenario 2: The training data in each class came from a mixture of 10 low-\\nvariance Gaussian distributions, with individual means themselves\\ndistributed as Gaussian.\\nA mixture of Gaussians is best described in terms of the generative\\nmodel. One ﬁrst generates a discrete variable that determines which of'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 32}, page_content='14 2. Overview of Supervised Learning\\nthe component Gaussians to use, and then generates an observation from\\nthe chosen density. In the case of one Gaussian per class, we will see in\\nChapter 4 that a linear decision boundary is the best one can do, and that\\nour estimate is almost optimal. The region of overlap is inevitable, and\\nfuture data to be predicted will be plagued by this overlap as well.\\nIn the case of mixtures of tightly clustered Gaussians the story is dif-\\nferent. A linear decision boundary is unlikely to be optimal, and in fact is\\nnot. The optimal decision boundary is nonlinear and disjoint, and as such\\nwill be much more diﬃcult to obtain.\\nWe now look at another classiﬁcation and regression procedure that is\\nin some sense at the opposite end of the spectrum to the linear model, and\\nfar better suited to the second scenario.\\n2.3.2 Nearest-Neighbor Methods\\nNearest-neighbor methods use those observations in the training set Tclos-\\nest in input space to xto form ˆY. Speciﬁcally, the k-nearest neighbor ﬁt\\nforˆYis deﬁned as follows:\\nˆY(x) =1\\nk∑\\nxi∈Nk(x)yi, (2.8)\\nwhere Nk(x) is the neighborhood of xdeﬁned by the kclosest points xiin\\nthe training sample. Closeness implies a metric, which for the moment we\\nassume is Euclidean distance. So, in words, we ﬁnd the kobservations with\\nxiclosest to xin input space, and average their responses.\\nIn Figure 2.2 we use the same training data as in Figure 2.1, and use\\n15-nearest-neighbor averaging of the binary coded response as the method\\nof ﬁtting. Thus ˆYis the proportion of ORANGE ’s in the neighborhood, and\\nso assigning class ORANGE toˆGifˆY >0.5 amounts to a majority vote in\\nthe neighborhood. The colored regions indicate all those points in input\\nspace classiﬁed as BLUEorORANGE by such a rule, in this case found by\\nevaluating the procedure on a ﬁne grid in input space. We see that the\\ndecision boundaries that separate the BLUEfrom the ORANGE regions are far\\nmore irregular, and respond to local clusters where one class dominates.\\nFigure 2.3 shows the results for 1-nearest-neighbor classiﬁcation: ˆYis\\nassigned the value yℓof the closest point xℓtoxin the training data. In\\nthis case the regions of classiﬁcation can be computed relatively easily, and\\ncorrespond to a Voronoi tessellation of the training data. Each point xi\\nhas an associated tile bounding the region for which it is the closest input\\npoint. For all points xin the tile, ˆG(x) =gi. The decision boundary is even\\nmore irregular than before.\\nThe method of k-nearest-neighbor averaging is deﬁned in exactly the\\nsame way for regression of a quantitative output Y, although k= 1 would\\nbe an unlikely choice.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 33}, page_content='2.3 Least Squares and Nearest Neighbors 15\\n15-Nearest Neighbor Classifier'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 33}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 33}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 33}, page_content='.. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 33}, page_content='... .. .. .. .. . .. . .. . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 33}, page_content='. . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 33}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.2. The same classiﬁcation example in two dimensions as in Fig-\\nure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE = 1)and\\nthen ﬁt by 15-nearest-neighbor averaging as in (2.8). The predicted class i s hence\\nchosen by majority vote amongst the 15-nearest neighbors.\\nIn Figure 2.2 we see that far fewer training observations are misclassiﬁed\\nthan in Figure 2.1. This should not give us too much comfort, though, since\\nin Figure 2.3 noneof the training data are misclassiﬁed. A little thought\\nsuggests that for k-nearest-neighbor ﬁts, the error on the training data\\nshould be approximately an increasing function of k, and will always be 0\\nfork= 1. An independent test set would give us a more satisfactory means\\nfor comparing the diﬀerent methods.\\nIt appears that k-nearest-neighbor ﬁts have a single parameter, the num-\\nber of neighbors k, compared to the pparameters in least-squares ﬁts. Al-\\nthough this is the case, we will see that the eﬀective number of parameters\\nofk-nearest neighbors is N/kand is generally bigger than p, and decreases\\nwith increasing k. To get an idea of why, note that if the neighborhoods\\nwere nonoverlapping, there would be N/kneighborhoods and we would ﬁt\\none parameter (a mean) in each neighborhood.\\nIt is also clear that we cannot use sum-of-squared errors on the training\\nset as a criterion for picking k, since we would always pick k= 1! It would\\nseem that k-nearest-neighbor methods would be more appropriate for the\\nmixture Scenario 2 described above, while for Gaussian data the decision\\nboundaries of k-nearest neighbors would be unnecessarily noisy.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 34}, page_content='16 2. Overview of Supervised Learning\\n1−Nearest Neighbor Classifier\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\nooooo\\nooo\\no\\no\\nooo\\noooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.3. The same classiﬁcation example in two dimensions as in Fig-\\nure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE = 1), and\\nthen predicted by 1-nearest-neighbor classiﬁcation.\\n2.3.3 From Least Squares to Nearest Neighbors\\nThe linear decision boundary from least squares is very smooth, and ap-\\nparently stable to ﬁt. It does appear to rely heavily on the assumption\\nthat a linear decision boundary is appropriate. In language we will develop\\nshortly, it has low variance and potentially high bias.\\nOn the other hand, the k-nearest-neighbor procedures do not appear to\\nrely on any stringent assumptions about the underlying data, and can adapt\\nto any situation. However, any particular subregion of the decision bound-\\nary depends on a handful of input points and their particular positions,\\nand is thus wiggly and unstable—high variance and low bias.\\nEach method has its own situations for which it works best; in particular\\nlinear regression is more appropriate for Scenario 1 above, while nearest\\nneighbors are more suitable for Scenario 2. The time has come to expose\\nthe oracle! The data in fact were simulated from a model somewhere be-\\ntween the two, but closer to Scenario 2. First we generated 10 means mk\\nfrom a bivariate Gaussian distribution N((1,0)T,I) and labeled this class\\nBLUE. Similarly, 10 more were drawn from N((0,1)T,I) and labeled class\\nORANGE . Then for each class we generated 100 observations as follows: for\\neach observation, we picked an mkat random with probability 1 /10, and'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 35}, page_content='2.3 Least Squares and Nearest Neighbors 17\\nDegrees of Freedom − N/kTest Error\\n0.10 0.15 0.20 0.25 0.30\\n  2   3   5   8  12  18  29  67 200151 101  69  45  31  21  11   7   5   3   1\\nTrain\\nTest\\nBayesk −  Number of Nearest Neighbors\\nLinear\\nFIGURE 2.4. Misclassiﬁcation curves for the simulation example used in Fi g-\\nures 2.1, 2.2 and 2.3. A single training sample of size 200was used, and a test\\nsample of size 10,000. The orange curves are test and the blue are training er-\\nror for k-nearest-neighbor classiﬁcation. The results for linear regres sion are the\\nbigger orange and blue squares at three degrees of freedom. The purple line is the\\noptimal Bayes error rate.\\nthen generated a N(mk,I/5), thus leading to a mixture of Gaussian clus-\\nters for each class. Figure 2.4 shows the results of classifying 10,000 new\\nobservations generated from the model. We compare the results for least\\nsquares and those for k-nearest neighbors for a range of values of k.\\nA large subset of the most popular techniques in use today are variants of\\nthese two simple procedures. In fact 1-nearest-neighbor, the simplest of all,\\ncaptures a large percentage of the market for low-dimensional problems.\\nThe following list describes some ways in which these simple procedures\\nhave been enhanced:\\n•Kernel methods use weights that decrease smoothly to zero with dis-\\ntance from the target point, rather than the eﬀective 0 /1 weights used\\nbyk-nearest neighbors.\\n•In high-dimensional spaces the distance kernels are modiﬁed to em-\\nphasize some variable more than others.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 36}, page_content='18 2. Overview of Supervised Learning\\n•Local regression ﬁts linear models by locally weighted least squares,\\nrather than ﬁtting constants locally.\\n•Linear models ﬁt to a basis expansion of the original inputs allow\\narbitrarily complex models.\\n•Projection pursuit and neural network models consist of sums of non-\\nlinearly transformed linear models.\\n2.4 Statistical Decision Theory\\nIn this section we develop a small amount of theory that provides a frame-\\nwork for developing models such as those discussed informally so far. We\\nﬁrst consider the case of a quantitative output, and place ourselves in the\\nworld of random variables and probability spaces. Let X∈IRpdenote a\\nreal valued random input vector, and Y∈IR a real valued random out-\\nput variable, with joint distribution Pr( X,Y). We seek a function f(X)\\nfor predicting Ygiven values of the input X. This theory requires a loss\\nfunction L(Y,f(X)) for penalizing errors in prediction, and by far the most\\ncommon and convenient is squared error loss :L(Y,f(X)) = ( Y−f(X))2.\\nThis leads us to a criterion for choosing f,\\nEPE(f) = E( Y−f(X))2(2.9)\\n=∫\\n[y−f(x)]2Pr(dx,dy ), (2.10)\\nthe expected (squared) prediction error . By conditioning1onX, we can\\nwrite EPE as\\nEPE(f) = E XEY|X(\\n[Y−f(X)]2|X)\\n(2.11)\\nand we see that it suﬃces to minimize EPE pointwise:\\nf(x) = argmincEY|X(\\n[Y−c]2|X=x)\\n. (2.12)\\nThe solution is\\nf(x) = E( Y|X=x), (2.13)\\nthe conditional expectation, also known as the regression function. Thus\\nthe best prediction of Yat any point X=xis the conditional mean, when\\nbest is measured by average squared error.\\nThe nearest-neighbor methods attempt to directly implement this recipe\\nusing the training data. At each point x, we might ask for the average of all\\n1Conditioning here amounts to factoring the joint density Pr (X, Y) = Pr( Y|X)Pr(X)\\nwhere Pr( Y|X) = Pr( Y, X)/Pr(X), and splitting up the bivariate integral accordingly.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 37}, page_content='2.4 Statistical Decision Theory 19\\nthose yis with input xi=x. Since there is typically at most one observation\\nat any point x, we settle for\\nˆf(x) = Ave( yi|xi∈Nk(x)), (2.14)\\nwhere “Ave” denotes average, and Nk(x) is the neighborhood containing\\nthekpoints in T closest to x. Two approximations are happening here:\\n•expectation is approximated by averaging over sample data;\\n•conditioning at a point is relaxed to conditioning on some region\\n“close” to the target point.\\nFor large training sample size N, the points in the neighborhood are likely\\nto be close to x, and as kgets large the average will get more stable.\\nIn fact, under mild regularity conditions on the joint probability distri-\\nbution Pr( X,Y), one can show that as N,k→ ∞ such that k/N→0,\\nˆf(x)→E(Y|X=x). In light of this, why look further, since it seems\\nwe have a universal approximator? We often do not have very large sam-\\nples. If the linear or some more structured model is appropriate, then we\\ncan usually get a more stable estimate than k-nearest neighbors, although\\nsuch knowledge has to be learned from the data as well. There are other\\nproblems though, sometimes disastrous. In Section 2.5 we see that as the\\ndimension pgets large, so does the metric size of the k-nearest neighbor-\\nhood. So settling for nearest neighborhood as a surrogate for conditioning\\nwill fail us miserably. The convergence above still holds, but the rateof\\nconvergence decreases as the dimension increases.\\nHow does linear regression ﬁt into this framework? The simplest explana-\\ntion is that one assumes that the regression function f(x) is approximately\\nlinear in its arguments:\\nf(x)≈xTβ. (2.15)\\nThis is a model-based approach—we specify a model for the regression func-\\ntion. Plugging this linear model for f(x) into EPE (2.9) and diﬀerentiating\\nwe can solve for βtheoretically:\\nβ= [E(XXT)]−1E(XY). (2.16)\\nNote we have notconditioned on X; rather we have used our knowledge\\nof the functional relationship to poolover values of X. The least squares\\nsolution (2.6) amounts to replacing the expectation in (2.16) by averages\\nover the training data.\\nSo both k-nearest neighbors and least squares end up approximating\\nconditional expectations by averages. But they diﬀer dramatically in terms\\nof model assumptions:\\n•Least squares assumes f(x) is well approximated by a globally linear\\nfunction.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 38}, page_content='20 2. Overview of Supervised Learning\\n•k-nearest neighbors assumes f(x) is well approximated by a locally\\nconstant function.\\nAlthough the latter seems more palatable, we have already seen that we\\nmay pay a price for this ﬂexibility.\\nMany of the more modern techniques described in this book are model\\nbased, although far more ﬂexible than the rigid linear model. For example,\\nadditive models assume that\\nf(X) =p∑\\nj=1fj(Xj). (2.17)\\nThis retains the additivity of the linear model, but each coordinate function\\nfjis arbitrary. It turns out that the optimal estimate for the additive model\\nuses techniques such as k-nearest neighbors to approximate univariate con-\\nditional expectations simultaneously for each of the coordinate functions.\\nThus the problems of estimating a conditional expectation in high dimen-\\nsions are swept away in this case by imposing some (often unrealistic) model\\nassumptions, in this case additivity.\\nAre we happy with the criterion (2.11)? What happens if we replace the\\nL2loss function with the L1:E|Y−f(X)|? The solution in this case is the\\nconditional median,\\nˆf(x) = median( Y|X=x), (2.18)\\nwhich is a diﬀerent measure of location, and its estimates are more robust\\nthan those for the conditional mean. L1criteria have discontinuities in\\ntheir derivatives, which have hindered their widespread use. Other more\\nresistant loss functions will be mentioned in later chapters, but squared\\nerror is analytically convenient and the most popular.\\nWhat do we do when the output is a categorical variable G? The same\\nparadigm works here, except we need a diﬀerent loss function for penalizing\\nprediction errors. An estimate ˆGwill assume values in G, the set of possible\\nclasses. Our loss function can be represented by a K×Kmatrix L, where\\nK= card( G).Lwill be zero on the diagonal and nonnegative elsewhere,\\nwhere L(k,ℓ) is the price paid for classifying an observation belonging to\\nclassGkasGℓ. Most often we use the zero–one loss function, where all\\nmisclassiﬁcations are charged a single unit. The expected prediction error\\nis\\nEPE = E[ L(G,ˆG(X))], (2.19)\\nwhere again the expectation is taken with respect to the joint distribution\\nPr(G,X). Again we condition, and can write EPE as\\nEPE = E XK∑\\nk=1L[Gk,ˆG(X)]Pr(Gk|X) (2.20)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 39}, page_content='2.4 Statistical Decision Theory 21\\nBayes Optimal Classifier'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 39}, page_content='... .. . . .. . . . .. . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 39}, page_content='. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 39}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 39}, page_content='. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 39}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nFIGURE 2.5. The optimal Bayes decision boundary for the simulation exampl e\\nof Figures 2.1, 2.2 and 2.3. Since the generating density is known for each class,\\nthis boundary can be calculated exactly (Exercise 2.2).\\nand again it suﬃces to minimize EPE pointwise:\\nˆG(x) = argming∈GK∑\\nk=1L(Gk,g)Pr(Gk|X=x). (2.21)\\nWith the 0–1 loss function this simpliﬁes to\\nˆG(x) = argming∈G[1−Pr(g|X=x)] (2.22)\\nor simply\\nˆG(X) =Gkif Pr(Gk|X=x) = max\\ng∈GPr(g|X=x). (2.23)\\nThis reasonable solution is known as the Bayes classiﬁer , and says that\\nwe classify to the most probable class, using the conditional (discrete) dis-\\ntribution Pr( G|X). Figure 2.5 shows the Bayes-optimal decision boundary\\nfor our simulation example. The error rate of the Bayes classiﬁer is called\\ntheBayes rate .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 40}, page_content='22 2. Overview of Supervised Learning\\nAgain we see that the k-nearest neighbor classiﬁer directly approximates\\nthis solution—a majority vote in a nearest neighborhood amounts to ex-\\nactly this, except that conditional probability at a point is relaxed to con-\\nditional probability within a neighborhood of a point, and probabilities ar e\\nestimated by training-sample proportions.\\nSuppose for a two-class problem we had taken the dummy-variable ap-\\nproach and coded Gvia a binary Y, followed by squared error loss estima-\\ntion. Then ˆf(X) = E( Y|X) = Pr( G=G1|X) ifG1corresponded to Y= 1.\\nLikewise for a K-class problem, E( Yk|X) = Pr( G=Gk|X). This shows\\nthat our dummy-variable regression procedure, followed by classiﬁcation to\\nthe largest ﬁtted value, is another way of representing the Bayes classiﬁer.\\nAlthough this theory is exact, in practice problems can occur, depending\\non the regression model used. For example, when linear regression is used,\\nˆf(X) need not be positive, and we might be suspicious about using it as\\nan estimate of a probability. We will discuss a variety of approaches to\\nmodeling Pr( G|X) in Chapter 4.\\n2.5 Local Methods in High Dimensions\\nWe have examined two learning techniques for prediction so far: the stable\\nbut biased linear model and the less stable but apparently less biased class\\nofk-nearest-neighbor estimates. It would seem that with a reasonably large\\nset of training data, we could always approximate the theoretically optimal\\nconditional expectation by k-nearest-neighbor averaging, since we should\\nbe able to ﬁnd a fairly large neighborhood of observations close to any x\\nand average them. This approach and our intuition breaks down in high\\ndimensions, and the phenomenon is commonly referred to as the curse\\nof dimensionality (Bellman, 1961). There are many manifestations of this\\nproblem, and we will examine a few here.\\nConsider the nearest-neighbor procedure for inputs uniformly distributed\\nin ap-dimensional unit hypercube, as in Figure 2.6. Suppose we send out a\\nhypercubical neighborhood about a target point to capture a fraction rof\\nthe observations. Since this corresponds to a fraction rof the unit volume,\\nthe expected edge length will be ep(r) =r1/p. In ten dimensions e10(0.01) =\\n0.63 and e10(0.1) = 0 .80, while the entire range for each input is only 1 .0.\\nSo to capture 1% or 10% of the data to form a local average, we must cover\\n63% or 80% of the range of each input variable. Such neighborhoods are no\\nlonger “local.” Reducing rdramatically does not help much either, since\\nthe fewer observations we average, the higher is the variance of our ﬁt.\\nAnother consequence of the sparse sampling in high dimensions is that\\nall sample points are close to an edge of the sample. Consider Ndata points\\nuniformly distributed in a p-dimensional unit ball centered at the origin.\\nSuppose we consider a nearest-neighbor estimate at the origin. The median'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 41}, page_content='2.5 Local Methods in High Dimensions 23\\n1\\n10Unit Cube\\nFraction of VolumeDistance\\n0.0 0.2 0.4 0.60.0 0.2 0.4 0.6 0.8 1.0p=1p=2p=3p=10\\nNeighborhood\\nFIGURE 2.6. The curse of dimensionality is well illustrated by a subcubica l\\nneighborhood for uniform data in a unit cube. The ﬁgure on the righ t shows the\\nside-length of the subcube needed to capture a fraction rof the volume of the data,\\nfor diﬀerent dimensions p. In ten dimensions we need to cover 80%of the range\\nof each coordinate to capture 10%of the data.\\ndistance from the origin to the closest data point is given by the expression\\nd(p,N) =(\\n1−1\\n21/N)1/p\\n(2.24)\\n(Exercise 2.3). A more complicated expression exists for the mean distance\\nto the closest point. For N= 500, p= 10 , d(p,N)≈0.52, more than\\nhalfway to the boundary. Hence most data points are closer to the boundary\\nof the sample space than to any other data point. The reason that this\\npresents a problem is that prediction is much more diﬃcult near the edges\\nof the training sample. One must extrapolate from neighboring sample\\npoints rather than interpolate between them.\\nAnother manifestation of the curse is that the sampling density is pro-\\nportional to N1/p, where pis the dimension of the input space and Nis the\\nsample size. Thus, if N1= 100 represents a dense sample for a single input\\nproblem, then N10= 10010is the sample size required for the same sam-\\npling density with 10 inputs. Thus in high dimensions all feasible training\\nsamples sparsely populate the input space.\\nLet us construct another uniform example. Suppose we have 1000 train-\\ning examples xigenerated uniformly on [ −1,1]p. Assume that the true\\nrelationship between XandYis\\nY=f(X) =e−8||X||2,\\nwithout any measurement error. We use the 1-nearest-neighbor rule to\\npredict y0at the test-point x0= 0. Denote the training set by T. We can'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 42}, page_content='24 2. Overview of Supervised Learning\\ncompute the expected prediction error at x0for our procedure, averaging\\nover all such samples of size 1000. Since the problem is deterministic, this\\nis the mean squared error (MSE) for estimating f(0):\\nMSE( x0) = E T[f(x0)−ˆy0]2\\n= E T[ˆy0−ET(ˆy0)]2+ [ET(ˆy0)−f(x0)]2\\n= Var T(ˆy0) + Bias2(ˆy0). (2.25)\\nFigure 2.7 illustrates the setup. We have broken down the MSE into two\\ncomponents that will become familiar as we proceed: variance and squared\\nbias. Such a decomposition is always possible and often useful, and is known\\nas the bias–variance decomposition . Unless the nearest neighbor is at 0,\\nˆy0will be smaller than f(0) in this example, and so the average estimate\\nwill be biased downward. The variance is due to the sampling variance of\\nthe 1-nearest neighbor. In low dimensions and with N= 1000, the nearest\\nneighbor is very close to 0, and so both the bias and variance are small. As\\nthe dimension increases, the nearest neighbor tends to stray further from\\nthe target point, and both bias and variance are incurred. By p= 10, for\\nmore than 99% of the samples the nearest neighbor is a distance greater\\nthan 0 .5 from the origin. Thus as pincreases, the estimate tends to be 0\\nmore often than not, and hence the MSE levels oﬀ at 1 .0, as does the bias,\\nand the variance starts dropping (an artifact of this example).\\nAlthough this is a highly contrived example, similar phenomena occur\\nmore generally. The complexity of functions of many variables can grow\\nexponentially with the dimension, and if we wish to be able to estimate\\nsuch functions with the same accuracy as function in low dimensions, then\\nwe need the size of our training set to grow exponentially as well. In this\\nexample, the function is a complex interaction of all pvariables involved.\\nThe dependence of the bias term on distance depends on the truth, and\\nit need not always dominate with 1-nearest neighbor. For example, if the\\nfunction always involves only a few dimensions as in Figure 2.8, then the\\nvariance can dominate instead.\\nSuppose, on the other hand, that we know that the relationship between\\nYandXis linear,\\nY=XTβ+ε, (2.26)\\nwhere ε∼N(0,σ2) and we ﬁt the model by least squares to the train-\\ning data. For an arbitrary test point x0, we have ˆ y0=xT\\n0ˆβ, which can\\nbe written as ˆ y0=xT\\n0β+∑N\\ni=1ℓi(x0)εi, where ℓi(x0) is the ith element\\nofX(XTX)−1x0. Since under this model the least squares estimates are'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 43}, page_content='2.5 Local Methods in High Dimensions 25\\nXf(X)\\n-1.0 -0.5 0.0 0.5 1.00.0 0.2 0.4 0.6 0.8 1.0•1-NN in One Dimension\\nX1X2\\n-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•••\\n•••\\n••\\n•\\n•••\\n•••\\n••\\n••\\n•\\n•\\n•••\\n••1-NN in One vs. Two Dimensions\\nDimensionAverage Distance to Nearest Neighbor\\n2 4 6 8 100.0 0.2 0.4 0.6 0.8••••••••••Distance to 1-NN vs. Dimension\\nDimensionMse\\n2 4 6 8 100.0 0.2 0.4 0.6 0.8 1.0• •••••••••\\n• •••••••• • • •••••••••MSE vs. Dimension\\n•MSE\\n•Variance\\n•Sq. Bias\\nFIGURE 2.7. A simulation example, demonstrating the curse of dimensional-\\nity and its eﬀect on MSE, bias and variance. The input features are u niformly\\ndistributed in [−1,1]pforp= 1, . . . ,10The top left panel shows the target func-\\ntion (no noise) in I R:f(X) =e−8||X||2, and demonstrates the error that 1-nearest\\nneighbor makes in estimating f(0). The training point is indicated by the blue tick\\nmark. The top right panel illustrates why the radius of the 1-nearest neighborhood\\nincreases with dimension p. The lower left panel shows the average radius of the\\n1-nearest neighborhoods. The lower-right panel shows the MSE, sq uared bias and\\nvariance curves as a function of dimension p.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 44}, page_content='26 2. Overview of Supervised Learning\\nXf(X)\\n-1.0 -0.5 0.0 0.5 1.00 1 2 3 4•1-NN in One Dimension\\nDimensionMSE\\n2 4 6 8 100.0 0.05 0.10 0.15 0.20 0.25••••••••••\\n••••••••••\\n• •• • ••••••MSE  vs. Dimension\\n•MSE\\n•Variance\\n•Sq. Bias\\nFIGURE 2.8. A simulation example with the same setup as in Figure 2.7. Here\\nthe function is constant in all but one dimension: F(X) =1\\n2(X1+ 1)3. The\\nvariance dominates.\\nunbiased, we ﬁnd that\\nEPE(x0) = E y0|x0ET(y0−ˆy0)2\\n= Var( y0|x0) + E T[ˆy0−ETˆy0]2+ [ETˆy0−xT\\n0β]2\\n= Var( y0|x0) + Var T(ˆy0) + Bias2(ˆy0)\\n=σ2+ ETxT\\n0(XTX)−1x0σ2+ 02. (2.27)\\nHere we have incurred an additional variance σ2in the prediction error,\\nsince our target is not deterministic. There is no bias, and the variance\\ndepends on x0. IfNis large and Twere selected at random, and assuming\\nE(X) = 0, then XTX→NCov(X) and\\nEx0EPE(x0)∼Ex0xT\\n0Cov(X)−1x0σ2/N+σ2\\n= trace[Cov( X)−1Cov(x0)]σ2/N+σ2\\n=σ2(p/N) +σ2. (2.28)\\nHere we see that the expected EPE increases linearly as a function of p,\\nwith slope σ2/N. IfNis large and/or σ2is small, this growth in vari-\\nance is negligible (0 in the deterministic case). By imposing some heavy\\nrestrictions on the class of models being ﬁtted, we have avoided the curse\\nof dimensionality. Some of the technical details in (2.27) and (2.28) are\\nderived in Exercise 2.5.\\nFigure 2.9 compares 1-nearest neighbor vs. least squares in two situa-\\ntions, both of which have the form Y=f(X) +ε,Xuniform as before,\\nandε∼N(0,1). The sample size is N= 500. For the orange curve, f(x)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 45}, page_content='2.5 Local Methods in High Dimensions 27\\nDimensionEPE Ratio\\n2 4 6 8 101.6 1.7 1.8 1.9 2.0 2.1••••••••••••••••••••Expected Prediction Error of 1NN vs. OLS\\n•Linear\\n•Cubic\\nFIGURE 2.9. The curves show the expected prediction error (at x0= 0) for\\n1-nearest neighbor relative to least squares for the model Y=f(X) +ε. For the\\norange curve, f(x) =x1, while for the blue curve f(x) =1\\n2(x1+ 1)3.\\nis linear in the ﬁrst coordinate, for the blue curve, cubic as in Figure 2.8.\\nShown is the relative EPE of 1-nearest neighbor to least squares, which\\nappears to start at around 2 for the linear case. Least squares is unbiased\\nin this case, and as discussed above the EPE is slightly above σ2= 1.\\nThe EPE for 1-nearest neighbor is always above 2, since the variance of\\nˆf(x0) in this case is at least σ2, and the ratio increases with dimension as\\nthe nearest neighbor strays from the target point. For the cubic case, least\\nsquares is biased, which moderates the ratio. Clearly we could manufacture\\nexamples where the bias of least squares would dominate the variance, and\\nthe 1-nearest neighbor would come out the winner.\\nBy relying on rigid assumptions, the linear model has no bias at all and\\nnegligible variance, while the error in 1-nearest neighbor is substantially\\nlarger. However, if the assumptions are wrong, all bets are oﬀ and the\\n1-nearest neighbor may dominate. We will see that there is a whole spec-\\ntrum of models between the rigid linear models and the extremely ﬂexible\\n1-nearest-neighbor models, each with their own assumptions and biases,\\nwhich have been proposed speciﬁcally to avoid the exponential growth in\\ncomplexity of functions in high dimensions by drawing heavily on these\\nassumptions.\\nBefore we delve more deeply, let us elaborate a bit on the concept of\\nstatistical models and see how they ﬁt into the prediction framework.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 46}, page_content='28 2. Overview of Supervised Learning\\n2.6 Statistical Models, Supervised Learning and\\nFunction Approximation\\nOur goal is to ﬁnd a useful approximation ˆf(x) to the function f(x) that\\nunderlies the predictive relationship between the inputs and outputs. In the\\ntheoretical setting of Section 2.4, we saw that squared error loss lead us\\nto the regression function f(x) = E( Y|X=x) for a quantitative response.\\nThe class of nearest-neighbor methods can be viewed as direct estimates\\nof this conditional expectation, but we have seen that they can fail in at\\nleast two ways:\\n•if the dimension of the input space is high, the nearest neighbors need\\nnot be close to the target point, and can result in large errors;\\n•if special structure is known to exist, this can be used to reduce both\\nthe bias and the variance of the estimates.\\nWe anticipate using other classes of models for f(x), in many cases specif-\\nically designed to overcome the dimensionality problems, and here we dis-\\ncuss a framework for incorporating them into the prediction problem.\\n2.6.1 A Statistical Model for the Joint Distribution Pr(X,Y)\\nSuppose in fact that our data arose from a statistical model\\nY=f(X) +ε, (2.29)\\nwhere the random error εhas E( ε) = 0 and is independent of X. Note that\\nfor this model, f(x) = E( Y|X=x), and in fact the conditional distribution\\nPr(Y|X) depends on Xonlythrough the conditional mean f(x).\\nThe additive error model is a useful approximation to the truth. For\\nmost systems the input–output pairs ( X,Y) will not have a deterministic\\nrelationship Y=f(X). Generally there will be other unmeasured variables\\nthat also contribute to Y, including measurement error. The additive model\\nassumes that we can capture all these departures from a deterministic re-\\nlationship via the error ε.\\nFor some problems a deterministic relationship does hold. Many of the\\nclassiﬁcation problems studied in machine learning are of this form, where\\nthe response surface can be thought of as a colored map deﬁned in IRp.\\nThe training data consist of colored examples from the map {xi,gi}, and\\nthe goal is to be able to color any point. Here the function is deterministic,\\nand the randomness enters through the xlocation of the training points.\\nFor the moment we will not pursue such problems, but will see that they\\ncan be handled by techniques appropriate for the error-based models.\\nThe assumption in (2.29) that the errors are independent and identically\\ndistributed is not strictly necessary, but seems to be at the back of our mind'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 47}, page_content='2.6 Statistical Models, Supervised Learning and Function Approxi mation 29\\nwhen we average squared errors uniformly in our EPE criterion. With such\\na model it becomes natural to use least squares as a data criterion for\\nmodel estimation as in (2.1). Simple modiﬁcations can be made to avoid\\nthe independence assumption; for example, we can have Var( Y|X=x) =\\nσ(x), and now both the mean and variance depend on X. In general the\\nconditional distribution Pr( Y|X) can depend on Xin complicated ways,\\nbut the additive error model precludes these.\\nSo far we have concentrated on the quantitative response. Additive error\\nmodels are typically not used for qualitative outputs G; in this case the tar-\\nget function p(X)isthe conditional density Pr( G|X), and this is modeled\\ndirectly. For example, for two-class data, it is often reasonable to assume\\nthat the data arise from independent binary trials, with the probability of\\none particular outcome being p(X), and the other 1 −p(X). Thus if Yis\\nthe 0–1 coded version of G, then E( Y|X=x) =p(x), but the variance\\ndepends on xas well: Var( Y|X=x) =p(x)[1−p(x)].\\n2.6.2 Supervised Learning\\nBefore we launch into more statistically oriented jargon, we present the\\nfunction-ﬁtting paradigm from a machine learning point of view. Suppose\\nfor simplicity that the errors are additive and that the model Y=f(X)+ε\\nis a reasonable assumption. Supervised learning attempts to learn fby\\nexample through a teacher . One observes the system under study, both\\nthe inputs and outputs, and assembles a training set of observations T=\\n(xi,yi), i= 1,... ,N . The observed input values to the system xiare also\\nfed into an artiﬁcial system, known as a learning algorithm (usually a com-\\nputer program), which also produces outputs ˆf(xi) in response to the in-\\nputs. The learning algorithm has the property that it can modify its in-\\nput/output relationship ˆfin response to diﬀerences yi−ˆf(xi) between the\\noriginal and generated outputs. This process is known as learning by exam-\\nple. Upon completion of the learning process the hope is that the artiﬁcial\\nand real outputs will be close enough to be useful for all sets of inputs likely\\nto be encountered in practice.\\n2.6.3 Function Approximation\\nThe learning paradigm of the previous section has been the motivation\\nfor research into the supervised learning problem in the ﬁelds of machine\\nlearning (with analogies to human reasoning) and neural networks (with\\nbiological analogies to the brain). The approach taken in applied mathe-\\nmatics and statistics has been from the perspective of function approxima-\\ntion and estimation. Here the data pairs {xi,yi}are viewed as points in a\\n(p+ 1)-dimensional Euclidean space. The function f(x) has domain equal\\nto the p-dimensional input subspace, and is related to the data via a model'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 48}, page_content='30 2. Overview of Supervised Learning\\nsuch as yi=f(xi) +εi. For convenience in this chapter we will assume the\\ndomain is IRp, ap-dimensional Euclidean space, although in general the\\ninputs can be of mixed type. The goal is to obtain a useful approximation\\ntof(x) for all xin some region of IRp, given the representations in T.\\nAlthough somewhat less glamorous than the learning paradigm, treating\\nsupervised learning as a problem in function approximation encourages the\\ngeometrical concepts of Euclidean spaces and mathematical concepts of\\nprobabilistic inference to be applied to the problem. This is the approach\\ntaken in this book.\\nMany of the approximations we will encounter have associated a set of\\nparameters θthat can be modiﬁed to suit the data at hand. For example,\\nthe linear model f(x) =xTβhasθ=β. Another class of useful approxi-\\nmators can be expressed as linear basis expansions\\nfθ(x) =K∑\\nk=1hk(x)θk, (2.30)\\nwhere the hkare a suitable set of functions or transformations of the input\\nvector x. Traditional examples are polynomial and trigonometric expan-\\nsions, where for example hkmight be x2\\n1,x1x2\\n2, cos(x1) and so on. We\\nalso encounter nonlinear expansions, such as the sigmoid transformation\\ncommon to neural network models,\\nhk(x) =1\\n1 + exp( −xTβk). (2.31)\\nWe can use least squares to estimate the parameters θinfθas we did\\nfor the linear model, by minimizing the residual sum-of-squares\\nRSS(θ) =N∑\\ni=1(yi−fθ(xi))2(2.32)\\nas a function of θ. This seems a reasonable criterion for an additive error\\nmodel. In terms of function approximation, we imagine our parameterized\\nfunction as a surface in p+ 1 space, and what we observe are noisy re-\\nalizations from it. This is easy to visualize when p= 2 and the vertical\\ncoordinate is the output y, as in Figure 2.10. The noise is in the output\\ncoordinate, so we ﬁnd the set of parameters such that the ﬁtted surface\\ngets as close to the observed points as possible, where close is measured by\\nthe sum of squared vertical errors in RSS( θ).\\nFor the linear model we get a simple closed form solution to the mini-\\nmization problem. This is also true for the basis function methods, if the\\nbasis functions themselves do not have any hidden parameters. Otherwise\\nthe solution requires either iterative methods or numerical optimization.\\nWhile least squares is generally very convenient, it is not the only crite-\\nrion used and in some cases would not make much sense. A more general'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 49}, page_content='2.6 Statistical Models, Supervised Learning and Function Approxi mation 31\\n•••\\n•••\\n••••\\n••\\n•••\\n••••\\n•••\\n••\\n•••\\n•\\n•\\n••\\n•\\n•• ••••\\n••\\n•••\\n•••\\n•\\n••\\n••\\n•••\\n•\\n••\\n•••\\n•\\n•••\\n••••\\n•••\\n•\\n••\\nFIGURE 2.10. Least squares ﬁtting of a function of two inputs. The parameters\\noffθ(x)are chosen so as to minimize the sum-of-squared vertical erro rs.\\nprinciple for estimation is maximum likelihood estimation . Suppose we have\\na random sample yi, i= 1,... ,N from a density Pr θ(y) indexed by some\\nparameters θ. The log-probability of the observed sample is\\nL(θ) =N∑\\ni=1log Pr θ(yi). (2.33)\\nThe principle of maximum likelihood assumes that the most reasonable\\nvalues for θare those for which the probability of the observed sample is\\nlargest. Least squares for the additive error model Y=fθ(X) +ε, with\\nε∼N(0,σ2), is equivalent to maximum likelihood using the conditional\\nlikelihood\\nPr(Y|X,θ) =N(fθ(X),σ2). (2.34)\\nSo although the additional assumption of normality seems more restrictive,\\nthe results are the same. The log-likelihood of the data is\\nL(θ) =−N\\n2log(2π)−Nlogσ−1\\n2σ2N∑\\ni=1(yi−fθ(xi))2, (2.35)\\nand the only term involving θis the last, which is RSS( θ) up to a scalar\\nnegative multiplier.\\nA more interesting example is the multinomial likelihood for the regres-\\nsion function Pr( G|X) for a qualitative output G. Suppose we have a model\\nPr(G=Gk|X=x) =pk,θ(x), k= 1,... ,K for the conditional probabil-\\nity of each class given X, indexed by the parameter vector θ. Then the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 50}, page_content='32 2. Overview of Supervised Learning\\nlog-likelihood (also referred to as the cross-entropy) is\\nL(θ) =N∑\\ni=1logpgi,θ(xi), (2.36)\\nand when maximized it delivers values of θthat best conform with the data\\nin this likelihood sense.\\n2.7 Structured Regression Models\\nWe have seen that although nearest-neighbor and other local methods focus\\ndirectly on estimating the function at a point, they face problems in high\\ndimensions. They may also be inappropriate even in low dimensions in\\ncases where more structured approaches can make more eﬃcient use of the\\ndata. This section introduces classes of such structured approaches. Before\\nwe proceed, though, we discuss further the need for such classes.\\n2.7.1 Diﬃculty of the Problem\\nConsider the RSS criterion for an arbitrary function f,\\nRSS(f) =N∑\\ni=1(yi−f(xi))2. (2.37)\\nMinimizing (2.37) leads to inﬁnitely many solutions: any function ˆfpassing\\nthrough the training points ( xi,yi) is a solution. Any particular solution\\nchosen might be a poor predictor at test points diﬀerent from the training\\npoints. If there are multiple observation pairs xi,yiℓ, ℓ= 1,... ,N iat each\\nvalue of xi, the risk is limited. In this case, the solutions pass through\\nthe average values of the yiℓat each xi; see Exercise 2.6. The situation is\\nsimilar to the one we have already visited in Section 2.4; indeed, (2.37) is\\nthe ﬁnite sample version of (2.11) on page 18. If the sample size Nwere\\nsuﬃciently large such that repeats were guaranteed and densely arranged,\\nit would seem that these solutions might all tend to the limiting conditional\\nexpectation.\\nIn order to obtain useful results for ﬁnite N, we must restrict the eligible\\nsolutions to (2.37) to a smaller set of functions. How to decide on the\\nnature of the restrictions is based on considerations outside of the data.\\nThese restrictions are sometimes encoded via the parametric representation\\noffθ, or may be built into the learning method itself, either implicitly or\\nexplicitly. These restricted classes of solutions are the major topic of this\\nbook. One thing should be clear, though. Any restrictions imposed on f\\nthat lead to a unique solution to (2.37) do not really remove the ambiguity'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 51}, page_content='2.8 Classes of Restricted Estimators 33\\ncaused by the multiplicity of solutions. There are inﬁnitely many possible\\nrestrictions, each leading to a unique solution, so the ambiguity has simply\\nbeen transferred to the choice of constraint.\\nIn general the constraints imposed by most learning methods can be\\ndescribed as complexity restrictions of one kind or another. This usually\\nmeans some kind of regular behavior in small neighborhoods of the input\\nspace. That is, for all input points xsuﬃciently close to each other in\\nsome metric, ˆfexhibits some special structure such as nearly constant,\\nlinear or low-order polynomial behavior. The estimator is then obtained by\\naveraging or polynomial ﬁtting in that neighborhood.\\nThe strength of the constraint is dictated by the neighborhood size. The\\nlarger the size of the neighborhood, the stronger the constraint, and the\\nmore sensitive the solution is to the particular choice of constraint. For\\nexample, local constant ﬁts in inﬁnitesimally small neighborhoods is no\\nconstraint at all; local linear ﬁts in very large neighborhoods is almost a\\nglobally linear model, and is very restrictive.\\nThe nature of the constraint depends on the metric used. Some methods,\\nsuch as kernel and local regression and tree-based methods, directly specify\\nthe metric and size of the neighborhood. The nearest-neighbor methods\\ndiscussed so far are based on the assumption that locally the function is\\nconstant; close to a target input x0, the function does not change much, and\\nso close outputs can be averaged to produce ˆf(x0). Other methods such\\nas splines, neural networks and basis-function methods implicitly deﬁne\\nneighborhoods of local behavior. In Section 5.4.1 we discuss the concept\\nof anequivalent kernel (see Figure 5.8 on page 157), which describes this\\nlocal dependence for any method linear in the outputs. These equivalent\\nkernels in many cases look just like the explicitly deﬁned weighting kernels\\ndiscussed above—peaked at the target point and falling away smoothly\\naway from it.\\nOne fact should be clear by now. Any method that attempts to pro-\\nduce locally varying functions in small isotropic neighborhoods will run\\ninto problems in high dimensions—again the curse of dimensionality. And\\nconversely, all methods that overcome the dimensionality problems have an\\nassociated—and often implicit or adaptive—metric for measuring neighbor-\\nhoods, which basically does not allow the neighborhood to be simultane-\\nously small in all directions.\\n2.8 Classes of Restricted Estimators\\nThe variety of nonparametric regression techniques or learning methods fall\\ninto a number of diﬀerent classes depending on the nature of the restrictions\\nimposed. These classes are not distinct, and indeed some methods fall in\\nseveral classes. Here we give a brief summary, since detailed descriptions'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 52}, page_content='34 2. Overview of Supervised Learning\\nare given in later chapters. Each of the classes has associated with it one\\nor more parameters, sometimes appropriately called smoothing parameters,\\nthat control the eﬀective size of the local neighborhood. Here we describe\\nthree broad classes.\\n2.8.1 Roughness Penalty and Bayesian Methods\\nHere the class of functions is controlled by explicitly penalizing RSS( f)\\nwith a roughness penalty\\nPRSS( f;λ) = RSS( f) +λJ(f). (2.38)\\nThe user-selected functional J(f) will be large for functions fthat vary too\\nrapidly over small regions of input space. For example, the popular cubic\\nsmoothing spline for one-dimensional inputs is the solution to the penalized\\nleast-squares criterion\\nPRSS( f;λ) =N∑\\ni=1(yi−f(xi))2+λ∫\\n[f′′(x)]2dx. (2.39)\\nThe roughness penalty here controls large values of the second derivative\\noff, and the amount of penalty is dictated by λ≥0. For λ= 0 no penalty\\nis imposed, and any interpolating function will do, while for λ=∞only\\nfunctions linear in xare permitted.\\nPenalty functionals Jcan be constructed for functions in any dimension,\\nand special versions can be created to impose special structure. For ex-\\nample, additive penalties J(f) =∑p\\nj=1J(fj) are used in conjunction with\\nadditive functions f(X) =∑p\\nj=1fj(Xj) to create additive models with\\nsmooth coordinate functions. Similarly, projection pursuit regression mod-\\nels have f(X) =∑M\\nm=1gm(αT\\nmX) for adaptively chosen directions αm, and\\nthe functions gmcan each have an associated roughness penalty.\\nPenalty function, or regularization methods, express our prior belief that\\nthe type of functions we seek exhibit a certain type of smooth behavior, and\\nindeed can usually be cast in a Bayesian framework. The penalty Jcorre-\\nsponds to a log-prior, and PRSS( f;λ) the log-posterior distribution, and\\nminimizing PRSS( f;λ) amounts to ﬁnding the posterior mode. We discuss\\nroughness-penalty approaches in Chapter 5 and the Bayesian paradigm in\\nChapter 8.\\n2.8.2 Kernel Methods and Local Regression\\nThese methods can be thought of as explicitly providing estimates of the re-\\ngression function or conditional expectation by specifying the nature of the\\nlocal neighborhood, and of the class of regular functions ﬁtted locally. The\\nlocal neighborhood is speciﬁed by a kernel function Kλ(x0,x) which assigns'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 53}, page_content='2.8 Classes of Restricted Estimators 35\\nweights to points xin a region around x0(see Figure 6.1 on page 192). For\\nexample, the Gaussian kernel has a weight function based on the Gaussian\\ndensity function\\nKλ(x0,x) =1\\nλexp[\\n−||x−x0||2\\n2λ]\\n(2.40)\\nand assigns weights to points that die exponentially with their squared\\nEuclidean distance from x0. The parameter λcorresponds to the variance\\nof the Gaussian density, and controls the width of the neighborhood. The\\nsimplest form of kernel estimate is the Nadaraya–Watson weighted averag e\\nˆf(x0) =∑N\\ni=1Kλ(x0,xi)yi∑N\\ni=1Kλ(x0,xi). (2.41)\\nIn general we can deﬁne a local regression estimate of f(x0) asfˆθ(x0),\\nwhere ˆθminimizes\\nRSS(fθ,x0) =N∑\\ni=1Kλ(x0,xi)(yi−fθ(xi))2, (2.42)\\nandfθis some parameterized function, such as a low-order polynomial.\\nSome examples are:\\n•fθ(x) =θ0, the constant function; this results in the Nadaraya–\\nWatson estimate in (2.41) above.\\n•fθ(x) =θ0+θ1xgives the popular local linear regression model.\\nNearest-neighbor methods can be thought of as kernel methods having a\\nmore data-dependent metric. Indeed, the metric for k-nearest neighbors is\\nKk(x,x0) =I(||x−x0|| ≤ ||x(k)−x0||),\\nwhere x(k)is the training observation ranked kth in distance from x0, and\\nI(S) is the indicator of the set S.\\nThese methods of course need to be modiﬁed in high dimensions, to avoid\\nthe curse of dimensionality. Various adaptations are discussed in Chapter 6.\\n2.8.3 Basis Functions and Dictionary Methods\\nThis class of methods includes the familiar linear and polynomial expan-\\nsions, but more importantly a wide variety of more ﬂexible models. The\\nmodel for fis a linear expansion of basis functions\\nfθ(x) =M∑\\nm=1θmhm(x), (2.43)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 54}, page_content='36 2. Overview of Supervised Learning\\nwhere each of the hmis a function of the input x, and the term linear here\\nrefers to the action of the parameters θ. This class covers a wide variety of\\nmethods. In some cases the sequence of basis functions is prescribed, such\\nas a basis for polynomials in xof total degree M.\\nFor one-dimensional x, polynomial splines of degree Kcan be represented\\nby an appropriate sequence of Mspline basis functions, determined in turn\\nbyM−Kknots. These produce functions that are piecewise polynomials\\nof degree Kbetween the knots, and joined up with continuity of degree\\nK−1 at the knots. As an example consider linear splines, or piecewise\\nlinear functions. One intuitively satisfying basis consists of the functions\\nb1(x) = 1, b2(x) =x, and bm+2(x) = ( x−tm)+,m= 1,... ,M −2,\\nwhere tmis the mth knot, and z+denotes positive part. Tensor products\\nof spline bases can be used for inputs with dimensions larger than one\\n(see Section 5.2, and the CART and MARS models in Chapter 9.) The\\nparameter θcan be the total degree of the polynomial or the number of\\nknots in the case of splines.\\nRadial basis functions are symmetric p-dimensional kernels located at\\nparticular centroids,\\nfθ(x) =M∑\\nm=1Kλm(θm,x)θm; (2.44)\\nfor example, the Gaussian kernel Kλ(θ,x) =e−||x−θ||2/2λis popular.\\nRadial basis functions have centroids θmand scales λmthat have to\\nbe determined. The spline basis functions have knots. In general we would\\nlike the data to dictate them as well. Including these as parameters changes\\nthe regression problem from a straightforward linear problem to a combi-\\nnatorially hard nonlinear problem. In practice, shortcuts such as greedy\\nalgorithms or two stage processes are used. Section 6.7 describes some such\\napproaches.\\nA single-layer feed-forward neural network model with linear output\\nweights can be thought of as an adaptive basis function method. The model\\nhas the form\\nfθ(x) =M∑\\nm=1βmσ(αT\\nmx+bm), (2.45)\\nwhere σ(x) = 1 /(1 +e−x) is known as the activation function. Here, as\\nin the projection pursuit model, the directions αmand the biasterms bm\\nhave to be determined, and their estimation is the meat of the computation.\\nDetails are give in Chapter 11.\\nThese adaptively chosen basis function methods are also known as dictio-\\nnarymethods, where one has available a possibly inﬁnite set or dictionary\\nDof candidate basis functions from which to choose, and models are built\\nup by employing some kind of search mechanism.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 55}, page_content='2.9 Model Selection and the Bias–Variance Tradeoﬀ 37\\n2.9 Model Selection and the Bias–Variance\\nTradeoﬀ\\nAll the models described above and many others discussed in later chapters\\nhave a smoothing orcomplexity parameter that has to be determined:\\n•the multiplier of the penalty term;\\n•the width of the kernel;\\n•or the number of basis functions.\\nIn the case of the smoothing spline, the parameter λindexes models ranging\\nfrom a straight line ﬁt to the interpolating model. Similarly a local degr ee-\\nmpolynomial model ranges between a degree- mglobal polynomial when\\nthe window size is inﬁnitely large, to an interpolating ﬁt when the window\\nsize shrinks to zero. This means that we cannot use residual sum-of-squares\\non the training data to determine these parameters as well, since we would\\nalways pick those that gave interpolating ﬁts and hence zero residuals. Such\\na model is unlikely to predict future data well at all.\\nThek-nearest-neighbor regression ﬁt ˆfk(x0) usefully illustrates the com-\\npeting forces that eﬀect the predictive ability of such approximations. Sup-\\npose the data arise from a model Y=f(X) +ε, with E( ε) = 0 and\\nVar(ε) =σ2. For simplicity here we assume that the values of xiin the\\nsample are ﬁxed in advance (nonrandom). The expected prediction error\\natx0, also known as testorgeneralization error, can be decomposed:\\nEPE k(x0) = E[( Y−ˆfk(x0))2|X=x0]\\n=σ2+ [Bias2(ˆfk(x0)) + Var T(ˆfk(x0))] (2.46)\\n=σ2+[\\nf(x0)−1\\nkk∑\\nℓ=1f(x(ℓ))]2\\n+σ2\\nk. (2.47)\\nThe subscripts in parentheses ( ℓ) indicate the sequence of nearest neighbors\\ntox0.\\nThere are three terms in this expression. The ﬁrst term σ2is the ir-\\nreducible error—the variance of the new test target—and is beyond our\\ncontrol, even if we know the true f(x0).\\nThe second and third terms are under our control, and make up the\\nmean squared error ofˆfk(x0) in estimating f(x0), which is broken down\\ninto a bias component and a variance component. The bias term is the\\nsquared diﬀerence between the true mean f(x0) and the expected value of\\nthe estimate—[E T(ˆfk(x0))−f(x0)]2—where the expectation averages the\\nrandomness in the training data. This term will most likely increase with\\nk, if the true function is reasonably smooth. For small kthe few closest\\nneighbors will have values f(x(ℓ)) close to f(x0), so their average should'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 56}, page_content='38 2. Overview of Supervised Learning\\nHigh Bias\\nLow VarianceLow Bias\\nHigh VariancePrediction Error\\nModel ComplexityTraining SampleTest Sample\\nLow High\\nFIGURE 2.11. Test and training error as a function of model complexity.\\nbe close to f(x0). As kgrows, the neighbors are further away, and then\\nanything can happen.\\nThe variance term is simply the variance of an average here, and de-\\ncreases as the inverse of k. So as kvaries, there is a bias–variance tradeoﬀ.\\nMore generally, as the model complexity of our procedure is increased,\\nthe variance tends to increase and the squared bias tends to decreases.\\nThe opposite behavior occurs as the model complexity is decreased. For\\nk-nearest neighbors, the model complexity is controlled by k.\\nTypically we would like to choose our model complexity to trade bias\\noﬀ with variance in such a way as to minimize the test error. An obvious\\nestimate of test error is the training error1\\nN∑\\ni(yi−ˆyi)2. Unfortunately\\ntraining error is not a good estimate of test error, as it does not properly\\naccount for model complexity.\\nFigure 2.11 shows the typical behavior of the test and training error, as\\nmodel complexity is varied. The training error tends to decrease whenever\\nwe increase the model complexity, that is, whenever we ﬁt the data harder.\\nHowever with too much ﬁtting, the model adapts itself too closely to the\\ntraining data, and will not generalize well (i.e., have large test error). In\\nthat case the predictions ˆf(x0) will have large variance, as reﬂected in the\\nlast term of expression (2.46). In contrast, if the model is not complex\\nenough, it will underﬁt and may have large bias, again resulting in poor\\ngeneralization. In Chapter 7 we discuss methods for estimating the test\\nerror of a prediction method, and hence estimating the optimal amount of\\nmodel complexity for a given prediction method and training set.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 57}, page_content='Exercises 39\\nBibliographic Notes\\nSome good general books on the learning problem are Duda et al. (2000),\\nBishop (1995),(Bishop, 2006), Ripley (1996), Cherkassky and Mulier (2 007)\\nand Vapnik (1996). Parts of this chapter are based on Friedman (1994b).\\nExercises\\nEx. 2.1 Suppose each of K-classes has an associated target tk, which is a\\nvector of all zeros, except a one in the kth position. Show that classifying to\\nthe largest element of ˆ yamounts to choosing the closest target, min k||tk−\\nˆy||, if the elements of ˆ ysum to one.\\nEx. 2.2 Show how to compute the Bayes decision boundary for the simula-\\ntion example in Figure 2.5.\\nEx. 2.3 Derive equation (2.24).\\nEx. 2.4 The edge eﬀect problem discussed on page 23 is not peculiar to\\nuniform sampling from bounded domains. Consider inputs drawn from a\\nspherical multinormal distribution X∼N(0,Ip). The squared distance\\nfrom any sample point to the origin has a χ2\\npdistribution with mean p.\\nConsider a prediction point x0drawn from this distribution, and let a=\\nx0/||x0||be an associated unit vector. Let zi=aTxibe the projection of\\neach of the training points on this direction.\\nShow that the ziare distributed N(0,1) with expected squared distance\\nfrom the origin 1, while the target point has expected squared distance p\\nfrom the origin.\\nHence for p= 10, a randomly drawn test point is about 3 .1 standard\\ndeviations from the origin, while all the training points are on average\\none standard deviation along direction a. So most prediction points see\\nthemselves as lying on the edge of the training set.\\nEx. 2.5\\n(a) Derive equation (2.27). The last line makes use of (3.8) through a\\nconditioning argument.\\n(b) Derive equation (2.28), making use of the cyclic property of the trace\\noperator [trace( AB) = trace( BA)], and its linearity (which allows us\\nto interchange the order of trace and expectation).\\nEx. 2.6 Consider a regression problem with inputs xiand outputs yi, and a\\nparameterized model fθ(x) to be ﬁt by least squares. Show that if there are\\nobservations with tiedoridentical values of x, then the ﬁt can be obtained\\nfrom a reduced weighted least squares problem.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 58}, page_content='40 2. Overview of Supervised Learning\\nEx. 2.7 Suppose we have a sample of Npairs xi,yidrawn i.i.d. from the\\ndistribution characterized as follows:\\nxi∼h(x),the design density\\nyi=f(xi) +εi, fis the regression function\\nεi∼(0,σ2) (mean zero, variance σ2)\\nWe construct an estimator for flinear in the yi,\\nˆf(x0) =N∑\\ni=1ℓi(x0;X)yi,\\nwhere the weights ℓi(x0;X) do not depend on the yi, but do depend on the\\nentire training sequence of xi, denoted here by X.\\n(a) Show that linear regression and k-nearest-neighbor regression are mem-\\nbers of this class of estimators. Describe explicitly the weights ℓi(x0;X)\\nin each of these cases.\\n(b) Decompose the conditional mean-squared error\\nEY|X(f(x0)−ˆf(x0))2\\ninto a conditional squared bias and a conditional variance component.\\nLikeX,Yrepresents the entire training sequence of yi.\\n(c) Decompose the (unconditional) mean-squared error\\nEY,X(f(x0)−ˆf(x0))2\\ninto a squared bias and a variance component.\\n(d) Establish a relationship between the squared biases and variances in\\nthe above two cases.\\nEx. 2.8 Compare the classiﬁcation performance of linear regression and k–\\nnearest neighbor classiﬁcation on the zipcode data. In particular, consider\\nonly the 2’s and3’s, and k= 1,3,5,7 and 15. Show both the training and\\ntest error for each choice. The zipcode data are available from the book\\nwebsitewww-stat.stanford.edu/ElemStatLearn .\\nEx. 2.9 Consider a linear regression model with pparameters, ﬁt by least\\nsquares to a set of training data ( x1,y1),... ,(xN,yN) drawn at random\\nfrom a population. Let ˆβbe the least squares estimate. Suppose we have\\nsome test data (˜ x1,˜y1),... ,(˜xM,˜yM) drawn at random from the same pop-\\nulation as the training data. If Rtr(β) =1\\nN∑N\\n1(yi−βTxi)2andRte(β) =\\n1\\nM∑M\\n1(˜yi−βT˜xi)2, prove that\\nE[Rtr(ˆβ)]≤E[Rte(ˆβ)],'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 59}, page_content='Exercises 41\\nwhere the expectations are over all that is random in each expression. [This\\nexercise was brought to our attention by Ryan Tibshirani, from a homework\\nassignment given by Andrew Ng.]'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 60}, page_content='42 2. Overview of Supervised Learning'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 61}, page_content='This is page 43\\nPrinter: Opaque this\\n3\\nLinear Methods for Regression\\n3.1 Introduction\\nA linear regression model assumes that the regression function E( Y|X) is\\nlinear in the inputs X1,... ,X p. Linear models were largely developed in\\nthe precomputer age of statistics, but even in today’s computer era there\\nare still good reasons to study and use them. They are simple and often\\nprovide an adequate and interpretable description of how the inputs aﬀect\\nthe output. For prediction purposes they can sometimes outperform fancier\\nnonlinear models, especially in situations with small numbers of training\\ncases, low signal-to-noise ratio or sparse data. Finally, linear methods can be\\napplied to transformations of the inputs and this considerably expands their\\nscope. These generalizations are sometimes called basis-function methods,\\nand are discussed in Chapter 5.\\nIn this chapter we describe linear methods for regression, while in the\\nnext chapter we discuss linear methods for classiﬁcation. On some topics we\\ngo into considerable detail, as it is our ﬁrm belief that an understanding\\nof linear methods is essential for understanding nonlinear ones. In fact,\\nmany nonlinear techniques are direct generalizations of the linear methods\\ndiscussed here.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 62}, page_content='44 3. Linear Methods for Regression\\n3.2 Linear Regression Models and Least Squares\\nAs introduced in Chapter 2, we have an input vector XT= (X1,X2,... ,X p),\\nand want to predict a real-valued output Y. The linear regression model\\nhas the form\\nf(X) =β0+p∑\\nj=1Xjβj. (3.1)\\nThe linear model either assumes that the regression function E( Y|X) is\\nlinear, or that the linear model is a reasonable approximation. Here the\\nβj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\\nfrom diﬀerent sources:\\n•quantitative inputs;\\n•transformations of quantitative inputs, such as log, square-root or\\nsquare;\\n•basis expansions, such as X2=X2\\n1,X3=X3\\n1, leading to a polynomial\\nrepresentation;\\n•numeric or “dummy” coding of the levels of qualitative inputs. For\\nexample, if Gis a ﬁve-level factor input, we might create Xj, j=\\n1,... ,5,such that Xj=I(G=j). Together this group of Xjrepre-\\nsents the eﬀect of Gby a set of level-dependent constants, since in∑5\\nj=1Xjβj, one of the Xjs is one, and the others are zero.\\n•interactions between variables, for example, X3=X1≤X2.\\nNo matter the source of the Xj, the model is linear in the parameters.\\nTypically we have a set of training data ( x1,y1)...(xN,yN) from which\\nto estimate the parameters β. Each xi= (xi1,xi2,... ,x ip)Tis a vector\\nof feature measurements for the ith case. The most popular estimation\\nmethod is least squares , in which we pick the coeﬃcients β= (β0,β1,... ,β p)T\\nto minimize the residual sum of squares\\nRSS(β) =N∑\\ni=1(yi−f(xi))2\\n=N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\n. (3.2)\\nFrom a statistical point of view, this criterion is reasonable if the tr aining\\nobservations ( xi,yi) represent independent random draws from their popu-\\nlation. Even if the xi’s were not drawn randomly, the criterion is still valid\\nif the yi’s are conditionally independent given the inputs xi. Figure 3.1\\nillustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 63}, page_content='3.2 Linear Regression Models and Least Squares 45\\n•• •\\n••••\\n••••\\n•••\\n••\\n•••\\n•\\n••\\n••\\n•••\\n••••••\\n••••\\n•••\\n••\\n••\\n•\\n••\\n••\\n••••\\n••\\n••\\n•\\n•••\\n•\\n••••\\n••••\\n••\\n••\\nX1X2Y\\nFIGURE 3.1. Linear least squares ﬁtting with X∈I R2. We seek the linear\\nfunction of Xthat minimizes the sum of squared residuals from Y.\\nspace occupied by the pairs ( X,Y). Note that (3.2) makes no assumptions\\nabout the validity of model (3.1); it simply ﬁnds the best linear ﬁt to the\\ndata. Least squares ﬁtting is intuitively satisfying no matter how the data\\narise; the criterion measures the average lack of ﬁt.\\nHow do we minimize (3.2)? Denote by XtheN×(p+ 1) matrix with\\neach row an input vector (with a 1 in the ﬁrst position), and similarly let\\nybe the N-vector of outputs in the training set. Then we can write the\\nresidual sum-of-squares as\\nRSS(β) = (y−Xβ)T(y−Xβ). (3.3)\\nThis is a quadratic function in the p+ 1 parameters. Diﬀerentiating with\\nrespect to βwe obtain\\n∂RSS\\n∂β=−2XT(y−Xβ)\\n∂2RSS\\n∂β∂βT= 2XTX.(3.4)\\nAssuming (for the moment) that Xhas full column rank, and hence XTX\\nis positive deﬁnite, we set the ﬁrst derivative to zero\\nXT(y−Xβ) = 0 (3.5)\\nto obtain the unique solution\\nˆβ= (XTX)−1XTy. (3.6)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 64}, page_content='46 3. Linear Methods for Regression\\nx1x2y\\nˆ y\\nFIGURE 3.2. TheN-dimensional geometry of least squares regression with two\\npredictors. The outcome vector yis orthogonally projected onto the hyperplane\\nspanned by the input vectors x1andx2. The projection ˆyrepresents the vector\\nof the least squares predictions\\nThe predicted values at an input vector x0are given by ˆf(x0) = (1 : x0)Tˆβ;\\nthe ﬁtted values at the training inputs are\\nˆy=Xˆβ=X(XTX)−1XTy, (3.7)\\nwhere ˆ yi=ˆf(xi). The matrix H=X(XTX)−1XTappearing in equation\\n(3.7) is sometimes called the “hat” matrix because it puts the hat on y.\\nFigure 3.2 shows a diﬀerent geometrical representation of the least squares\\nestimate, this time in IRN. We denote the column vectors of Xbyx0,x1,... ,xp,\\nwithx0≡1. For much of what follows, this ﬁrst column is treated like any\\nother. These vectors span a subspace of IRN, also referred to as the column\\nspace of X. We minimize RSS( β) =∥y−Xβ∥2by choosing ˆβso that the\\nresidual vector y−ˆyis orthogonal to this subspace. This orthogonality is\\nexpressed in (3.5), and the resulting estimate ˆyis hence the orthogonal pro-\\njection ofyonto this subspace. The hat matrix Hcomputes the orthogonal\\nprojection, and hence it is also known as a projection matrix.\\nIt might happen that the columns of Xare not linearly independent, so\\nthatXis not of full rank. This would occur, for example, if two of the\\ninputs were perfectly correlated, (e.g., x2= 3x1). Then XTXis singular\\nand the least squares coeﬃcients ˆβare not uniquely deﬁned. However,\\nthe ﬁtted values ˆy=Xˆβare still the projection of yonto the column\\nspace of X; there is just more than one way to express that projection\\nin terms of the column vectors of X. The non-full-rank case occurs most\\noften when one or more qualitative inputs are coded in a redundant fashion.\\nThere is usually a natural way to resolve the non-unique representation,\\nby recoding and/or dropping redundant columns in X. Most regression\\nsoftware packages detect these redundancies and automatically implement'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 65}, page_content='3.2 Linear Regression Models and Least Squares 47\\nsome strategy for removing them. Rank deﬁciencies can also occur in signal\\nand image analysis, where the number of inputs pcan exceed the number\\nof training cases N. In this case, the features are typically reduced by\\nﬁltering or else the ﬁtting is controlled by regularization (Section 5.2.3 and\\nChapter 18).\\nUp to now we have made minimal assumptions about the true distribu-\\ntion of the data. In order to pin down the sampling properties of ˆβ, we now\\nassume that the observations yiare uncorrelated and have constant vari-\\nanceσ2, and that the xiare ﬁxed (non random). The variance–covariance\\nmatrix of the least squares parameter estimates is easily derived from (3.6 )\\nand is given by\\nVar(ˆβ) = (XTX)−1σ2. (3.8)\\nTypically one estimates the variance σ2by\\nˆσ2=1\\nN−p−1N∑\\ni=1(yi−ˆyi)2.\\nTheN−p−1 rather than Nin the denominator makes ˆ σ2an unbiased\\nestimate of σ2: E(ˆσ2) =σ2.\\nTo draw inferences about the parameters and the model, additional as-\\nsumptions are needed. We now assume that (3.1) is the correct model for\\nthe mean; that is, the conditional expectation of Yis linear in X1,... ,X p.\\nWe also assume that the deviations of Yaround its expectation are additive\\nand Gaussian. Hence\\nY= E( Y|X1,... ,X p) +ε\\n=β0+p∑\\nj=1Xjβj+ε, (3.9)\\nwhere the error εis a Gaussian random variable with expectation zero and\\nvariance σ2, written ε∼N(0,σ2).\\nUnder (3.9), it is easy to show that\\nˆβ∼N(β,(XTX)−1σ2). (3.10)\\nThis is a multivariate normal distribution with mean vector and variance–\\ncovariance matrix as shown. Also\\n(N−p−1)ˆσ2∼σ2χ2\\nN−p−1, (3.11)\\na chi-squared distribution with N−p−1 degrees of freedom. In addition ˆβ\\nand ˆσ2are statistically independent. We use these distributional properties\\nto form tests of hypothesis and conﬁdence intervals for the parameters βj.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 66}, page_content='48 3. Linear Methods for Regression\\nZTail Probabilities\\n2.0 2.2 2.4 2.6 2.8 3.00.01 0.02 0.03 0.04 0.05 0.06t30\\nt100\\nnormal\\nFIGURE 3.3. The tail probabilities Pr(|Z|> z)for three distributions, t30,t100\\nand standard normal. Shown are the appropriate quantiles for test ing signiﬁcance\\nat the p= 0.05and0.01levels. The diﬀerence between tand the standard normal\\nbecomes negligible for Nbigger than about 100.\\nTo test the hypothesis that a particular coeﬃcient βj= 0, we form the\\nstandardized coeﬃcient or Z-score\\nzj=ˆβj\\nˆσ√vj, (3.12)\\nwhere vjis the jth diagonal element of ( XTX)−1. Under the null hypothesis\\nthatβj= 0,zjis distributed as tN−p−1(atdistribution with N−p−1\\ndegrees of freedom), and hence a large (absolute) value of zjwill lead to\\nrejection of this null hypothesis. If ˆ σis replaced by a known value σ, then\\nzjwould have a standard normal distribution. The diﬀerence between the\\ntail quantiles of a t-distribution and a standard normal become negligible\\nas the sample size increases, and so we typically use the normal quantiles\\n(see Figure 3.3).\\nOften we need to test for the signiﬁcance of groups of coeﬃcients simul-\\ntaneously. For example, to test if a categorical variable with klevels can\\nbe excluded from a model, we need to test whether the coeﬃcients of the\\ndummy variables used to represent the levels can all be set to zero. Here\\nwe use the Fstatistic,\\nF=(RSS 0−RSS1)/(p1−p0)\\nRSS1/(N−p1−1), (3.13)\\nwhere RSS 1is the residual sum-of-squares for the least squares ﬁt of the big-\\nger model with p1+1 parameters, and RSS 0the same for the nested smaller\\nmodel with p0+1 parameters, having p1−p0parameters constrained to be'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 67}, page_content='3.2 Linear Regression Models and Least Squares 49\\nzero. The Fstatistic measures the change in residual sum-of-squares per\\nadditional parameter in the bigger model, and it is normalized by an esti-\\nmate of σ2. Under the Gaussian assumptions, and the null hypothesis that\\nthe smaller model is correct, the Fstatistic will have a Fp1−p0,N−p1−1dis-\\ntribution. It can be shown (Exercise 3.1) that the zjin (3.12) are equivalent\\nto the Fstatistic for dropping the single coeﬃcient βjfrom the model. For\\nlargeN, the quantiles of the Fp1−p0,N−p1−1approach those of the χ2\\np1−p0.\\nSimilarly, we can isolate βjin (3.10) to obtain a 1 −2αconﬁdence interval\\nforβj:\\n(ˆβj−z(1−α)v1\\n2\\njˆσ,ˆβj+z(1−α)v1\\n2\\njˆσ). (3.14)\\nHerez(1−α)is the 1 −αpercentile of the normal distribution:\\nz(1−0.025)= 1.96,\\nz(1−.05)= 1.645,etc.\\nHence the standard practice of reporting ˆβ±2≤se(ˆβ) amounts to an ap-\\nproximate 95% conﬁdence interval. Even if the Gaussian error assumption\\ndoes not hold, this interval will be approximately correct, with its coverage\\napproaching 1 −2αas the sample size N→ ∞.\\nIn a similar fashion we can obtain an approximate conﬁdence set for the\\nentire parameter vector β, namely\\nCβ={β|(ˆβ−β)TXTX(ˆβ−β)≤ˆσ2χ2\\np+1(1−α)}, (3.15)\\nwhere χ2\\nℓ(1−α)is the 1 −αpercentile of the chi-squared distribution on ℓ\\ndegrees of freedom: for example, χ2\\n5(1−0.05)= 11.1,χ2\\n5(1−0.1)= 9.2. This\\nconﬁdence set for βgenerates a corresponding conﬁdence set for the true\\nfunction f(x) =xTβ, namely {xTβ|β∈Cβ}(Exercise 3.2; see also Fig-\\nure 5.4 in Section 5.2.2 for examples of conﬁdence bands for functions).\\n3.2.1 Example: Prostate Cancer\\nThe data for this example come from a study by Stamey et al. (1989). They\\nexamined the correlation between the level of prostate-speciﬁc antigen and\\na number of clinical measures in men who were about to receive a radical\\nprostatectomy. The variables are log cancer volume ( lcavol ), log prostate\\nweight ( lweight ),age, log of the amount of benign prostatic hyperplasia\\n(lbph), seminal vesicle invasion ( svi), log of capsular penetration ( lcp),\\nGleason score ( gleason ), and percent of Gleason scores 4 or 5 ( pgg45).\\nThe correlation matrix of the predictors given in Table 3.1 shows many\\nstrong correlations. Figure 1.1 (page 3) of Chapter 1 is a scatterplot matr ix\\nshowing every pairwise plot between the variables. We see that sviis a\\nbinary variable, and gleason is an ordered categorical variable. We see, for'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 68}, page_content='50 3. Linear Methods for Regression\\nTABLE 3.1. Correlations of predictors in the prostate cancer data.\\nlcavol lweight age lbph svi lcp gleason\\nlweight 0.300\\nage 0.286 0.317\\nlbph 0.063 0.437 0.287\\nsvi 0.593 0.181 0.129 −0.139\\nlcp 0.692 0.157 0.173 −0.089 0.671\\ngleason 0.426 0.024 0.366 0.033 0.307 0.476\\npgg45 0.483 0.074 0.276 −0.030 0.481 0.663 0.757\\nTABLE 3.2. Linear model ﬁt to the prostate cancer data. The Z score is the\\ncoeﬃcient divided by its standard error (3.12). Roughly a Zscore larger than two\\nin absolute value is signiﬁcantly nonzero at the p= 0.05level.\\nTerm Coeﬃcient Std. Error ZScore\\nIntercept 2.46 0.09 27.60\\nlcavol 0.68 0.13 5.37\\nlweight 0.26 0.10 2.75\\nage −0.14 0.10 −1.40\\nlbph 0.21 0.10 2.06\\nsvi 0.31 0.12 2.47\\nlcp −0.29 0.15 −1.87\\ngleason −0.02 0.15 −0.15\\npgg45 0.27 0.15 1.74\\nexample, that both lcavol andlcpshow a strong relationship with the\\nresponse lpsa, and with each other. We need to ﬁt the eﬀects jointly to\\nuntangle the relationships between the predictors and the response.\\nWe ﬁt a linear model to the log of prostate-speciﬁc antigen, lpsa, after\\nﬁrst standardizing the predictors to have unit variance. We randomly split\\nthe dataset into a training set of size 67 and a test set of size 30. We ap-\\nplied least squares estimation to the training set, producing the estimates,\\nstandard errors and Z-scores shown in Table 3.2. The Z-scores are deﬁned\\nin (3.12), and measure the eﬀect of dropping that variable from the model.\\nAZ-score greater than 2 in absolute value is approximately signiﬁcant at\\nthe 5% level. (For our example, we have nine parameters, and the 0 .025 tail\\nquantiles of the t67−9distribution are ±2.002!) The predictor lcavol shows\\nthe strongest eﬀect, with lweight andsvialso strong. Notice that lcpis\\nnot signiﬁcant, once lcavol is in the model (when used in a model without\\nlcavol ,lcpis strongly signiﬁcant). We can also test for the exclusion of\\na number of terms at once, using the F-statistic (3.13). For example, we\\nconsider dropping all the non-signiﬁcant terms in Table 3.2, namely age,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 69}, page_content='3.2 Linear Regression Models and Least Squares 51\\nlcp,gleason , andpgg45. We get\\nF=(32.81−29.43)/(9−5)\\n29.43/(67−9)= 1.67, (3.16)\\nwhich has a p-value of 0 .17 (Pr( F4,58>1.67) = 0 .17), and hence is not\\nsigniﬁcant.\\nThe mean prediction error on the test data is 0 .521. In contrast, predic-\\ntion using the mean training value of lpsahas a test error of 1 .057, which\\nis called the “base error rate.” Hence the linear model reduces the base\\nerror rate by about 50%. We will return to this example later to compare\\nvarious selection and shrinkage methods.\\n3.2.2 The Gauss–Markov Theorem\\nOne of the most famous results in statistics asserts that the least squares\\nestimates of the parameters βhave the smallest variance among all linear\\nunbiased estimates. We will make this precise here, and also make clear\\nthat the restriction to unbiased estimates is not necessarily a wise one. This\\nobservation will lead us to consider biased estimates such as ridge regression\\nlater in the chapter. We focus on estimation of any linear combination of\\nthe parameters θ=aTβ; for example, predictions f(x0) =xT\\n0βare of this\\nform. The least squares estimate of aTβis\\nˆθ=aTˆβ=aT(XTX)−1XTy. (3.17)\\nConsidering Xto be ﬁxed, this is a linear function cT\\n0yof the response\\nvector y. If we assume that the linear model is correct, aTˆβis unbiased\\nsince\\nE(aTˆβ) = E( aT(XTX)−1XTy)\\n=aT(XTX)−1XTXβ\\n=aTβ. (3.18)\\nThe Gauss–Markov theorem states that if we have any other linear estima-\\ntor˜θ=cTythat is unbiased for aTβ, that is, E( cTy) =aTβ, then\\nVar(aTˆβ)≤Var(cTy). (3.19)\\nThe proof (Exercise 3.3) uses the triangle inequality. For simplicity we hav e\\nstated the result in terms of estimation of a single parameter aTβ, but with\\na few more deﬁnitions one can state it in terms of the entire parameter\\nvector β(Exercise 3.3).\\nConsider the mean squared error of an estimator ˜θin estimating θ:\\nMSE( ˜θ) = E( ˜θ−θ)2\\n= Var( ˜θ) + [E( ˜θ)−θ]2. (3.20)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 70}, page_content='52 3. Linear Methods for Regression\\nThe ﬁrst term is the variance, while the second term is the squared bias.\\nThe Gauss-Markov theorem implies that the least squares estimator has the\\nsmallest mean squared error of all linear estimators with no bias. However ,\\nthere may well exist a biased estimator with smaller mean squared error.\\nSuch an estimator would trade a little bias for a larger reduction in varia nce.\\nBiased estimates are commonly used. Any method that shrinks or sets to\\nzero some of the least squares coeﬃcients may result in a biased estimate.\\nWe discuss many examples, including variable subset selection and ridge\\nregression, later in this chapter. From a more pragmatic point of view, m ost\\nmodels are distortions of the truth, and hence are biased; picking the right\\nmodel amounts to creating the right balance between bias and variance.\\nWe go into these issues in more detail in Chapter 7.\\nMean squared error is intimately related to prediction accuracy, as dis-\\ncussed in Chapter 2. Consider the prediction of the new response at input\\nx0,\\nY0=f(x0) +ε0. (3.21)\\nThen the expected prediction error of an estimate ˜f(x0) =xT\\n0˜βis\\nE(Y0−˜f(x0))2=σ2+ E(xT\\n0˜β−f(x0))2\\n=σ2+ MSE( ˜f(x0)). (3.22)\\nTherefore, expected prediction error and mean squared error diﬀer only by\\nthe constant σ2, representing the variance of the new observation y0.\\n3.2.3 Multiple Regression from Simple Univariate Regressi on\\nThe linear model (3.1) with p >1 inputs is called the multiple linear\\nregression model . The least squares estimates (3.6) for this model are best\\nunderstood in terms of the estimates for the univariate (p= 1) linear\\nmodel, as we indicate in this section.\\nSuppose ﬁrst that we have a univariate model with no intercept, that is,\\nY=Xβ+ε. (3.23)\\nThe least squares estimate and residuals are\\nˆβ=∑N\\n1xiyi∑N\\n1x2\\ni,\\nri=yi−xiˆβ.(3.24)\\nIn convenient vector notation, we let y= (y1,... ,y N)T,x= (x1,... ,x N)T\\nand deﬁne\\n⟨x,y⟩=N∑\\ni=1xiyi,\\n=xTy, (3.25)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 71}, page_content='3.2 Linear Regression Models and Least Squares 53\\ntheinner product between xandy1. Then we can write\\nˆβ=⟨x,y⟩\\n⟨x,x⟩,\\nr=y−xˆβ.(3.26)\\nAs we will see, this simple univariate regression provides the building block\\nfor multiple linear regression. Suppose next that the inputs x1,x2,... ,xp\\n(the columns of the data matrix X) are orthogonal; that is ⟨xj,xk⟩= 0\\nfor all j̸=k. Then it is easy to check that the multiple least squares esti-\\nmates ˆβjare equal to ⟨xj,y⟩/⟨xj,xj⟩—the univariate estimates. In other\\nwords, when the inputs are orthogonal, they have no eﬀect on each other’s\\nparameter estimates in the model.\\nOrthogonal inputs occur most often with balanced, designed experiments\\n(where orthogonality is enforced), but almost never with observational\\ndata. Hence we will have to orthogonalize them in order to carry this idea\\nfurther. Suppose next that we have an intercept and a single input x. Then\\nthe least squares coeﬃcient of xhas the form\\nˆβ1=⟨x−¯x1,y⟩\\n⟨x−¯x1,x−¯x1⟩, (3.27)\\nwhere ¯ x=∑\\nixi/N, and1=x0, the vector of Nones. We can view the\\nestimate (3.27) as the result of two applications of the simple regression\\n(3.26). The steps are:\\n1. regress xon1to produce the residual z=x−¯x1;\\n2. regress yon the residual zto give the coeﬃcient ˆβ1.\\nIn this procedure, “regress bona” means a simple univariate regression of b\\nonawith no intercept, producing coeﬃcient ˆ γ=⟨a,b⟩/⟨a,a⟩and residual\\nvector b−ˆγa. We say that bis adjusted for a, or is “orthogonalized” with\\nrespect to a.\\nStep 1 orthogonalizes xwith respect to x0=1. Step 2 is just a simple\\nunivariate regression, using the orthogonal predictors 1andz. Figure 3.4\\nshows this process for two general inputs x1andx2. The orthogonalization\\ndoes not change the subspace spanned by x1andx2, it simply produces an\\northogonal basis for representing it.\\nThis recipe generalizes to the case of pinputs, as shown in Algorithm 3.1.\\nNote that the inputs z0,... ,zj−1in step 2 are orthogonal, hence the simple\\nregression coeﬃcients computed there are in fact also the multiple regres-\\nsion coeﬃcients.\\n1The inner-product notation is suggestive of generalizatio ns of linear regression to\\ndiﬀerent metric spaces, as well as to probability spaces.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 72}, page_content='54 3. Linear Methods for Regression\\nx1x2y\\nˆ yz z z z z\\nFIGURE 3.4. Least squares regression by orthogonalization of the inputs. The\\nvector x2is regressed on the vector x1, leaving the residual vector z. The regres-\\nsion of yonzgives the multiple regression coeﬃcient of x2. Adding together the\\nprojections of yon each of x1andzgives the least squares ﬁt ˆy.\\nAlgorithm 3.1 Regression by Successive Orthogonalization.\\n1. Initialize z0=x0=1.\\n2. For j= 1,2,... ,p\\nRegress xjonz0,z1,... ,,zj−1to produce coeﬃcients ˆ γℓj=\\n⟨zℓ,xj⟩/⟨zℓ,zℓ⟩,ℓ= 0,... ,j −1 and residual vector zj=\\nxj−∑j−1\\nk=0ˆγkjzk.\\n3. Regress yon the residual zpto give the estimate ˆβp.\\nThe result of this algorithm is\\nˆβp=⟨zp,y⟩\\n⟨zp,zp⟩. (3.28)\\nRe-arranging the residual in step 2, we can see that each of the xjis a linear\\ncombination of the zk, k≤j. Since the zjare all orthogonal, they form\\na basis for the column space of X, and hence the least squares projection\\nonto this subspace is ˆy. Since zpalone involves xp(with coeﬃcient 1), we\\nsee that the coeﬃcient (3.28) is indeed the multiple regression coeﬃcient of\\nyonxp. This key result exposes the eﬀect of correlated inputs in multiple\\nregression. Note also that by rearranging the xj, any one of them could\\nbe in the last position, and a similar results holds. Hence stated more\\ngenerally, we have shown that the jth multiple regression coeﬃcient is the\\nunivariate regression coeﬃcient of yonxj≤012...(j−1)(j+1)...,p, the residual\\nafter regressing xjonx0,x1,... ,xj−1,xj+1,... ,xp:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 73}, page_content='3.2 Linear Regression Models and Least Squares 55\\nThe multiple regression coeﬃcient ˆβjrepresents the additional\\ncontribution of xjony, afterxjhas been adjusted for x0,x1,... ,xj−1,\\nxj+1,... ,xp.\\nIfxpis highly correlated with some of the other xk’s, the residual vector\\nzpwill be close to zero, and from (3.28) the coeﬃcient ˆβpwill be very\\nunstable. This will be true for all the variables in the correlated set. In\\nsuch situations, we might have all the Z-scores (as in Table 3.2) be smal l—\\nany one of the set can be deleted—yet we cannot delete them all. From\\n(3.28) we also obtain an alternate formula for the variance estimates ( 3.8),\\nVar(ˆβp) =σ2\\n⟨zp,zp⟩=σ2\\n∥zp∥2. (3.29)\\nIn other words, the precision with which we can estimate ˆβpdepends on\\nthe length of the residual vector zp; this represents how much of xpis\\nunexplained by the other xk’s.\\nAlgorithm 3.1 is known as the Gram–Schmidt procedure for multiple\\nregression, and is also a useful numerical strategy for computing the esti-\\nmates. We can obtain from it not just ˆβp, but also the entire multiple least\\nsquares ﬁt, as shown in Exercise 3.4.\\nWe can represent step 2 of Algorithm 3.1 in matrix form:\\nX=ZΓ, (3.30)\\nwhereZhas as columns the zj(in order), and Γis the upper triangular ma-\\ntrix with entries ˆ γkj. Introducing the diagonal matrix Dwithjth diagonal\\nentry Djj=∥zj∥, we get\\nX=ZD−1DΓ\\n=QR, (3.31)\\nthe so-called QRdecomposition of X. Here Qis anN×(p+1) orthogonal\\nmatrix, QTQ=I, andRis a (p+ 1)×(p+ 1) upper triangular matrix.\\nTheQRdecomposition represents a convenient orthogonal basis for the\\ncolumn space of X. It is easy to see, for example, that the least squares\\nsolution is given by\\nˆβ=R−1QTy, (3.32)\\nˆy=QQTy. (3.33)\\nEquation (3.32) is easy to solve because Ris upper triangular\\n(Exercise 3.4).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 74}, page_content='56 3. Linear Methods for Regression\\n3.2.4 Multiple Outputs\\nSuppose we have multiple outputs Y1,Y2,... ,Y Kthat we wish to predict\\nfrom our inputs X0,X1,X2,... ,X p. We assume a linear model for each\\noutput\\nYk=β0k+p∑\\nj=1Xjβjk+εk (3.34)\\n=fk(X) +εk. (3.35)\\nWith Ntraining cases we can write the model in matrix notation\\nY=XB+E. (3.36)\\nHereYis the N×Kresponse matrix, with ikentry yik,Xis the N×(p+1)\\ninput matrix, Bis the ( p+ 1)×Kmatrix of parameters and Eis the\\nN×Kmatrix of errors. A straightforward generalization of the univariat e\\nloss function (3.2) is\\nRSS(B) =K∑\\nk=1N∑\\ni=1(yik−fk(xi))2(3.37)\\n= tr[( Y−XB)T(Y−XB)]. (3.38)\\nThe least squares estimates have exactly the same form as before\\nˆB= (XTX)−1XTY. (3.39)\\nHence the coeﬃcients for the kth outcome are just the least squares es-\\ntimates in the regression of ykonx0,x1,... ,xp. Multiple outputs do not\\naﬀect one another’s least squares estimates.\\nIf the errors ε= (ε1,... ,ε K) in (3.34) are correlated, then it might seem\\nappropriate to modify (3.37) in favor of a multivariate version. Speciﬁca lly,\\nsuppose Cov( ε) =Σ, then the multivariate weighted criterion\\nRSS(B;Σ) =N∑\\ni=1(yi−f(xi))TΣ−1(yi−f(xi)) (3.40)\\narises naturally from multivariate Gaussian theory. Here f(x) is the vector\\nfunction ( f1(x),... ,f K(x)), and yithe vector of Kresponses for observa-\\ntioni. However, it can be shown that again the solution is given by (3.39);\\nKseparate regressions that ignore the correlations (Exercise 3.11). If the Σi\\nvary among observations, then this is no longer the case, and the solution\\nforBno longer decouples.\\nIn Section 3.7 we pursue the multiple outcome problem, and consider\\nsituations where it does pay to combine the regressions.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 75}, page_content='3.3 Subset Selection 57\\n3.3 Subset Selection\\nThere are two reasons why we are often not satisﬁed with the least squares\\nestimates (3.6).\\n•The ﬁrst is prediction accuracy : the least squares estimates often have\\nlow bias but large variance. Prediction accuracy can sometimes be\\nimproved by shrinking or setting some coeﬃcients to zero. By doing\\nso we sacriﬁce a little bit of bias to reduce the variance of the predicted\\nvalues, and hence may improve the overall prediction accuracy.\\n•The second reason is interpretation . With a large number of predic-\\ntors, we often would like to determine a smaller subset that exhibit\\nthe strongest eﬀects. In order to get the “big picture,” we are willing\\nto sacriﬁce some of the small details.\\nIn this section we describe a number of approaches to variable subset selec-\\ntion with linear regression. In later sections we discuss shrinkage and hybrid\\napproaches for controlling variance, as well as other dimension-reduction\\nstrategies. These all fall under the general heading model selection . Model\\nselection is not restricted to linear models; Chapter 7 covers this topic in\\nsome detail.\\nWith subset selection we retain only a subset of the variables, and elim-\\ninate the rest from the model. Least squares regression is used to estimate\\nthe coeﬃcients of the inputs that are retained. There are a number of dif-\\nferent strategies for choosing the subset.\\n3.3.1 Best-Subset Selection\\nBest subset regression ﬁnds for each k∈ {0,1,2,... ,p }the subset of size k\\nthat gives smallest residual sum of squares (3.2). An eﬃcient algorithm—\\ntheleaps and bounds procedure (Furnival and Wilson, 1974)—makes this\\nfeasible for pas large as 30 or 40. Figure 3.5 shows all the subset models\\nfor the prostate cancer example. The lower boundary represents the models\\nthat are eligible for selection by the best-subsets approach. Note that the\\nbest subset of size 2, for example, need not include the variable that was\\nin the best subset of size 1 (for this example all the subsets are nested).\\nThe best-subset curve (red lower boundary in Figure 3.5) is necessarily\\ndecreasing, so cannot be used to select the subset size k. The question of\\nhow to choose kinvolves the tradeoﬀ between bias and variance, along with\\nthe more subjective desire for parsimony. There are a number of criteria\\nthat one may use; typically we choose the smallest model that minimizes\\nan estimate of the expected prediction error.\\nMany of the other approaches that we discuss in this chapter are similar,\\nin that they use the training data to produce a sequence of models varying\\nin complexity and indexed by a single parameter. In the next section we use'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 76}, page_content='58 3. Linear Methods for Regression\\nSubset Size kResidual Sum−of−Squares\\n0 20 40 60 80 100\\n0 1 2 3 4 5 6 7 8•\\n••••••••\\n••••••••••••••••••••••••••\\n•••••••••••••••••••••••••••••••••••••••••••••••••\\n••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•••••••••••••••••••••••••••••••••••••••••••••\\n••••••••••••••••••••••••••\\n••••••••\\n••\\n•\\n•••••• •\\nFIGURE 3.5. All possible subset models for the prostate cancer example. At\\neach subset size is shown the residual sum-of-squares for ea ch model of that size.\\ncross-validation to estimate prediction error and select k; the AIC criterion\\nis a popular alternative. We defer more detailed discussion of these and\\nother approaches to Chapter 7.\\n3.3.2 Forward- and Backward-Stepwise Selection\\nRather than search through all possible subsets (which becomes infeasible\\nforpmuch larger than 40), we can seek a good path through them. Forward-\\nstepwise selection starts with the intercept, and then sequentially adds into\\nthe model the predictor that most improves the ﬁt. With many candidate\\npredictors, this might seem like a lot of computation; however, clever up-\\ndating algorithms can exploit the QR decomposition for the current ﬁt to\\nrapidly establish the next candidate (Exercise 3.9). Like best-subset re-\\ngression, forward stepwise produces a sequence of models indexed by k, the\\nsubset size, which must be determined.\\nForward-stepwise selection is a greedy algorithm , producing a nested se-\\nquence of models. In this sense it might seem sub-optimal compared to\\nbest-subset selection. However, there are several reasons why it might be\\npreferred:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 77}, page_content='3.3 Subset Selection 59\\n•Computational; for large pwe cannot compute the best subset se-\\nquence, but we can always compute the forward stepwise sequence\\n(even when p≫N).\\n•Statistical; a price is paid in variance for selecting the best subset\\nof each size; forward stepwise is a more constrained search, and will\\nhave lower variance, but perhaps more bias.\\n0 5 10 15 20 25 300.65 0.70 0.75 0.80 0.85 0.90 0.95Best Subset\\nForward Stepwise\\nBackward Stepwise\\nForward StagewiseE||ˆβ(k)−β||2\\nSubset Size k\\nFIGURE 3.6. Comparison of four subset-selection techniques on a simulat ed lin-\\near regression problem Y=XTβ+ε. There are N= 300 observations on p= 31\\nstandard Gaussian variables, with pairwise correlations all equal to 0.85. For10of\\nthe variables, the coeﬃcients are drawn at random from a N(0,0.4)distribution;\\nthe rest are zero. The noise ε∼N(0,6.25), resulting in a signal-to-noise ratio of\\n0.64. Results are averaged over 50simulations. Shown is the mean-squared error\\nof the estimated coeﬃcient ˆβ(k)at each step from the true β.\\nBackward-stepwise selection starts with the full model, and sequentially\\ndeletes the predictor that has the least impact on the ﬁt. The candidate for\\ndropping is the variable with the smallest Z-score (Exercise 3.10). Backw ard\\nselection can only be used when N > p , while forward stepwise can always\\nbe used.\\nFigure 3.6 shows the results of a small simulation study to compare\\nbest-subset regression with the simpler alternatives forward and backward\\nselection. Their performance is very similar, as is often the case. Included in\\nthe ﬁgure is forward stagewise regression (next section), which takes longer\\nto reach minimum error.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 78}, page_content='60 3. Linear Methods for Regression\\nOn the prostate cancer example, best-subset, forward and backward se-\\nlection all gave exactly the same sequence of terms.\\nSome software packages implement hybrid stepwise-selection strategies\\nthat consider both forward and backward moves at each step, and select\\nthe “best” of the two. For example in the Rpackage the stepfunction uses\\nthe AIC criterion for weighing the choices, which takes proper account of\\nthe number of parameters ﬁt; at each step an add or drop will be performed\\nthat minimizes the AIC score.\\nOther more traditional packages base the selection on F-statistics, adding\\n“signiﬁcant” terms, and dropping “non-signiﬁcant” terms. These are out\\nof fashion, since they do not take proper account of the multiple testing\\nissues. It is also tempting after a model search to print out a summary of\\nthe chosen model, such as in Table 3.2; however, the standard errors are\\nnot valid, since they do not account for the search process. The bootstrap\\n(Section 8.2) can be useful in such settings.\\nFinally, we note that often variables come in groups (such as the dummy\\nvariables that code a multi-level categorical predictor). Smart stepwise pro-\\ncedures (such as stepinR) will add or drop whole groups at a time, taking\\nproper account of their degrees-of-freedom.\\n3.3.3 Forward-Stagewise Regression\\nForward-stagewise regression (FS) is even more constrained than forward-\\nstepwise regression. It starts like forward-stepwise regression, wit h an in-\\ntercept equal to ¯ y, and centered predictors with coeﬃcients initially all 0.\\nAt each step the algorithm identiﬁes the variable most correlated with the\\ncurrent residual. It then computes the simple linear regression coeﬃcient\\nof the residual on this chosen variable, and then adds it to the current co-\\neﬃcient for that variable. This is continued till none of the variables have\\ncorrelation with the residuals—i.e. the least-squares ﬁt when N > p .\\nUnlike forward-stepwise regression, none of the other variables are ad-\\njusted when a term is added to the model. As a consequence, forward\\nstagewise can take many more than psteps to reach the least squares ﬁt,\\nand historically has been dismissed as being ineﬃcient. It turns out that\\nthis “slow ﬁtting” can pay dividends in high-dimensional problems. We\\nsee in Section 3.8.1 that both forward stagewise and a variant which is\\nslowed down even further are quite competitive, especially in very high-\\ndimensional problems.\\nForward-stagewise regression is included in Figure 3.6. In this example it\\ntakes over 1000 steps to get all the correlations below 10−4. For subset size\\nk, we plotted the error for the last step for which there where knonzero\\ncoeﬃcients. Although it catches up with the best ﬁt, it takes longer to\\ndo so.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 79}, page_content='3.4 Shrinkage Methods 61\\n3.3.4 Prostate Cancer Data Example (Continued)\\nTable 3.3 shows the coeﬃcients from a number of diﬀerent selection and\\nshrinkage methods. They are best-subset selection using an all-subsets search,\\nridge regression , thelasso,principal components regression andpartial least\\nsquares . Each method has a complexity parameter, and this was chosen to\\nminimize an estimate of prediction error based on tenfold cross-validation;\\nfull details are given in Section 7.10. Brieﬂy, cross-validation works by divid-\\ning the training data randomly into ten equal parts. The learning method\\nis ﬁt—for a range of values of the complexity parameter—to nine-tenths of\\nthe data, and the prediction error is computed on the remaining one-tenth.\\nThis is done in turn for each one-tenth of the data, and the ten prediction\\nerror estimates are averaged. From this we obtain an estimated prediction\\nerror curve as a function of the complexity parameter.\\nNote that we have already divided these data into a training set of size\\n67 and a test set of size 30. Cross-validation is applied to the training set,\\nsince selecting the shrinkage parameter is part of the training process. The\\ntest set is there to judge the performance of the selected model.\\nThe estimated prediction error curves are shown in Figure 3.7. Many of\\nthe curves are very ﬂat over large ranges near their minimum. Included\\nare estimated standard error bands for each estimated error rate, based on\\nthe ten error estimates computed by cross-validation. We have used the\\n“one-standard-error” rule—we pick the most parsimonious model within\\none standard error of the minimum (Section 7.10, page 244). Such a rule\\nacknowledges the fact that the tradeoﬀ curve is estimated with error, and\\nhence takes a conservative approach.\\nBest-subset selection chose to use the two predictors lcvol andlweight .\\nThe last two lines of the table give the average prediction error (and its\\nestimated standard error) over the test set.\\n3.4 Shrinkage Methods\\nBy retaining a subset of the predictors and discarding the rest, subset selec-\\ntion produces a model that is interpretable and has possibly lower predic-\\ntion error than the full model. However, because it is a discrete process—\\nvariables are either retained or discarded—it often exhibits high variance,\\nand so doesn’t reduce the prediction error of the full model. Shrinkage\\nmethods are more continuous, and don’t suﬀer as much from high\\nvariability.\\n3.4.1 Ridge Regression\\nRidge regression shrinks the regression coeﬃcients by imposing a penalty\\non their size. The ridge coeﬃcients minimize a penalized residual sum of'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 80}, page_content='62 3. Linear Methods for Regression\\nSubset SizeCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•••••••All Subsets\\nDegrees of FreedomCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•\\n••••••Ridge Regression\\nShrinkage Factor sCV Error\\n0.0 0.2 0.4 0.6 0.8 1.00.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n•\\n•\\n••••••Lasso\\nNumber of DirectionsCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n••••••••Principal Components Regression\\nNumber of  DirectionsCV Error\\n0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\\n••••••• •Partial Least Squares\\nFIGURE 3.7. Estimated prediction error curves and their standard errors f or\\nthe various selection and shrinkage methods. Each curve is plo tted as a function\\nof the corresponding complexity parameter for that method. The horizontal axis\\nhas been chosen so that the model complexity increases as we mov e from left to\\nright. The estimates of prediction error and their standard er rors were obtained by\\ntenfold cross-validation; full details are given in Section 7. 10. The least complex\\nmodel within one standard error of the best is chosen, indicated b y the purple\\nvertical broken lines.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 81}, page_content='3.4 Shrinkage Methods 63\\nTABLE 3.3. Estimated coeﬃcients and test error results, for diﬀerent subs et\\nand shrinkage methods applied to the prostate data. The blank ent ries correspond\\nto variables omitted.\\nTerm LS Best Subset Ridge Lasso PCR PLS\\nIntercept 2.465 2.477 2.452 2.468 2.497 2.452\\nlcavol 0.680 0.740 0.420 0.533 0.543 0.419\\nlweight 0.263 0.316 0.238 0.169 0.289 0.344\\nage −0.141 −0.046 −0.152 −0.026\\nlbph 0.210 0.162 0.002 0.214 0.220\\nsvi 0.305 0.227 0.094 0.315 0.243\\nlcp −0.288 0.000 −0.051 0.079\\ngleason −0.021 0.040 0.232 0.011\\npgg45 0.267 0.133 −0.056 0.084\\nTest Error 0.521 0.492 0.492 0.479 0.449 0.528\\nStd Error 0.179 0.143 0.165 0.164 0.105 0.152\\nsquares,\\nˆβridge= argmin\\nβ{N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1β2\\nj}\\n. (3.41)\\nHereλ≥0 is a complexity parameter that controls the amount of shrink-\\nage: the larger the value of λ, the greater the amount of shrinkage. The\\ncoeﬃcients are shrunk toward zero (and each other). The idea of penaliz-\\ning by the sum-of-squares of the parameters is also used in neural networks,\\nwhere it is known as weight decay (Chapter 11).\\nAn equivalent way to write the ridge problem is\\nˆβridge= argmin\\nβN∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\n,\\nsubject top∑\\nj=1β2\\nj≤t,(3.42)\\nwhich makes explicit the size constraint on the parameters. There is a one-\\nto-one correspondence between the parameters λin (3.41) and tin (3.42).\\nWhen there are many correlated variables in a linear regression model,\\ntheir coeﬃcients can become poorly determined and exhibit high variance.\\nA wildly large positive coeﬃcient on one variable can be canceled by a\\nsimilarly large negative coeﬃcient on its correlated cousin. By imposing a\\nsize constraint on the coeﬃcients, as in (3.42), this problem is alleviated.\\nThe ridge solutions are not equivariant under scaling of the inputs, and\\nso one normally standardizes the inputs before solving (3.41). In addition,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 82}, page_content='64 3. Linear Methods for Regression\\nnotice that the intercept β0has been left out of the penalty term. Penal-\\nization of the intercept would make the procedure depend on the origin\\nchosen for Y; that is, adding a constant cto each of the targets yiwould\\nnot simply result in a shift of the predictions by the same amount c. It\\ncan be shown (Exercise 3.5) that the solution to (3.41) can be separated\\ninto two parts, after reparametrization using centered inputs: each xijgets\\nreplaced by xij−¯xj. We estimate β0by ¯y=1\\nN∑N\\n1yi. The remaining co-\\neﬃcients get estimated by a ridge regression without intercept, using the\\ncentered xij. Henceforth we assume that this centering has been done, so\\nthat the input matrix Xhasp(rather than p+ 1) columns.\\nWriting the criterion in (3.41) in matrix form,\\nRSS(λ) = (y−Xβ)T(y−Xβ) +λβTβ, (3.43)\\nthe ridge regression solutions are easily seen to be\\nˆβridge= (XTX+λI)−1XTy, (3.44)\\nwhereIis the p×pidentity matrix. Notice that with the choice of quadratic\\npenalty βTβ, the ridge regression solution is again a linear function of\\ny. The solution adds a positive constant to the diagonal of XTXbefore\\ninversion. This makes the problem nonsingular, even if XTXis not of full\\nrank, and was the main motivation for ridge regression when it was ﬁrst\\nintroduced in statistics (Hoerl and Kennard, 1970). Traditional descriptions\\nof ridge regression start with deﬁnition (3.44). We choose to motivat e it via\\n(3.41) and (3.42), as these provide insight into how it works.\\nFigure 3.8 shows the ridge coeﬃcient estimates for the prostate can-\\ncer example, plotted as functions of df( λ), the eﬀective degrees of freedom\\nimplied by the penalty λ(deﬁned in (3.50) on page 68). In the case of or-\\nthonormal inputs, the ridge estimates are just a scaled version of the least\\nsquares estimates, that is, ˆβridge=ˆβ/(1 +λ).\\nRidge regression can also be derived as the mean or mode of a poste-\\nrior distribution, with a suitably chosen prior distribution. In detail, sup-\\nposeyi∼N(β0+xT\\niβ,σ2), and the parameters βjare each distributed as\\nN(0,τ2), independently of one another. Then the (negative) log-posterior\\ndensity of β, with τ2andσ2assumed known, is equal to the expression\\nin curly braces in (3.41), with λ=σ2/τ2(Exercise 3.6). Thus the ridge\\nestimate is the mode of the posterior distribution; since the distribution is\\nGaussian, it is also the posterior mean.\\nThesingular value decomposition (SVD) of the centered input matrix X\\ngives us some additional insight into the nature of ridge regression. This de-\\ncomposition is extremely useful in the analysis of many statistical metho ds.\\nThe SVD of the N×pmatrix Xhas the form\\nX=UDVT. (3.45)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 83}, page_content='3.4 Shrinkage Methods 65Coefficients\\n0 2 4 6 8−0.2 0.0 0.2 0.4 0.6•\\n•••••\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n•\\n••••••\\n•lcavol\\n••••••••••••••••••••••••\\n•lweight\\n•••••••••••••••••••••••••\\nage••••••••••••••••••••••••\\n•lbph••••••••••••••••••••••••\\n•svi\\n••••••••••••••••••••••••\\n•\\nlcp••••••••••••••••••••••••\\n•gleason•\\n•••••••••••••••••••••••\\n•pgg45\\ndf(λ)\\nFIGURE 3.8. Proﬁles of ridge coeﬃcients for the prostate cancer example, as\\nthe tuning parameter λis varied. Coeﬃcients are plotted versus df(λ), the eﬀective\\ndegrees of freedom. A vertical line is drawn at df = 5 .0, the value chosen by\\ncross-validation.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 84}, page_content='66 3. Linear Methods for Regression\\nHereUandVareN×pandp×porthogonal matrices, with the columns\\nofUspanning the column space of X, and the columns of Vspanning the\\nrow space. Dis ap×pdiagonal matrix, with diagonal entries d1≥d2≥\\n≤≤≤ ≥ dp≥0 called the singular values of X. If one or more values dj= 0,\\nXis singular.\\nUsing the singular value decomposition we can write the least squares\\nﬁtted vector as\\nXˆβls=X(XTX)−1XTy\\n=UUTy, (3.46)\\nafter some simpliﬁcation. Note that UTyare the coordinates of ywith\\nrespect to the orthonormal basis U. Note also the similarity with (3.33);\\nQandUare generally diﬀerent orthogonal bases for the column space of\\nX(Exercise 3.8).\\nNow the ridge solutions are\\nXˆβridge=X(XTX+λI)−1XTy\\n=U D(D2+λI)−1D UTy\\n=p∑\\nj=1ujd2\\nj\\nd2\\nj+λuT\\njy, (3.47)\\nwhere the ujare the columns of U. Note that since λ≥0, we have d2\\nj/(d2\\nj+\\nλ)≤1. Like linear regression, ridge regression computes the coordinates of\\nywith respect to the orthonormal basis U. It then shrinks these coordinates\\nby the factors d2\\nj/(d2\\nj+λ). This means that a greater amount of shrinkage\\nis applied to the coordinates of basis vectors with smaller d2\\nj.\\nWhat does a small value of d2\\njmean? The SVD of the centered matrix\\nXis another way of expressing the principal components of the variables\\ninX. The sample covariance matrix is given by S=XTX/N, and from\\n(3.45) we have\\nXTX=VD2VT, (3.48)\\nwhich is the eigen decomposition ofXTX(and of S, up to a factor N).\\nThe eigenvectors vj(columns of V) are also called the principal compo-\\nnents (or Karhunen–Loeve) directions of X. The ﬁrst principal component\\ndirection v1has the property that z1=Xv1has the largest sample vari-\\nance amongst all normalized linear combinations of the columns of X. This\\nsample variance is easily seen to be\\nVar(z1) = Var( Xv1) =d2\\n1\\nN, (3.49)\\nand in fact z1=Xv1=u1d1. The derived variable z1is called the ﬁrst\\nprincipal component of X, and hence u1is the normalized ﬁrst principal'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 85}, page_content='3.4 Shrinkage Methods 67\\n-4 -2 0 2 4-4 -2 0 2 4ooo\\nooooooo\\no\\no\\nooo\\noo\\noo\\noo\\nooo\\no\\no\\noooo\\noo\\no\\nooo\\nooo\\nooooo\\noo\\nooo\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\nooo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\no\\noooo\\nooo\\no\\noo\\no\\noo\\noo\\no\\noo\\noooooo\\nooo\\noo\\nooo\\noo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\nooo\\noo\\no\\noooo\\no\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\noo\\noo\\no\\noo\\noo\\noooo\\no\\nooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooLargest Principal\\nComponent\\nSmallest Principal\\nComponent\\nX1X2\\nFIGURE 3.9. Principal components of some input data points. The largest prin-\\ncipal component is the direction that maximizes the variance of t he projected data,\\nand the smallest principal component minimizes that variance. Rid ge regression\\nprojects yonto these components, and then shrinks the coeﬃcients of the low–\\nvariance components more than the high-variance components.\\ncomponent. Subsequent principal components zjhave maximum variance\\nd2\\nj/N, subject to being orthogonal to the earlier ones. Conversely the last\\nprincipal component has minimum variance. Hence the small singular val-\\nuesdjcorrespond to directions in the column space of Xhaving small\\nvariance, and ridge regression shrinks these directions the most.\\nFigure 3.9 illustrates the principal components of some data points in\\ntwo dimensions. If we consider ﬁtting a linear surface over this domain\\n(theY-axis is sticking out of the page), the conﬁguration of the data allow\\nus to determine its gradient more accurately in the long direction than\\nthe short. Ridge regression protects against the potentially high variance\\nof gradients estimated in the short directions. The implicit assumption is\\nthat the response will tend to vary most in the directions of high variance\\nof the inputs. This is often a reasonable assumption, since predictors are\\noften chosen for study because they vary with the response variable, but\\nneed not hold in general.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 86}, page_content='68 3. Linear Methods for Regression\\nIn Figure 3.7 we have plotted the estimated prediction error versus the\\nquantity\\ndf(λ) = tr[ X(XTX+λI)−1XT],\\n= tr( Hλ)\\n=p∑\\nj=1d2\\nj\\nd2\\nj+λ. (3.50)\\nThis monotone decreasing function of λis the eﬀective degrees of freedom\\nof the ridge regression ﬁt. Usually in a linear-regression ﬁt with pvariables,\\nthe degrees-of-freedom of the ﬁt is p, the number of free parameters. The\\nidea is that although all pcoeﬃcients in a ridge ﬁt will be non-zero, they\\nare ﬁt in a restricted fashion controlled by λ. Note that df( λ) =pwhen\\nλ= 0 (no regularization) and df( λ)→0 asλ→ ∞ . Of course there\\nis always an additional one degree of freedom for the intercept, which was\\nremoved apriori . This deﬁnition is motivated in more detail in Section 3.4.4\\nand Sections 7.4–7.6. In Figure 3.7 the minimum occurs at df( λ) = 5 .0.\\nTable 3.3 shows that ridge regression reduces the test error of the full least\\nsquares estimates by a small amount.\\n3.4.2 The Lasso\\nThe lasso is a shrinkage method like ridge, with subtle but important dif-\\nferences. The lasso estimate is deﬁned by\\nˆβlasso= argmin\\nβN∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\nsubject top∑\\nj=1|βj| ≤t. (3.51)\\nJust as in ridge regression, we can re-parametrize the constant β0by stan-\\ndardizing the predictors; the solution for ˆβ0is ¯y, and thereafter we ﬁt a\\nmodel without an intercept (Exercise 3.5). In the signal processing litera-\\nture, the lasso is also known as basis pursuit (Chen et al., 1998).\\nWe can also write the lasso problem in the equivalent Lagrangian form\\nˆβlasso= argmin\\nβ{1\\n2N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1|βj|}\\n.(3.52)\\nNotice the similarity to the ridge regression problem (3.42) or (3.41) : the\\nL2ridge penalty∑p\\n1β2\\njis replaced by the L1lasso penalty∑p\\n1|βj|. This\\nlatter constraint makes the solutions nonlinear in the yi, and there is no\\nclosed form expression as in ridge regression. Computing the lasso solution'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 87}, page_content='3.4 Shrinkage Methods 69\\nis a quadratic programming problem, although we see in Section 3.4.4 that\\neﬃcient algorithms are available for computing the entire path of solutions\\nasλis varied, with the same computational cost as for ridge regression.\\nBecause of the nature of the constraint, making tsuﬃciently small will\\ncause some of the coeﬃcients to be exactly zero. Thus the lasso does a kind\\nof continuous subset selection. If tis chosen larger than t0=∑p\\n1|ˆβj|(where\\nˆβj=ˆβls\\nj, the least squares estimates), then the lasso estimates are the ˆβj’s.\\nOn the other hand, for t=t0/2 say, then the least squares coeﬃcients are\\nshrunk by about 50% on average. However, the nature of the shrinkage\\nis not obvious, and we investigate it further in Section 3.4.4 below. Like\\nthe subset size in variable subset selection, or the penalty parameter in\\nridge regression, tshould be adaptively chosen to minimize an estimate of\\nexpected prediction error.\\nIn Figure 3.7, for ease of interpretation, we have plotted the lasso pre-\\ndiction error estimates versus the standardized parameter s=t/∑p\\n1|ˆβj|.\\nA value ˆ s≈0.36 was chosen by 10-fold cross-validation; this caused four\\ncoeﬃcients to be set to zero (ﬁfth column of Table 3.3). The resulting\\nmodel has the second lowest test error, slightly lower than the full least\\nsquares model, but the standard errors of the test error estimates (last line\\nof Table 3.3) are fairly large.\\nFigure 3.10 shows the lasso coeﬃcients as the standardized tuning pa-\\nrameter s=t/∑p\\n1|ˆβj|is varied. At s= 1.0 these are the least squares\\nestimates; they decrease to 0 as s→0. This decrease is not always strictly\\nmonotonic, although it is in this example. A vertical line is drawn at\\ns= 0.36, the value chosen by cross-validation.\\n3.4.3 Discussion: Subset Selection, Ridge Regression and t he\\nLasso\\nIn this section we discuss and compare the three approaches discussed so far\\nfor restricting the linear regression model: subset selection, ridge regression\\nand the lasso.\\nIn the case of an orthonormal input matrix Xthe three procedures have\\nexplicit solutions. Each method applies a simple transformation to the leas t\\nsquares estimate ˆβj, as detailed in Table 3.4.\\nRidge regression does a proportional shrinkage. Lasso translates each\\ncoeﬃcient by a constant factor λ, truncating at zero. This is called “soft\\nthresholding,” and is used in the context of wavelet-based smoothing in Sec-\\ntion 5.9. Best-subset selection drops all variables with coeﬃcients smaller\\nthan the Mth largest; this is a form of “hard-thresholding.”\\nBack to the nonorthogonal case; some pictures help understand their re-\\nlationship. Figure 3.11 depicts the lasso (left) and ridge regression (righ t)\\nwhen there are only two parameters. The residual sum of squares has ellip-\\ntical contours, centered at the full least squares estimate. The constraint'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 88}, page_content='70 3. Linear Methods for Regression\\n0.0 0.2 0.4 0.6 0.8 1.0−0.2 0.0 0.2 0.4 0.6\\nShrinkage Factor sCoefficientslcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\nFIGURE 3.10. Proﬁles of lasso coeﬃcients, as the tuning parameter tis varied.\\nCoeﬃcients are plotted versus s=t/Pp\\n1|ˆβj|. A vertical line is drawn at s= 0.36,\\nthe value chosen by cross-validation. Compare Figure 3.8 on p age 65; the lasso\\nproﬁles hit zero, while those for ridge do not. The proﬁles are pi ece-wise linear,\\nand so are computed only at the points displayed; see Section 3.4. 4 for details.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 89}, page_content='3.4 Shrinkage Methods 71\\nTABLE 3.4. Estimators of βjin the case of orthonormal columns of X.Mandλ\\nare constants chosen by the corresponding techniques; signdenotes the sign of its\\nargument ( ±1), and x+denotes “positive part” of x. Below the table, estimators\\nare shown by broken red lines. The 45◦line in gray shows the unrestricted estimate\\nfor reference.\\nEstimator Formula\\nBest subset (size M)ˆβj≤I(|ˆβj| ≥ |ˆβ(M)|)\\nRidge ˆβj/(1 +λ)\\nLasso sign( ˆβj)(|ˆβj| −λ)+\\n(0,0) (0,0) (0,0)|ˆβ(M)|λBest Subset Ridge Lasso\\nβ^β^ 2. . β\\n1β2\\nβ1β\\nFIGURE 3.11. Estimation picture for the lasso (left) and ridge regression\\n(right). Shown are contours of the error and constraint functions. T he solid blue\\nareas are the constraint regions |β1|+|β2| ≤tandβ2\\n1+β2\\n2≤t2, respectively,\\nwhile the red ellipses are the contours of the least squares er ror function.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 90}, page_content='72 3. Linear Methods for Regression\\nregion for ridge regression is the disk β2\\n1+β2\\n2≤t, while that for lasso is\\nthe diamond |β1|+|β2| ≤t. Both methods ﬁnd the ﬁrst point where the\\nelliptical contours hit the constraint region. Unlike the disk, the diamond\\nhas corners; if the solution occurs at a corner, then it has one parameter\\nβjequal to zero. When p >2, the diamond becomes a rhomboid, and has\\nmany corners, ﬂat edges and faces; there are many more opportunities for\\nthe estimated parameters to be zero.\\nWe can generalize ridge regression and the lasso, and view them as Bayes\\nestimates. Consider the criterion\\n˜β= argmin\\nβ{N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2+λp∑\\nj=1|βj|q}\\n(3.53)\\nforq≥0. The contours of constant value of∑\\nj|βj|qare shown in Fig-\\nure 3.12, for the case of two inputs.\\nThinking of |βj|qas the log-prior density for βj, these are also the equi-\\ncontours of the prior distribution of the parameters. The value q= 0 corre-\\nsponds to variable subset selection, as the penalty simply counts the number\\nof nonzero parameters; q= 1 corresponds to the lasso, while q= 2 to ridge\\nregression. Notice that for q≤1, the prior is not uniform in direction, but\\nconcentrates more mass in the coordinate directions. The prior correspond-\\ning to the q= 1 case is an independent double exponential (or Laplace)\\ndistribution for each input, with density (1 /2τ)exp(−|β|/τ) and τ= 1/λ.\\nThe case q= 1 (lasso) is the smallest qsuch that the constraint region\\nis convex; non-convex constraint regions make the optimization problem\\nmore diﬃcult.\\nIn this view, the lasso, ridge regression and best subset selection are\\nBayes estimates with diﬀerent priors. Note, however, that they are derived\\nas posterior modes, that is, maximizers of the posterior. It is more com mon\\nto use the mean of the posterior as the Bayes estimate. Ridge regression is\\nalso the posterior mean, but the lasso and best subset selection are not.\\nLooking again at the criterion (3.53), we might try using other values\\nofqbesides 0, 1, or 2. Although one might consider estimating qfrom\\nthe data, our experience is that it is not worth the eﬀort for the extra\\nvariance incurred. Values of q∈(1,2) suggest a compromise between the\\nlasso and ridge regression. Although this is the case, with q >1,|βj|qis\\ndiﬀerentiable at 0, and so does not share the ability of lasso ( q= 1) for\\nq= 4 q= 2 q= 1 q= 0.5 q= 0.1\\nFIGURE 3.12. Contours of constant value ofP\\nj|βj|qfor given values of q.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 91}, page_content='3.4 Shrinkage Methods 73\\nq= 1.2 α= 0.2\\nLq Elastic Net\\nFIGURE 3.13. Contours of constant value ofP\\nj|βj|qforq= 1.2(left plot),\\nand the elastic-net penaltyP\\nj(αβ2\\nj+(1−α)|βj|)forα= 0.2(right plot). Although\\nvisually very similar, the elastic-net has sharp (non-diﬀerent iable) corners, while\\ntheq= 1.2penalty does not.\\nsetting coeﬃcients exactly to zero. Partly for this reason as well as for\\ncomputational tractability, Zou and Hastie (2005) introduced the elastic-\\nnetpenalty\\nλp∑\\nj=1(\\nαβ2\\nj+ (1−α)|βj|)\\n, (3.54)\\na diﬀerent compromise between ridge and lasso. Figure 3.13 compares the\\nLqpenalty with q= 1.2 and the elastic-net penalty with α= 0.2; it is\\nhard to detect the diﬀerence by eye. The elastic-net selects variables like\\nthe lasso, and shrinks together the coeﬃcients of correlated predictors like\\nridge. It also has considerable computational advantages over the Lqpenal-\\nties. We discuss the elastic-net further in Section 18.4.\\n3.4.4 Least Angle Regression\\nLeast angle regression (LAR) is a relative newcomer (Efron et al., 2004) ,\\nand can be viewed as a kind of “democratic” version of forward stepwise\\nregression (Section 3.3.2). As we will see, LAR is intimately connected\\nwith the lasso, and in fact provides an extremely eﬃcient algorithm for\\ncomputing the entire lasso path as in Figure 3.10.\\nForward stepwise regression builds a model sequentially, adding one vari-\\nable at a time. At each step, it identiﬁes the best variable to include in the\\nactive set , and then updates the least squares ﬁt to include all the active\\nvariables.\\nLeast angle regression uses a similar strategy, but only enters “as much”\\nof a predictor as it deserves. At the ﬁrst step it identiﬁes the variable\\nmost correlated with the response. Rather than ﬁt this variable completely,\\nLAR moves the coeﬃcient of this variable continuously toward its least-\\nsquares value (causing its correlation with the evolving residual to decrease\\nin absolute value). As soon as another variable “catches up” in terms of\\ncorrelation with the residual, the process is paused. The second variable\\nthen joins the active set, and their coeﬃcients are moved together in a way\\nthat keeps their correlations tied and decreasing. This process is continued'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 92}, page_content='74 3. Linear Methods for Regression\\nuntil all the variables are in the model, and ends at the full least-squares\\nﬁt. Algorithm 3.2 provides the details. The termination condition in step 5\\nrequires some explanation. If p > N −1, the LAR algorithm reaches a zero\\nresidual solution after N−1 steps (the −1 is because we have centered the\\ndata).\\nAlgorithm 3.2 Least Angle Regression.\\n1. Standardize the predictors to have mean zero and unit norm. Start\\nwith the residual r=y−¯y,β1,β2,... ,β p= 0.\\n2. Find the predictor xjmost correlated with r.\\n3. Move βjfrom 0 towards its least-squares coeﬃcient ⟨xj,r⟩, until some\\nother competitor xkhas as much correlation with the current residual\\nas does xj.\\n4. Move βjandβkin the direction deﬁned by their joint least squares\\ncoeﬃcient of the current residual on ( xj,xk), until some other com-\\npetitor xlhas as much correlation with the current residual.\\n5. Continue in this way until all ppredictors have been entered. After\\nmin(N−1,p) steps, we arrive at the full least-squares solution.\\nSuppose Akis the active set of variables at the beginning of the kth\\nstep, and let βAkbe the coeﬃcient vector for these variables at this step;\\nthere will be k−1 nonzero values, and the one just entered will be zero. If\\nrk=y−XAkβAkis the current residual, then the direction for this step is\\nδk= (XT\\nAkXAk)−1XT\\nAkrk. (3.55)\\nThe coeﬃcient proﬁle then evolves as βAk(α) =βAk+α≤δk. Exercise 3.23\\nveriﬁes that the directions chosen in this fashion do what is claimed: keep\\nthe correlations tied and decreasing. If the ﬁt vector at the beginning of\\nthis step is ˆfk, then it evolves as ˆfk(α) =ˆfk+α≤uk, where uk=XAkδk\\nis the new ﬁt direction. The name “least angle” arises from a geometrical\\ninterpretation of this process; ukmakes the smallest (and equal) angle\\nwith each of the predictors in Ak(Exercise 3.24). Figure 3.14 shows the\\nabsolute correlations decreasing and joining ranks with each step of the\\nLAR algorithm, using simulated data.\\nBy construction the coeﬃcients in LAR change in a piecewise linear fash-\\nion. Figure 3.15 [left panel] shows the LAR coeﬃcient proﬁle evolving as a\\nfunction of their L1arc length2. Note that we do not need to take small\\n2TheL1arc-length of a diﬀerentiable curve β(s) fors∈[0, S] is given by TV( β, S) =RS\\n0||˙β(s)||1ds,where ˙β(s) =∂β(s)/∂s. For the piecewise-linear LAR coeﬃcient proﬁle,\\nthis amounts to summing the L1norms of the changes in coeﬃcients from step to step.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 93}, page_content='3.4 Shrinkage Methods 75\\n0 5 10 150.0 0.1 0.2 0.3 0.4v2 v6 v4 v5 v3 v1\\nL1Arc LengthAbsolute Correlations\\nFIGURE 3.14. Progression of the absolute correlations during each step of t he\\nLAR procedure, using a simulated data set with six predictors . The labels at the\\ntop of the plot indicate which variables enter the active set at each step. The step\\nlength are measured in units of L1arc length.\\n0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Least Angle Regression\\n0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Lasso\\nL1Arc Length L1Arc Length\\nCoeﬃcientsCoeﬃcients\\nFIGURE 3.15. Left panel shows the LAR coeﬃcient proﬁles on the simulated\\ndata, as a function of the L1arc length. The right panel shows the Lasso proﬁle.\\nThey are identical until the dark-blue coeﬃcient crosses zero a t an arc length of\\nabout 18.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 94}, page_content='76 3. Linear Methods for Regression\\nsteps and recheck the correlations in step 3; using knowledge of the covari-\\nance of the predictors and the piecewise linearity of the algorithm, we can\\nwork out the exact step length at the beginning of each step (Exercise 3.25).\\nThe right panel of Figure 3.15 shows the lasso coeﬃcient proﬁles on the\\nsame data. They are almost identical to those in the left panel, and diﬀer\\nfor the ﬁrst time when the blue coeﬃcient passes back through zero. For the\\nprostate data, the LAR coeﬃcient proﬁle turns out to be identical to the\\nlasso proﬁle in Figure 3.10, which never crosses zero. These observations\\nlead to a simple modiﬁcation of the LAR algorithm that gives the entire\\nlasso path, which is also piecewise-linear.\\nAlgorithm 3.2a Least Angle Regression: Lasso Modiﬁcation .\\n4a. If a non-zero coeﬃcient hits zero, drop its variable from the active set\\nof variables and recompute the current joint least squares direction.\\nThe LAR(lasso) algorithm is extremely eﬃcient, requiring the same order\\nof computation as that of a single least squares ﬁt using the ppredictors.\\nLeast angle regression always takes psteps to get to the full least squares\\nestimates. The lasso path can have more than psteps, although the two\\nare often quite similar. Algorithm 3.2 with the lasso modiﬁcation 3. 2a is\\nan eﬃcient way of computing the solution to any lasso problem, especially\\nwhen p≫N. Osborne et al. (2000a) also discovered a piecewise-linear path\\nfor computing the lasso, which they called a homotopy algorithm.\\nWe now give a heuristic argument for why these procedures are so similar.\\nAlthough the LAR algorithm is stated in terms of correlations, if the input\\nfeatures are standardized, it is equivalent and easier to work with inner-\\nproducts. Suppose Ais the active set of variables at some stage in the\\nalgorithm, tied in their absolute inner-product with the current residuals\\ny−Xβ. We can express this as\\nxT\\nj(y−Xβ) =γ≤sj,∀j∈ A (3.56)\\nwhere sj∈ {−1,1}indicates the sign of the inner-product, and γis the\\ncommon value. Also |xT\\nk(y−Xβ)| ≤γ∀k̸∈ A. Now consider the lasso\\ncriterion (3.52), which we write in vector form\\nR(β) =1\\n2||y−Xβ||2\\n2+λ||β||1. (3.57)\\nLetBbe the active set of variables in the solution for a given value of λ.\\nFor these variables R(β) is diﬀerentiable, and the stationarity conditions\\ngive\\nxT\\nj(y−Xβ) =λ≤sign(βj),∀j∈ B (3.58)\\nComparing (3.58) with (3.56), we see that they are identical only if the\\nsign of βjmatches the sign of the inner product. That is why the LAR'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 95}, page_content='3.4 Shrinkage Methods 77\\nalgorithm and lasso start to diﬀer when an active coeﬃcient passes through\\nzero; condition (3.58) is violated for that variable, and it is kicked out o f the\\nactive set B. Exercise 3.23 shows that these equations imply a piecewise-\\nlinear coeﬃcient proﬁle as λdecreases. The stationarity conditions for the\\nnon-active variables require that\\n|xT\\nk(y−Xβ)| ≤λ,∀k̸∈ B, (3.59)\\nwhich again agrees with the LAR algorithm.\\nFigure 3.16 compares LAR and lasso to forward stepwise and stagewise\\nregression. The setup is the same as in Figure 3.6 on page 59, except here\\nN= 100 here rather than 300, so the problem is more diﬃcult. We see\\nthat the more aggressive forward stepwise starts to overﬁt quite earl y (well\\nbefore the 10 true variables can enter the model), and ultimately performs\\nworse than the slower forward stagewise regression. The behavior of LAR\\nand lasso is similar to that of forward stagewise regression. Increment al\\nforward stagewise is similar to LAR and lasso, and is described in Sec-\\ntion 3.8.1.\\nDegrees-of-Freedom Formula for LAR and Lasso\\nSuppose that we ﬁt a linear model via the least angle regression procedure,\\nstopping at some number of steps k < p, or equivalently using a lasso bound\\ntthat produces a constrained version of the full least squares ﬁt. How many\\nparameters, or “degrees of freedom” have we used?\\nConsider ﬁrst a linear regression using a subset of kfeatures. If this subset\\nis prespeciﬁed in advance without reference to the training data, then the\\ndegrees of freedom used in the ﬁtted model is deﬁned to be k. Indeed, in\\nclassical statistics, the number of linearly independent parameters is what\\nis meant by “degrees of freedom.” Alternatively, suppose that we carry out\\na best subset selection to determine the “optimal” set of kpredictors. Then\\nthe resulting model has kparameters, but in some sense we have used up\\nmore than kdegrees of freedom.\\nWe need a more general deﬁnition for the eﬀective degrees of freedom of\\nan adaptively ﬁtted model. We deﬁne the degrees of freedom of the ﬁtted\\nvector ˆy= (ˆy1,ˆy2,... ,ˆyN) as\\ndf(ˆy) =1\\nσ2N∑\\ni=1Cov(ˆyi,yi). (3.60)\\nHere Cov(ˆ yi,yi) refers to the sampling covariance between the predicted\\nvalue ˆ yiand its corresponding outcome value yi. This makes intuitive sense:\\nthe harder that we ﬁt to the data, the larger this covariance and hence\\ndf(ˆy). Expression (3.60) is a useful notion of degrees of freedom, one that\\ncan be applied to any model prediction ˆy. This includes models that are'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 96}, page_content='78 3. Linear Methods for Regression\\n0.0 0.2 0.4 0.6 0.8 1.00.55 0.60 0.65Forward Stepwise\\nLAR\\nLasso\\nForward Stagewise\\nIncremental Forward StagewiseE||ˆβ(k)−β||2\\nFraction of L1arc-length\\nFIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward\\nstagewise (FS) and incremental forward stagewise (FS 0) regression. The setup\\nis the same as in Figure 3.6, except N= 100 here rather than 300. Here the\\nslower FS regression ultimately outperforms forward stepw ise. LAR and lasso\\nshow similar behavior to FS and FS 0. Since the procedures take diﬀerent numbers\\nof steps (across simulation replicates and methods), we plot the MSE as a function\\nof the fraction of total L1arc-length toward the least-squares ﬁt.\\nadaptively ﬁtted to the training data. This deﬁnition is motivated and\\ndiscussed further in Sections 7.4–7.6.\\nNow for a linear regression with kﬁxed predictors, it is easy to show\\nthat df( ˆy) =k. Likewise for ridge regression, this deﬁnition leads to the\\nclosed-form expression (3.50) on page 68: df( ˆy) = tr( Sλ). In both these\\ncases, (3.60) is simple to evaluate because the ﬁt ˆy=Hλyis linear in y.\\nIf we think about deﬁnition (3.60) in the context of a best subset selection\\nof size k, it seems clear that df( ˆy) will be larger than k, and this can be\\nveriﬁed by estimating Cov(ˆ yi,yi)/σ2directly by simulation. However there\\nis no closed form method for estimating df( ˆy) for best subset selection.\\nFor LAR and lasso, something magical happens. These techniques are\\nadaptive in a smoother way than best subset selection, and hence estimation\\nof degrees of freedom is more tractable. Speciﬁcally it can be shown that\\nafter the kth step of the LAR procedure, the eﬀective degrees of freedom of\\nthe ﬁt vector is exactly k. Now for the lasso, the (modiﬁed) LAR procedure'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 97}, page_content='3.5 Methods Using Derived Input Directions 79\\noften takes more than psteps, since predictors can drop out. Hence the\\ndeﬁnition is a little diﬀerent; for the lasso, at any stage df( ˆy) approximately\\nequals the number of predictors in the model. While this approximation\\nworks reasonably well anywhere in the lasso path, for each kit works best\\nat the lastmodel in the sequence that contains kpredictors. A detailed\\nstudy of the degrees of freedom for the lasso may be found in Zou et al.\\n(2007).\\n3.5 Methods Using Derived Input Directions\\nIn many situations we have a large number of inputs, often very correlated.\\nThe methods in this section produce a small number of linear combinations\\nZm, m= 1,... ,M of the original inputs Xj, and the Zmare then used in\\nplace of the Xjas inputs in the regression. The methods diﬀer in how the\\nlinear combinations are constructed.\\n3.5.1 Principal Components Regression\\nIn this approach the linear combinations Zmused are the principal com-\\nponents as deﬁned in Section 3.4.1 above.\\nPrincipal component regression forms the derived input columns zm=\\nXvm, and then regresses yonz1,z2,... ,zMfor some M≤p. Since the zm\\nare orthogonal, this regression is just a sum of univariate regressions:\\nˆypcr\\n(M)= ¯y1+M∑\\nm=1ˆθmzm, (3.61)\\nwhere ˆθm=⟨zm,y⟩/⟨zm,zm⟩. Since the zmare each linear combinations\\nof the original xj, we can express the solution (3.61) in terms of coeﬃcients\\nof thexj(Exercise 3.13):\\nˆβpcr(M) =M∑\\nm=1ˆθmvm. (3.62)\\nAs with ridge regression, principal components depend on the scaling of\\nthe inputs, so typically we ﬁrst standardize them. Note that if M=p, we\\nwould just get back the usual least squares estimates, since the columns of\\nZ=UDspan the column space of X. ForM < p we get a reduced regres-\\nsion. We see that principal components regression is very similar to ridge\\nregression: both operate via the principal components of the input ma-\\ntrix. Ridge regression shrinks the coeﬃcients of the principal components\\n(Figure 3.17), shrinking more depending on the size of the corresponding\\neigenvalue; principal components regression discards the p−Msmallest\\neigenvalue components. Figure 3.17 illustrates this.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 98}, page_content='80 3. Linear Methods for Regression\\nIndexShrinkage Factor\\n2 4 6 80.0 0.2 0.4 0.6 0.8 1.0•\\n••\\n••••\\n•• • • • • • •\\n• •ridge\\npcr\\nFIGURE 3.17. Ridge regression shrinks the regression coeﬃcients of the pri n-\\ncipal components, using shrinkage factors d2\\nj/(d2\\nj+λ)as in (3.47). Principal\\ncomponent regression truncates them. Shown are the shrinkage and t runcation\\npatterns corresponding to Figure 3.7, as a function of the princip al component\\nindex.\\nIn Figure 3.7 we see that cross-validation suggests seven terms; the re-\\nsulting model has the lowest test error in Table 3.3.\\n3.5.2 Partial Least Squares\\nThis technique also constructs a set of linear combinations of the inputs\\nfor regression, but unlike principal components regression it uses y(in ad-\\ndition to X) for this construction. Like principal component regression,\\npartial least squares (PLS) is not scale invariant, so we assume that eac h\\nxjis standardized to have mean 0 and variance 1. PLS begins by com-\\nputing ˆ ϕ1j=⟨xj,y⟩for each j. From this we construct the derived input\\nz1=∑\\njˆϕ1jxj, which is the ﬁrst partial least squares direction. Hence\\nin the construction of each zm, the inputs are weighted by the strength\\nof their univariate eﬀect on y3. The outcome yis regressed on z1giving\\ncoeﬃcient ˆθ1, and then we orthogonalize x1,... ,xpwith respect to z1. We\\ncontinue this process, until M≤pdirections have been obtained. In this\\nmanner, partial least squares produces a sequence of derived, orthogonal\\ninputs or directions z1,z2,... ,zM. As with principal-component regres-\\nsion, if we were to construct all M=pdirections, we would get back a\\nsolution equivalent to the usual least squares estimates; using M < p di-\\nrections produces a reduced regression. The procedure is described fully in\\nAlgorithm 3.3.\\n3Since the xjare standardized, the ﬁrst directions ˆ ϕ1jare the univariate regression\\ncoeﬃcients (up to an irrelevant constant); this is not the ca se for subsequent directions.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 99}, page_content='3.5 Methods Using Derived Input Directions 81\\nAlgorithm 3.3 Partial Least Squares.\\n1. Standardize each xjto have mean zero and variance one. Set ˆy(0)=\\n¯y1, andx(0)\\nj=xj, j= 1,... ,p .\\n2. For m= 1,2,... ,p\\n(a)zm=∑p\\nj=1ˆϕmjx(m−1)\\nj, where ˆ ϕmj=⟨x(m−1)\\nj,y⟩.\\n(b)ˆθm=⟨zm,y⟩/⟨zm,zm⟩.\\n(c)ˆy(m)=ˆy(m−1)+ˆθmzm.\\n(d) Orthogonalize each x(m−1)\\nj with respect to zm:x(m)\\nj=x(m−1)\\nj−\\n[⟨zm,x(m−1)\\nj⟩/⟨zm,zm⟩]zm,j= 1,2,... ,p .\\n3. Output the sequence of ﬁtted vectors {ˆy(m)}p\\n1. Since the {zℓ}m\\n1are\\nlinear in the original xj, so is ˆy(m)=Xˆβpls(m). These linear coeﬃ-\\ncients can be recovered from the sequence of PLS transformations.\\nIn the prostate cancer example, cross-validation chose M= 2 PLS direc-\\ntions in Figure 3.7. This produced the model given in the rightmost column\\nof Table 3.3.\\nWhat optimization problem is partial least squares solving? Since it uses\\nthe response yto construct its directions, its solution path is a nonlinear\\nfunction of y. It can be shown (Exercise 3.15) that partial least squares\\nseeks directions that have high variance andhave high correlation with the\\nresponse, in contrast to principal components regression which keys only\\non high variance (Stone and Brooks, 1990; Frank and Friedman, 1993). In\\nparticular, the mth principal component direction vmsolves:\\nmax αVar(Xα) (3.63)\\nsubject to ||α||= 1, αTSvℓ= 0, ℓ= 1,... ,m −1,\\nwhereSis the sample covariance matrix of the xj. The conditions αTSvℓ=\\n0 ensures that zm=Xαis uncorrelated with all the previous linear com-\\nbinations zℓ=Xvℓ. The mth PLS direction ˆ ϕmsolves:\\nmax αCorr2(y,Xα)Var(Xα) (3.64)\\nsubject to ||α||= 1, αTSˆϕℓ= 0, ℓ= 1,... ,m −1.\\nFurther analysis reveals that the variance aspect tends to dominate, and\\nso partial least squares behaves much like ridge regression and principal\\ncomponents regression. We discuss this further in the next section.\\nIf the input matrix Xis orthogonal, then partial least squares ﬁnds the\\nleast squares estimates after m= 1 steps. Subsequent steps have no eﬀect'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 100}, page_content='82 3. Linear Methods for Regression\\nsince the ˆ ϕmjare zero for m >1 (Exercise 3.14). It can also be shown that\\nthe sequence of PLS coeﬃcients for m= 1,2,... ,p represents the conjugate\\ngradient sequence for computing the least squares solutions (Exercise 3.18).\\n3.6 Discussion: A Comparison of the Selection and\\nShrinkage Methods\\nThere are some simple settings where we can understand better the rela-\\ntionship between the diﬀerent methods described above. Consider an exam-\\nple with two correlated inputs X1andX2, with correlation ρ. We assume\\nthat the true regression coeﬃcients are β1= 4 and β2= 2. Figure 3.18\\nshows the coeﬃcient proﬁles for the diﬀerent methods, as their tuning pa-\\nrameters are varied. The top panel has ρ= 0.5, the bottom panel ρ=−0.5.\\nThe tuning parameters for ridge and lasso vary over a continuous range,\\nwhile best subset, PLS and PCR take just two discrete steps to the least\\nsquares solution. In the top panel, starting at the origin, ridge regression\\nshrinks the coeﬃcients together until it ﬁnally converges to least squares.\\nPLS and PCR show similar behavior to ridge, although are discrete and\\nmore extreme. Best subset overshoots the solution and then backtracks.\\nThe behavior of the lasso is intermediate to the other methods. When the\\ncorrelation is negative (lower panel), again PLS and PCR roughly track\\nthe ridge path, while all of the methods are more similar to one another.\\nIt is interesting to compare the shrinkage behavior of these diﬀerent\\nmethods. Recall that ridge regression shrinks all directions, but shrinks\\nlow-variance directions more. Principal components regression leaves M\\nhigh-variance directions alone, and discards the rest. Interestingly, it can\\nbe shown that partial least squares also tends to shrink the low-variance\\ndirections, but can actually inﬂate some of the higher variance directions.\\nThis can make PLS a little unstable, and cause it to have slightly higher\\nprediction error compared to ridge regression. A full study is given in Frank\\nand Friedman (1993). These authors conclude that for minimizing predic-\\ntion error, ridge regression is generally preferable to variable subset selec-\\ntion, principal components regression and partial least squares. However\\nthe improvement over the latter two methods was only slight.\\nTo summarize, PLS, PCR and ridge regression tend to behave similarly.\\nRidge regression may be preferred because it shrinks smoothly, rather than\\nin discrete steps. Lasso falls somewhere between ridge regression and best\\nsubset regression, and enjoys some of the properties of each.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 101}, page_content='3.6 Discussion: A Comparison of the Selection and Shrinkage Methods 83\\n0 1 2 3 4 5 6-1 0 1 2 3Least Squares\\n0Ridge\\nLasso\\nBest SubsetPLS PCR\\n•\\n0 1 2 3 4 5 6-1 0 1 2 3Least Squares\\nRidge\\nBest Subset\\nPLS\\nPCRLasso•\\n0ρ= 0.5\\nρ=−0.5\\nβ1β1β2 β2\\nFIGURE 3.18. Coeﬃcient proﬁles from diﬀerent methods for a simple problem:\\ntwo inputs with correlation ±0.5, and the true regression coeﬃcients β= (4,2).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 102}, page_content='84 3. Linear Methods for Regression\\n3.7 Multiple Outcome Shrinkage and Selection\\nAs noted in Section 3.2.4, the least squares estimates in a multiple-output\\nlinear model are simply the individual least squares estimates for each of\\nthe outputs.\\nTo apply selection and shrinkage methods in the multiple output case,\\none could apply a univariate technique individually to each outcome or si-\\nmultaneously to all outcomes. With ridge regression, for example, we could\\napply formula (3.44) to each of the Kcolumns of the outcome matrix Y,\\nusing possibly diﬀerent parameters λ, or apply it to all columns using the\\nsame value of λ. The former strategy would allow diﬀerent amounts of\\nregularization to be applied to diﬀerent outcomes but require estimation\\nofkseparate regularization parameters λ1,... ,λ k, while the latter would\\npermit all koutputs to be used in estimating the sole regularization pa-\\nrameter λ.\\nOther more sophisticated shrinkage and selection strategies that exploit\\ncorrelations in the diﬀerent responses can be helpful in the multiple output\\ncase. Suppose for example that among the outputs we have\\nYk=f(X) +εk (3.65)\\nYℓ=f(X) +εℓ; (3.66)\\ni.e., (3.65) and (3.66) share the same structural part f(X) in their models.\\nIt is clear in this case that we should pool our observations on YkandYl\\nto estimate the common f.\\nCombining responses is at the heart of canonical correlation analysis\\n(CCA), a data reduction technique developed for the multiple output case.\\nSimilar to PCA, CCA ﬁnds a sequence of uncorrelated linear combina-\\ntionsXvm, m= 1,... ,M of the xj, and a corresponding sequence of\\nuncorrelated linear combinations Yumof the responses yk, such that the\\ncorrelations\\nCorr2(Yum,Xvm) (3.67)\\nare successively maximized. Note that at most M= min( K,p) directions\\ncan be found. The leading canonical response variates are those linear com-\\nbinations (derived responses) best predicted by the xj; in contrast, the\\ntrailing canonical variates can be poorly predicted by the xj, and are can-\\ndidates for being dropped. The CCA solution is computed using a general-\\nized SVD of the sample cross-covariance matrix YTX/N(assuming Yand\\nXare centered; Exercise 3.20).\\nReduced-rank regression (Izenman, 1975; van der Merwe and Zidek, 1980)\\nformalizes this approach in terms of a regression model that explicitly pool s\\ninformation. Given an error covariance Cov( ε) =Σ, we solve the following'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 103}, page_content='3.7 Multiple Outcome Shrinkage and Selection 85\\nrestricted multivariate regression problem:\\nˆBrr(m) = argmin\\nrank(B)=mN∑\\ni=1(yi−BTxi)TΣ−1(yi−BTxi). (3.68)\\nWithΣreplaced by the estimate YTY/N, one can show (Exercise 3.21)\\nthat the solution is given by a CCA of YandX:\\nˆBrr(m) =ˆBUmU−\\nm, (3.69)\\nwhereUmis the K×msub-matrix of Uconsisting of the ﬁrst mcolumns,\\nandUis the K×Mmatrix of leftcanonical vectors u1,u2,... ,u M.U−\\nm\\nis its generalized inverse. Writing the solution as\\nˆBrr(M) = (XTX)−1XT(YU m)U−\\nm, (3.70)\\nwe see that reduced-rank regression performs a linear regression on the\\npooled response matrix YU m, and then maps the coeﬃcients (and hence\\nthe ﬁts as well) back to the original response space. The reduced-rank ﬁts\\nare given by\\nˆYrr(m) =X(XTX)−1XTYU mU−\\nm\\n=HYP m,(3.71)\\nwhere His the usual linear regression projection operator, and Pmis the\\nrank-mCCA response projection operator. Although a better estimate of\\nΣwould be ( Y−XˆB)T(Y−XˆB)/(N−pK), one can show that the solution\\nremains the same (Exercise 3.22).\\nReduced-rank regression borrows strength among responses by truncat-\\ning the CCA. Breiman and Friedman (1997) explored with some success\\nshrinkage of the canonical variates between XandY, a smooth version of\\nreduced rank regression. Their proposal has the form (compare (3.69))\\nˆBc+w=ˆBUΛU−1, (3.72)\\nwhere Λis a diagonal shrinkage matrix (the “c+w” stands for “Curds\\nand Whey,” the name they gave to their procedure). Based on optimal\\nprediction in the population setting, they show that Λhas diagonal entries\\nλm=c2\\nm\\nc2m+p\\nN(1−c2m), m= 1,... ,M, (3.73)\\nwhere cmis the mth canonical correlation coeﬃcient. Note that as the ratio\\nof the number of input variables to sample size p/Ngets small, the shrink-\\nage factors approach 1. Breiman and Friedman (1997) proposed modiﬁed\\nversions of Λbased on training data and cross-validation, but the general\\nform is the same. Here the ﬁtted response has the form\\nˆYc+w=HYSc+w, (3.74)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 104}, page_content='86 3. Linear Methods for Regression\\nwhere Sc+w=UΛU−1is the response shrinkage operator.\\nBreiman and Friedman (1997) also suggested shrinking in both the Y\\nspace and Xspace. This leads to hybrid shrinkage models of the form\\nˆYridge,c+w=AλYSc+w, (3.75)\\nwhereAλ=X(XTX+λI)−1XTis the ridge regression shrinkage operator,\\nas in (3.46) on page 66. Their paper and the discussions thereof contain\\nmany more details.\\n3.8 More on the Lasso and Related Path\\nAlgorithms\\nSince the publication of the LAR algorithm (Efron et al., 2004) there has\\nbeen a lot of activity in developing algorithms for ﬁtting regularization\\npaths for a variety of diﬀerent problems. In addition, L1regularization has\\ntaken on a life of its own, leading to the development of the ﬁeld compressed\\nsensing in the signal-processing literature. (Donoho, 2006a; Candes, 2006).\\nIn this section we discuss some related proposals and other path algorithms,\\nstarting oﬀ with a precursor to the LAR algorithm.\\n3.8.1 Incremental Forward Stagewise Regression\\nHere we present another LAR-like algorithm, this time focused on forward\\nstagewise regression. Interestingly, eﬀorts to understand a ﬂexible nonlinear\\nregression procedure (boosting) led to a new algorithm for linear models\\n(LAR). In reading the ﬁrst edition of this book and the forward stagewise\\nAlgorithm 3.4 Incremental Forward Stagewise Regression—FS ǫ.\\n1. Start with the residual requal to yandβ1,β2,... ,β p= 0. All the\\npredictors are standardized to have mean zero and unit norm.\\n2. Find the predictor xjmost correlated with r\\n3. Update βj←βj+δj, where δj=ǫ≤sign[⟨xj,r⟩] and ǫ >0 is a small\\nstep size, and set r←r−δjxj.\\n4. Repeat steps 2 and 3 many times, until the residuals are uncorrelated\\nwith all the predictors.\\nAlgorithm 16.1 of Chapter 164, our colleague Brad Efron realized that with\\n4In the ﬁrst edition, this was Algorithm 10.4 in Chapter 10.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 105}, page_content='3.8 More on the Lasso and Related Path Algorithms 87−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0 50 100 150 200\\n−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0.0 0.5 1.0 1.5 2.0FSǫ FS0\\nIteration\\nCoeﬃcientsCoeﬃcients\\nL1Arc-length of Coeﬃcients\\nFIGURE 3.19. Coeﬃcient proﬁles for the prostate data. The left panel shows\\nincremental forward stagewise regression with step size ǫ= 0.01. The right panel\\nshows the inﬁnitesimal version FS 0obtained letting ǫ→0. This proﬁle was ﬁt by\\nthe modiﬁcation 3.2b to the LAR Algorithm 3.2. In this example t he FS 0proﬁles\\nare monotone, and hence identical to those of lasso and LAR.\\nlinear models, one could explicitly construct the piecewise-linear lasso paths\\nof Figure 3.10. This led him to propose the LAR procedure of Section 3.4.4,\\nas well as the incremental version of forward-stagewise regression presented\\nhere.\\nConsider the linear-regression version of the forward-stagewise boosting\\nalgorithm 16.1 proposed in Section 16.1 (page 608). It generates a coeﬃcient\\nproﬁle by repeatedly updating (by a small amount ǫ) the coeﬃcient of the\\nvariable most correlated with the current residuals. Algorithm 3.4 gives\\nthe details. Figure 3.19 (left panel) shows the progress of the algorithm on\\nthe prostate data with step size ǫ= 0.01. If δj=⟨xj,r⟩(the least-squares\\ncoeﬃcient of the residual on jth predictor), then this is exactly the usual\\nforward stagewise procedure (FS) outlined in Section 3.3.3.\\nHere we are mainly interested in small values of ǫ. Letting ǫ→0 gives\\nthe right panel of Figure 3.19, which in this case is identical to the lasso\\npath in Figure 3.10. We call this limiting procedure inﬁnitesimal forward\\nstagewise regression or FS 0. This procedure plays an important role in\\nnon-linear, adaptive methods like boosting (Chapters 10 and 16) and is the\\nversion of incremental forward stagewise regression that is most amenabl e\\nto theoretical analysis. B¨ uhlmann and Hothorn (2007) refer to the same\\nprocedure as “L2boost”, because of its connections to boosting.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 106}, page_content='88 3. Linear Methods for Regression\\nEfron originally thought that the LAR Algorithm 3.2 was an implemen-\\ntation of FS 0, allowing each tied predictor a chance to update their coeﬃ-\\ncients in a balanced way, while remaining tied in correlation. However, he\\nthen realized that the LAR least-squares ﬁt amongst the tied predictors\\ncan result in coeﬃcients moving in the opposite direction to their correla-\\ntion, which cannot happen in Algorithm 3.4. The following modiﬁcation of\\nthe LAR algorithm implements FS 0:\\nAlgorithm 3.2b Least Angle Regression: FS 0Modiﬁcation .\\n4. Find the new direction by solving the constrained least squares prob-\\nlem\\nmin\\nb||r−XAb||2\\n2subject to bjsj≥0, j∈ A,\\nwhere sjis the sign of ⟨xj,r⟩.\\nThe modiﬁcation amounts to a non-negative least squares ﬁt, keeping the\\nsigns of the coeﬃcients the same as those of the correlations. One can show\\nthat this achieves the optimal balancing of inﬁnitesimal “update turns”\\nfor the variables tied for maximal correlation (Hastie et al., 2007) . Like\\nlasso, the entire FS 0path can be computed very eﬃciently via the LAR\\nalgorithm.\\nAs a consequence of these results, if the LAR proﬁles are monotone non-\\nincreasing or non-decreasing, as they are in Figure 3.19, then all three\\nmethods—LAR, lasso, and FS 0—give identical proﬁles. If the proﬁles are\\nnot monotone but do not cross the zero axis, then LAR and lasso are\\nidentical.\\nSince FS 0is diﬀerent from the lasso, it is natural to ask if it optimizes\\na criterion. The answer is more complex than for lasso; the FS 0coeﬃcient\\nproﬁle is the solution to a diﬀerential equation. While the lasso makes op-\\ntimal progress in terms of reducing the residual sum-of-squares per unit\\nincrease in L1-norm of the coeﬃcient vector β, FS0is optimal per unit\\nincrease in L1arc-length traveled along the coeﬃcient path. Hence its co-\\neﬃcient path is discouraged from changing directions too often.\\nFS0is more constrained than lasso, and in fact can be viewed as a mono-\\ntone version of the lasso; see Figure 16.3 on page 614 for a dramatic exa m-\\nple. FS 0may be useful in p≫Nsituations, where its coeﬃcient proﬁles\\nare much smoother and hence have less variance than those of lasso. More\\ndetails on FS 0are given in Section 16.2.3 and Hastie et al. (2007). Fig-\\nure 3.16 includes FS 0where its performance is very similar to that of the\\nlasso.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 107}, page_content='3.8 More on the Lasso and Related Path Algorithms 89\\n3.8.2 Piecewise-Linear Path Algorithms\\nThe least angle regression procedure exploits the piecewise linear nature of\\nthe lasso solution paths. It has led to similar “path algorithms” for o ther\\nregularized problems. Suppose we solve\\nˆβ(λ) = argminβ[R(β) +λJ(β)], (3.76)\\nwith\\nR(β) =N∑\\ni=1L(yi,β0+p∑\\nj=1xijβj), (3.77)\\nwhere both the loss function Land the penalty function Jare convex.\\nThen the following are suﬃcient conditions for the solution path ˆβ(λ) to\\nbe piecewise linear (Rosset and Zhu, 2007):\\n1.Ris quadratic or piecewise-quadratic as a function of β, and\\n2.Jis piecewise linear in β.\\nThis also implies (in principle) that the solution path can be eﬃciently\\ncomputed. Examples include squared- and absolute-error loss, “Huberized”\\nlosses, and the L1,L∞penalties on β. Another example is the “hinge loss”\\nfunction used in the support vector machine. There the loss is piecewise\\nlinear, and the penalty is quadratic. Interestingly, this leads to a piecewise-\\nlinear path algorithm in the dual space ; more details are given in Sec-\\ntion 12.3.5.\\n3.8.3 The Dantzig Selector\\nCandes and Tao (2007) proposed the following criterion:\\nminβ||β||1subject to ||XT(y−Xβ)||∞≤s. (3.78)\\nThey call the solution the Dantzig selector (DS). It can be written equiva-\\nlently as\\nminβ||XT(y−Xβ)||∞subject to ||β||1≤t. (3.79)\\nHere|| ≤ ||∞denotes the L∞norm, the maximum absolute value of the\\ncomponents of the vector. In this form it resembles the lasso, replacing\\nsquared error loss by the maximum absolute value of its gradient. Note\\nthat as tgets large, both procedures yield the least squares solution if\\nN < p . Ifp≥N, they both yield the least squares solution with minimum\\nL1norm. However for smaller values of t, the DS procedure produces a\\ndiﬀerent path of solutions than the lasso.\\nCandes and Tao (2007) show that the solution to DS is a linear pro-\\ngramming problem; hence the name Dantzig selector, in honor of the late'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 108}, page_content='90 3. Linear Methods for Regression\\nGeorge Dantzig, the inventor of the simplex method for linear program-\\nming. They also prove a number of interesting mathematical properties for\\nthe method, related to its ability to recover an underlying sparse coeﬃ-\\ncient vector. These same properties also hold for the lasso, as shown later\\nby Bickel et al. (2008).\\nUnfortunately the operating properties of the DS method are somewhat\\nunsatisfactory. The method seems similar in spirit to the lasso, especiall y\\nwhen we look at the lasso’s stationary conditions (3.58). Like the LAR a l-\\ngorithm, the lasso maintains the same inner product (and correlation) with\\nthe current residual for all variables in the active set, and moves their co-\\neﬃcients to optimally decrease the residual sum of squares. In the process,\\nthis common correlation is decreased monotonically (Exercise 3.23), and at\\nall times this correlation is larger than that for non-active variables. The\\nDantzig selector instead tries to minimize the maximum inner product of\\nthe current residual with all the predictors. Hence it can achieve a smaller\\nmaximum than the lasso, but in the process a curious phenomenon can\\noccur. If the size of the active set is m, there will be mvariables tied with\\nmaximum correlation. However, these need not coincide with the active set!\\nHence it can include a variable in the model that has smaller correlation\\nwith the current residual than some of the excluded variables (Efron et\\nal., 2007). This seems unreasonable and may be responsible for its some-\\ntimes inferior prediction accuracy. Efron et al. (2007) also show that DS\\ncan yield extremely erratic coeﬃcient paths as the regularization parameter\\nsis varied.\\n3.8.4 The Grouped Lasso\\nIn some problems, the predictors belong to pre-deﬁned groups; for example\\ngenes that belong to the same biological pathway, or collections of indicator\\n(dummy) variables for representing the levels of a categorical predictor. In\\nthis situation it may be desirable to shrink and select the members of a\\ngroup together. The grouped lasso is one way to achieve this. Suppose that\\ntheppredictors are divided into Lgroups, with pℓthe number in group\\nℓ. For ease of notation, we use a matrix Xℓto represent the predictors\\ncorresponding to the ℓth group, with corresponding coeﬃcient vector βℓ.\\nThe grouped-lasso minimizes the convex criterion\\nmin\\nβ∈I Rp(\\n||y−β01−L∑\\nℓ=1Xℓβℓ||2\\n2+λL∑\\nℓ=1√pℓ||βℓ||2)\\n, (3.80)\\nwhere the√pℓterms accounts for the varying group sizes, and || ≤ ||2is\\nthe Euclidean norm (not squared). Since the Euclidean norm of a vector\\nβℓis zero only if all of its components are zero, this procedure encourages\\nsparsity at both the group and individual levels. That is, for some values of\\nλ, an entire group of predictors may drop out of the model. This procedure'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 109}, page_content='3.8 More on the Lasso and Related Path Algorithms 91\\nwas proposed by Bakin (1999) and Lin and Zhang (2006), and studied and\\ngeneralized by Yuan and Lin (2007). Generalizations include more general\\nL2norms ||η||K= (ηTKη)1/2, as well as allowing overlapping groups of\\npredictors (Zhao et al., 2008). There are also connections to methods for\\nﬁtting sparse additive models (Lin and Zhang, 2006; Ravikumar et al.,\\n2008).\\n3.8.5 Further Properties of the Lasso\\nA number of authors have studied the ability of the lasso and related pro-\\ncedures to recover the correct model, as Nandpgrow. Examples of this\\nwork include Knight and Fu (2000), Greenshtein and Ritov (2004), Tropp\\n(2004), Donoho (2006b), Meinshausen (2007), Meinshausen and B¨ uhlmann\\n(2006), Tropp (2006), Zhao and Yu (2006), Wainwright (2006), and Bunea\\net al. (2007). For example Donoho (2006b) focuses on the p > N case and\\nconsiders the lasso solution as the bound tgets large. In the limit this gives\\nthe solution with minimum L1norm among all models with zero training\\nerror. He shows that under certain assumptions on the model matrix X, if\\nthe true model is sparse, this solution identiﬁes the correct predictors with\\nhigh probability.\\nMany of the results in this area assume a condition on the model matrix\\nof the form\\n||(XSTXS)−1XSTXSc||∞≤(1−ǫ) for some ǫ∈(0,1]. (3.81)\\nHereSindexes the subset of features with non-zero coeﬃcients in the true\\nunderlying model, and XSare the columns of Xcorresponding to those\\nfeatures. Similarly Scare the features with true coeﬃcients equal to zero,\\nandXScthe corresponding columns. This says that the least squares coef-\\nﬁcients for the columns of XSconXSare not too large, that is, the “good”\\nvariables Sare not too highly correlated with the nuisance variables Sc.\\nRegarding the coeﬃcients themselves, the lasso shrinkage causes the esti-\\nmates of the non-zero coeﬃcients to be biased towards zero, and in general\\nthey are not consistent5. One approach for reducing this bias is to run\\nthe lasso to identify the set of non-zero coeﬃcients, and then ﬁt an un-\\nrestricted linear model to the selected set of features. This is not always\\nfeasible, if the selected set is large. Alternatively, one can use the lasso to\\nselect the set of non-zero predictors, and then apply the lasso again, but\\nusing only the selected predictors from the ﬁrst step. This is known as the\\nrelaxed lasso (Meinshausen, 2007). The idea is to use cross-validation to\\nestimate the initial penalty parameter for the lasso, and then again for a\\nsecond penalty parameter applied to the selected set of predictors. Since\\n5Statistical consistency means as the sample size grows, the estimates converge to\\nthe true values.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 110}, page_content='92 3. Linear Methods for Regression\\nthe variables in the second step have less “competition” from noise vari-\\nables, cross-validation will tend to pick a smaller value for λ, and hence\\ntheir coeﬃcients will be shrunken less than those in the initial estimate.\\nAlternatively, one can modify the lasso penalty function so that larger co-\\neﬃcients are shrunken less severely; the smoothly clipped absolute deviation\\n(SCAD) penalty of Fan and Li (2005) replaces λ|β|byJa(β,λ), where\\ndJa(β,λ)\\ndβ=λ≤sign(β)[\\nI(|β| ≤λ) +(aλ− |β|)+\\n(a−1)λI(|β|> λ)]\\n(3.82)\\nfor some a≥2. The second term in square-braces reduces the amount of\\nshrinkage in the lasso for larger values of β, with ultimately no shrinkage\\nasa→ ∞. Figure 3.20 shows the SCAD penalty, along with the lasso and\\n−4 −2 0 2 40 1 2 3 4 5\\n−4 −2 0 2 40.0 0.5 1.0 1.5 2.0 2.5\\n−4 −2 0 2 40.5 1.0 1.5 2.0|β| SCAD |β|1−ν\\nβ β β\\nFIGURE 3.20. The lasso and two alternative non-convex penalties designed to\\npenalize large coeﬃcients less. For SCAD we use λ= 1anda= 4, and ν=1\\n2in\\nthe last panel.\\n|β|1−ν. However this criterion is non-convex, which is a drawback since it\\nmakes the computation much more diﬃcult. The adaptive lasso (Zou, 2006)\\nuses a weighted penalty of the form∑p\\nj=1wj|βj|where wj= 1/|ˆβj|ν,ˆβjis\\nthe ordinary least squares estimate and ν >0. This is a practical approxi-\\nmation to the |β|qpenalties ( q= 1−νhere) discussed in Section 3.4.3. The\\nadaptive lasso yields consistent estimates of the parameters while retaining\\nthe attractive convexity property of the lasso.\\n3.8.6 Pathwise Coordinate Optimization\\nAn alternate approach to the LARS algorithm for computing the lasso\\nsolution is simple coordinate descent. This idea was proposed by Fu (1998)\\nand Daubechies et al. (2004), and later studied and generalized by Friedman\\net al. (2007), Wu and Lange (2008) and others. The idea is to ﬁx the penalty\\nparameter λin the Lagrangian form (3.52) and optimize successively over\\neach parameter, holding the other parameters ﬁxed at their current values.\\nSuppose the predictors are all standardized to have mean zero and unit\\nnorm. Denote by ˜βk(λ) the current estimate for βkat penalty parameter'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 111}, page_content='3.9 Computational Considerations 93\\nλ. We can rearrange (3.52) to isolate βj,\\nR(˜β(λ),βj) =1\\n2N∑\\ni=1(\\nyi−∑\\nk̸=jxik˜βk(λ)−xijβj)2\\n+λ∑\\nk̸=j|˜βk(λ)|+λ|βj|,\\n(3.83)\\nwhere we have suppressed the intercept and introduced a factor1\\n2for con-\\nvenience. This can be viewed as a univariate lasso problem with response\\nvariable the partial residual yi−˜y(j)\\ni=yi−∑\\nk̸=jxik˜βk(λ). This has an\\nexplicit solution, resulting in the update\\n˜βj(λ)←S(N∑\\ni=1xij(yi−˜y(j)\\ni),λ)\\n. (3.84)\\nHereS(t,λ) = sign( t)(|t|−λ)+is the soft-thresholding operator in Table 3.4\\non page 71. The ﬁrst argument to S(≤) is the simple least-squares coeﬃcient\\nof the partial residual on the standardized variable xij. Repeated iteration\\nof (3.84)—cycling through each variable in turn until convergence—yields\\nthe lasso estimate ˆβ(λ).\\nWe can also use this simple algorithm to eﬃciently compute the lasso\\nsolutions at a grid of values of λ. We start with the smallest value λmax\\nfor which ˆβ(λmax) = 0, decrease it a little and cycle through the variables\\nuntil convergence. Then λis decreased again and the process is repeated,\\nusing the previous solution as a “warm start” for the new value of λ. This\\ncan be faster than the LARS algorithm, especially in large problems. A\\nkey to its speed is the fact that the quantities in (3.84) can be updated\\nquickly as jvaries, and often the update is to leave ˜βj= 0. On the other\\nhand, it delivers solutions over a grid of λvalues, rather than the entire\\nsolution path. The same kind of algorithm can be applied to the elastic\\nnet, the grouped lasso and many other models in which the penalty is a\\nsum of functions of the individual parameters (Friedman et al., 2010). It\\ncan also be applied, with some substantial modiﬁcations, to the fused lasso\\n(Section 18.4.2); details are in Friedman et al. (2007).\\n3.9 Computational Considerations\\nLeast squares ﬁtting is usually done via the Cholesky decomposition of\\nthe matrix XTXor a QR decomposition of X. With Nobservations and p\\nfeatures, the Cholesky decomposition requires p3+Np2/2 operations, while\\nthe QR decomposition requires Np2operations. Depending on the relative\\nsize of Nandp, the Cholesky can sometimes be faster; on the other hand,\\nit can be less numerically stable (Lawson and Hansen, 1974). Computation\\nof the lasso via the LAR algorithm has the same order of computation as\\na least squares ﬁt.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 112}, page_content='94 3. Linear Methods for Regression\\nBibliographic Notes\\nLinear regression is discussed in many statistics books, for example, Seber\\n(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regression wa s\\nintroduced by Hoerl and Kennard (1970), while the lasso was proposed by\\nTibshirani (1996). Around the same time, lasso-type penalties were pro-\\nposed in the basis pursuit method for signal processing (Chen et al., 1998).\\nThe least angle regression procedure was proposed in Efron et al. (2004);\\nrelated to this is the earlier homotopy procedure of Osborne et al. (2000a)\\nand Osborne et al. (2000b). Their algorithm also exploits the piecewise\\nlinearity used in the LAR/lasso algorithm, but lacks its transparency. The\\ncriterion for the forward stagewise criterion is discussed in Hastie et a l.\\n(2007). Park and Hastie (2007) develop a path algorithm similar to l east\\nangle regression for generalized regression models. Partial least squares\\nwas introduced by Wold (1975). Comparisons of shrinkage methods may\\nbe found in Copas (1983) and Frank and Friedman (1993).\\nExercises\\nEx. 3.1 Show that the Fstatistic (3.13) for dropping a single coeﬃcient\\nfrom a model is equal to the square of the corresponding z-score (3.12).\\nEx. 3.2 Given data on two variables XandY, consider ﬁtting a cubic\\npolynomial regression model f(X) =∑3\\nj=0βjXj. In addition to plotting\\nthe ﬁtted curve, you would like a 95% conﬁdence band about the curve.\\nConsider the following two approaches:\\n1. At each point x0, form a 95% conﬁdence interval for the linear func-\\ntionaTβ=∑3\\nj=0βjxj\\n0.\\n2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\\nconﬁdence intervals for f(x0).\\nHow do these approaches diﬀer? Which band is likely to be wider? Conduct\\na small simulation experiment to compare the two methods.\\nEx. 3.3 Gauss–Markov theorem:\\n(a) Prove the Gauss–Markov theorem: the least squares estimate of a\\nparameter aTβhas variance no bigger than that of any other linear\\nunbiased estimate of aTβ(Section 3.2.2).\\n(b) The matrix inequality B⪯Aholds if A−Bis positive semideﬁnite.\\nShow that if ˆVis the variance-covariance matrix of the least squares\\nestimate of βand˜Vis the variance-covariance matrix of any other\\nlinear unbiased estimate, then ˆV⪯˜V.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 113}, page_content='Exercises 95\\nEx. 3.4 Show how the vector of least squares coeﬃcients can be obtained\\nfrom a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Rep-\\nresent your solution in terms of the QR decomposition of X.\\nEx. 3.5 Consider the ridge regression problem (3.41). Show that this prob-\\nlem is equivalent to the problem\\nˆβc= argmin\\nβc{N∑\\ni=1[\\nyi−βc\\n0−p∑\\nj=1(xij−¯xj)βc\\nj]2+λp∑\\nj=1βc\\nj2}\\n.(3.85)\\nGive the correspondence between βcand the original βin (3.41). Char-\\nacterize the solution to this modiﬁed criterion. Show that a similar result\\nholds for the lasso.\\nEx. 3.6 Show that the ridge regression estimate is the mean (and mode)\\nof the posterior distribution, under a Gaussian prior β∼N(0,τI), and\\nGaussian sampling model y∼N(Xβ,σ2I). Find the relationship between\\nthe regularization parameter λin the ridge formula, and the variances τ\\nandσ2.\\nEx. 3.7 Assume yi∼N(β0+xT\\niβ,σ2),i= 1,2,... ,N , and the parameters\\nβjare each distributed as N(0,τ2), independently of one another. Assuming\\nσ2andτ2are known, show that the (minus) log-posterior density of βis\\nproportional to∑N\\ni=1(yi−β0−∑\\njxijβj)2+λ∑p\\nj=1β2\\njwhere λ=σ2/τ2.\\nEx. 3.8 Consider the QR decomposition of the uncentered N×(p+ 1)\\nmatrix X(whose ﬁrst column is all ones), and the SVD of the N×p\\ncentered matrix ˜X. Show that Q2andUspan the same subspace, where\\nQ2is the sub-matrix of Qwith the ﬁrst column removed. Under what\\ncircumstances will they be the same, up to sign ﬂips?\\nEx. 3.9 Forward stepwise regression. Suppose we have the QR decomposi-\\ntion for the N×qmatrix X1in a multiple regression problem with response\\ny, and we have an additional p−qpredictors in the matrix X2. Denote the\\ncurrent residual by r. We wish to establish which one of these additional\\nvariables will reduce the residual-sum-of squares the most when included\\nwith those in X1. Describe an eﬃcient procedure for doing this.\\nEx. 3.10 Backward stepwise regression. Suppose we have the multiple re-\\ngression ﬁt of yonX, along with the standard errors and Z-scores as in\\nTable 3.2. We wish to establish which variable, when dropped, will increase\\nthe residual sum-of-squares the least. How would you do this?\\nEx. 3.11 Show that the solution to the multivariate linear regression prob-\\nlem (3.40) is given by (3.39). What happens if the covariance matrices Σi\\nare diﬀerent for each observation?'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 114}, page_content='96 3. Linear Methods for Regression\\nEx. 3.12 Show that the ridge regression estimates can be obtained by\\nordinary least squares regression on an augmented data set. We augment\\nthe centered matrix Xwithpadditional rows√\\nλI, and augment ywithp\\nzeros. By introducing artiﬁcial data having response value zero, the ﬁtting\\nprocedure is forced to shrink the coeﬃcients toward zero. This is related to\\nthe idea of hintsdue to Abu-Mostafa (1995), where model constraints are\\nimplemented by adding artiﬁcial data examples that satisfy them.\\nEx. 3.13 Derive the expression (3.62), and show that ˆβpcr(p) =ˆβls.\\nEx. 3.14 Show that in the orthogonal case, PLS stops after m= 1 steps,\\nbecause subsequent ˆ ϕmjin step 2 in Algorithm 3.3 are zero.\\nEx. 3.15 Verify expression (3.64), and hence show that the partial least\\nsquares directions are a compromise between the ordinary regression coef-\\nﬁcient and the principal component directions.\\nEx. 3.16 Derive the entries in Table 3.4, the explicit forms for estimators\\nin the orthogonal case.\\nEx. 3.17 Repeat the analysis of Table 3.3 on the spam data discussed in\\nChapter 1.\\nEx. 3.18 Read about conjugate gradient algorithms (Murray et al., 1981, for\\nexample), and establish a connection between these algorithms and partial\\nleast squares.\\nEx. 3.19 Show that ∥ˆβridge∥increases as its tuning parameter λ→0. Does\\nthe same property hold for the lasso and partial least squares estimates?\\nFor the latter, consider the “tuning parameter” to be the successive steps\\nin the algorithm.\\nEx. 3.20 Consider the canonical-correlation problem (3.67). Show that the\\nleading pair of canonical variates u1andv1solve the problem\\nmax\\nuT(YTY)u=1\\nvT(XTX)v=1uT(YTX)v, (3.86)\\na generalized SVD problem. Show that the solution is given by u1=\\n(YTY)−1\\n2u∗\\n1, and v1= (XTX)−1\\n2v∗\\n1, where u∗\\n1andv∗\\n1are the leading left\\nand right singular vectors in\\n(YTY)−1\\n2(YTX)(XTX)−1\\n2=U∗D∗V∗T. (3.87)\\nShow that the entire sequence um, vm, m= 1,... ,min(K,p) is also given\\nby (3.87).\\nEx. 3.21 Show that the solution to the reduced-rank regression problem\\n(3.68), with Σestimated by YTY/N, is given by (3.69). Hint: Transform'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 115}, page_content='Exercises 97\\nYtoY∗=YΣ−1\\n2, and solved in terms of the canonical vectors u∗\\nm. Show\\nthatUm=Σ−1\\n2U∗\\nm, and a generalized inverse is U−\\nm=U∗\\nmTΣ1\\n2.\\nEx. 3.22 Show that the solution in Exercise 3.21 does not change if Σis\\nestimated by the more natural quantity ( Y−XˆB)T(Y−XˆB)/(N−pK).\\nEx. 3.23 Consider a regression problem with all variables and response hav-\\ning mean zero and standard deviation one. Suppose also that each variable\\nhas identical absolute correlation with the response:\\n1\\nN|⟨xj,y⟩|=λ, j= 1,... ,p.\\nLetˆβbe the least-squares coeﬃcient of yonX, and let u(α) =αXˆβfor\\nα∈[0,1] be the vector that moves a fraction αtoward the least squares ﬁt\\nu. LetRSSbe the residual sum-of-squares from the full least squares ﬁt.\\n(a) Show that\\n1\\nN|⟨xj,y−u(α)⟩|= (1−α)λ, j= 1,... ,p,\\nand hence the correlations of each xjwith the residuals remain equal\\nin magnitude as we progress toward u.\\n(b) Show that these correlations are all equal to\\nλ(α) =(1−α)√\\n(1−α)2+α(2−α)\\nN≤RSS≤λ,\\nand hence they decrease monotonically to zero.\\n(c) Use these results to show that the LAR algorithm in Section 3.4.4\\nkeeps the correlations tied and monotonically decreasing, as claimed\\nin (3.55).\\nEx. 3.24 LAR directions. Using the notation around equation (3.55) on\\npage 74, show that the LAR direction makes an equal angle with each of\\nthe predictors in Ak.\\nEx. 3.25 LAR look-ahead (Efron et al., 2004, Sec. 2). Starting at the be-\\nginning of the kth step of the LAR algorithm, derive expressions to identify\\nthe next variable to enter the active set at step k+1, and the value of αat\\nwhich this occurs (using the notation around equation (3.55) on page 74).\\nEx. 3.26 Forward stepwise regression enters the variable at each step that\\nmost reduces the residual sum-of-squares. LAR adjusts variables that have\\nthe most (absolute) correlation with the current residuals. Show that these\\ntwo entry criteria are not necessarily the same. [Hint: let xj.Abe the jth'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 116}, page_content='98 3. Linear Methods for Regression\\nvariable, linearly adjusted for all the variables currently in the model. Show\\nthat the ﬁrst criterion amounts to identifying the jfor which Cor( xj.A,r)\\nis largest in magnitude.\\nEx. 3.27 Lasso and LAR : Consider the lasso problem in Lagrange multiplier\\nform: with L(β) =1\\n2∑\\ni(yi−∑\\njxijβj)2, we minimize\\nL(β) +λ∑\\nj|βj| (3.88)\\nfor ﬁxed λ >0.\\n(a) Setting βj=β+\\nj−β−\\njwithβ+\\nj,β−\\nj≥0, expression (3.88) becomes\\nL(β) +λ∑\\nj(β+\\nj+β−\\nj). Show that the Lagrange dual function is\\nL(β) +λ∑\\nj(β+\\nj+β−\\nj)−∑\\njλ+\\njβ+\\nj−∑\\njλ−\\njβ−\\nj (3.89)\\nand the Karush–Kuhn–Tucker optimality conditions are\\n∇L(β)j+λ−λ+\\nj= 0\\n−∇L(β)j+λ−λ−\\nj= 0\\nλ+\\njβ+\\nj= 0\\nλ−\\njβ−\\nj= 0,\\nalong with the non-negativity constraints on the parameters and all\\nthe Lagrange multipliers.\\n(b) Show that |∇L(β)j| ≤λ∀j,and that the KKT conditions imply one\\nof the following three scenarios:\\nλ= 0⇒ ∇ L(β)j= 0∀j\\nβ+\\nj>0, λ > 0⇒λ+\\nj= 0,∇L(β)j=−λ <0, β−\\nj= 0\\nβ−\\nj>0, λ > 0⇒λ−\\nj= 0,∇L(β)j=λ >0, β+\\nj= 0.\\nHence show that for any “active” predictor having βj̸= 0, we must\\nhave∇L(β)j=−λifβj>0, and ∇L(β)j=λifβj<0. Assuming\\nthe predictors are standardized, relate λto the correlation between\\nthejth predictor and the current residuals.\\n(c) Suppose that the set of active predictors is unchanged for λ0≥λ≥λ1.\\nShow that there is a vector γ0such that\\nˆβ(λ) =ˆβ(λ0)−(λ−λ0)γ0 (3.90)\\nThus the lasso solution path is linear as λranges from λ0toλ1(Efron\\net al., 2004; Rosset and Zhu, 2007).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 117}, page_content='Exercises 99\\nEx. 3.28 Suppose for a given tin (3.51), the ﬁtted lasso coeﬃcient for\\nvariable Xjisˆβj=a. Suppose we augment our set of variables with an\\nidentical copy X∗\\nj=Xj. Characterize the eﬀect of this exact collinearity\\nby describing the set of solutions for ˆβjandˆβ∗\\nj, using the same value of t.\\nEx. 3.29 Suppose we run a ridge regression with parameter λon a single\\nvariable X, and get coeﬃcient a. We now include an exact copy X∗=X,\\nand reﬁt our ridge regression. Show that both coeﬃcients are identical, and\\nderive their value. Show in general that if mcopies of a variable Xjare\\nincluded in a ridge regression, their coeﬃcients are all the same.\\nEx. 3.30 Consider the elastic-net optimization problem:\\nmin\\nβ||y−Xβ||2+λ[\\nα||β||2\\n2+ (1−α)||β||1]\\n. (3.91)\\nShow how one can turn this into a lasso problem, using an augmented\\nversion of Xandy.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 118}, page_content='100 3. Linear Methods for Regression'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 119}, page_content='This is page 101\\nPrinter: Opaque this\\n4\\nLinear Methods for Classiﬁcation\\n4.1 Introduction\\nIn this chapter we revisit the classiﬁcation problem and focus on linear\\nmethods for classiﬁcation. Since our predictor G(x) takes values in a dis-\\ncrete set G, we can always divide the input space into a collection of regions\\nlabeled according to the classiﬁcation. We saw in Chapter 2 that the bound-\\naries of these regions can be rough or smooth, depending on the prediction\\nfunction. For an important class of procedures, these decision boundaries\\nare linear; this is what we will mean by linear methods for classiﬁcation.\\nThere are several diﬀerent ways in which linear decision boundaries can\\nbe found. In Chapter 2 we ﬁt linear regression models to the class indicator\\nvariables, and classify to the largest ﬁt. Suppose there are Kclasses, for\\nconvenience labeled 1 ,2,... ,K , and the ﬁtted linear model for the kth\\nindicator response variable is ˆfk(x) =ˆβk0+ˆβT\\nkx. The decision boundary\\nbetween class kandℓis that set of points for which ˆfk(x) =ˆfℓ(x), that is,\\nthe set {x: (ˆβk0−ˆβℓ0) + (ˆβk−ˆβℓ)Tx= 0}, an aﬃne set or hyperplane1\\nSince the same is true for any pair of classes, the input space is divided\\ninto regions of constant classiﬁcation, with piecewise hyperplanar decision\\nboundaries. This regression approach is a member of a class of methods\\nthat model discriminant functions δk(x) for each class, and then classify x\\nto the class with the largest value for its discriminant function. Methods\\n1Strictly speaking, a hyperplane passes through the origin, while an aﬃne set need\\nnot. We sometimes ignore the distinction and refer in genera l to hyperplanes.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 120}, page_content='102 4. Linear Methods for Classiﬁcation\\nthat model the posterior probabilities Pr( G=k|X=x) are also in this\\nclass. Clearly, if either the δk(x) or Pr( G=k|X=x) are linear in x, then\\nthe decision boundaries will be linear.\\nActually, all we require is that some monotone transformation of δkor\\nPr(G=k|X=x) be linear for the decision boundaries to be linear. For\\nexample, if there are two classes, a popular model for the posterior proba-\\nbilities is\\nPr(G= 1|X=x) =exp(β0+βTx)\\n1 + exp( β0+βTx),\\nPr(G= 2|X=x) =1\\n1 + exp( β0+βTx).(4.1)\\nHere the monotone transformation is the logittransformation: log[ p/(1−p)],\\nand in fact we see that\\nlogPr(G= 1|X=x)\\nPr(G= 2|X=x)=β0+βTx. (4.2)\\nThe decision boundary is the set of points for which the log-odds are zero,\\nand this is a hyperplane deﬁned by{\\nx|β0+βTx= 0}\\n. We discuss two very\\npopular but diﬀerent methods that result in linear log-odds or logits: linear\\ndiscriminant analysis and linear logistic regression. Although they diﬀer in\\ntheir derivation, the essential diﬀerence between them is in the way the\\nlinear function is ﬁt to the training data.\\nA more direct approach is to explicitly model the boundaries between\\nthe classes as linear. For a two-class problem in a p-dimensional input\\nspace, this amounts to modeling the decision boundary as a hyperplane—in\\nother words, a normal vector and a cut-point. We will look at two methods\\nthat explicitly look for “separating hyperplanes.” The ﬁrst is the well-\\nknown perceptron model of Rosenblatt (1958), with an algorithm that ﬁnds\\na separating hyperplane in the training data, if one exists. The second\\nmethod, due to Vapnik (1996), ﬁnds an optimally separating hyperplane if\\none exists, else ﬁnds a hyperplane that minimizes some measure of overlap\\nin the training data. We treat the separable case here, and defer treatment\\nof the nonseparable case to Chapter 12.\\nWhile this entire chapter is devoted to linear decision boundaries, there is\\nconsiderable scope for generalization. For example, we can expand our vari-\\nable set X1,... ,X pby including their squares and cross-products X2\\n1,X2\\n2,... ,\\nX1X2,..., thereby adding p(p+1)/2 additional variables. Linear functions\\nin the augmented space map down to quadratic functions in the original\\nspace—hence linear decision boundaries to quadratic decision boundaries.\\nFigure 4.1 illustrates the idea. The data are the same: the left plot uses\\nlinear decision boundaries in the two-dimensional space shown, while the\\nright plot uses linear decision boundaries in the augmented ﬁve-dimensional\\nspace described above. This approach can be used with any basis transfor-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 121}, page_content='4.2 Linear Regression of an Indicator Matrix 103\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\nFIGURE 4.1. The left plot shows some data from three classes, with linear\\ndecision boundaries found by linear discriminant analysis. The ri ght plot shows\\nquadratic decision boundaries. These were obtained by ﬁnding line ar boundaries\\nin the ﬁve-dimensional space X1, X2, X1X2, X2\\n1, X2\\n2. Linear inequalities in this\\nspace are quadratic inequalities in the original space.\\nmation h(X) where h: IRp↦→IRqwithq > p, and will be explored in later\\nchapters.\\n4.2 Linear Regression of an Indicator Matrix\\nHere each of the response categories are coded via an indicator variable.\\nThus if GhasKclasses, there will be Ksuch indicators Yk, k= 1,... ,K ,\\nwithYk= 1 if G=kelse 0. These are collected together in a vector\\nY= (Y1,... ,Y K), and the Ntraining instances of these form an N×K\\nindicator response matrix Y.Yis a matrix of 0’s and 1’s, with each row\\nhaving a single 1. We ﬁt a linear regression model to each of the columns\\nofYsimultaneously, and the ﬁt is given by\\nˆY=X(XTX)−1XTY. (4.3)\\nChapter 3 has more details on linear regression. Note that we have a coeﬃ-\\ncient vector for each response column yk, and hence a ( p+1)×Kcoeﬃcient\\nmatrix ˆB= (XTX)−1XTY. HereXis the model matrix with p+1 columns\\ncorresponding to the pinputs, and a leading column of 1’s for the intercept.\\nA new observation with input xis classiﬁed as follows:\\n•compute the ﬁtted output ˆf(x)T= (1,xT)ˆB, aKvector;\\n•identify the largest component and classify accordingly:\\nˆG(x) = argmaxk∈Gˆfk(x). (4.4)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 122}, page_content='104 4. Linear Methods for Classiﬁcation\\nWhat is the rationale for this approach? One rather formal justiﬁcation\\nis to view the regression as an estimate of conditional expectation. For the\\nrandom variable Yk,E(Yk|X=x) = Pr( G=k|X=x), so conditional\\nexpectation of each of the Ykseems a sensible goal. The real issue is: how\\ngood an approximation to conditional expectation is the rather rigid linear\\nregression model? Alternatively, are the ˆfk(x) reasonable estimates of the\\nposterior probabilities Pr( G=k|X=x), and more importantly, does this\\nmatter?\\nIt is quite straightforward to verify that∑\\nk∈Gˆfk(x) = 1 for any x, as\\nlong as there is an intercept in the model (column of 1’s in X). However,\\ntheˆfk(x) can be negative or greater than 1, and typically some are. This\\nis a consequence of the rigid nature of linear regression, especially if we\\nmake predictions outside the hull of the training data. These violations in\\nthemselves do not guarantee that this approach will not work, and in fact\\non many problems it gives similar results to more standard linear meth-\\nods for classiﬁcation. If we allow linear regression onto basis expansions\\nh(X) of the inputs, this approach can lead to consistent estimates of the\\nprobabilities. As the size of the training set Ngrows bigger, we adaptively\\ninclude more basis elements so that linear regression onto these basis func-\\ntions approaches conditional expectation. We discuss such approaches in\\nChapter 5.\\nA more simplistic viewpoint is to construct targets tkfor each class,\\nwhere tkis the kth column of the K×Kidentity matrix. Our prediction\\nproblem is to try and reproduce the appropriate target for an observation.\\nWith the same coding as before, the response vector yi(ith row of Y) for\\nobservation ihas the value yi=tkifgi=k. We might then ﬁt the linear\\nmodel by least squares:\\nmin\\nBN∑\\ni=1||yi−[(1,xT\\ni)B]T||2. (4.5)\\nThe criterion is a sum-of-squared Euclidean distances of the ﬁtted vectors\\nfrom their targets. A new observation is classiﬁed by computing its ﬁtted\\nvector ˆf(x) and classifying to the closest target:\\nˆG(x) = argmin\\nk||ˆf(x)−tk||2. (4.6)\\nThis is exactly the same as the previous approach:\\n•The sum-of-squared-norm criterion is exactly the criterion for multi-\\nple response linear regression, just viewed slightly diﬀerently. Since\\na squared norm is itself a sum of squares, the components decouple\\nand can be rearranged as a separate linear model for each element.\\nNote that this is only possible because there is nothing in the model\\nthat binds the diﬀerent responses together.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 123}, page_content='4.2 Linear Regression of an Indicator Matrix 105\\nLinear Regression\\n1\\n11\\n1\\n1\\n11111\\n11\\n1\\n11111\\n111\\n11 111\\n111\\n1111\\n111\\n1\\n11111\\n111\\n11\\n1111\\n11\\n11\\n11111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n1\\n1\\n11\\n111\\n111111\\n11\\n111\\n11 1\\n111\\n1\\n1\\n11\\n1\\n11\\n11\\n11\\n111\\n111\\n11\\n11\\n11\\n1\\n11\\n1111\\n11\\n1\\n111\\n11111\\n1\\n1111\\n11\\n111\\n111\\n111\\n1\\n11\\n1111\\n11\\n1\\n111 11\\n11\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n1\\n111\\n1\\n11\\n11\\n1\\n111\\n111 111\\n11\\n11\\n11\\n111111\\n1\\n11\\n11\\n11\\n1111\\n1111\\n111\\n1\\n11 1\\n111\\n1111\\n11\\n111\\n111\\n11\\n1\\n11\\n111\\n111\\n111\\n11\\n11\\n11\\n111\\n11 1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n111\\n1\\n11111\\n11111\\n1\\n11\\n111\\n1\\n11\\n111 1\\n11\\n1\\n11 11\\n1\\n111\\n111\\n11\\n1\\n111\\n1\\n1\\n1111\\n1111\\n11\\n11\\n1111\\n11\\n1\\n11\\n11\\n11\\n1 1\\n111\\n1 11\\n1\\n11\\n11\\n11 1\\n11\\n1\\n1111111\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n111\\n1111\\n11\\n11\\n11\\n1 1\\n11\\n11111\\n1\\n11\\n11\\n111\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n111\\n111\\n11\\n111\\n11\\n111\\n1\\n11\\n1\\n11\\n11\\n1\\n111\\n1111\\n11\\n1\\n11\\n1\\n11\\n1 12\\n22\\n2\\n222\\n22\\n222\\n2\\n22\\n222\\n222\\n222\\n2222\\n2\\n22222\\n2\\n2\\n222\\n2\\n222\\n2\\n222\\n2222\\n2\\n22\\n22\\n22\\n22222\\n2222\\n2\\n222\\n222\\n22\\n2\\n22\\n22\\n222\\n2\\n2222222\\n22\\n22\\n222\\n222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n2 2\\n22\\n2222\\n2\\n2\\n222\\n2222\\n22\\n222\\n222\\n222\\n222\\n22\\n222\\n2\\n2\\n222\\n222\\n2\\n22\\n2222\\n222\\n2222\\n22\\n222\\n2\\n2222\\n222\\n22\\n2\\n222\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22222\\n2222\\n22\\n222\\n22\\n22\\n22\\n2222\\n2222\\n2\\n22\\n22\\n22\\n222\\n22222\\n22\\n22\\n2\\n22\\n2\\n2\\n22\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n22\\n2\\n222\\n22\\n2222\\n2222\\n222\\n222\\n222\\n2222\\n2\\n222\\n2\\n22\\n222\\n222\\n22\\n222 22\\n222\\n222\\n22222\\n22 222\\n222\\n2\\n22\\n22 2\\n2\\n2222\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n222 22\\n2\\n222\\n2222\\n222\\n2\\n2 2222 2\\n22\\n22\\n2222\\n22\\n222\\n2\\n22\\n22\\n2\\n22\\n22\\n222\\n22\\n22\\n2222\\n22\\n2222\\n2222\\n2\\n222\\n22 2\\n22\\n2\\n22\\n22222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n2\\n2 22 222\\n22\\n22\\n222\\n22 22\\n222\\n22\\n22\\n22 22\\n22\\n223\\n33\\n33\\n33\\n3\\n3\\n333\\n3\\n33\\n3\\n333\\n33\\n3\\n33\\n333\\n33\\n333\\n3\\n333\\n3333\\n3\\n33\\n33\\n333\\n33\\n33\\n33\\n3\\n33\\n33\\n33333\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n3\\n33\\n3\\n333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n3333\\n33\\n3333\\n3\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n333\\n33\\n33\\n3333\\n3\\n33\\n33\\n333\\n3\\n3\\n33\\n33\\n3\\n33333\\n33\\n333\\n333\\n3333\\n33\\n33\\n333\\n3\\n33\\n33\\n33\\n333\\n3\\n3\\n3333\\n333\\n333\\n3\\n333\\n3\\n3\\n33333333333\\n33\\n333\\n3\\n33\\n333\\n33\\n33\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n333\\n3\\n3333\\n333\\n33\\n33\\n33\\n333\\n33\\n3\\n33\\n33\\n33\\n33\\n33333\\n33\\n33\\n33\\n33\\n3 33\\n333\\n3\\n33 33\\n3\\n33\\n333\\n333333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3333\\n3\\n33\\n3\\n3\\n33333\\n333\\n3333\\n3\\n33\\n333\\n3\\n333\\n3\\n333\\n33\\n3\\n33\\n33\\n3333\\n333\\n33\\n33\\n33 3\\n3\\n3333\\n333\\n3\\n3\\n3\\n333\\n33\\n3\\n333\\n33\\n3\\n3333\\n33\\n3\\n33\\n3333\\n333\\n33\\n333\\n33\\n3\\n3\\n33\\n333\\n3\\n3\\n3333333\\n3333\\n33\\n3\\n3\\n33\\n3\\n3333\\n3\\n33\\n33 3\\n33\\n3\\n3333\\n333\\n33\\n3\\n3\\n333\\n33\\n3\\n3333\\n3\\n33Linear Discriminant Analysis\\n1\\n11\\n1\\n1\\n11111\\n11\\n1\\n11111\\n111\\n11 111\\n111\\n1111\\n111\\n1\\n11111\\n111\\n11\\n1111\\n11\\n11\\n11111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n1\\n1\\n11\\n111\\n111111\\n11\\n111\\n11 1\\n111\\n1\\n1\\n11\\n1\\n11\\n11\\n11\\n111\\n111\\n11\\n11\\n11\\n1\\n11\\n1111\\n11\\n1\\n111\\n11111\\n1\\n1111\\n11\\n111\\n111\\n111\\n1\\n11\\n1111\\n11\\n1\\n111 11\\n11\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n1\\n111\\n1\\n11\\n11\\n1\\n111\\n111 111\\n11\\n11\\n11\\n111111\\n1\\n11\\n11\\n11\\n1111\\n1111\\n111\\n1\\n11 1\\n111\\n1111\\n11\\n111\\n111\\n11\\n1\\n11\\n111\\n111\\n111\\n11\\n11\\n11\\n111\\n11 1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n1\\n11\\n11\\n1\\n111\\n1\\n11111\\n11111\\n1\\n11\\n111\\n1\\n11\\n111 1\\n11\\n1\\n11 11\\n1\\n111\\n111\\n11\\n1\\n111\\n1\\n1\\n1111\\n1111\\n11\\n11\\n1111\\n11\\n1\\n11\\n11\\n11\\n1 1\\n111\\n1 11\\n1\\n11\\n11\\n11 1\\n11\\n1\\n1111111\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n111\\n1111\\n11\\n11\\n11\\n1 1\\n11\\n11111\\n1\\n11\\n11\\n111\\n11\\n11\\n11\\n1\\n1\\n11\\n11\\n111\\n111\\n11\\n111\\n11\\n111\\n1\\n11\\n1\\n11\\n11\\n1\\n111\\n1111\\n11\\n1\\n11\\n1\\n11\\n1 12\\n22\\n2\\n222\\n22\\n222\\n2\\n22\\n222\\n222\\n222\\n2222\\n2\\n22222\\n2\\n2\\n222\\n2\\n222\\n2\\n222\\n2222\\n2\\n22\\n22\\n22\\n22222\\n2222\\n2\\n222\\n222\\n22\\n2\\n22\\n22\\n222\\n2\\n2222222\\n22\\n22\\n222\\n222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n2 2\\n22\\n2222\\n2\\n2\\n222\\n2222\\n22\\n222\\n222\\n222\\n222\\n22\\n222\\n2\\n2\\n222\\n222\\n2\\n22\\n2222\\n222\\n2222\\n22\\n222\\n2\\n2222\\n222\\n22\\n2\\n222\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22222\\n2222\\n22\\n222\\n22\\n22\\n22\\n2222\\n2222\\n2\\n22\\n22\\n22\\n222\\n22222\\n22\\n22\\n2\\n22\\n2\\n2\\n22\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n22\\n2\\n222\\n22\\n2222\\n2222\\n222\\n222\\n222\\n2222\\n2\\n222\\n2\\n22\\n222\\n222\\n22\\n222 22\\n222\\n222\\n22222\\n22 222\\n222\\n2\\n22\\n22 2\\n2\\n2222\\n2\\n22\\n22\\n2\\n22\\n22\\n22\\n222 22\\n2\\n222\\n2222\\n222\\n2\\n2 2222 2\\n22\\n22\\n2222\\n22\\n222\\n2\\n22\\n22\\n2\\n22\\n22\\n222\\n22\\n22\\n2222\\n22\\n2222\\n2222\\n2\\n222\\n22 2\\n22\\n2\\n22\\n22222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n2\\n2 22 222\\n22\\n22\\n222\\n22 22\\n222\\n22\\n22\\n22 22\\n22\\n223\\n33\\n33\\n33\\n3\\n3\\n333\\n3\\n33\\n3\\n333\\n33\\n3\\n33\\n333\\n33\\n333\\n3\\n333\\n3333\\n3\\n33\\n33\\n333\\n33\\n33\\n33\\n3\\n33\\n33\\n33333\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n3\\n33\\n3\\n333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n3333\\n33\\n3333\\n3\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n333\\n33\\n33\\n3333\\n3\\n33\\n33\\n333\\n3\\n3\\n33\\n33\\n3\\n33333\\n33\\n333\\n333\\n3333\\n33\\n33\\n333\\n3\\n33\\n33\\n33\\n333\\n3\\n3\\n3333\\n333\\n333\\n3'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 123}, page_content='3\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n3\\n33\\n3\\n333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n3333\\n33\\n3333\\n3\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n333\\n33\\n33\\n3333\\n3\\n33\\n33\\n333\\n3\\n3\\n33\\n33\\n3\\n33333\\n33\\n333\\n333\\n3333\\n33\\n33\\n333\\n3\\n33\\n33\\n33\\n333\\n3\\n3\\n3333\\n333\\n333\\n3\\n333\\n3\\n3\\n33333333333\\n33\\n333\\n3\\n33\\n333\\n33\\n33\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n333\\n3\\n3333\\n333\\n33\\n33\\n33\\n333\\n33\\n3\\n33\\n33\\n33\\n33\\n33333\\n33\\n33\\n33\\n33\\n3 33\\n333\\n3\\n33 33\\n3\\n33\\n333\\n333333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3333\\n3\\n33\\n3\\n3\\n33333\\n333\\n3333\\n3\\n33\\n333\\n3\\n333\\n3\\n333\\n33\\n3\\n33\\n33\\n3333\\n333\\n33\\n33\\n33 3\\n3\\n3333\\n333\\n3\\n3\\n3\\n333\\n33\\n3\\n333\\n33\\n3\\n3333\\n33\\n3\\n33\\n3333\\n333\\n33\\n333\\n33\\n3\\n3\\n33\\n333\\n3\\n3\\n3333333\\n3333\\n33\\n3\\n3\\n33\\n3\\n3333\\n3\\n33\\n33 3\\n33\\n3\\n3333\\n333\\n33\\n3\\n3\\n333\\n33\\n3\\n3333\\n3\\n33\\nX1 X1\\nX2X2\\nFIGURE 4.2. The data come from three classes in I R2and are easily separated\\nby linear decision boundaries. The right plot shows the boundar ies found by linear\\ndiscriminant analysis. The left plot shows the boundaries found b y linear regres-\\nsion of the indicator response variables. The middle class is c ompletely masked\\n(never dominates).\\n•The closest target classiﬁcation rule (4.6) is easily seen to be exactly\\nthe same as the maximum ﬁtted component criterion (4.4), but does\\nrequire that the ﬁtted values sum to 1.\\nThere is a serious problem with the regression approach when the number\\nof classes K≥3, especially prevalent when Kis large. Because of the rigid\\nnature of the regression model, classes can be masked by others. Figure 4.2\\nillustrates an extreme situation when K= 3. The three classes are perfectly\\nseparated by linear decision boundaries, yet linear regression misses the\\nmiddle class completely.\\nIn Figure 4.3 we have projected the data onto the line joining the three\\ncentroids (there is no information in the orthogonal direction in this case),\\nand we have included and coded the three response variables Y1,Y2and\\nY3. The three regression lines (left panel) are included, and we see that\\nthe line corresponding to the middle class is horizontal and its ﬁtted values\\nare never dominant! Thus, observations from class 2 are classiﬁed either\\nas class 1 or class 3. The right panel uses quadratic regression rather than\\nlinear regression. For this simple example a quadratic rather than linear\\nﬁt (for the middle class at least) would solve the problem. However, it\\ncan be seen that if there were four rather than three classes lined up like\\nthis, a quadratic would not come down fast enough, and a cubic would\\nbe needed as well. A loose but general rule is that if K≥3 classes are\\nlined up, polynomial terms up to degree K−1 might be needed to resolve\\nthem. Note also that these are polynomials along the derived direction'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 124}, page_content='106 4. Linear Methods for Classiﬁcation\\n111\\n111\\n1\\n1\\n1111\\n11\\n11\\n1\\n11\\n11111\\n1\\n11111\\n11\\n1\\n11\\n11\\n11\\n11111\\n111\\n111\\n1\\n1111\\n111\\n111\\n111\\n111111\\n11\\n1\\n11\\n111\\n1111\\n11\\n111\\n111\\n111\\n111\\n111\\n1\\n11\\n11\\n111\\n111\\n11\\n11\\n1111\\n11\\n1\\n111\\n1\\n11\\n1\\n11\\n1\\n11\\n11\\n111\\n1\\n1111\\n111\\n111\\n12222 2222222 2 2 222 2 2 222222 222222 2 2 2222 22 22222 2222222222222 22 22222 222 2222222 2 22 222 2 22222 22 22 222 222222 22222222 22 2 222222222 22222222 222 222 2 222222222222222 2\\n3\\n3\\n33\\n3\\n3333\\n33\\n33\\n33\\n333\\n333\\n3\\n3\\n33333\\n3\\n33\\n333\\n33\\n333333\\n333333\\n3\\n3333\\n3\\n33\\n3\\n33\\n3\\n33\\n333\\n33\\n33\\n33333\\n33\\n3\\n3333\\n33\\n333\\n33\\n3333\\n33333\\n3333\\n33333\\n33\\n3\\n33\\n33\\n33\\n33\\n33\\n333\\n3\\n333\\n333\\n333\\n3333\\n3\\n3333\\n333\\n3\\n33333\\n0.00.51.0\\n0.0 0.2 0.4 0.6 0.8 1.0111\\n111\\n1\\n1\\n1111\\n11\\n11\\n1\\n11\\n11111\\n1\\n11\\n111\\n11\\n1\\n11\\n11\\n11\\n11111\\n11\\n1\\n111\\n1\\n1111\\n111\\n111\\n111\\n111111\\n111\\n11111\\n111111\\n111\\n111\\n111\\n111\\n111\\n1\\n1111111 111\\n111111111111\\n1111 1111111 11111111111111112\\n2\\n22\\n2\\n2222\\n22\\n22\\n22\\n222\\n222\\n2\\n2\\n22222\\n2\\n22\\n222\\n22\\n222222\\n222222\\n2\\n22222\\n2\\n222\\n222222222222222\\n22\\n22222222222222222 22 22\\n22 22\\n22\\n22\\n222\\n222\\n22\\n22\\n2222\\n22\\n2\\n222\\n2\\n22\\n2\\n22\\n2\\n22\\n22\\n222\\n2\\n222\\n222\\n222\\n2333333\\n333333\\n3333\\n33333333\\n333333\\n33\\n33333\\n3 333333333333 3333\\n33\\n3\\n33\\n3\\n33\\n3333 333\\n33333\\n33\\n3\\n333333333\\n33\\n3333\\n33333\\n3333\\n33333\\n33\\n3\\n33\\n33\\n33\\n33\\n33\\n333\\n3\\n333\\n333\\n333\\n3333\\n3\\n333\\n3\\n33\\n3\\n33333\\n0.00.51.0\\n0.0 0.2 0.4 0.6 0.8 1.0Degree = 1; Error = 0.33 Degree = 2; Error = 0.04\\nFIGURE 4.3. The eﬀects of masking on linear regression in I Rfor a three-class\\nproblem. The rug plot at the base indicates the positions and class membership of\\neach observation. The three curves in each panel are the ﬁtted re gressions to the\\nthree-class indicator variables; for example, for the blue cl ass,yblueis1for the\\nblue observations, and 0for the green and orange. The ﬁts are linear and quadratic\\npolynomials. Above each plot is the training error rate. The Bay es error rate is\\n0.025for this problem, as is the LDA error rate.\\npassing through the centroids, which can have arbitrary orientation. So in\\np-dimensional input space, one would need general polynomial terms and\\ncross-products of total degree K−1,O(pK−1) terms in all, to resolve such\\nworst-case scenarios.\\nThe example is extreme, but for large Kand small psuch maskings\\nnaturally occur. As a more realistic illustration, Figure 4.4 is a project ion\\nof the training data for a vowel recognition problem onto an informative\\ntwo-dimensional subspace. There are K= 11 classes in p= 10 dimensions.\\nThis is a diﬃcult classiﬁcation problem, and the best methods achieve\\naround 40% errors on the test data. The main point here is summarized in\\nTable 4.1; linear regression has an error rate of 67%, while a close relat ive,\\nlinear discriminant analysis, has an error rate of 56%. It seems that mask ing\\nhas hurt in this case. While all the other methods in this chapter are based\\non linear functions of xas well, they use them in such a way that avoids\\nthis masking problem.\\n4.3 Linear Discriminant Analysis\\nDecision theory for classiﬁcation (Section 2.4) tells us that we need to know\\nthe class posteriors Pr( G|X) for optimal classiﬁcation. Suppose fk(x) is\\nthe class-conditional density of Xin class G=k, and let πkbe the prior\\nprobability of class k, with∑K\\nk=1πk= 1. A simple application of Bayes'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 125}, page_content='4.3 Linear Discriminant Analysis 107\\nCoordinate 1 for Training DataCoordinate 2 for Training Data\\n-4 -2 0 2 4-6 -4 -2 0 2 4oooo oo\\nooooooo\\no\\noooo\\noo\\no\\no\\no\\no\\noooooooooooo\\no\\no\\no\\no\\no ooooooo\\nooooooo\\no\\noo\\no\\nooooo\\no\\nooooooo\\noo\\noooo\\no\\no\\nooooooooooooo\\no\\noooooo\\no\\nooo\\noooo\\noooo\\no\\nooooo\\noooooo o\\no\\no\\noooooooooo\\no\\noo\\no\\noo\\nooooooooooooo\\noooooooooooooooooo\\noooooooooooo\\no\\no\\nooooooo\\no\\no\\nooooooo\\nooooo\\nooooooo\\no\\no\\noooooooooooooooo\\no\\no\\no\\no\\noooooooo\\noooo\\noo\\noooooooooo\\no\\nooo\\no\\no\\no\\noooo\\no\\no\\nooooo\\no\\no o\\no\\no\\no\\no\\nooooooooo oooo\\no\\no\\noooooo\\no\\noooooooo\\no\\noo\\no\\noooooooo\\no\\no\\no\\no\\nooo\\nooooooooooooooo\\nooo\\noooooooo\\no\\nooo\\nooooooooooooo\\no\\noooo\\nooooo\\nooooo\\no\\noo\\no\\no\\no\\no\\nooo\\nooo\\no\\nooooo\\no\\noooo\\no\\nooooooooooooo\\no ooooooooooooooooo\\no\\no\\nooooo\\nooooooooooo\\no\\no\\nooo\\nooo\\noo oo\\noooooo\\noooooo\\nooooooooo\\no\\no\\nooooooo\\noooooooooooo\\noo\\no\\no\\noo\\n••••••••••••••\\n••\\n••\\n••••Linear Discriminant Analysis\\nFIGURE 4.4. A two-dimensional plot of the vowel training data. There are\\neleven classes with X∈I R10, and this is the best view in terms of a LDA model\\n(Section 4.3.3). The heavy circles are the projected mean vec tors for each class.\\nThe class overlap is considerable.\\nTABLE 4.1. Training and test error rates using a variety of linear techniques\\non the vowel data. There are eleven classes in ten dimensions, o f which three\\naccount for 90%of the variance (via a principal components analysis). We see\\nthat linear regression is hurt by masking, increasing the test and training error\\nby over 10%.\\nTechnique Error Rates\\nTraining Test\\nLinear regression 0.48 0.67\\nLinear discriminant analysis 0.32 0.56\\nQuadratic discriminant analysis 0.01 0.53\\nLogistic regression 0.22 0.51'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 126}, page_content='108 4. Linear Methods for Classiﬁcation\\ntheorem gives us\\nPr(G=k|X=x) =fk(x)πk∑K\\nℓ=1fℓ(x)πℓ. (4.7)\\nWe see that in terms of ability to classify, having the fk(x) is almost equiv-\\nalent to having the quantity Pr( G=k|X=x).\\nMany techniques are based on models for the class densities:\\n•linear and quadratic discriminant analysis use Gaussian densities;\\n•more ﬂexible mixtures of Gaussians allow for nonlinear decision bound-\\naries (Section 6.8);\\n•general nonparametric density estimates for each class density allow\\nthe most ﬂexibility (Section 6.6.2);\\n•Naive Bayes models are a variant of the previous case, and assume\\nthat each of the class densities are products of marginal densities;\\nthat is, they assume that the inputs are conditionally independent in\\neach class (Section 6.6.3).\\nSuppose that we model each class density as multivariate Gaussian\\nfk(x) =1\\n(2π)p/2|Σk|1/2e−1\\n2(x−θk)TΣ−1\\nk(x−θk). (4.8)\\nLinear discriminant analysis (LDA) arises in the special case when we\\nassume that the classes have a common covariance matrix Σk=Σ∀k. In\\ncomparing two classes kandℓ, it is suﬃcient to look at the log-ratio, and\\nwe see that\\nlogPr(G=k|X=x)\\nPr(G=ℓ|X=x)= logfk(x)\\nfℓ(x)+ logπk\\nπℓ\\n= logπk\\nπℓ−1\\n2(θk+θℓ)TΣ−1(θk−θℓ)\\n+xTΣ−1(θk−θℓ),(4.9)\\nan equation linear in x. The equal covariance matrices cause the normal-\\nization factors to cancel, as well as the quadratic part in the exponents.\\nThis linear log-odds function implies that the decision boundary between\\nclasses kandℓ—the set where Pr( G=k|X=x) = Pr( G=ℓ|X=x)—is\\nlinear in x; inpdimensions a hyperplane. This is of course true for any pair\\nof classes, so all the decision boundaries are linear. If we divide IRpinto\\nregions that are classiﬁed as class 1, class 2, etc., these regions will be sep-\\narated by hyperplanes. Figure 4.5 (left panel) shows an idealized example\\nwith three classes and p= 2. Here the data do arise from three Gaus-\\nsian distributions with a common covariance matrix. We have included in'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 127}, page_content='4.3 Linear Discriminant Analysis 109\\n+++\\n3\\n21\\n11\\n233\\n3\\n123\\n32\\n11211\\n33\\n12 1\\n23\\n23\\n3\\n12\\n211\\n1\\n13\\n222\\n21 3\\n2 23\\n13\\n13\\n32\\n13\\n3\\n23\\n133\\n2133\\n22\\n3\\n22\\n21\\n11\\n11\\n2\\n133\\n1\\n13\\n32\\n222 3\\n12\\nFIGURE 4.5. The left panel shows three Gaussian distributions, with the sa me\\ncovariance and diﬀerent means. Included are the contours of constant density\\nenclosing 95% of the probability in each case. The Bayes decision boundari es\\nbetween each pair of classes are shown (broken straight lines ), and the Bayes\\ndecision boundaries separating all three classes are the thic ker solid lines (a subset\\nof the former). On the right we see a sample of 30drawn from each Gaussian\\ndistribution, and the ﬁtted LDA decision boundaries.\\nthe ﬁgure the contours corresponding to 95% highest probability density,\\nas well as the class centroids. Notice that the decision boundaries are not\\nthe perpendicular bisectors of the line segments joining the centroids. This\\nwould be the case if the covariance Σwere spherical σ2I, and the class\\npriors were equal. From (4.9) we see that the linear discriminant functions\\nδk(x) =xTΣ−1θk−1\\n2θT\\nkΣ−1θk+ logπk (4.10)\\nare an equivalent description of the decision rule, with G(x) = argmaxkδk(x).\\nIn practice we do not know the parameters of the Gaussian distributions,\\nand will need to estimate them using our training data:\\n•ˆπk=Nk/N, where Nkis the number of class- kobservations;\\n•ˆθk=∑\\ngi=kxi/Nk;\\n•ˆΣ=∑K\\nk=1∑\\ngi=k(xi−ˆθk)(xi−ˆθk)T/(N−K).\\nFigure 4.5 (right panel) shows the estimated decision boundaries based on\\na sample of size 30 each from three Gaussian distributions. Figure 4.1 on\\npage 103 is another example, but here the classes are not Gaussian.\\nWith two classes there is a simple correspondence between linear dis-\\ncriminant analysis and classiﬁcation by linear least squares, as in (4.5) .\\nThe LDA rule classiﬁes to class 2 if\\nxTˆΣ−1(ˆθ2−ˆθ1)>1\\n2ˆθT\\n2ˆΣ−1ˆθ2−1\\n2ˆθT\\n1ˆΣ−1ˆθ1+ log( N1/N)−log(N2/N)\\n(4.11)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 128}, page_content='110 4. Linear Methods for Classiﬁcation\\nand class 1 otherwise. Suppose we code the targets in the two classes as +1\\nand−1, respectively. It is easy to show that the coeﬃcient vector from least\\nsquares is proportional to the LDA direction given in (4.11) (Exercise 4. 2).\\n[In fact, this correspondence occurs for any (distinct) coding of the targets;\\nsee Exercise 4.2]. However unless N1=N2the intercepts are diﬀerent and\\nhence the resulting decision rules are diﬀerent.\\nSince this derivation of the LDA direction via least squares does not use a\\nGaussian assumption for the features, its applicability extends beyond the\\nrealm of Gaussian data. However the derivation of the particular intercept\\nor cut-point given in (4.11) doesrequire Gaussian data. Thus it makes\\nsense to instead choose the cut-point that empirically minimizes training\\nerror for a given dataset. This is something we have found to work well in\\npractice, but have not seen it mentioned in the literature.\\nWith more than two classes, LDA is not the same as linear regression of\\nthe class indicator matrix, and it avoids the masking problems associated\\nwith that approach (Hastie et al., 1994). A correspondence between regres-\\nsion and LDA can be established through the notion of optimal scoring ,\\ndiscussed in Section 12.5.\\nGetting back to the general discriminant problem (4.8), if the Σkare\\nnot assumed to be equal, then the convenient cancellations in (4.9) do not\\noccur; in particular the pieces quadratic in xremain. We then get quadratic\\ndiscriminant functions (QDA),\\nδk(x) =−1\\n2log|Σk| −1\\n2(x−θk)TΣ−1\\nk(x−θk) + log πk. (4.12)\\nThe decision boundary between each pair of classes kandℓis described by\\na quadratic equation {x:δk(x) =δℓ(x)}.\\nFigure 4.6 shows an example (from Figure 4.1 on page 103) where the\\nthree classes are Gaussian mixtures (Section 6.8) and the decision bound-\\naries are approximated by quadratic equations in x. Here we illustrate\\ntwo popular ways of ﬁtting these quadratic boundaries. The right plot\\nuses QDA as described here, while the left plot uses LDA in the enlarged\\nﬁve-dimensional quadratic polynomial space. The diﬀerences are generally\\nsmall; QDA is the preferred approach, with the LDA method a convenient\\nsubstitute2.\\nThe estimates for QDA are similar to those for LDA, except that separate\\ncovariance matrices must be estimated for each class. When pis large this\\ncan mean a dramatic increase in parameters. Since the decision boundaries\\nare functions of the parameters of the densities, counting the number of\\nparameters must be done with care. For LDA, it seems there are ( K−\\n1)×(p+ 1) parameters, since we only need the diﬀerences δk(x)−δK(x)\\n2For this ﬁgure and many similar ﬁgures in the book we compute t he decision bound-\\naries by an exhaustive contouring method. We compute the dec ision rule on a ﬁne lattice\\nof points, and then use contouring algorithms to compute the boundaries.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 129}, page_content='4.3 Linear Discriminant Analysis 111\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\n1\\n11\\n11\\n111\\n1\\n111\\n1\\n11\\n11\\n1111 1\\n111\\n1\\n11\\n111\\n1111\\n11\\n1\\n11\\n111\\n11\\n1111\\n1 1\\n111\\n11\\n1\\n1\\n1 1\\n11\\n1111\\n11111\\n1\\n11\\n111\\n111\\n1111\\n1\\n111\\n11\\n11\\n11\\n1\\n11\\n11\\n111\\n11\\n1\\n1\\n11\\n111\\n111\\n11\\n11\\n11\\n11 1\\n11\\n11 111\\n1 11\\n1\\n11\\n1\\n11\\n1\\n11\\n1\\n11 1\\n1\\n11\\n11\\n1111\\n1111\\n1\\n111\\n11\\n111\\n1\\n111\\n111\\n1111\\n11\\n1 11\\n111\\n11\\n1\\n11\\n11\\n11\\n111\\n12\\n22\\n22\\n222\\n2\\n2\\n22\\n2 2\\n2\\n22\\n22\\n22\\n2222\\n2\\n22\\n222\\n2\\n22\\n2 2\\n22\\n222\\n2\\n222\\n22222\\n22\\n2\\n222\\n22\\n2\\n22\\n22\\n22\\n22\\n22\\n2222\\n22\\n22\\n222\\n222\\n22222\\n22\\n222\\n22\\n22\\n22\\n2\\n22\\n2222\\n22\\n2\\n22\\n2\\n222\\n2\\n222\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n2222 2\\n222\\n22\\n2\\n22\\n22\\n222\\n222\\n2\\n22\\n22\\n22222\\n22\\n222\\n22\\n2\\n222\\n22\\n22\\n222\\n222\\n22\\n2\\n22\\n22\\n22222\\n22\\n3\\n33\\n33\\n33\\n3\\n3333\\n33\\n3\\n33\\n3\\n333\\n33\\n33\\n333\\n333\\n3\\n3\\n33\\n333\\n333\\n333\\n333\\n3333\\n333\\n3333\\n3\\n33333\\n33\\n3\\n33\\n3\\n33\\n3333\\n333\\n3\\n333\\n333\\n33\\n333\\n3\\n3333\\n33\\n333\\n33\\n3\\n3333\\n333\\n33333\\n3\\n3\\n3\\n333\\n33\\n3\\n33\\n3\\n33\\n33\\n333\\n33\\n3\\n3333\\n33\\n333\\n3\\n333\\n33333\\n33\\n333\\n33\\n33\\n3333\\n333\\n33\\n33\\n3\\n33\\n33\\n3\\n33\\n33\\n3\\n333\\n3\\n333\\n33\\n33\\n33\\nFIGURE 4.6. Two methods for ﬁtting quadratic boundaries. The left plot show s\\nthe quadratic decision boundaries for the data in Figure 4.1 ( obtained using LDA\\nin the ﬁve-dimensional space X1, X2, X1X2, X2\\n1, X2\\n2). The right plot shows the\\nquadratic decision boundaries found by QDA. The diﬀerences are small, as is\\nusually the case.\\nbetween the discriminant functions where Kis some pre-chosen class (here\\nwe have chosen the last), and each diﬀerence requires p+ 1 parameters3.\\nLikewise for QDA there will be ( K−1)× {p(p+ 3)/2 + 1}parameters.\\nBoth LDA and QDA perform well on an amazingly large and diverse set\\nof classiﬁcation tasks. For example, in the STATLOG project (Michie et\\nal., 1994) LDA was among the top three classiﬁers for 7 of the 22 datasets,\\nQDA among the top three for four datasets, and one of the pair were in the\\ntop three for 10 datasets. Both techniques are widely used, and entire books\\nare devoted to LDA. It seems that whatever exotic tools are the rage of the\\nday, we should always have available these two simple tools. The question\\narises why LDA and QDA have such a good track record. The reason is not\\nlikely to be that the data are approximately Gaussian, and in addition for\\nLDA that the covariances are approximately equal. More likely a reason is\\nthat the data can only support simple decision boundaries such as linear or\\nquadratic, and the estimates provided via the Gaussian models are stable.\\nThis is a bias variance tradeoﬀ—we can put up with the bias of a linear\\ndecision boundary because it can be estimated with much lower variance\\nthan more exotic alternatives. This argument is less believable for QDA,\\nsince it can have many parameters itself, although perhaps fewer than the\\nnon-parametric alternatives.\\n3Although we ﬁt the covariance matrix ˆΣto compute the LDA discriminant functions,\\na much reduced function of it is all that is required to estima te the O(p) parameters\\nneeded to compute the decision boundaries.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 130}, page_content='112 4. Linear Methods for Classiﬁcation\\nMisclassification Rate\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Regularized Discriminant Analysis on the Vowel Data\\nTest Data\\nTrain Data\\nα\\nFIGURE 4.7. Test and training errors for the vowel data, using regularized\\ndiscriminant analysis with a series of values of α∈[0,1]. The optimum for the\\ntest data occurs around α= 0.9, close to quadratic discriminant analysis.\\n4.3.1 Regularized Discriminant Analysis\\nFriedman (1989) proposed a compromise between LDA and QDA, which\\nallows one to shrink the separate covariances of QDA toward a common\\ncovariance as in LDA. These methods are very similar in ﬂavor to ridge\\nregression. The regularized covariance matrices have the form\\nˆΣk(α) =αˆΣk+ (1−α)ˆΣ, (4.13)\\nwhere ˆΣis the pooled covariance matrix as used in LDA. Here α∈[0,1]\\nallows a continuum of models between LDA and QDA, and needs to be\\nspeciﬁed. In practice αcan be chosen based on the performance of the\\nmodel on validation data, or by cross-validation.\\nFigure 4.7 shows the results of RDA applied to the vowel data. Both\\nthe training and test error improve with increasing α, although the test\\nerror increases sharply after α= 0.9. The large discrepancy between the\\ntraining and test error is partly due to the fact that there are many repeat\\nmeasurements on a small number of individuals, diﬀerent in the training\\nand test set.\\nSimilar modiﬁcations allow ˆΣitself to be shrunk toward the scalar\\ncovariance,\\nˆΣ(γ) =γˆΣ+ (1−γ)ˆσ2I (4.14)\\nforγ∈[0,1]. Replacing ˆΣin (4.13) by ˆΣ(γ) leads to a more general family\\nof covariances ˆΣ(α,γ) indexed by a pair of parameters.\\nIn Chapter 12, we discuss other regularized versions of LDA, which are\\nmore suitable when the data arise from digitized analog signals and images.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 131}, page_content='4.3 Linear Discriminant Analysis 113\\nIn these situations the features are high-dimensional and correlated, and the\\nLDA coeﬃcients can be regularized to be smooth or sparse in the original\\ndomain of the signal. This leads to better generalization and allows for\\neasier interpretation of the coeﬃcients. In Chapter 18 we also deal with\\nvery high-dimensional problems, where for example the features are gene-\\nexpression measurements in microarray studies. There the methods focus\\non the case γ= 0 in (4.14), and other severely regularized versions of LDA.\\n4.3.2 Computations for LDA\\nAs a lead-in to the next topic, we brieﬂy digress on the computations\\nrequired for LDA and especially QDA. Their computations are simpliﬁed\\nby diagonalizing ˆΣorˆΣk. For the latter, suppose we compute the eigen-\\ndecomposition for each ˆΣk=UkDkUT\\nk, where Ukisp×porthonormal,\\nandDka diagonal matrix of positive eigenvalues dkℓ. Then the ingredients\\nforδk(x) (4.12) are\\n•(x−ˆθk)TˆΣ−1\\nk(x−ˆθk) = [UT\\nk(x−ˆθk)]TD−1\\nk[UT\\nk(x−ˆθk)];\\n•log|ˆΣk|=∑\\nℓlogdkℓ.\\nIn light of the computational steps outlined above, the LDA classiﬁer\\ncan be implemented by the following pair of steps:\\n•Sphere the data with respect to the common covariance estimate ˆΣ:\\nX∗←D−1\\n2UTX, where ˆΣ=UDUT. The common covariance esti-\\nmate of X∗will now be the identity.\\n•Classify to the closest class centroid in the transformed space, modulo\\nthe eﬀect of the class prior probabilities πk.\\n4.3.3 Reduced-Rank Linear Discriminant Analysis\\nSo far we have discussed LDA as a restricted Gaussian classiﬁer. Part of\\nits popularity is due to an additional restriction that allows us to view\\ninformative low-dimensional projections of the data.\\nTheKcentroids in p-dimensional input space lie in an aﬃne subspace\\nof dimension ≤K−1, and if pis much larger than K, this will be a con-\\nsiderable drop in dimension. Moreover, in locating the closest centroid, we\\ncan ignore distances orthogonal to this subspace, since they will contribute\\nequally to each class. Thus we might just as well project the X∗onto this\\ncentroid-spanning subspace HK−1, and make distance comparisons there.\\nThus there is a fundamental dimension reduction in LDA, namely, that we\\nneed only consider the data in a subspace of dimension at most K−1.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 132}, page_content='114 4. Linear Methods for Classiﬁcation\\nIfK= 3, for instance, this could allow us to view the data in a two-\\ndimensional plot, color-coding the classes. In doing so we would not have\\nrelinquished any of the information needed for LDA classiﬁcation.\\nWhat if K >3? We might then ask for a L < K −1 dimensional subspace\\nHL⊆HK−1optimal for LDA in some sense. Fisher deﬁned optimal to\\nmean that the projected centroids were spread out as much as possible in\\nterms of variance. This amounts to ﬁnding principal component subspaces\\nof the centroids themselves (principal components are described brieﬂy in\\nSection 3.5.1, and in more detail in Section 14.5.1). Figure 4.4 shows such an\\noptimal two-dimensional subspace for the vowel data. Here there are eleven\\nclasses, each a diﬀerent vowel sound, in a ten-dimensional input space. The\\ncentroids require the full space in this case, since K−1 =p, but we have\\nshown an optimal two-dimensional subspace. The dimensions are ordered,\\nso we can compute additional dimensions in sequence. Figure 4.8 shows four\\nadditional pairs of coordinates, also known as canonical ordiscriminant\\nvariables. In summary then, ﬁnding the sequences of optimal subspaces\\nfor LDA involves the following steps:\\n•compute the K×pmatrix of class centroids Mand the common\\ncovariance matrix W(forwithin-class covariance);\\n•compute M∗=MW−1\\n2using the eigen-decomposition of W;\\n•compute B∗, the covariance matrix of M∗(Bforbetween-class covari-\\nance), and its eigen-decomposition B∗=V∗DBV∗T. The columns\\nv∗\\nℓofV∗in sequence from ﬁrst to last deﬁne the coordinates of the\\noptimal subspaces.\\nCombining all these operations the ℓthdiscriminant variable is given by\\nZℓ=vT\\nℓXwithvℓ=W−1\\n2v∗\\nℓ.\\nFisher arrived at this decomposition via a diﬀerent route, without refer-\\nring to Gaussian distributions at all. He posed the problem:\\nFind the linear combination Z=aTXsuch that the between-\\nclass variance is maximized relative to the within-class var iance.\\nAgain, the between class variance is the variance of the class means of\\nZ, and the within class variance is the pooled variance about the means.\\nFigure 4.9 shows why this criterion makes sense. Although the direction\\njoining the centroids separates the means as much as possible (i.e., max-\\nimizes the between-class variance), there is considerable overlap between\\nthe projected classes due to the nature of the covariances. By taking the\\ncovariance into account as well, a direction with minimum overlap can be\\nfound.\\nThe between-class variance of ZisaTBaand the within-class variance\\naTWa, where Wis deﬁned earlier, and Bis the covariance matrix of the\\nclass centroid matrix M. Note that B+W=T, where Tis the total\\ncovariance matrix of X, ignoring class information.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 133}, page_content='4.3 Linear Discriminant Analysis 115\\nCoordinate 1 Coordinate 3 \\n-4 -2 0 2 4-2 0 2o\\nooooo\\no\\no\\nooo\\nooo\\nooooooo\\no\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\no\\nooo\\nooooo\\noooooo ooooooo\\no\\no\\nooo oo\\no\\no\\nooo\\no\\no\\noooooooooooo\\noooo\\noooooo\\noo\\nooo\\noooooooo\\nooo\\noooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\noo\\noooooooo\\noooo\\noooooooooo\\noo\\nooo\\no\\nooo\\noooooo\\nooo\\noooooo\\no\\no\\noooooo\\noooooooooooo\\no\\nooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooooo\\no\\nooooo\\no\\nooooooo\\noooo\\no\\noooo\\noo\\nooooo\\noooo\\no\\noo\\nooooo\\no\\no\\no\\nooo\\no\\nooooo\\nooooooo\\no\\noooo\\noooooo\\noooo\\no\\no\\noo ooo\\noo\\nooo\\no\\noooo\\no\\nooooooo\\no\\noo\\noo\\no\\nooo\\nooooo\\no\\nooooo\\no\\noo\\no\\noo\\no\\noooooo\\no\\noo\\nooooooooooo\\noo\\noooooo\\noo\\no\\no\\noooo\\no\\nooooo\\no\\no\\no\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\no\\no\\nooo\\nooooooooooo\\noooo\\nooo\\noooooo\\no\\no\\noooooooo\\nooo\\noooooo\\no\\nooooo\\nooo\\no\\nooo\\no\\noooooooooo oooo ooooooo\\nooooooo\\n••••••••\\n••••••••••••••\\nCoordinate 2 Coordinate 3 \\n-6 -4 -2 0 2 4-2 0 2o\\nooooo\\no\\no\\nooo\\nooo\\nooooooo\\no\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\no\\nooo\\nooooo\\nooooooooooooo\\no\\no\\nooo oo\\no\\no\\nooo\\no\\no\\noooooooooooo\\noooo\\noooooo\\noo\\nooo\\noooooooo\\nooo\\noooo\\nooooo\\noooo\\noooo\\noo\\noooo\\no\\noo\\noooooooo\\noooo\\noooooooooo\\noo\\nooo\\no\\nooo\\noooooo\\nooo\\noooooo\\no\\no\\noooooo\\noooooooooooo\\no\\nooo\\noo\\nooo\\nooooo\\noo\\nooo\\nooooo\\no\\nooooo\\no\\nooooooo\\noooo\\no\\noooo\\noo\\nooooo\\noooo\\no\\noo\\nooooo\\no\\no\\no\\nooo\\no\\nooooo\\nooooooo\\no\\noooo\\noooooo\\noooo\\no\\no\\noo ooo\\noo\\nooo\\no\\noooo\\no\\nooooooo\\no\\noo\\noo\\no\\nooo\\nooooo\\no\\nooooo\\no\\noo\\no\\noo\\no\\noooooo\\no\\noo\\nooooooooooo\\noo\\noooooo\\noo\\no\\no\\noooo\\no\\nooooo\\no\\no\\no\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\no\\no\\nooo\\nooooooooooo\\noo oo\\nooo\\noooooo\\no\\no\\noooooooo\\nooo\\noooooo\\no\\nooooo\\nooo\\no\\nooo\\no\\noooooooooo ooooooooooo\\nooooooo\\n••••••••\\n••••••••••••••\\nCoordinate 1 Coordinate 7 \\n-4 -2 0 2 4-3 -2 -1 0 1 2 3ooo ooo\\no\\nooooooo\\noooo\\nooooooooooooooo\\nooo\\nooo\\no\\no\\nooooooooooooo\\noooo\\no\\nooooo\\no\\nooooooo\\no\\no\\nooooo\\nooooooooooo\\nooo\\no\\noooo\\noooo\\noooo\\noo\\noo\\no\\nooo\\noooooooooo\\nooo\\nooooo\\noooooooooooo\\noo oooo\\noooooo\\noooooo\\noooooooo\\no\\nooooooooo\\no\\nooooooooooo\\no\\nooooo\\noooooo\\noo\\noooo\\nooooooooooo\\nooo o\\nooo\\nooooooooo\\nooo\\noooooo\\noooo\\noooooooo\\no\\noooooo\\nooo\\nooo\\no\\noooo\\nooooooooooo\\no\\noooooo\\nooooo\\nooooooooooooo\\no\\nooooooo\\noooo\\noooooooo\\noooo\\noooooo\\no\\nooooo\\nooooo\\no\\noooooo\\noooooooooooo\\nooooo\\nooooooooooooo\\nooo\\no\\nooo\\noo\\no\\noooooooo\\nooooooo\\nooo\\noo\\noooooooooooo\\nooooooooo\\nooo\\noo\\nooooo\\no\\nooo\\no\\no\\nooo\\no\\noo\\no\\noooooo\\nooooo\\noo\\noo\\no\\noooooo\\nooo\\noo\\nooooooooooo\\no\\noooooo\\no\\nooooooooooo\\noooo\\no\\noo\\n••••••••••••••••••••••\\nCoordinate 9 Coordinate 10 \\n-2 -1 0 1 2 3-2 -1 0 1 2oo\\no\\no\\noooooooo\\nooooo\\no\\nooooooo\\nooooo\\nooooo\\no\\nooooooo\\no\\no\\no\\noo\\noo\\no\\nooo\\noooooooooo\\no\\no\\noooooooooooo o\\noo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\noooooo o\\noo\\noooo\\noooo\\nooooooo\\noooooo\\noo\\no\\noooooo\\no\\noo\\no\\noooooooooooooooooo\\nooo\\nooo\\nooo\\noo\\nooooooo\\no\\noooo\\nooooo\\no\\nooooooooooo\\nooooo\\nooooooooooooo\\no\\noooo\\nooooo\\no\\noo\\no\\no\\noooo\\nooooo\\no\\nooo\\no\\noooo\\noo\\no\\noo\\noo\\noooo\\noooo\\no\\nooooooooooo\\no\\nooo\\noo\\no\\nooo\\no\\no oo\\noooooooo\\nooo\\nooooo\\nooooo\\no\\no\\nooo\\noooooooo\\nooo\\noo\\noo\\noooooooooooo\\no\\no\\no\\noo\\nooooooooo\\noooo\\no\\noooooooooo\\nooo\\no\\no\\no\\no\\no\\noo\\nooooooo\\no\\no\\noo\\no\\no\\noooooooooo\\noooo\\nooo\\noooooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooooo\\nooooooooooooo\\nooooo\\no\\no\\nooo\\nooooooo\\nooo\\no\\noo\\noooo\\nooooo\\noo\\no\\no\\no\\noooooooooooooooo\\nooo\\no\\no ooooo\\noo\\noo\\no\\no\\no\\no••••••••••••••••••••••Linear Discriminant Analysis\\nFIGURE 4.8. Four projections onto pairs of canonical variates. Notice that a s\\nthe rank of the canonical variates increases, the centroids becom e less spread out.\\nIn the lower right panel they appear to be superimposed, and the classes most\\nconfused.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 134}, page_content='116 4. Linear Methods for Classiﬁcation\\n++\\n++\\nFIGURE 4.9. Although the line joining the centroids deﬁnes the direction of\\ngreatest centroid spread, the projected data overlap becaus e of the covariance\\n(left panel). The discriminant direction minimizes this overla p for Gaussian data\\n(right panel).\\nFisher’s problem therefore amounts to maximizing the Rayleigh quotient ,\\nmax\\naaTBa\\naTWa, (4.15)\\nor equivalently\\nmax\\naaTBasubject to aTWa= 1. (4.16)\\nThis is a generalized eigenvalue problem, with agiven by the largest\\neigenvalue of W−1B. It is not hard to show (Exercise 4.1) that the optimal\\na1is identical to v1deﬁned above. Similarly one can ﬁnd the next direction\\na2, orthogonal in Wtoa1, such that aT\\n2Ba2/aT\\n2Wa2is maximized; the\\nsolution is a2=v2, and so on. The aℓare referred to as discriminant\\ncoordinates , not to be confused with discriminant functions. They are also\\nreferred to as canonical variates , since an alternative derivation of these\\nresults is through a canonical correlation analysis of the indicator response\\nmatrix Yon the predictor matrix X. This line is pursued in Section 12.5.\\nTo summarize the developments so far:\\n•Gaussian classiﬁcation with common covariances leads to linear deci-\\nsion boundaries. Classiﬁcation can be achieved by sphering the data\\nwith respect to W, and classifying to the closest centroid (modulo\\nlogπk) in the sphered space.\\n•Since only the relative distances to the centroids count, one can con-\\nﬁne the data to the subspace spanned by the centroids in the sphered\\nspace.\\n•This subspace can be further decomposed into successively optimal\\nsubspaces in term of centroid separation. This decomposition is iden-\\ntical to the decomposition due to Fisher.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 135}, page_content='4.3 Linear Discriminant Analysis 117\\nDimensionMisclassification Rate\\n2 4 6 8 100.3 0.4 0.5 0.6 0.7LDA and Dimension Reduction on the Vowel Data\\n•\\n••••• • ••••\\n•\\n• •\\n•\\n•••••Test Data\\nTrain Data\\nFIGURE 4.10. Training and test error rates for the vowel data, as a function\\nof the dimension of the discriminant subspace. In this case the b est error rate is\\nfor dimension 2. Figure 4.11 shows the decision boundaries in this space.\\nThe reduced subspaces have been motivated as a data reduction (for\\nviewing) tool. Can they also be used for classiﬁcation, and what is the\\nrationale? Clearly they can, as in our original derivation; we simply limit\\nthe distance-to-centroid calculations to the chosen subspace. One can show\\nthat this is a Gaussian classiﬁcation rule with the additional restriction\\nthat the centroids of the Gaussians lie in a L-dimensional subspace of IRp.\\nFitting such a model by maximum likelihood, and then constructing the\\nposterior probabilities using Bayes’ theorem amounts to the classiﬁcation\\nrule described above (Exercise 4.8).\\nGaussian classiﬁcation dictates the log πkcorrection factor in the dis-\\ntance calculation. The reason for this correction can be seen in Figure 4.9.\\nThe misclassiﬁcation rate is based on the area of overlap between the two\\ndensities. If the πkare equal (implicit in that ﬁgure), then the optimal\\ncut-point is midway between the projected means. If the πkare not equal,\\nmoving the cut-point toward the smaller class will improve the error rate.\\nAs mentioned earlier for two classes, one can derive the linear rule using\\nLDA (or any other method), and then choose the cut-point to minimize\\nmisclassiﬁcation error over the training data.\\nAs an example of the beneﬁt of the reduced-rank restriction, we return\\nto the vowel data. There are 11 classes and 10 variables, and hence 10\\npossible dimensions for the classiﬁer. We can compute the training and\\ntest error in each of these hierarchical subspaces; Figure 4.10 shows the\\nresults. Figure 4.11 shows the decision boundaries for the classiﬁer based\\non the two-dimensional LDA solution.\\nThere is a close connection between Fisher’s reduced rank discriminant\\nanalysis and regression of an indicator response matrix. It turns out that'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 136}, page_content='118 4. Linear Methods for Classiﬁcation\\noooo\\noo o\\noo\\noo\\no\\noo o\\noo\\noooo\\no\\noo\\nooooo\\noo\\nooo\\no\\nooo\\noooo\\noo\\noooo\\no\\nooo\\no\\noo\\nooo\\no\\no\\no\\nooo\\no\\noo\\noo oo\\noo\\nooo\\no\\noo\\noo\\noo\\no\\noo\\no\\noooo\\no\\noo\\noo\\noo\\noooo\\nooo\\no\\no\\no\\no\\noo\\no\\noo\\noo\\no\\no\\nooo\\no\\nooo\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\noo\\no\\nooo\\no\\no\\nooo\\no\\nooo\\noo\\noo\\noooo\\noo\\noo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\no o\\nooo\\no\\no\\no\\noo\\noooo\\noo\\noo\\no\\noo\\nooo\\no\\noo\\noo\\noo\\no oo\\nooo\\no\\noo\\noo\\noooo\\noo\\noo\\noo\\no\\noo\\no\\noo\\noooo\\no\\nooo\\noooooo\\nooo\\noo\\noo\\nooooo\\nooo\\no\\nooo\\no oo\\noo\\nooo\\no\\noooo\\noo\\no\\nooo\\no\\nooo\\nooooo\\no\\noo\\noo\\no oo\\noo\\noo\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\noooo\\no\\no\\noooo\\no\\noo\\noo\\no\\noo\\noo\\no\\noo\\nooo\\noooo\\noo\\noo\\nooo\\noo\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noo\\no\\noooo\\noooo\\no\\noo\\noo\\no\\nooo\\nooooo\\nooo\\noo\\noo\\noo\\noo\\no\\no\\no\\no\\no\\noo\\noo\\no\\noo\\nooo\\no\\no o\\noo\\no o\\noo\\no\\noo\\no\\nooo\\noo\\nooo\\no\\noo\\no\\noo o\\nooo\\noo\\noo\\no\\noo\\noo\\noo\\noo\\nooo\\nooo\\no\\noo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\nooo\\noo\\noo\\no\\noo\\no\\no\\no\\nCanonical Coordinate 1Canonical Coordinate 2Classification in Reduced Subspace\\n••••••••••••••\\n••\\n••\\n••••\\nFIGURE 4.11. Decision boundaries for the vowel training data, in the two-di-\\nmensional subspace spanned by the ﬁrst two canonical variates. Note that in\\nany higher-dimensional subspace, the decision boundaries are h igher-dimensional\\naﬃne planes, and could not be represented as lines.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 137}, page_content='4.4 Logistic Regression 119\\nLDA amounts to the regression followed by an eigen-decomposition of\\nˆYTY. In the case of two classes, there is a single discriminant variable\\nthat is identical up to a scalar multiplication to either of the columns of ˆY.\\nThese connections are developed in Chapter 12. A related fact is that if one\\ntransforms the original predictors XtoˆY, then LDA using ˆYis identical\\nto LDA in the original space (Exercise 4.3).\\n4.4 Logistic Regression\\nThe logistic regression model arises from the desire to model the posterior\\nprobabilities of the Kclasses via linear functions in x, while at the same\\ntime ensuring that they sum to one and remain in [0 ,1]. The model has\\nthe form\\nlogPr(G= 1|X=x)\\nPr(G=K|X=x)=β10+βT\\n1x\\nlogPr(G= 2|X=x)\\nPr(G=K|X=x)=β20+βT\\n2x\\n...\\nlogPr(G=K−1|X=x)\\nPr(G=K|X=x)=β(K−1)0+βT\\nK−1x.(4.17)\\nThe model is speciﬁed in terms of K−1 log-odds or logit transformations\\n(reﬂecting the constraint that the probabilities sum to one). Although the\\nmodel uses the last class as the denominator in the odds-ratios, the choice\\nof denominator is arbitrary in that the estimates are equivariant under this\\nchoice. A simple calculation shows that\\nPr(G=k|X=x) =exp(βk0+βT\\nkx)\\n1 +∑K−1\\nℓ=1exp(βℓ0+βT\\nℓx), k= 1,... ,K −1,\\nPr(G=K|X=x) =1\\n1 +∑K−1\\nℓ=1exp(βℓ0+βT\\nℓx), (4.18)\\nand they clearly sum to one. To emphasize the dependence on the entire pa-\\nrameter set θ={β10,βT\\n1,... ,β (K−1)0,βT\\nK−1}, we denote the probabilities\\nPr(G=k|X=x) =pk(x;θ).\\nWhen K= 2, this model is especially simple, since there is only a single\\nlinear function. It is widely used in biostatistical applications where binary\\nresponses (two classes) occur quite frequently. For example, patients survive\\nor die, have heart disease or not, or a condition is present or absent.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 138}, page_content='120 4. Linear Methods for Classiﬁcation\\n4.4.1 Fitting Logistic Regression Models\\nLogistic regression models are usually ﬁt by maximum likelihood, using the\\nconditional likelihood of Ggiven X. Since Pr( G|X) completely speciﬁes the\\nconditional distribution, the multinomial distribution is appropriate. The\\nlog-likelihood for Nobservations is\\nℓ(θ) =N∑\\ni=1logpgi(xi;θ), (4.19)\\nwhere pk(xi;θ) = Pr( G=k|X=xi;θ).\\nWe discuss in detail the two-class case, since the algorithms simplify\\nconsiderably. It is convenient to code the two-class givia a 0 /1 response yi,\\nwhere yi= 1 when gi= 1, and yi= 0 when gi= 2. Let p1(x;θ) =p(x;θ),\\nandp2(x;θ) = 1−p(x;θ). The log-likelihood can be written\\nℓ(β) =N∑\\ni=1{\\nyilogp(xi;β) + (1 −yi)log(1 −p(xi;β))}\\n=N∑\\ni=1{\\nyiβTxi−log(1 + eβTxi)}\\n. (4.20)\\nHereβ={β10,β1}, and we assume that the vector of inputs xiincludes\\nthe constant term 1 to accommodate the intercept.\\nTo maximize the log-likelihood, we set its derivatives to zero. These score\\nequations are\\n∂ℓ(β)\\n∂β=N∑\\ni=1xi(yi−p(xi;β)) = 0 , (4.21)\\nwhich are p+1 equations nonlinear inβ. Notice that since the ﬁrst compo-\\nnent of xiis 1, the ﬁrst score equation speciﬁes that∑N\\ni=1yi=∑N\\ni=1p(xi;β);\\ntheexpected number of class ones matches the observed number (and hence\\nalso class twos.)\\nTo solve the score equations (4.21), we use the Newton–Raphson algo-\\nrithm, which requires the second-derivative or Hessian matrix\\n∂2ℓ(β)\\n∂β∂βT=−N∑\\ni=1xixiTp(xi;β)(1−p(xi;β)). (4.22)\\nStarting with βold, a single Newton update is\\nβnew=βold−(∂2ℓ(β)\\n∂β∂βT)−1∂ℓ(β)\\n∂β, (4.23)\\nwhere the derivatives are evaluated at βold.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 139}, page_content='4.4 Logistic Regression 121\\nIt is convenient to write the score and Hessian in matrix notation. Let\\nydenote the vector of yivalues, XtheN×(p+ 1) matrix of xivalues,\\npthe vector of ﬁtted probabilities with ith element p(xi;βold) andWa\\nN×Ndiagonal matrix of weights with ith diagonal element p(xi;βold)(1−\\np(xi;βold)). Then we have\\n∂ℓ(β)\\n∂β=XT(y−p) (4.24)\\n∂2ℓ(β)\\n∂β∂βT=−XTWX (4.25)\\nThe Newton step is thus\\nβnew=βold+ (XTWX)−1XT(y−p)\\n= (XTWX)−1XTW(\\nXβold+W−1(y−p))\\n= (XTWX)−1XTWz. (4.26)\\nIn the second and third line we have re-expressed the Newton step as a\\nweighted least squares step, with the response\\nz=Xβold+W−1(y−p), (4.27)\\nsometimes known as the adjusted response . These equations get solved re-\\npeatedly, since at each iteration pchanges, and hence so does Wandz.\\nThis algorithm is referred to as iteratively reweighted least squares or IRLS,\\nsince each iteration solves the weighted least squares problem:\\nβnew←arg min\\nβ(z−Xβ)TW(z−Xβ). (4.28)\\nIt seems that β= 0 is a good starting value for the iterative procedure,\\nalthough convergence is never guaranteed. Typically the algorithm does\\nconverge, since the log-likelihood is concave, but overshooting can occur.\\nIn the rare cases that the log-likelihood decreases, step size halving will\\nguarantee convergence.\\nFor the multiclass case ( K≥3) the Newton algorithm can also be ex-\\npressed as an iteratively reweighted least squares algorithm, but with a\\nvector ofK−1 responses and a nondiagonal weight matrix per observation.\\nThe latter precludes any simpliﬁed algorithms, and in this case it is numer-\\nically more convenient to work with the expanded vector θdirectly (Ex-\\nercise 4.4). Alternatively coordinate-descent methods (Section 3.8.6) can\\nbe used to maximize the log-likelihood eﬃciently. The Rpackageglmnet\\n(Friedman et al., 2010) can ﬁt very large logistic regression problems ef-\\nﬁciently, both in Nandp. Although designed to ﬁt regularized models,\\noptions allow for unregularized ﬁts.\\nLogistic regression models are used mostly as a data analysis and infer-\\nence tool, where the goal is to understand the role of the input variables'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 140}, page_content='122 4. Linear Methods for Classiﬁcation\\nTABLE 4.2. Results from a logistic regression ﬁt to the South African hear t\\ndisease data.\\nCoeﬃcient Std. Error ZScore\\n(Intercept) −4.130 0 .964 −4.285\\nsbp 0.006 0 .006 1 .023\\ntobacco 0.080 0 .026 3 .034\\nldl 0.185 0 .057 3 .219\\nfamhist 0.939 0 .225 4 .178\\nobesity -0.035 0 .029 −1.187\\nalcohol 0.001 0 .004 0 .136\\nage 0.043 0 .010 4 .184\\ninexplaining the outcome. Typically many models are ﬁt in a search for a\\nparsimonious model involving a subset of the variables, possibly with some\\ninteractions terms. The following example illustrates some of the issues\\ninvolved.\\n4.4.2 Example: South African Heart Disease\\nHere we present an analysis of binary data to illustrate the traditional\\nstatistical use of the logistic regression model. The data in Figure 4.1 2 are a\\nsubset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried\\nout in three rural areas of the Western Cape, South Africa (Rousseauw et\\nal., 1983). The aim of the study was to establish the intensity of ischemic\\nheart disease risk factors in that high-incidence region. The data represent\\nwhite males between 15 and 64, and the response variable is the presence or\\nabsence of myocardial infarction (MI) at the time of the survey (the overall\\nprevalence of MI was 5.1% in this region). There are 160 cases in our data\\nset, and a sample of 302 controls. These data are described in more detail\\nin Hastie and Tibshirani (1987).\\nWe ﬁt a logistic-regression model by maximum likelihood, giving the\\nresults shown in Table 4.2. This summary includes Zscores for each of the\\ncoeﬃcients in the model (coeﬃcients divided by their standard errors); a\\nnonsigniﬁcant Zscore suggests a coeﬃcient can be dropped from the model.\\nEach of these correspond formally to a test of the null hypothesis that the\\ncoeﬃcient in question is zero, while all the others are not (also known as\\nthe Wald test). A Zscore greater than approximately 2 in absolute value\\nis signiﬁcant at the 5% level.\\nThere are some surprises in this table of coeﬃcients, which must be in-\\nterpreted with caution. Systolic blood pressure ( sbp) is not signiﬁcant! Nor\\nisobesity , and its sign is negative. This confusion is a result of the corre-\\nlation between the set of predictors. On their own, both sbpandobesity\\nare signiﬁcant, and with positive sign. However, in the presence of many'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 141}, page_content='4.4 Logistic Regression 123\\nsbp0 10 20 30\\no\\no\\noo\\nooo\\noooo\\no\\noo\\nooooo\\no\\nooo\\noo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooooo\\noooooooooooo\\nooooo\\nooooooooo\\noo\\nooooo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noooooo\\noooo\\noooooo\\nooo\\noo\\nooo\\noo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\noooo\\no\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\nooooo\\nooo\\noooooo\\nooo\\nooo\\nooo\\noo\\nooo\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\no oo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\nooooooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\noooooooooooo\\nooo\\noooo\\noooo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooooooooooooo\\nooooo\\nooooooooo\\noo\\nooooo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noooooo\\noooooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\nooooo\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooo oo\\noooooo\\nooo\\noooooo\\noo\\noooo\\nooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\noooo ooo\\nooooo\\noo\\noooo\\noooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooo oo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0.0 0.4 0.8\\no\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooo\\noo\\nooo o ooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\nooo ooo\\nooo\\noo\\nooo oo\\noooo\\noooooo o ooo oo\\noooo o\\no ooo\\no oooo\\noo\\no oo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noo\\no ooo\\noooooooo oo\\no oo\\noo\\noo ooo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\nooo\\nooo\\noooo\\no\\nooo o\\noo oo\\noo ooooo\\noo o ooooooo\\nooo\\noo\\noo oo\\noo oo\\noooooooo\\nooooo\\no\\no ooo o\\noo\\noo\\nooo\\noo oo\\noooooooo\\noooo oo\\nooo\\nooo\\nooo\\noo\\nooooooooo\\noooooo\\no oo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\no oo\\noo\\nooo\\nooooo\\nooo\\noo oo\\nooo\\no\\no oo\\nooo\\nooo oo oo\\no oooo\\noo\\nooo o\\no o oo\\no\\no o oo\\nooo\\nooo\\no ooo\\noooo\\noooo\\no oo ooo\\no\\nooo\\noooooo\\noooo\\no ooo\\noo\\noooo\\nooooo\\nooo o ooo\\nooo\\noo o ooooo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo\\noooo\\no\\noo\\noo ooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooooooooooooooooo\\nooooo\\nooooooooo\\noo\\nooo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooo\\noooooo\\nooo\\noo\\nooooo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\noooo\\no\\noooooooo\\nooooooo\\noooo\\noooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooooo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooooo\\noooooo\\nooo\\noooooo\\noo\\nooo\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooo\\noooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\noooo\\noooo\\noo\\no\\nooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0 50 100\\no\\no\\noo\\nooo\\noooo\\no\\noo\\nooooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\noooooooo\\nooooooooooooo\\nooooo\\noooo\\nooooo\\noo\\nooooo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooooo\\nooo ooo\\noo\\noooo\\noooooooooo\\nooo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\no\\nooo\\nooooo\\noooooooo\\nooooooo\\noooooooooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooo\\noo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noooooooo\\noooooo\\nooo\\noooooo\\noo\\noooo\\nooooo\\noooooo\\nooo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooooooo\\nooooo\\noo\\noooooooo\\nooooo\\noooooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\noooooo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo\\n100 160 220o\\no\\noo\\nooo\\no ooo\\no\\noo\\nooooo\\no\\nooooo\\nooooooo\\nooo\\noo\\nooo\\noo\\noo\\nooo\\noooooo\\nooo\\noo\\nooooooo o\\noooooooooo ooo\\nooooo\\noooo\\nooooo\\noo\\nooo\\noo\\noo\\noooo\\noo\\noooo\\noo\\nooooo\\no ooooo\\noooooo\\noooo\\noooo oo\\nooo\\noo\\nooooo\\nooooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noo\\noo\\no\\nooo\\nooooo\\nooooooo\\no\\nooo oooo\\noo ooooo ooo\\nooo\\noo\\noooo\\noooo\\noooooooo\\nooo\\noo\\no\\nooooo\\noo\\noo\\nooo\\noooo\\noo ooo\\nooo\\noooooo\\nooo\\noooooo\\noo\\noooooo\\nooo\\noooo oo\\nooo\\noooo\\noo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\no o\\nooo\\nooooo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\nooo oooo\\nooooo\\noooooo\\noo oo\\nooooo\\nooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\no\\nooo\\nooo\\nooo\\noooo\\noooo\\noo\\noooo\\nooooo\\nooooooo\\nooo\\noooooooo\\noo\\noo\\nooo\\no\\nooo0 10 20 30o\\noooo\\nooooooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooo\\noo\\nooo\\nooooooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooo oo\\nooooooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooooo\\noo\\no oooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\noooooo\\nooooo\\nooo\\noooooo\\nooo'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 141}, page_content='oooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooo oo\\nooooooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooooo\\noo\\no oooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\noooooo\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooo\\noo\\noo\\nooooooo\\noooooooooooooo\\no\\nooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooo oo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\noo\\noooooo\\nooo\\noo\\noooooooo\\nooo\\no\\noooo\\nooooo\\noooo\\noo\\noo o\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooo oooooooo\\nooo\\noooooooooooooooo oo\\notobaccoo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooooo\\nooo\\nooooooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\noo\\nooooooo\\nooooo\\noooooooo oo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooooo\\noo\\nooooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\nooo\\nooo\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooooo\\noo\\noooo\\nooo\\noooooooooooooo\\no\\nooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\noo ooo\\nooo\\nooo\\nooooooo\\noo\\nooooooooo\\noo\\noooo oo\\noo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooooooooooo\\nooo\\noooooooooo\\noooooooo\\noo\\no ooo\\noo o\\noooo\\nooo\\nooo\\noo\\noo\\no oo\\nooo\\no\\nooooooooo\\noo\\no o ooooo\\nooo\\noooo\\noo\\noo\\no o o oo\\noooo\\noo\\noooo\\no ooo oo\\nooooo\\no oo\\noo ooooooo oooo\\no\\no o ooooo\\noooo\\nooo\\noooo\\noooooo o\\no oooo\\nooooooo o oo\\no oooo oooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\noooooo o oooooo\\no oooo\\noo\\noo\\no o oo oo o oo oooo ooo ooooo\\noooooo\\noooo\\no ooo oo oo\\noooo\\noooo\\no o\\nooo oooo\\no ooooo\\nooooo\\nooo\\noo oooo\\nooo\\noo\\noo oo\\noo\\noo\\nooooooo\\noooooooooooooo\\no\\nooo o o oo\\no\\noooo\\nooo\\noooo\\no\\noo oo\\noo o oo\\nooooo\\nooo o\\nooooo\\noo oo o\\nooo\\nooo\\nooooooo\\noooo oo oo\\nooo\\noo\\no ooo oooo\\no oo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\nooo\\noo\\noo\\noo\\noooo\\noo\\noooo oo\\nooooo o o o ooo\\no oo\\noo oo oooooo\\no ooo oooo\\noo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooooooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooo\\noo\\nooo\\nooooo\\noooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooooo\\nooooooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooo\\noo\\noo\\nooooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noooo\\noo\\nooooooo\\noooooo\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooooo\\noo\\nooooooo\\noooooooooooooo\\no\\nooooooo\\no\\noooo\\nooooooo\\no\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\noooooooo\\nooo\\noo\\noooooooo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\nooooooo oooo\\nooo\\noooooooooo\\noooooooo\\noo\\noooo\\nooo\\noooo\\no oo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooooooo\\noo\\nooooooo\\nooooooo\\noo\\noo\\nooooo\\noooo\\noo\\noooo\\noooooo\\nooooo\\nooo\\nooooooooooooo\\no\\nooooooo\\noooo\\nooo\\noo\\nooooooooo\\nooooo\\noooooooooo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\no oooo\\nooooooooooooo\\nooooo\\noo\\noo\\nooooooooooooooooo\\noooo\\noooooo\\noooo\\noooooooo\\noooo\\noo oo\\noo\\nooooooo\\nooooo o\\nooooo\\nooo\\noooooo\\nooo\\noo\\noooo\\noo\\noo\\noooo\\nooo\\noooooooooooooo\\noooooooo\\no\\noooo\\nooo\\noooo\\no\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\nooooooooooo\\noo\\no ooo oo\\noo\\nooo\\no\\noo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\no ooooooo ooo\\nooo\\noooooooooooooooooo\\noo\\noooo\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\nooooo\\noooo\\noo\\nooooooo\\nooo\\noooo\\noo\\noo\\no o ooo\\noooo\\noo\\noooo\\noo oooo\\nooooo\\no oo\\nooo oooooo oooo\\no\\nooooooo\\noooo\\nooo\\noo\\noooo ooooo\\nooooo\\no ooooooo oo\\nooo\\noooooo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\nooooooooooooo\\nooooo\\noo\\noo\\nooooooooooooooooooooo\\noooo oo\\noooo\\no ooooooo\\noooo\\noooo\\noo\\nooooo\\noo\\noooooo\\nooooo\\nooo\\nooo ooo\\nooo\\noo\\noooo\\noo\\noo\\noooo\\nooo\\nooooooooo\\nooooo\\nooooo ooo\\no\\noooo\\nooo\\noooo\\nooooo\\nooo oo\\no oooo\\noooo\\nooooo\\nooooo\\nooo\\nooo\\nooooooo\\nooooooooooo\\noo\\noooooooo\\nooo\\nooo\\noo\\nooooo\\noooo\\noo\\nooo\\nooo\\no\\no\\noo\\noo\\noo\\noo\\noooo\\noo\\no\\nooooo\\noooooooooooooo\\noooooooooo\\nooo o oooo\\no\\noooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\no\\nooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\nooooooooo\\noooooo\\noooo\\nooo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooooo\\no\\nooooooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\nooooooo\\nooooo\\no\\nooooooo\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noooooo\\noo\\noooooo\\noooooooooo\\nooo\\noooooo\\nooo\\nooo\\noo\\nooooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\nooooo ooooo\\noo\\noooo\\nooo\\noooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\nooooo'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 141}, page_content='ooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noooooo\\noo\\noooooo\\noooooooooo\\nooo\\noooooo\\nooo\\nooo\\noo\\nooooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\nooooo ooooo\\noo\\noooo\\nooo\\noooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\no oo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\noooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooooooooo\\noooooo\\noooo\\nooo\\nooo\\no oooooo\\nooo\\noo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\noo\\noo\\nooooo\\no\\nooo\\noooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\no\\noooo\\noo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooo oooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\nooo\\nooo\\noo\\noooooo\\noooooo\\noooo\\nooo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\noooo\\noooooo\\noo\\no\\nooo\\nooo\\noooo\\no\\nooo\\no\\noo\\noldl\\noo oo\\noo\\noooo\\noo\\noo o oo\\nooo\\nooo\\nooo\\no\\no oo\\noo ooo\\noo\\noooo\\no\\noo oooo\\nooo\\noo\\no oo\\noooo\\nooo o\\no oo\\noo\\noooooo\\noo oo oo\\noo\\noooo oo\\nooo\\nooo\\noooo\\noo\\noo oo\\nooo o\\noo o\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\no o ooooooo\\nooo ooo\\nooooo ooooo\\nooooooo\\nooo\\noo\\no oo\\no oo\\nooo\\no\\noo\\nooooo\\noooo\\nooooo\\no\\nooo\\no oo ooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\no\\noooooo\\noo ooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\no oooo\\nooooo\\no ooo\\noooo\\noooooooooooo\\noo\\noo\\nooooo o oo\\noo\\no oo\\noo oo\\no ooo\\nooooo ooooooo\\nooo\\no oo\\nooooo\\no\\noo\\noo\\nooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\noooooo\\noooo\\no o o\\nooo ooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\nooo ooo\\no\\nooooo\\noo\\noo\\noo\\noo oooooooo\\noo oo\\noooo\\no ooooo\\noo\\noooo\\nooo\\noo oo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noo o ooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\nooooooooo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\noo ooooooo\\noooooo\\noo\\noo\\nooo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo oo\\no\\nooooooooooo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\nooooooooo\\nooooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooo\\noooo\\noooo\\noooooooooooo\\nooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noooooo\\noo\\noooooo\\noooooooooo\\nooo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\noooooooooo\\noo\\noooo\\nooo\\noooo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooo\\noo\\nooo oo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\noooo\\no\\noooooo\\nooo\\noo\\nooo\\noooo\\noooo\\nooooo\\noooooo\\noooooo\\noo\\nooooooo\\noo\\nooo\\noooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\nooooooooo\\noooooo\\noooo\\no oo\\nooo\\nooooooo\\nooo\\noo\\nooo\\nooo\\nooo\\no\\noo\\nooooo\\noooo\\nooooo\\no\\nooo\\noooooo\\noo\\no\\no\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noo ooooo\\nooooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\nooooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooooooo\\noooo\\noooooooooooo\\no oo\\nooo\\noo\\nooo\\no\\nooo\\no\\nooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\noooooo\\nooooo oo\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooo oooooo\\noooo\\noooo\\noooooo\\noo\\noooo\\nooooooo\\no\\noooo\\noo\\no\\n2 6 10 14oooo\\noo\\noooo\\noo\\nooooo\\nooo\\nooo\\nooo\\no\\nooo\\nooooo\\noo\\no\\nooo\\no\\noo o ooo\\nooo\\noo\\nooo\\noooo\\noooo\\no oooo\\noooooo\\noooooo\\noo\\noooooo\\noooooo\\no ooo\\noo\\noooo\\noooo\\nooo\\noo\\noo\\nooo\\noo oo\\nooo\\nooo\\noo\\no oo\\noo\\nooo oooooo\\noooooo\\no oooo oo\\nooo\\no ooo ooo\\nooo\\noo\\no oo\\nooo\\noooo\\noo\\nooooo\\noo\\noo\\nooooo\\no\\nooooooooo\\noo\\no\\no\\nooo\\noooo\\noooo\\nooo\\nooo\\noo\\nooooooo\\no oooo\\no\\noooooo\\no\\noo\\nooo\\nooo\\noo ooo\\nooooo\\noooo\\noooo\\noooooooooooo\\noo\\noo\\noooooooo\\noo\\nooo\\noooo\\noooo\\nooo ooooooooo\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooo\\no\\nooo\\nooo\\noo\\noooo\\noo\\noooooooooo\\noo o\\noooooo\\nooo\\nooo\\noo\\no\\noooooooo\\noo\\noooooo\\no\\nooooo\\noo\\noo\\noo\\noooooooooo\\noooo\\noooo\\noooooo\\noo\\noooo\\nooooooo\\no\\noooo\\noo\\no0.0 0.4 0.8o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\no oo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooooo ooooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\nooooo o ooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\nooooooo o oo\\noo\\nooo\\nooo o\\nooooooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\nooooo o\\noo\\noooo\\noooooo oo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooooo oo\\nooo\\no oooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\noo ooo o\\nooooo\\noooo\\noo\\noooo\\nooo o\\nooo\\no oo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\no oooo\\noooo\\noo\\noo\\nooo\\noo ooooo\\noo\\noo\\noooooo\\noo\\noo\\noo'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 141}, page_content='ooo\\noo ooo o\\nooooo\\noooo\\noo\\noooo\\nooo o\\nooo\\no oo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\no oooo\\noooo\\noo\\noo\\nooo\\noo ooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\nooooo o ooooooooo\\noo oooo\\nooooooooo\\no ooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\noooo\\noooo ooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\no ooo\\noo\\noooo\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noo oo\\noo o ooooo\\nooo\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\nooooo o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\nooooooooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\noooo\\nooo oooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\noooo\\noo\\noo ooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooo oooo\\nooo\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\nooo oo\\nfamhisto\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooooo\\nooo\\no ooo\\nooo\\nooo\\noooooo\\noo o\\noooo\\noo\\noo ooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\nooooo\\noooo\\noo\\noo\\nooo\\nooo oooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\nooooooooooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\noooo\\nooooooo\\noooo\\nooo\\nooo\\noo\\noooooooo\\noo\\nooooo\\noooooo\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\noo o\\noo\\nooo\\noooooo\\noooo\\noo\\noooo\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noooooooo\\nooo\\nooooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\nooooo o\\nooooo\\noooo\\noo\\noooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooooo\\nooo\\noooo\\nooo\\nooo\\noo oooo\\nooo\\noooo\\noo\\nooooo\\noooo\\nooo\\noo\\noo\\noooo\\noooooooooo\\noo\\noo\\noo o oo\\noooo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\no ooooooo ooooooo\\noooooo\\nooooooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooo\\noo\\noo\\noooooooooo\\noo\\nooo\\nooo o\\nooo o ooo\\noooo\\nooo\\nooo\\noo\\no ooooooo\\noo\\nooooo\\no ooooo\\noo\\noooo\\noooooooo\\nooooooooo\\nooooooo\\nooo\\nooo\\noooo\\noo\\noooo\\noo\\nooo\\noo\\nooo\\noooooo\\noooo\\noo\\nooo o\\noooo\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\noo o\\noo o\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\no ooooooo\\nooo\\noo ooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\nooo o\\noooo\\nooo\\nooooo o\\nooooo\\noo oo\\noo\\no ooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noooooo oo\\nooo\\no ooo\\no oo\\nooo\\noooooo\\nooo\\noooo\\noo\\noo ooo\\noooo\\nooo\\noo\\noo\\noooo\\no o o ooooooo\\noo\\noo\\noo ooo\\noooo\\noo\\noo\\nooo\\no o ooooo\\noo\\noo\\noooooo\\noo\\noo\\noo\\noooooo\\noooooooo\\no oooo o ooo oooooo\\noo oooo\\noo o oooooo\\noooo\\nooo\\nooo\\nooo\\nooo\\no oo\\noo o\\noo\\noo\\no oo oooo o oo\\noo\\nooo\\noooo\\noo ooooo\\noo oo\\nooo\\nooo\\noo\\no o oooooo\\noo\\noo ooo\\noo oooo\\noo\\noooo\\noooooooo\\nooooooo oo\\noooo ooo\\nooo\\nooo\\noooo\\noo\\no ooo\\noo\\nooo\\noo\\nooo\\nooo ooo\\noooo\\noo\\noooo\\no oo o\\noo\\nooooooo\\nooo\\noo\\noooo\\noooo\\nooo\\noo o\\nooo\\nooooo\\noooo\\nooooo\\nooooo\\noo\\nooooo\\noooo\\noo oooooo\\nooo\\noo ooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noooo\\noooo\\noooo\\nooo\\nooooo\\noooo\\noo\\nooooo\\nooooooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\noooooo\\nooooooo\\noooooo\\no\\noooooo\\nooooo\\nooooooooo\\no\\nooooo\\nooooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooo oo\\nooooo\\noo\\noooo\\nooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooooo\\noo\\noooo\\no\\nooooooooo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\noooo\\nooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooooooo\\noo\\nooooooooo\\nooooooooo\\noo\\noo\\noooo\\noo\\noo\\noooooo\\nooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noo oo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\noooooo\\nooooooo\\nooooooo'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 141}, page_content='ooooooo\\noo\\nooooooooo\\nooooooooo\\noo\\noo\\noooo\\noo\\noo\\noooooo\\nooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noo oo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\noooooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noooooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\nooooooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooooo\\noo\\noooooooooo oo\\noo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooo oooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noo\\noo\\nooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\noooo\\nooooo\\nooooooo\\noo\\noooo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\noooo\\noooo\\noooooo\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\noooo\\nooooo\\no\\noooooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\nooooooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\nooo oo\\noo\\nooooo\\nooo oo oooo\\nooooooo\\noo o\\nooooo\\nooo\\nooo\\noooo\\nooo\\noo ooo\\noooooo o ooo o\\nooo\\noo\\noooo\\noo\\nooooo\\noo\\no o\\noo\\noo\\noo\\nooo\\noo\\noooooooo ooo\\noo\\nooo o\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\nooooo o ooo\\nooo\\noo oo o\\nooooooo oo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooo oooo\\noo\\no ooo\\nooo oo\\no oooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooo o ooooo\\nooooo\\noooo oo\\no\\no oo oo o\\noooooo\\no oo\\noooo\\no\\nooo\\noooo\\no o\\noooo\\nooooooo\\nooooo\\nooooo\\no\\noooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\no oooo oo oo\\nooo\\nooo\\nooo\\noo ooooo\\nooooooo\\noooooo\\nooooo\\no oooooo oo\\no\\noo\\noooooooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\noo ooo\\nooo\\nooo\\noooo\\no\\noo\\no oo\\nooooo\\nooooo\\noo\\nooo oooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\noobesity\\noooo\\noo\\noooo\\noooooooooo\\nooooo\\noo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooooo\\noooo\\nooooooooo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noo\\nooooooo\\nooo\\nooooo\\nooooooooo\\noooo\\noooooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\nooooooo\\noo\\noooo\\nooooo\\nooooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\no\\nooo\\noooo\\nooooo\\no\\nooooooo\\nooooo\\nooooo\\nooooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noo\\nooooo\\nooooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\noooooo\\noooo\\no\\noo\\nooo\\nooo oo\\nooooo\\noo\\nooooooooo oo\\noooo\\noo\\nooo\\noooo\\no\\noo\\noo\\noo\\no\\n15 25 35 45oooo\\noo\\nooooo\\nooo oooooo\\nooooooo\\nooo\\nooooo\\nooo\\nooo\\noooo\\nooo\\nooooo\\noooooo ooooo\\nooo\\noo\\noooo\\noo\\nooooooo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooooooooooo\\noo\\noooo\\noo\\noo\\noooo\\noooooo\\noo\\noo\\noo\\noooo\\nooooo\\nooo\\nooooo\\noooo\\nooo oo\\noooo\\nooo ooooo\\no\\nooo\\nooo\\nooooo\\no\\noo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooooooo\\noo\\noo\\noo\\noooo\\noo\\noo\\nooooooooo\\nooooo\\noooooo\\no\\noooooo\\noooooo\\nooo\\noooo\\no\\nooo\\noooo\\noo\\nooo\\no\\nooooooo\\nooooo\\nooooo\\no\\noooo\\nooo\\no\\nooo\\nooo\\noooo\\no\\nooooooooo\\nooo\\nooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooo\\nooooooooo\\no\\noo\\noooooo\\noooo\\nooo\\nooo\\noooooo\\nooo\\noo\\nooooo\\nooo\\nooo\\noooo\\no\\noo\\nooo\\nooooo\\nooooo\\noo\\noooo\\nooooooo\\noooo\\noo\\nooo\\nooooo\\noo\\noo\\noo\\no0 50 100o\\noooo\\noooooo\\noooooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooooooo\\nooo\\noooo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooooooooo\\no\\noooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooooooooo\\no\\noo\\noo\\noo\\nooooooooo oooooooooooooo\\nooooooooo\\nooooooo\\no\\nooo\\nooooo\\noooooooooooooooo\\noo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooooo\\nooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooo oooooooooo\\nooo\\no\\noo\\nooo\\noooooo\\noo\\nooo\\nooooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooo\\noo\\nooooo\\no ooo\\noooo\\noo\\noooo\\no\\noooooo ooooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noo oooo\\noo\\nooo\\no ooooo\\nooooooooo\\nooo\\noooo\\nooooo\\nooo\\noooo\\noooooooooo\\nooooooo\\nooo\\noooo\\noooooo\\nooooooooooo\\nooo\\noooooo\\nooooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooooooooo\\no\\noo\\noo\\noo\\nooooooooooooooooooooooo\\nooooooooo\\nooooooo\\no\\nooo\\nooo\\noo\\noo\\noooooooooooooooo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\noooooooooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\nooooooooo\\nooooo\\nooo\\no\\noo\\nooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\noo ooooo oooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noooooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooo oooo\\nooo\\noo\\noo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooo'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 141}, page_content='ooooooooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\noo ooooo oooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noooooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooo oooo\\nooo\\noo\\noo\\noooooooooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooo\\noooooo\\nooooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooooooooooooooooo\\noooooooooooooooo\\no\\nooo\\nooo\\noo\\noo\\noooooooooooooo\\noo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\noooooooooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooooooooooooo\\nooo\\no\\noo\\nooo\\noooooo\\noo\\nooo\\nooooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\nooooooooo\\noooo\\noo\\noooo\\no\\noooooooooo ooooo\\noo\\nooooo\\no\\noo\\nooo\\noooo o\\noo\\no ooo\\noo oooo\\no oo oo o\\noo\\nooo\\no o o o oo\\no oooooooo\\nooo\\no ooooo ooo\\no oo\\noooo\\nooo o o ooooo\\nooooooo\\no ooo ooo\\no o oooo\\no oo oooooooo\\noooo o oooo\\noo ooo o o oo\\noo\\noo\\noooooo\\no ooo\\nooo\\nooo\\no oo\\noooo\\noo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooo ooo ooooooooooo\\no oooo o ooooooo ooo\\no\\nooo\\no oo\\noo\\nooooooooooo ooo oo\\noo\\nooo\\noooo\\noooooo\\noo\\no oo oo ooo\\noo oooo\\nooo\\noooo\\no ooooo\\no o ooo ooooooo\\nooo\\noo ooo\\noooo\\noo\\nooo\\noo oo o oo\\noooo\\noo\\nooo\\no\\noo\\nooo\\noo\\no oo o oooooooooo\\nooo\\no\\noo\\no oo\\noo o ooooo\\nooo\\noo\\noo ooo\\noo\\nooo o oo\\no\\noo\\noo\\noo\\nooo\\no oo\\noo ooo\\noooo\\noo oo\\no\\nooooo o ooo\\no oooooooo\\noooo\\noo\\no ooo\\no\\no ooooo o oooooo oo\\noo\\no oooo\\no\\noo\\nooo\\nooooo\\noo\\noooo\\noooooo\\noooooo\\noo\\nooo\\nooooooooooooooo\\nooo\\nooooo oooo\\nooo\\noooo\\nooooooo\\nooo\\nooooooo\\nooooooo\\noooooo\\nooooooooooo\\nooooooooo\\no\\noooooooo\\noo\\noo\\noooooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noo oooo\\no\\noo\\noo\\noo\\nooooooooooooooooooooooo\\noooooo oooooooooo\\no\\nooo\\nooo\\noo\\nooooooooooo ooooo\\noo\\nooo\\noooo\\noooooo\\noo\\noooooooo\\noooooo\\nooo\\noooo\\noooooo\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooooo\\nooo\\no\\noo\\nooo\\no\\noo\\nooo\\noo\\noooooooooooooo\\nooo\\no\\noo\\nooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\nooooooooo\\noooo\\noo\\noooo\\no\\nooooooooooooooo\\noo\\nooooo\\no\\noo\\nooo\\nooooo\\noalcoholo\\noooo\\noooooo\\nooo ooo\\noo\\nooo\\noooooo\\nooooooooo\\nooo\\nooooo oooo\\nooo\\noooo\\nooo o ooo\\nooo\\nooooooo\\nooooooo\\noooooo\\noooo o oooooo\\nooooooooo\\nooooo oooo\\noo\\noo\\noo oooo\\no ooo\\nooo\\nooo\\nooo\\noooo\\noo\\nooooo\\noooooo\\no\\noo\\noo\\noo\\nooooooooooooooooo o ooooo\\nooooooooo\\nooooooo\\no\\nooo\\nooooo\\noooo o oooooo ooooo\\noo\\nooo\\noo oo\\nooo o\\noo\\noo\\noooooooo\\no oooo\\no\\nooo\\noooo\\no oooo o\\noooooooooooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooooo oooooo\\noo\\nooo\\no\\noo\\nooo\\noo\\noooo ooooo\\nooooo\\nooo\\nooo\\nooo\\noooooooo\\nooo\\noooo ooo\\noo\\noooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\nooooo\\noooo\\noooo\\no\\nooooooooo\\noo ooo\\noooo\\noooo\\noo\\noooo\\no\\no ooooo ooooooooo\\noo\\nooooo\\no\\noo\\nooo\\no oooo\\no\\n100 160 220oo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\no o\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\nooooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\no\\noooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\noooooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\nooooo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\nooooo\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\noo o\\no\\nooo\\noo\\nooo\\noo\\nooooo\\no\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\no\\nooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooo\\noo\\noo\\noo\\nooo\\noo\\no\\no\\nooooo\\nooooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\noooooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\noooo o\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\nooo\\noo\\noo\\n2 6 10 14oo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 141}, page_content='ooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooooo\\no\\no\\noooo o\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\nooo\\no\\nooooo\\no\\nooo\\noooooooo\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\no\\no\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\nooo\\noo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\no oo o\\nooo\\noooo o o\\noooo o\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\noo o\\no\\nooo\\noo\\nooo\\noo\\noooo oo\\noo\\noo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooo oo\\no\\no\\noo o o o\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\noooo oooo\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\noo o\\noooo o\\noo\\nooo\\noo\\no\\nooo\\noo\\noo o\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\nooooo o\\noo o\\no\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\no oo\\no o\\nooooo ooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\noo o\\noo\\noo\\n15 25 35 45oo\\noo\\no\\no\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\nooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooo oo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noooo\\noooo\\noo\\noo\\noo\\nooooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\noooooo\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\nooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\nooooooooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\noooo\\no\\nooooo\\noooo\\noo\\noo\\noo\\nooo\\no\\nooo\\nooooo\\noooo\\no\\nooooo\\nooo\\noo\\nooo\\noo\\no\\no\\noo\\nooo\\no\\noooo\\no\\noo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noooo\\no\\no\\nooo\\nooo\\no\\nooo\\noo\\nooo\\noo\\noooooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noo\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooooo\\no\\noooooo\\noo\\noo\\nooo\\noo\\no\\noooooo\\nooooo\\no\\no\\nooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\nooo\\noooo\\no oooo\\no\\nooo\\nooooooo\\no\\no\\noo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\no\\nooooo\\no\\nooooo\\noo\\nooooo\\noooo\\noo\\noooo\\no\\nooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\nooo\\nooooo\\nooo\\nooooo\\noo\\nooo\\noo\\no\\nooo\\noo\\nooo\\noo\\nooo\\no\\noooooo\\no\\noo\\noo\\noo\\noo\\no\\noo\\noooo\\no\\noo\\noooo\\no\\noo\\noo\\noooooo\\nooo\\no\\nooooo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\noo\\noooooooooo\\no\\noo\\no\\nooo\\nooo\\no\\nooo\\no\\nooo\\noo\\noo\\n20 40 60\\n20 40 60age\\nFIGURE 4.12. A scatterplot matrix of the South African heart disease data.\\nEach plot shows a pair of risk factors, and the cases and controls are color coded\\n(red is a case). The variable family history of heart disease ( famhist )is binary\\n(yes or no).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 142}, page_content='124 4. Linear Methods for Classiﬁcation\\nTABLE 4.3. Results from stepwise logistic regression ﬁt to South African heart\\ndisease data.\\nCoeﬃcient Std. Error Zscore\\n(Intercept) −4.204 0 .498 −8.45\\ntobacco 0.081 0 .026 3 .16\\nldl 0.168 0 .054 3 .09\\nfamhist 0.924 0 .223 4 .14\\nage 0.044 0 .010 4 .52\\nother correlated variables, they are no longer needed (and can even get a\\nnegative sign).\\nAt this stage the analyst might do some model selection; ﬁnd a subset\\nof the variables that are suﬃcient for explaining their joint eﬀect on the\\nprevalence of chd. One way to proceed by is to drop the least signiﬁcant co-\\neﬃcient, and reﬁt the model. This is done repeatedly until no further terms\\ncan be dropped from the model. This gave the model shown in Table 4.3.\\nA better but more time-consuming strategy is to reﬁt each of the models\\nwith one variable removed, and then perform an analysis of deviance to\\ndecide which variable to exclude. The residual deviance of a ﬁtted model\\nis minus twice its log-likelihood, and the deviance between two models is\\nthe diﬀerence of their individual residual deviances (in analogy to sums-of-\\nsquares). This strategy gave the same ﬁnal model as above.\\nHow does one interpret a coeﬃcient of 0 .081 (Std. Error = 0 .026) for\\ntobacco , for example? Tobacco is measured in total lifetime usage in kilo-\\ngrams, with a median of 1 .0kg for the controls and 4 .1kg for the cases. Thus\\nan increase of 1kg in lifetime tobacco usage accounts for an increase in the\\nodds of coronary heart disease of exp(0 .081) = 1 .084 or 8 .4%. Incorporat-\\ning the standard error we get an approximate 95% conﬁdence interval of\\nexp(0.081±2×0.026) = (1 .03,1.14).\\nWe return to these data in Chapter 5, where we see that some of the\\nvariables have nonlinear eﬀects, and when modeled appropriately, are not\\nexcluded from the model.\\n4.4.3 Quadratic Approximations and Inference\\nThe maximum-likelihood parameter estimates ˆβsatisfy a self-consistency\\nrelationship: they are the coeﬃcients of a weighted least squares ﬁt, where\\nthe responses are\\nzi=xT\\niˆβ+(yi−ˆpi)\\nˆpi(1−ˆpi), (4.29)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 143}, page_content='4.4 Logistic Regression 125\\nand the weights are wi= ˆpi(1−ˆpi), both depending on ˆβitself. Apart from\\nproviding a convenient algorithm, this connection with least squares has\\nmore to oﬀer:\\n•The weighted residual sum-of-squares is the familiar Pearson chi-\\nsquare statistic\\nN∑\\ni=1(yi−ˆpi)2\\nˆpi(1−ˆpi), (4.30)\\na quadratic approximation to the deviance.\\n•Asymptotic likelihood theory says that if the model is correct, then\\nˆβis consistent (i.e., converges to the trueβ).\\n•A central limit theorem then shows that the distribution of ˆβcon-\\nverges to N(β,(XTWX)−1). This and other asymptotics can be de-\\nrived directly from the weighted least squares ﬁt by mimicking normal\\ntheory inference.\\n•Model building can be costly for logistic regression models, because\\neach model ﬁtted requires iteration. Popular shortcuts are the Rao\\nscore test which tests for inclusion of a term, and the Wald test which\\ncan be used to test for exclusion of a term. Neither of these require\\niterative ﬁtting, and are based on the maximum-likelihood ﬁt of the\\ncurrent model. It turns out that both of these amount to adding\\nor dropping a term from the weighted least squares ﬁt, using the\\nsame weights. Such computations can be done eﬃciently, without\\nrecomputing the entire weighted least squares ﬁt.\\nSoftware implementations can take advantage of these connections. For\\nexample, the generalized linear modeling software in R (which includes lo-\\ngistic regression as part of the binomial family of models) exploits them\\nfully. GLM (generalized linear model) objects can be treated as linear model\\nobjects, and all the tools available for linear models can be applied auto-\\nmatically.\\n4.4.4 L1Regularized Logistic Regression\\nTheL1penalty used in the lasso (Section 3.4.2) can be used for variable\\nselection and shrinkage with any linear regression model. For logistic re-\\ngression, we would maximize a penalized version of (4.20):\\nmax\\nβ0,β\\uf8f1\\n\\uf8f2\\n\\uf8f3N∑\\ni=1[\\nyi(β0+βTxi)−log(1 + eβ0+βTxi)]\\n−λp∑\\nj=1|βj|\\uf8fc\\n\\uf8fd\\n\\uf8fe.(4.31)\\nAs with the lasso, we typically do not penalize the intercept term, and stan-\\ndardize the predictors for the penalty to be meaningful. Criterion (4.31) is'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 144}, page_content='126 4. Linear Methods for Classiﬁcation\\nconcave, and a solution can be found using nonlinear programming meth-\\nods (Koh et al., 2007, for example). Alternatively, using the same quadratic\\napproximations that were used in the Newton algorithm in Section 4.4.1,\\nwe can solve (4.31) by repeated application of a weighted lasso algorit hm.\\nInterestingly, the score equations [see (4.24)] for the variables with non-zer o\\ncoeﬃcients have the form\\nxT\\nj(y−p) =λ≤sign(βj), (4.32)\\nwhich generalizes (3.58) in Section 3.4.4; the active variables are tied in\\ntheirgeneralized correlation with the residuals.\\nPath algorithms such as LAR for lasso are more diﬃcult, because the\\ncoeﬃcient proﬁles are piecewise smooth rather than linear. Nevertheless,\\nprogress can be made using quadratic approximations.\\n******************************* ************** ****** *************************************************** ****************************************** **************************************************************************** *****************\\n0.0 0.5 1.0 1.5 2.00.0 0.2 0.4 0.6******************************* * ******************************************************************************************************************************************************************************************** *****************\\n******************************* ************** * ****************************************************************************************************************************************************************************** *****************\\n****************************** ********************************************************************************************************************************************************************************************** *****************\\n******************************* ************** ****** *************************************************** *************************************************** ************************ ******************************************* ************************************************ ************** ****** *************************************************** *************************************************** ************************ *************************** *************** * ********************************************************************************************************************************************************************************************************************************************* *****************\\nobesityalcoholsbptobaccoldlfamhistage1 2 4 5 6 7Coeﬃcients βj(λ)\\n||β(λ)||1\\nFIGURE 4.13. L1regularized logistic regression coeﬃcients for the South\\nAfrican heart disease data, plotted as a function of the L1norm. The variables\\nwere all standardized to have unit variance. The proﬁles are comp uted exactly at\\neach of the plotted points.\\nFigure 4.13 shows the L1regularization path for the South African\\nheart disease data of Section 4.4.2. This was produced using the Rpackage\\nglmpath (Park and Hastie, 2007), which uses predictor–corrector methods\\nof convex optimization to identify the exact values of λat which the active\\nset of non-zero coeﬃcients changes (vertical lines in the ﬁgure). Here the\\nproﬁles look almost linear; in other examples the curvature will be more\\nvisible.\\nCoordinate descent methods (Section 3.8.6) are very eﬃcient for comput-\\ning the coeﬃcient proﬁles on a grid of values for λ. TheRpackageglmnet'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 145}, page_content='4.4 Logistic Regression 127\\n(Friedman et al., 2010) can ﬁt coeﬃcient paths for very large logistic re-\\ngression problems eﬃciently (large in Norp). Their algorithms can exploit\\nsparsity in the predictor matrix X, which allows for even larger problems.\\nSee Section 18.4 for more details, and a discussion of L1-regularized multi-\\nnomial models.\\n4.4.5 Logistic Regression or LDA?\\nIn Section 4.3 we ﬁnd that the log-posterior odds between class kandK\\nare linear functions of x(4.9):\\nlogPr(G=k|X=x)\\nPr(G=K|X=x)= logπk\\nπK−1\\n2(θk+θK)TΣ−1(θk−θK)\\n+xTΣ−1(θk−θK)\\n=αk0+αT\\nkx. (4.33)\\nThis linearity is a consequence of the Gaussian assumption for the class\\ndensities, as well as the assumption of a common covariance matrix. The\\nlinear logistic model (4.17) by construction has linear logits:\\nlogPr(G=k|X=x)\\nPr(G=K|X=x)=βk0+βT\\nkx. (4.34)\\nIt seems that the models are the same. Although they have exactly the same\\nform, the diﬀerence lies in the way the linear coeﬃcients are estimated. The\\nlogistic regression model is more general, in that it makes less assumptio ns.\\nWe can write the joint density ofXandGas\\nPr(X,G=k) = Pr( X)Pr(G=k|X), (4.35)\\nwhere Pr( X) denotes the marginal density of the inputs X. For both LDA\\nand logistic regression, the second term on the right has the logit-linear\\nform\\nPr(G=k|X=x) =eβk0+βT\\nkx\\n1 +∑K−1\\nℓ=1eβℓ0+βT\\nℓx, (4.36)\\nwhere we have again arbitrarily chosen the last class as the reference.\\nThe logistic regression model leaves the marginal density of Xas an arbi-\\ntrary density function Pr( X), and ﬁts the parameters of Pr( G|X) by max-\\nimizing the conditional likelihood —the multinomial likelihood with proba-\\nbilities the Pr( G=k|X). Although Pr( X) is totally ignored, we can think\\nof this marginal density as being estimated in a fully nonparametric and\\nunrestricted fashion, using the empirical distribution function which places\\nmass 1 /Nat each observation.\\nWith LDA we ﬁt the parameters by maximizing the full log-likelihood,\\nbased on the joint density\\nPr(X,G=k) =φ(X;θk,Σ)πk, (4.37)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 146}, page_content='128 4. Linear Methods for Classiﬁcation\\nwhere φis the Gaussian density function. Standard normal theory leads\\neasily to the estimates ˆ θk,ˆΣ, and ˆ πkgiven in Section 4.3. Since the linear\\nparameters of the logistic form (4.33) are functions of the Gaussian para m-\\neters, we get their maximum-likelihood estimates by plugging in the corre-\\nsponding estimates. However, unlike in the conditional case, the marginal\\ndensity Pr( X) does play a role here. It is a mixture density\\nPr(X) =K∑\\nk=1πkφ(X;θk,Σ), (4.38)\\nwhich also involves the parameters.\\nWhat role can this additional component/restriction play? By relying\\non the additional model assumptions, we have more information about the\\nparameters, and hence can estimate them more eﬃciently (lower variance).\\nIf in fact the true fk(x) are Gaussian, then in the worst case ignoring this\\nmarginal part of the likelihood constitutes a loss of eﬃciency of about 30%\\nasymptotically in the error rate (Efron, 1975). Paraphrasing: with 3 0%\\nmore data, the conditional likelihood will do as well.\\nFor example, observations far from the decision boundary (which are\\ndown-weighted by logistic regression) play a role in estimating the common\\ncovariance matrix. This is not all good news, because it also means that\\nLDA is not robust to gross outliers.\\nFrom the mixture formulation, it is clear that even observations without\\nclass labels have information about the parameters. Often it is expensive\\nto generate class labels, but unclassiﬁed observations come cheaply. By\\nrelying on strong model assumptions, such as here, we can use both types\\nof information.\\nThe marginal likelihood can be thought of as a regularizer, requiring\\nin some sense that class densities be visible from this marginal view. For\\nexample, if the data in a two-class logistic regression model can be per-\\nfectly separated by a hyperplane, the maximum likelihood estimates of the\\nparameters are undeﬁned (i.e., inﬁnite; see Exercise 4.5). The LDA coeﬃ-\\ncients for the same data will be well deﬁned, since the marginal likelihood\\nwill not permit these degeneracies.\\nIn practice these assumptions are never correct, and often some of the\\ncomponents of Xare qualitative variables. It is generally felt that logistic\\nregression is a safer, more robust bet than the LDA model, relying on fewer\\nassumptions. It is our experience that the models give very similar results,\\neven when LDA is used inappropriately, such as with qualitative predictors.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 147}, page_content='4.5 Separating Hyperplanes 129\\nFIGURE 4.14. A toy example with two classes separable by a hyperplane. The\\norange line is the least squares solution, which misclassiﬁes one of the training\\npoints. Also shown are two blue separating hyperplanes found by t heperceptron\\nlearning algorithm with diﬀerent random starts.\\n4.5 Separating Hyperplanes\\nWe have seen that linear discriminant analysis and logistic regression bot h\\nestimate linear decision boundaries in similar but slightly diﬀerent ways.\\nFor the rest of this chapter we describe separating hyperplane classiﬁers.\\nThese procedures construct linear decision boundaries that explicitly try\\nto separate the data into diﬀerent classes as well as possible. They provide\\nthe basis for support vector classiﬁers, discussed in Chapter 12. The math-\\nematical level of this section is somewhat higher than that of the previous\\nsections.\\nFigure 4.14 shows 20 data points in two classes in IR2. These data can be\\nseparated by a linear boundary. Included in the ﬁgure (blue lines) are two\\nof the inﬁnitely many possible separating hyperplanes . The orange line is\\nthe least squares solution to the problem, obtained by regressing the −1/1\\nresponse YonX(with intercept); the line is given by\\n{x:ˆβ0+ˆβ1x1+ˆβ2x2= 0}. (4.39)\\nThis least squares solution does not do a perfect job in separating the\\npoints, and makes one error. This is the same boundary found by LDA,\\nin light of its equivalence with linear regression in the two-class case (Sec-\\ntion 4.3 and Exercise 4.2).\\nClassiﬁers such as (4.39), that compute a linear combination of the input\\nfeatures and return the sign, were called perceptrons in the engineering liter-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 148}, page_content='130 4. Linear Methods for Classiﬁcation\\nx0x\\nβ∗β0+βTx= 0\\nFIGURE 4.15. The linear algebra of a hyperplane (aﬃne set).\\nature in the late 1950s (Rosenblatt, 1958). Perceptrons set the foundations\\nfor the neural network models of the 1980s and 1990s.\\nBefore we continue, let us digress slightly and review some vector algebra.\\nFigure 4.15 depicts a hyperplane or aﬃne set Ldeﬁned by the equation\\nf(x) =β0+βTx= 0; since we are in IR2this is a line.\\nHere we list some properties:\\n1. For any two points x1andx2lying in L,βT(x1−x2) = 0, and hence\\nβ∗=β/||β||is the vector normal to the surface of L.\\n2. For any point x0inL,βTx0=−β0.\\n3. The signed distance of any point xtoLis given by\\nβ∗T(x−x0) =1\\n∥β∥(βTx+β0)\\n=1\\n||f′(x)||f(x). (4.40)\\nHence f(x) is proportional to the signed distance from xto the hyperplane\\ndeﬁned by f(x) = 0.\\n4.5.1 Rosenblatt’s Perceptron Learning Algorithm\\nTheperceptron learning algorithm tries to ﬁnd a separating hyperplane by\\nminimizing the distance of misclassiﬁed points to the decision boundary. If'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 149}, page_content='4.5 Separating Hyperplanes 131\\na response yi= 1 is misclassiﬁed, then xT\\niβ+β0<0, and the opposite for\\na misclassiﬁed response with yi=−1. The goal is to minimize\\nD(β,β0) =−∑\\ni∈Myi(xT\\niβ+β0), (4.41)\\nwhere Mindexes the set of misclassiﬁed points. The quantity is non-\\nnegative and proportional to the distance of the misclassiﬁed points to\\nthe decision boundary deﬁned by βTx+β0= 0. The gradient (assuming\\nMis ﬁxed) is given by\\n∂D(β,β0)\\n∂β=−∑\\ni∈Myixi, (4.42)\\n∂D(β,β0)\\n∂β0=−∑\\ni∈Myi. (4.43)\\nThe algorithm in fact uses stochastic gradient descent to minimize this\\npiecewise linear criterion. This means that rather than computing the sum\\nof the gradient contributions of each observation followed by a step in the\\nnegative gradient direction, a step is taken after each observation is visit ed.\\nHence the misclassiﬁed observations are visited in some sequence, and the\\nparameters βare updated via\\n(\\nβ\\nβ0)\\n←(\\nβ\\nβ0)\\n+ρ(\\nyixi\\nyi)\\n. (4.44)\\nHereρis the learning rate, which in this case can be taken to be 1 without\\nloss in generality. If the classes are linearly separable, it can be shown that\\nthe algorithm converges to a separating hyperplane in a ﬁnite number of\\nsteps (Exercise 4.6). Figure 4.14 shows two solutions to a toy problem, eac h\\nstarted at a diﬀerent random guess.\\nThere are a number of problems with this algorithm, summarized in\\nRipley (1996):\\n•When the data are separable, there are many solutions, and which\\none is found depends on the starting values.\\n•The “ﬁnite” number of steps can be very large. The smaller the gap,\\nthe longer the time to ﬁnd it.\\n•When the data are not separable, the algorithm will not converge,\\nand cycles develop. The cycles can be long and therefore hard to\\ndetect.\\nThe second problem can often be eliminated by seeking a hyperplane not\\nin the original space, but in a much enlarged space obtained by creating'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 150}, page_content='132 4. Linear Methods for Classiﬁcation\\nmany basis-function transformations of the original variables. This is a nal-\\nogous to driving the residuals in a polynomial regression problem down\\nto zero by making the degree suﬃciently large. Perfect separation cannot\\nalways be achieved: for example, if observations from two diﬀerent classes\\nshare the same input. It may not be desirable either, since the resulting\\nmodel is likely to be overﬁt and will not generalize well. We return to this\\npoint at the end of the next section.\\nA rather elegant solution to the ﬁrst problem is to add additional con-\\nstraints to the separating hyperplane.\\n4.5.2 Optimal Separating Hyperplanes\\nTheoptimal separating hyperplane separates the two classes and maximizes\\nthe distance to the closest point from either class (Vapnik, 1996). Not only\\ndoes this provide a unique solution to the separating hyperplane problem,\\nbut by maximizing the margin between the two classes on the training data,\\nthis leads to better classiﬁcation performance on test data.\\nWe need to generalize criterion (4.41). Consider the optimization problem\\nmax\\nβ,β0,||β||=1M\\nsubject to yi(xT\\niβ+β0)≥M, i= 1,... ,N.(4.45)\\nThe set of conditions ensure that all the points are at least a signed\\ndistance Mfrom the decision boundary deﬁned by βandβ0, and we seek\\nthe largest such Mand associated parameters. We can get rid of the ||β||=\\n1 constraint by replacing the conditions with\\n1\\n||β||yi(xT\\niβ+β0)≥M, (4.46)\\n(which redeﬁnes β0) or equivalently\\nyi(xT\\niβ+β0)≥M||β||. (4.47)\\nSince for any βandβ0satisfying these inequalities, any positively scaled\\nmultiple satisﬁes them too, we can arbitrarily set ||β||= 1/M. Thus (4.45)\\nis equivalent to\\nmin\\nβ,β01\\n2||β||2\\nsubject to yi(xT\\niβ+β0)≥1, i= 1,... ,N.(4.48)\\nIn light of (4.40), the constraints deﬁne an empty slab or margin around the\\nlinear decision boundary of thickness 1 /||β||. Hence we choose βandβ0to\\nmaximize its thickness. This is a convex optimization problem (quadratic'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 151}, page_content='4.5 Separating Hyperplanes 133\\ncriterion with linear inequality constraints). The Lagrange (primal) func-\\ntion, to be minimized w.r.t. βandβ0, is\\nLP=1\\n2||β||2−N∑\\ni=1αi[yi(xT\\niβ+β0)−1]. (4.49)\\nSetting the derivatives to zero, we obtain:\\nβ=N∑\\ni=1αiyixi, (4.50)\\n0 =N∑\\ni=1αiyi, (4.51)\\nand substituting these in (4.49) we obtain the so-called Wolfe dual\\nLD=N∑\\ni=1αi−1\\n2N∑\\ni=1N∑\\nk=1αiαkyiykxT\\nixk\\nsubject to αi≥0. (4.52)\\nThe solution is obtained by maximizing LDin the positive orthant, a sim-\\npler convex optimization problem, for which standard software can be used.\\nIn addition the solution must satisfy the Karush–Kuhn–Tucker conditions,\\nwhich include (4.50), (4.51), (4.52) and\\nαi[yi(xT\\niβ+β0)−1] = 0 ∀i. (4.53)\\nFrom these we can see that\\n•ifαi>0, then yi(xT\\niβ+β0) = 1, or in other words, xiis on the\\nboundary of the slab;\\n•ifyi(xT\\niβ+β0)>1,xiis not on the boundary of the slab, and αi= 0.\\nFrom (4.50) we see that the solution vector βis deﬁned in terms of a linear\\ncombination of the support points xi—those points deﬁned to be on the\\nboundary of the slab via αi>0. Figure 4.16 shows the optimal separating\\nhyperplane for our toy example; there are three support points. Likewise,\\nβ0is obtained by solving (4.53) for any of the support points.\\nThe optimal separating hyperplane produces a function ˆf(x) =xTˆβ+ˆβ0\\nfor classifying new observations:\\nˆG(x) = sign ˆf(x). (4.54)\\nAlthough none of the training observations fall in the margin (by con-\\nstruction), this will not necessarily be the case for test observations. The'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 152}, page_content='134 4. Linear Methods for Classiﬁcation\\nFIGURE 4.16. The same data as in Figure 4.14. The shaded region delineates\\nthe maximum margin separating the two classes. There are three support points\\nindicated, which lie on the boundary of the margin, and the optima l separating\\nhyperplane (blue line) bisects the slab. Included in the ﬁgure is the boundary found\\nusing logistic regression (red line), which is very close to th e optimal separating\\nhyperplane (see Section 12.3.3).\\nintuition is that a large margin on the training data will lead to good\\nseparation on the test data.\\nThe description of the solution in terms of support points seems to sug-\\ngest that the optimal hyperplane focuses more on the points that count,\\nand is more robust to model misspeciﬁcation. The LDA solution, on the\\nother hand, depends on all of the data, even points far away from the de-\\ncision boundary. Note, however, that the identiﬁcation of these support\\npoints required the use of all the data. Of course, if the classes are really\\nGaussian, then LDA is optimal, and separating hyperplanes will pay a price\\nfor focusing on the (noisier) data at the boundaries of the classes.\\nIncluded in Figure 4.16 is the logistic regression solution to this prob-\\nlem, ﬁt by maximum likelihood. Both solutions are similar in this case.\\nWhen a separating hyperplane exists, logistic regression will always ﬁnd\\nit, since the log-likelihood can be driven to 0 in this case (Exercise 4.5).\\nThe logistic regression solution shares some other qualitative features with\\nthe separating hyperplane solution. The coeﬃcient vector is deﬁned by a\\nweighted least squares ﬁt of a zero-mean linearized response on the input\\nfeatures, and the weights are larger for points near the decision boundary\\nthan for those further away.\\nWhen the data are not separable, there will be no feasible solution to\\nthis problem, and an alternative formulation is needed. Again one can en-\\nlarge the space using basis transformations, but this can lead to artiﬁcial'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 153}, page_content='Exercises 135\\nseparation through over-ﬁtting. In Chapter 12 we discuss a more attractive\\nalternative known as the support vector machine , which allows for overlap,\\nbut minimizes a measure of the extent of this overlap.\\nBibliographic Notes\\nGood general texts on classiﬁcation include Duda et al. (2000), Hand\\n(1981), McLachlan (1992) and Ripley (1996). Mardia et al. (1979) have\\na concise discussion of linear discriminant analysis. Michie et al. (1994)\\ncompare a large number of popular classiﬁers on benchmark datasets. Lin-\\near separating hyperplanes are discussed in Vapnik (1996). Our account of\\nthe perceptron learning algorithm follows Ripley (1996).\\nExercises\\nEx. 4.1 Show how to solve the generalized eigenvalue problem max aTBa\\nsubject to aTWa= 1 by transforming to a standard eigenvalue problem.\\nEx. 4.2 Suppose we have features x∈IRp, a two-class response, with class\\nsizesN1,N2, and the target coded as −N/N 1,N/N 2.\\n(a) Show that the LDA rule classiﬁes to class 2 if\\nxTˆΣ−1(ˆθ2−ˆθ1)>1\\n2ˆθT\\n2ˆΣ−1ˆθ2−1\\n2ˆθT\\n1ˆΣ−1ˆθ1+ log(N1\\nN)\\n−log(N2\\nN)\\n,\\nand class 1 otherwise.\\n(b) Consider minimization of the least squares criterion\\nN∑\\ni=1(yi−β0−βTxi)2. (4.55)\\nShow that the solution ˆβsatisﬁes\\n[\\n(N−2)ˆΣ+N1N2\\nNˆΣB]\\nβ=N(ˆθ2−ˆθ1) (4.56)\\n(after simpliﬁcation),where ˆΣB= (ˆθ2−ˆθ1)(ˆθ2−ˆθ1)T.\\n(c) Hence show that ˆΣBβis in the direction (ˆ θ2−ˆθ1) and thus\\nˆβ∝ˆΣ−1(ˆθ2−ˆθ1). (4.57)\\nTherefore the least squares regression coeﬃcient is identical to the\\nLDA coeﬃcient, up to a scalar multiple.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 154}, page_content='136 4. Linear Methods for Classiﬁcation\\n(d) Show that this result holds for any (distinct) coding of the two classes.\\n(e) Find the solution ˆβ0, and hence the predicted values ˆf=ˆβ0+ˆβTx.\\nConsider the following rule: classify to class 2 if ˆ yi>0 and class\\n1 otherwise. Show this is not the same as the LDA rule unless the\\nclasses have equal numbers of observations.\\n(Fisher, 1936; Ripley, 1996)\\nEx. 4.3 Suppose we transform the original predictors XtoˆYvia linear\\nregression. In detail, let ˆY=X(XTX)−1XTY=XˆB, where Yis the\\nindicator response matrix. Similarly for any input x∈IRp, we get a trans-\\nformed vector ˆ y=ˆBTx∈IRK. Show that LDA using ˆYis identical to\\nLDA in the original space.\\nEx. 4.4 Consider the multilogit model with Kclasses (4.17). Let βbe the\\n(p+ 1)(K−1)-vector consisting of all the coeﬃcients. Deﬁne a suitably\\nenlarged version of the input vector xto accommodate this vectorized co-\\neﬃcient matrix. Derive the Newton-Raphson algorithm for maximizing the\\nmultinomial log-likelihood, and describe how you would implement this\\nalgorithm.\\nEx. 4.5 Consider a two-class logistic regression problem with x∈IR. Char-\\nacterize the maximum-likelihood estimates of the slope and intercept pa-\\nrameter if the sample xifor the two classes are separated by a point x0∈IR.\\nGeneralize this result to (a) x∈IRp(see Figure 4.16), and (b) more than\\ntwo classes.\\nEx. 4.6 Suppose we have Npoints xiin IRpin general position, with class\\nlabels yi∈ {−1,1}. Prove that the perceptron learning algorithm converges\\nto a separating hyperplane in a ﬁnite number of steps:\\n(a) Denote a hyperplane by f(x) =βT\\n1x+β0= 0, or in more compact\\nnotation βTx∗= 0, where x∗= (x,1) and β= (β1,β0). Let zi=\\nx∗\\ni/||x∗\\ni||. Show that separability implies the existence of a βsepsuch\\nthatyiβT\\nsepzi≥1∀i\\n(b) Given a current βold, the perceptron algorithm identiﬁes a point zithat\\nis misclassiﬁed, and produces the update βnew←βold+yizi. Show\\nthat||βnew−βsep||2≤ ||βold−βsep||2−1, and hence that the algorithm\\nconverges to a separating hyperplane in no more than ||βstart−βsep||2\\nsteps (Ripley, 1996).\\nEx. 4.7 Consider the criterion\\nD∗(β,β0) =−N∑\\ni=1yi(xT\\niβ+β0), (4.58)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 155}, page_content='Exercises 137\\na generalization of (4.41) where we sum over all the observations. Consider\\nminimizing D∗subject to ||β||= 1. Describe this criterion in words. Does\\nit solve the optimal separating hyperplane problem?\\nEx. 4.8 Consider the multivariate Gaussian model X|G=k∼N(θk,Σ),\\nwith the additional restriction that rank {θk}K\\n1=L < max(K−1,p).\\nDerive the constrained MLEs for the θkandΣ. Show that the Bayes clas-\\nsiﬁcation rule is equivalent to classifying in the reduced subspace computed\\nby LDA (Hastie and Tibshirani, 1996b).\\nEx. 4.9 Write a computer program to perform a quadratic discriminant\\nanalysis by ﬁtting a separate Gaussian model per class. Try it out on the\\nvowel data, and compute the misclassiﬁcation error for the test data. The\\ndata can be found in the book website www-stat.stanford.edu/ElemStatLearn .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 156}, page_content='138 4. Linear Methods for Classiﬁcation'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 157}, page_content='This is page 139\\nPrinter: Opaque this\\n5\\nBasis Expansions and Regularization\\n5.1 Introduction\\nWe have already made use of models linear in the input features, both for\\nregression and classiﬁcation. Linear regression, linear discriminant analysi s,\\nlogistic regression and separating hyperplanes all rely on a linear model.\\nIt is extremely unlikely that the true function f(X) is actually linear in\\nX. In regression problems, f(X) = E( Y|X) will typically be nonlinear and\\nnonadditive in X, and representing f(X) by a linear model is usually a con-\\nvenient, and sometimes a necessary, approximation. Convenient because a\\nlinear model is easy to interpret, and is the ﬁrst-order Taylor approxima-\\ntion to f(X). Sometimes necessary, because with Nsmall and/or plarge,\\na linear model might be all we are able to ﬁt to the data without overﬁt-\\nting. Likewise in classiﬁcation, a linear, Bayes-optimal decision boundary\\nimplies that some monotone transformation of Pr( Y= 1|X) is linear in X.\\nThis is inevitably an approximation.\\nIn this chapter and the next we discuss popular methods for moving\\nbeyond linearity. The core idea in this chapter is to augment/replace the\\nvector of inputs Xwith additional variables, which are transformations of\\nX, and then use linear models in this new space of derived input features.\\nDenote by hm(X) : IRp↦→IR the mth transformation of X,m=\\n1,... ,M . We then model\\nf(X) =M∑\\nm=1βmhm(X), (5.1)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 158}, page_content='140 5. Basis Expansions and Regularization\\nalinear basis expansion inX. The beauty of this approach is that once the\\nbasis functions hmhave been determined, the models are linear in these\\nnew variables, and the ﬁtting proceeds as before.\\nSome simple and widely used examples of the hmare the following:\\n•hm(X) =Xm, m= 1,... ,p recovers the original linear model.\\n•hm(X) =X2\\njorhm(X) =XjXkallows us to augment the inputs with\\npolynomial terms to achieve higher-order Taylor expansions. Note,\\nhowever, that the number of variables grows exponentially in the de-\\ngree of the polynomial. A full quadratic model in pvariables requires\\nO(p2) square and cross-product terms, or more generally O(pd) for a\\ndegree- dpolynomial.\\n•hm(X) = log( Xj),√\\nXj,...permits other nonlinear transformations\\nof single inputs. More generally one can use similar functions involv-\\ning several inputs, such as hm(X) =||X||.\\n•hm(X) =I(Lm≤Xk< Um), an indicator for a region of Xk. By\\nbreaking the range of Xkup into Mksuch nonoverlapping regions\\nresults in a model with a piecewise constant contribution for Xk.\\nSometimes the problem at hand will call for particular basis functions hm,\\nsuch as logarithms or power functions. More often, however, we use the basis\\nexpansions as a device to achieve more ﬂexible representations for f(X).\\nPolynomials are an example of the latter, although they are limited by\\ntheir global nature—tweaking the coeﬃcients to achieve a functional form\\nin one region can cause the function to ﬂap about madly in remote regions.\\nIn this chapter we consider more useful families of piecewise-polynomials\\nandsplines that allow for local polynomial representations. We also discuss\\nthewavelet bases, especially useful for modeling signals and images. These\\nmethods produce a dictionary Dconsisting of typically a very large number\\n|D|of basis functions, far more than we can aﬀord to ﬁt to our data. Along\\nwith the dictionary we require a method for controlling the complexity\\nof our model, using basis functions from the dictionary. There are three\\ncommon approaches:\\n•Restriction methods, where we decide before-hand to limit the class\\nof functions. Additivity is an example, where we assume that our\\nmodel has the form\\nf(X) =p∑\\nj=1fj(Xj)\\n=p∑\\nj=1Mj∑\\nm=1βjmhjm(Xj). (5.2)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 159}, page_content='5.2 Piecewise Polynomials and Splines 141\\nThe size of the model is limited by the number of basis functions Mj\\nused for each component function fj.\\n•Selection methods, which adaptively scan the dictionary and include\\nonly those basis functions hmthat contribute signiﬁcantly to the ﬁt of\\nthe model. Here the variable selection techniques discussed in Chap-\\nter 3 are useful. The stagewise greedy approaches such as CART,\\nMARS and boosting fall into this category as well.\\n•Regularization methods where we use the entire dictionary but re-\\nstrict the coeﬃcients. Ridge regression is a simple example of a regu-\\nlarization approach, while the lasso is both a regularization and selec-\\ntion method. Here we discuss these and more sophisticated methods\\nfor regularization.\\n5.2 Piecewise Polynomials and Splines\\nWe assume until Section 5.7 that Xis one-dimensional. A piecewise poly-\\nnomial function f(X) is obtained by dividing the domain of Xinto contigu-\\nous intervals, and representing fby a separate polynomial in each interval.\\nFigure 5.1 shows two simple piecewise polynomials. The ﬁrst is piecewise\\nconstant, with three basis functions:\\nh1(X) =I(X < ξ 1), h 2(X) =I(ξ1≤X < ξ 2), h 3(X) =I(ξ2≤X).\\nSince these are positive over disjoint regions, the least squares estimate o f\\nthe model f(X) =∑3\\nm=1βmhm(X) amounts to ˆβm=¯Ym, the mean of Y\\nin the mth region.\\nThe top right panel shows a piecewise linear ﬁt. Three additional basis\\nfunctions are needed: hm+3=hm(X)X, m = 1,... ,3. Except in special\\ncases, we would typically prefer the third panel, which is also piecewise\\nlinear, but restricted to be continuous at the two knots. These continu-\\nity restrictions lead to linear constraints on the parameters; for example,\\nf(ξ−\\n1) =f(ξ+\\n1) implies that β1+ξ1β4=β2+ξ1β5. In this case, since there\\nare two restrictions, we expect to get back two parameters, leaving four free\\nparameters.\\nA more direct way to proceed in this case is to use a basis that incorpo-\\nrates the constraints:\\nh1(X) = 1, h 2(X) =X, h 3(X) = (X−ξ1)+, h 4(X) = (X−ξ2)+,\\nwhere t+denotes the positive part. The function h3is shown in the lower\\nright panel of Figure 5.1. We often prefer smoother functions, and these\\ncan be achieved by increasing the order of the local polynomial. Figure 5.2\\nshows a series of piecewise-cubic polynomials ﬁt to the same data, with'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 160}, page_content='142 5. Basis Expansions and Regularization\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOPiecewise Constant\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOPiecewise Linear\\nOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOO\\nO\\nO\\nOO\\nO\\nOOContinuous Piecewise Linear Piecewise-linear Basis Function\\n•\\n•••\\n••••\\n••\\n•• • ••\\n••••\\n••\\n•••\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n•••••\\n•••\\n•\\n••\\n••\\nξ1 ξ1ξ1 ξ1\\nξ2 ξ2ξ2 ξ2\\n(X−ξ1)+\\nFIGURE 5.1. The top left panel shows a piecewise constant function ﬁt to some\\nartiﬁcial data. The broken vertical lines indicate the positio ns of the two knots\\nξ1andξ2. The blue curve represents the true function, from which the dat a were\\ngenerated with Gaussian noise. The remaining two panels show piec ewise lin-\\near functions ﬁt to the same data—the top right unrestricted, and t he lower left\\nrestricted to be continuous at the knots. The lower right panel sh ows a piecewise–\\nlinear basis function, h3(X) = ( X−ξ1)+, continuous at ξ1. The black points\\nindicate the sample evaluations h3(xi), i= 1, . . . , N .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 161}, page_content='5.2 Piecewise Polynomials and Splines 143\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOODiscontinuous\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous First Derivative\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOOO\\nO\\nO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOOOO\\nO\\nOO\\nOOOContinuous Second DerivativePiecewise Cubic Polynomials\\nξ1 ξ1ξ1 ξ1\\nξ2 ξ2ξ2 ξ2\\nFIGURE 5.2. A series of piecewise-cubic polynomials, with increasing orde rs of\\ncontinuity.\\nincreasing orders of continuity at the knots. The function in the lower\\nright panel is continuous, and has continuous ﬁrst and second derivatives\\nat the knots. It is known as a cubic spline . Enforcing one more order of\\ncontinuity would lead to a global cubic polynomial. It is not hard to show\\n(Exercise 5.1) that the following basis represents a cubic spline with knots\\natξ1andξ2:\\nh1(X) = 1, h 3(X) =X2, h5(X) = (X−ξ1)3\\n+,\\nh2(X) =X, h 4(X) =X3, h6(X) = (X−ξ2)3\\n+.(5.3)\\nThere are six basis functions corresponding to a six-dimensional linear space\\nof functions. A quick check conﬁrms the parameter count: (3 regions) ×(4\\nparameters per region) −(2 knots) ×(3 constraints per knot)= 6.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 162}, page_content='144 5. Basis Expansions and Regularization\\nMore generally, an order- Mspline with knots ξj, j= 1,... ,K is a\\npiecewise-polynomial of order M, and has continuous derivatives up to\\norder M−2. A cubic spline has M= 4. In fact the piecewise-constant\\nfunction in Figure 5.1 is an order-1 spline, while the continuous piece-\\nwise linear function is an order-2 spline. Likewise the general form for the\\ntruncated-power basis set would be\\nhj(X) = Xj−1, j= 1,... ,M,\\nhM+ℓ(X) = ( X−ξℓ)M−1\\n+, ℓ= 1,... ,K.\\nIt is claimed that cubic splines are the lowest-order spline for which the\\nknot-discontinuity is not visible to the human eye. There is seldom any\\ngood reason to go beyond cubic-splines, unless one is interested in smooth\\nderivatives. In practice the most widely used orders are M= 1,2 and 4.\\nThese ﬁxed-knot splines are also known as regression splines . One needs\\nto select the order of the spline, the number of knots and their placement.\\nOne simple approach is to parameterize a family of splines by the number\\nof basis functions or degrees of freedom, and have the observations xide-\\ntermine the positions of the knots. For example, the expression bs(x,df=7)\\ninRgenerates a basis matrix of cubic-spline functions evaluated at the N\\nobservations in x, with the 7 −3 = 41interior knots at the appropriate per-\\ncentiles of x(20,40,60 and 80th.) One can be more explicit, however; bs(x,\\ndegree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,\\nwith three interior knots, and returns an N×4 matrix.\\nSince the space of spline functions of a particular order and knot sequence\\nis a vector space, there are many equivalent bases for representing them\\n(just as there are for ordinary polynomials.) While the truncated power\\nbasis is conceptually simple, it is not too attractive numerically: powers of\\nlarge numbers can lead to severe rounding problems. The B-spline basis,\\ndescribed in the Appendix to this chapter, allows for eﬃcient computations\\neven when the number of knots Kis large.\\n5.2.1 Natural Cubic Splines\\nWe know that the behavior of polynomials ﬁt to data tends to be erratic\\nnear the boundaries, and extrapolation can be dangerous. These problems\\nare exacerbated with splines. The polynomials ﬁt beyond the boundary\\nknots behave even more wildly than the corresponding global polynomials\\nin that region. This can be conveniently summarized in terms of the point-\\nwise variance of spline functions ﬁt by least squares (see the example in the\\nnext section for details on these variance calculations). Figure 5.3 compares\\n1A cubic spline with four knots is eight-dimensional. The bs() function omits by\\ndefault the constant term in the basis, since terms like this are typically included with\\nother terms in the model.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 163}, page_content='5.2 Piecewise Polynomials and Splines 145\\nXPointwise Variances\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5 0.6•\\n••\\n•\\n••\\n•••••••••••••• •••• ••••••••••••••••••••••••••\\n•••••\\n•••••••••••••••••••••••••••••••••••••••••••\\n•\\n••\\n•\\n••••••••••••••••••••••••••\\n•\\n••••••••••••••••••\\n•••••••••••••••••••• ••••••••••••••••••••••••• ••••Global Linear\\nGlobal Cubic Polynomial\\nCubic Spline - 2 knots\\nNatural Cubic Spline - 6 knots\\nFIGURE 5.3. Pointwise variance curves for four diﬀerent models, with Xcon-\\nsisting of 50points drawn at random from U[0,1], and an assumed error model\\nwith constant variance. The linear and cubic polynomial ﬁts have two and four\\ndegrees of freedom, respectively, while the cubic spline and na tural cubic spline\\neach have six degrees of freedom. The cubic spline has two knots at0.33and0.66,\\nwhile the natural spline has boundary knots at 0.1and0.9, and four interior knots\\nuniformly spaced between them.\\nthe pointwise variances for a variety of diﬀerent models. The explosion of\\nthe variance near the boundaries is clear, and inevitably is worst for cubic\\nsplines.\\nAnatural cubic spline adds additional constraints, namely that the func-\\ntion is linear beyond the boundary knots. This frees up four degrees of\\nfreedom (two constraints each in both boundary regions), which can be\\nspent more proﬁtably by sprinkling more knots in the interior region. This\\ntradeoﬀ is illustrated in terms of variance in Figure 5.3. There will be a\\nprice paid in bias near the boundaries, but assuming the function is lin-\\near near the boundaries (where we have less information anyway) is often\\nconsidered reasonable.\\nA natural cubic spline with Kknots is represented by Kbasis functions.\\nOne can start from a basis for cubic splines, and derive the reduced ba-\\nsis by imposing the boundary constraints. For example, starting from the\\ntruncated power series basis described in Section 5.2, we arrive at (Exer-\\ncise 5.4):\\nN1(X) = 1, N 2(X) =X, N k+2(X) =dk(X)−dK−1(X),(5.4)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 164}, page_content='146 5. Basis Expansions and Regularization\\nwhere\\ndk(X) =(X−ξk)3\\n+−(X−ξK)3\\n+\\nξK−ξk. (5.5)\\nEach of these basis functions can be seen to have zero second and third\\nderivative for X≥ξK.\\n5.2.2 Example: South African Heart Disease (Continued)\\nIn Section 4.4.2 we ﬁt linear logistic regression models to the South African\\nheart disease data. Here we explore nonlinearities in the functions using\\nnatural splines. The functional form of the model is\\nlogit[Pr(chd|X)] =θ0+h1(X1)Tθ1+h2(X2)Tθ2+≤≤≤+hp(Xp)Tθp,(5.6)\\nwhere each of the θjare vectors of coeﬃcients multiplying their associated\\nvector of natural spline basis functions hj.\\nWe use four natural spline bases for each term in the model. For example,\\nwithX1representing sbp,h1(X1) is a basis consisting of four basis func-\\ntions. This actually implies three rather than two interior knots (chosen at\\nuniform quantiles of sbp), plus two boundary knots at the extremes of the\\ndata, since we exclude the constant term from each of the hj.\\nSincefamhist is a two-level factor, it is coded by a simple binary or\\ndummy variable, and is associated with a single coeﬃcient in the ﬁt of the\\nmodel.\\nMore compactly we can combine all pvectors of basis functions (and\\nthe constant term) into one big vector h(X), and then the model is simply\\nh(X)Tθ, with total number of parameters df = 1 +∑p\\nj=1dfj, the sum of\\nthe parameters in each component term. Each basis function is evaluated\\nat each of the Nsamples, resulting in a N×df basis matrix H. At this\\npoint the model is like any other linear logistic model, and the algorithms\\ndescribed in Section 4.4.1 apply.\\nWe carried out a backward stepwise deletion process, dropping terms\\nfrom this model while preserving the group structure of each term, rather\\nthan dropping one coeﬃcient at a time. The AIC statistic (Section 7.5) was\\nused to drop terms, and all the terms remaining in the ﬁnal model would\\ncause AIC to increase if deleted from the model (see Table 5.1). Figure 5.4\\nshows a plot of the ﬁnal model selected by the stepwise regression. The\\nfunctions displayed are ˆfj(Xj) =hj(Xj)Tˆθjfor each variable Xj. The\\ncovariance matrix Cov( ˆθ) =Σis estimated by ˆΣ= (HTWH)−1, where W\\nis the diagonal weight matrix from the logistic regression. Hence vj(Xj) =\\nVar[ˆfj(Xj)] =hj(Xj)TˆΣjjhj(Xj) is the pointwise variance function of ˆfj,\\nwhere Cov( ˆθj) =ˆΣjjis the appropriate sub-matrix of ˆΣ. The shaded region\\nin each panel is deﬁned by ˆfj(Xj)±2√\\nvj(Xj).\\nThe AIC statistic is slightly more generous than the likelihood-ratio tes t\\n(deviance test). Both sbpandobesity are included in this model, while'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 165}, page_content='5.2 Piecewise Polynomials and Splines 147\\n100 120 140 160 180 200 220-2 0 2 4\\n0 5 10 15 20 25 300 2 4 6 8\\n2 4 6 8 10 12 14-4 -2 0 2 4\\n-4 -2 0 2 4\\nAbsent Present\\n15 20 25 30 35 40 45-2 0 2 4 6\\n20 30 40 50 60-6 -4 -2 0 2ˆf(sbp)\\nsbp\\nˆf(tobacco )\\ntobaccoˆf(ldl)\\nldlˆf(obesity )\\nobesity\\nˆf(age)\\nageˆf(famhist )\\nfamhist\\nFIGURE 5.4. Fitted natural-spline functions for each of the terms in the ﬁnal\\nmodel selected by the stepwise procedure. Included are pointw ise standard-error\\nbands. The rug plot at the base of each ﬁgure indicates the location of each of the\\nsample values for that variable (jittered to break ties).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 166}, page_content='148 5. Basis Expansions and Regularization\\nTABLE 5.1. Final logistic regression model, after stepwise deletion of natural\\nsplines terms. The column labeled “LRT” is the likelihood-ra tio test statistic when\\nthat term is deleted from the model, and is the change in deviance from the full\\nmodel (labeled “none”).\\nTerms Df Deviance AIC LRT P-value\\nnone 458.09 502.09\\nsbp 4 467.16 503.16 9.076 0.059\\ntobacco 4 470.48 506.48 12.387 0.015\\nldl 4 472.39 508.39 14.307 0.006\\nfamhist 1 479.44 521.44 21.356 0.000\\nobesity 4 466.24 502.24 8.147 0.086\\nage 4 481.86 517.86 23.768 0.000\\nthey were not in the linear model. The ﬁgure explains why, since their\\ncontributions are inherently nonlinear. These eﬀects at ﬁrst may come as\\na surprise, but an explanation lies in the nature of the retrospective data.\\nThese measurements were made sometime after the patients suﬀered a\\nheart attack, and in many cases they had already beneﬁted from a healthier\\ndiet and lifestyle, hence the apparent increase in risk at low values for\\nobesity andsbp. Table 5.1 shows a summary of the selected model.\\n5.2.3 Example: Phoneme Recognition\\nIn this example we use splines to reduce ﬂexibility rather than increase it;\\nthe application comes under the general heading of functional modeling. In\\nthe top panel of Figure 5.5 are displayed a sample of 15 log-periodograms\\nfor each of the two phonemes “aa” and “ao” measured at 256 frequencies.\\nThe goal is to use such data to classify a spoken phoneme. These two\\nphonemes were chosen because they are diﬃcult to separate.\\nThe input feature is a vector xof length 256, which we can think of as\\na vector of evaluations of a function X(f) over a grid of frequencies f. In\\nreality there is a continuous analog signal which is a function of frequency,\\nand we have a sampled version of it.\\nThe gray lines in the lower panel of Figure 5.5 show the coeﬃcients of\\na linear logistic regression model ﬁt by maximum likelihood to a training\\nsample of 1000 drawn from the total of 695 “aa”s and 1022 “ao”s. The\\ncoeﬃcients are also plotted as a function of frequency, and in fact we can\\nthink of the model in terms of its continuous counterpart\\nlogPr(aa|X)\\nPr(ao|X)=∫\\nX(f)β(f)df, (5.7)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 167}, page_content='5.2 Piecewise Polynomials and Splines 149\\nFrequencyLog-periodogram\\n0 50 100 150 200 2500 5 10 15 20 25Phoneme Examples\\naa\\nao\\nFrequencyLogistic Regression Coefficients\\n0 50 100 150 200 250-0.4 -0.2 0.0 0.2 0.4Phoneme Classification: Raw and Restricted Logistic Regression\\nFIGURE 5.5. The top panel displays the log-periodogram as a function of fre -\\nquency for 15examples each of the phonemes “aa” and “ao” sampled from a total\\nof695“aa”s and 1022“ao”s. Each log-periodogram is measured at 256uniformly\\nspaced frequencies. The lower panel shows the coeﬃcients (as a f unction of fre-\\nquency) of a logistic regression ﬁt to the data by maximum likeli hood, using the\\n256log-periodogram values as inputs. The coeﬃcients are restric ted to be smooth\\nin the red curve, and are unrestricted in the jagged gray curve.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 168}, page_content='150 5. Basis Expansions and Regularization\\nwhich we approximate by\\n256∑\\nj=1X(fj)β(fj) =256∑\\nj=1xjβj. (5.8)\\nThe coeﬃcients compute a contrast functional, and will have appreciable\\nvalues in regions of frequency where the log-periodograms diﬀer between\\nthe two classes.\\nThe gray curves are very rough. Since the input signals have fairly strong\\npositive autocorrelation, this results in negative autocorrelation in t he co-\\neﬃcients. In addition the sample size eﬀectively provides only four obser-\\nvations per coeﬃcient.\\nApplications such as this permit a natural regularization. We force the\\ncoeﬃcients to vary smoothly as a function of frequency. The red curve in the\\nlower panel of Figure 5.5 shows such a smooth coeﬃcient curve ﬁt to these\\ndata. We see that the lower frequencies oﬀer the most discriminatory power.\\nNot only does the smoothing allow easier interpretation of the contrast, it\\nalso produces a more accurate classiﬁer:\\nRaw Regularized\\nTraining error 0.080 0.185\\nTest error 0.255 0.158\\nThe smooth red curve was obtained through a very simple use of natural\\ncubic splines. We can represent the coeﬃcient function as an expansion of\\nsplines β(f) =∑M\\nm=1hm(f)θm. In practice this means that β=Hθwhere,\\nHis ap×Mbasis matrix of natural cubic splines, deﬁned on the set of\\nfrequencies. Here we used M= 12 basis functions, with knots uniformly\\nplaced over the integers 1 ,2,... ,256 representing the frequencies. Since\\nxTβ=xTHθ, we can simply replace the input features xby their ﬁltered\\nversions x∗=HTx, and ﬁt θby linear logistic regression on the x∗. The\\nred curve is thus ˆβ(f) =h(f)Tˆθ.\\n5.3 Filtering and Feature Extraction\\nIn the previous example, we constructed a p×Mbasis matrix H, and then\\ntransformed our features xinto new features x∗=HTx. These ﬁltered\\nversions of the features were then used as inputs into a learning procedure:\\nin the previous example, this was linear logistic regression.\\nPreprocessing of high-dimensional features is a very general and pow-\\nerful method for improving the performance of a learning algorithm. The\\npreprocessing need not be linear as it was above, but can be a general'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 169}, page_content='5.4 Smoothing Splines 151\\n(nonlinear) function of the form x∗=g(x). The derived features x∗can\\nthen be used as inputs into any (linear or nonlinear) learning procedure.\\nFor example, for signal or image recognition a popular approach is to ﬁrst\\ntransform the raw features via a wavelet transform x∗=HTx(Section 5.9)\\nand then use the features x∗as inputs into a neural network (Chapter 11).\\nWavelets are eﬀective in capturing discrete jumps or edges, and the neural\\nnetwork is a powerful tool for constructing nonlinear functions of these\\nfeatures for predicting the target variable. By using domain knowledge\\nto construct appropriate features, one can often improve upon a learning\\nmethod that has only the raw features xat its disposal.\\n5.4 Smoothing Splines\\nHere we discuss a spline basis method that avoids the knot selection prob-\\nlem completely by using a maximal set of knots. The complexity of the ﬁt\\nis controlled by regularization. Consider the following problem: among all\\nfunctions f(x) with two continuous derivatives, ﬁnd one that minimizes the\\npenalized residual sum of squares\\nRSS(f,λ) =N∑\\ni=1{yi−f(xi)}2+λ∫\\n{f′′(t)}2dt, (5.9)\\nwhere λis a ﬁxed smoothing parameter . The ﬁrst term measures closeness\\nto the data, while the second term penalizes curvature in the function, and\\nλestablishes a tradeoﬀ between the two. Two special cases are:\\nλ= 0 : fcan be any function that interpolates the data.\\nλ=∞:the simple least squares line ﬁt, since no second derivative can\\nbe tolerated.\\nThese vary from very rough to very smooth, and the hope is that λ∈(0,∞)\\nindexes an interesting class of functions in between.\\nThe criterion (5.9) is deﬁned on an inﬁnite-dimensional function space—\\nin fact, a Sobolev space of functions for which the second term is deﬁned.\\nRemarkably, it can be shown that (5.9) has an explicit, ﬁnite-dimensional,\\nunique minimizer which is a natural cubic spline with knots at the unique\\nvalues of the xi, i= 1,... ,N (Exercise 5.7). At face value it seems that\\nthe family is still over-parametrized, since there are as many as Nknots,\\nwhich implies Ndegrees of freedom. However, the penalty term translates\\nto a penalty on the spline coeﬃcients, which are shrunk some of the way\\ntoward the linear ﬁt.\\nSince the solution is a natural spline, we can write it as\\nf(x) =N∑\\nj=1Nj(x)θj, (5.10)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 170}, page_content='152 5. Basis Expansions and Regularization\\nAgeRelative Change in Spinal BMD\\n10 15 20 25-0.05 0.0 0.05 0.10 0.15 0.20••\\n••\\n••\\n••\\n•\\n••\\n••\\n••\\n••\\n•\\n•\\n•\\n••••\\n••\\n•\\n••\\n•\\n•\\n••\\n•••\\n••\\n•••\\n•••\\n•\\n•\\n•\\n••\\n•\\n••••\\n•••••\\n••\\n•••••\\n•\\n•\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n••\\n••••\\n•\\n•••\\n••\\n•••••\\n•\\n••\\n••\\n•\\n•\\n••••\\n• ••• ••••\\n••\\n••\\n•••\\n•\\n••\\n•••••\\n•\\n•••••\\n••\\n•••\\n•\\n• •••\\n•••\\n••••\\n••\\n••••\\n• •\\n••••\\n••• •\\n••••\\n•\\n•••\\n•\\n••••••\\n••\\n•\\n••\\n•\\n•\\n••\\n••\\n•\\n••\\n•\\n• •••\\n•\\n•\\n••••••\\n•\\n••\\n••\\n••••\\n•\\n••\\n••\\n•\\n••••\\n••\\n•\\n•••\\n••\\n•\\n•••\\n•\\n• •••\\n• ••\\n•••••\\n•\\n••\\n•••\\n•\\n••••\\n••\\n•\\n••\\n•••\\n•\\n•••\\n•\\n••\\n•••••\\n••\\n•\\n••••••\\n•\\n•\\n•••\\n• ••\\n•••••••\\n•\\n•••\\n••\\n•\\n•••\\n•\\n•••\\n••\\n••\\n••\\n••\\n•• ••\\n•\\n••••\\n•\\n•\\n•••\\n••\\n• •\\n•••\\n•\\n•••\\n• •\\n••\\n•\\n•••\\n••••\\n•\\n••••\\n••\\n•\\n••\\n••••\\n••••\\n•\\n••\\n•\\n•\\n••••\\n•\\n••\\n•\\n••••\\n• ••\\n•\\n••••\\n•\\n••\\n•••\\n•\\n••\\n•••\\n•\\n•\\n••\\n••\\n•\\n•\\n••\\n••\\n••\\n•••\\n•••\\n•\\n••\\n•\\n•••\\n•\\n•\\n•••\\n••\\n••\\n••\\n•\\n•••\\n•••\\n• ••••\\n••\\n••\\n••\\n•\\n•Male\\nFemale\\nFIGURE 5.6. The response is the relative change in bone mineral density mea-\\nsured at the spine in adolescents, as a function of age. A separat e smoothing spline\\nwas ﬁt to the males and females, with λ≈0.00022. This choice corresponds to\\nabout 12degrees of freedom.\\nwhere the Nj(x) are an N-dimensional set of basis functions for repre-\\nsenting this family of natural splines (Section 5.2.1 and Exercise 5.4). The\\ncriterion thus reduces to\\nRSS(θ,λ) = (y−Nθ)T(y−Nθ) +λθTΩNθ, (5.11)\\nwhere {N}ij=Nj(xi) and {ΩN}jk=∫\\nN′′\\nj(t)N′′\\nk(t)dt. The solution is\\neasily seen to be\\nˆθ= (NTN+λΩN)−1NTy, (5.12)\\na generalized ridge regression. The ﬁtted smoothing spline is given by\\nˆf(x) =N∑\\nj=1Nj(x)ˆθj. (5.13)\\nEﬃcient computational techniques for smoothing splines are discussed in\\nthe Appendix to this chapter.\\nFigure 5.6 shows a smoothing spline ﬁt to some data on bone mineral\\ndensity (BMD) in adolescents. The response is relative change in spinal\\nBMD over two consecutive visits, typically about one year apart. The data\\nare color coded by gender, and two separate curves were ﬁt. This simple'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 171}, page_content='5.4 Smoothing Splines 153\\nsummary reinforces the evidence in the data that the growth spurt for\\nfemales precedes that for males by about two years. In both cases the\\nsmoothing parameter λwas approximately 0 .00022; this choice is discussed\\nin the next section.\\n5.4.1 Degrees of Freedom and Smoother Matrices\\nWe have not yet indicated how λis chosen for the smoothing spline. Later\\nin this chapter we describe automatic methods using techniques such as\\ncross-validation. In this section we discuss intuitive ways of prespecifying\\nthe amount of smoothing.\\nA smoothing spline with prechosen λis an example of a linear smoother\\n(as in linear operator). This is because the estimated parameters in (5.12)\\nare a linear combination of the yi. Denote by ˆftheN-vector of ﬁtted values\\nˆf(xi) at the training predictors xi. Then\\nˆf=N(NTN+λΩN)−1NTy\\n=Sλy. (5.14)\\nAgain the ﬁt is linear in y, and the ﬁnite linear operator Sλis known as\\nthesmoother matrix . One consequence of this linearity is that the recipe\\nfor producing ˆffromydoes not depend on yitself;Sλdepends only on\\nthexiandλ.\\nLinear operators are familiar in more traditional least squares ﬁtting as\\nwell. Suppose Bξis aN×Mmatrix of Mcubic-spline basis functions\\nevaluated at the Ntraining points xi, with knot sequence ξ, and M≪N.\\nThen the vector of ﬁtted spline values is given by\\nˆf=Bξ(BT\\nξBξ)−1BT\\nξy\\n=Hξy. (5.15)\\nHere the linear operator Hξis a projection operator, also known as the hat\\nmatrix in statistics. There are some important similarities and di ﬀerences\\nbetween HξandSλ:\\n•Both are symmetric, positive semideﬁnite matrices.\\n•HξHξ=Hξ(idempotent), while SλSλ⪯Sλ, meaning that the right-\\nhand side exceeds the left-hand side by a positive semideﬁnite matrix.\\nThis is a consequence of the shrinking nature of Sλ, which we discuss\\nfurther below.\\n•Hξhas rank M, while Sλhas rank N.\\nThe expression M= trace( Hξ) gives the dimension of the projection space,\\nwhich is also the number of basis functions, and hence the number of pa-\\nrameters involved in the ﬁt. By analogy we deﬁne the eﬀective degrees of'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 172}, page_content='154 5. Basis Expansions and Regularization\\nfreedom of a smoothing spline to be\\ndfλ= trace( Sλ), (5.16)\\nthe sum of the diagonal elements of Sλ. This very useful deﬁnition allows\\nus a more intuitive way to parameterize the smoothing spline, and indeed\\nmany other smoothers as well, in a consistent fashion. For example, in Fig-\\nure 5.6 we speciﬁed df λ= 12 for each of the curves, and the corresponding\\nλ≈0.00022 was derived numerically by solving trace( Sλ) = 12. There are\\nmany arguments supporting this deﬁnition of degrees of freedom, and we\\ncover some of them here.\\nSinceSλis symmetric (and positive semideﬁnite), it has a real eigen-\\ndecomposition. Before we proceed, it is convenient to rewrite Sλin the\\nReinsch form\\nSλ= (I+λK)−1, (5.17)\\nwhere Kdoes not depend on λ(Exercise 5.9). Since ˆf=Sλysolves\\nmin\\nf(y−f)T(y−f) +λfTKf, (5.18)\\nKis known as the penalty matrix , and indeed a quadratic form in Khas\\na representation in terms of a weighted sum of squared (divided) second\\ndiﬀerences. The eigen-decomposition of Sλis\\nSλ=N∑\\nk=1ρk(λ)ukuT\\nk (5.19)\\nwith\\nρk(λ) =1\\n1 +λdk, (5.20)\\nanddkthe corresponding eigenvalue of K. Figure 5.7 (top) shows the re-\\nsults of applying a cubic smoothing spline to some air pollution data (128\\nobservations). Two ﬁts are given: a smoother ﬁt corresponding to a larger\\npenalty λand a rougher ﬁt for a smaller penalty. The lower panels repre-\\nsent the eigenvalues (lower left) and some eigenvectors (lower right) of the\\ncorresponding smoother matrices. Some of the highlights of the eigenrep-\\nresentation are the following:\\n•The eigenvectors are not aﬀected by changes in λ, and hence the whole\\nfamily of smoothing splines (for a particular sequence x) indexed by\\nλhave the same eigenvectors.\\n•Sλy=∑N\\nk=1ukρk(λ)⟨uk,y⟩, and hence the smoothing spline oper-\\nates by decomposing yw.r.t. the (complete) basis {uk}, and diﬀer-\\nentially shrinking the contributions using ρk(λ). This is to be con-\\ntrasted with a basis-regression method, where the components are'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 173}, page_content='5.4 Smoothing Splines 155\\nDaggot Pressure GradientOzone Concentration\\n-50 0 50 1000 10 20 30•••\\n••••••\\n••\\n••••••\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n•\\n••\\n•\\n••\\n•••\\n•\\n••\\n••\\n••\\n••\\n••••\\n•••••\\n•\\n•••\\n•••\\n••\\n•\\n••\\n•••\\n••\\n••\\n•••\\n••••\\n••\\n••\\n•••\\n•\\n••••\\n••\\n••\\n••\\n•••\\n•••\\n•••\\n••\\n•\\n••\\n••••\\n••\\n•••\\n•\\n•\\n•\\n•\\nOrderEigenvalues\\n5 10 15 20 25-0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2•••\\n•\\n•\\n•\\n••••••••••• •• • • •• • ••••••••\\n•\\n•\\n•\\n•\\n•\\n•••••••••••••df=5\\ndf=11\\n-50 0 50 100 -50 0 50 100\\nFIGURE 5.7. (Top:) Smoothing spline ﬁt of ozone concentration versus Daggot\\npressure gradient. The two ﬁts correspond to diﬀerent values of t he smoothing\\nparameter, chosen to achieve ﬁve and eleven eﬀective degrees o f freedom, deﬁned\\nby dfλ=trace(Sλ). (Lower left:) First 25eigenvalues for the two smoothing-spline\\nmatrices. The ﬁrst two are exactly 1, and all are ≥0. (Lower right:) Third to\\nsixth eigenvectors of the spline smoother matrices. In each cas e,ukis plotted\\nagainst x, and as such is viewed as a function of x. The rugat the base of the\\nplots indicate the occurrence of data points. The damped functio ns represent the\\nsmoothed versions of these functions (using the 5df smoother).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 174}, page_content='156 5. Basis Expansions and Regularization\\neither left alone, or shrunk to zero—that is, a projection matrix such\\nasHξabove has Meigenvalues equal to 1, and the rest are 0. For\\nthis reason smoothing splines are referred to as shrinking smoothers,\\nwhile regression splines are projection smoothers (see Figure 3.17 on\\npage 80).\\n•The sequence of uk, ordered by decreasing ρk(λ), appear to increase\\nin complexity. Indeed, they have the zero-crossing behavior of polyno-\\nmials of increasing degree. Since Sλuk=ρk(λ)uk, we see how each of\\nthe eigenvectors themselves are shrunk by the smoothing spline: the\\nhigher the complexity, the more they are shrunk. If the domain of X\\nis periodic, then the ukare sines and cosines at diﬀerent frequencies.\\n•The ﬁrst two eigenvalues are always one, and they correspond to the\\ntwo-dimensional eigenspace of functions linear in x(Exercise 5.11),\\nwhich are never shrunk.\\n•The eigenvalues ρk(λ) = 1/(1 +λdk) are an inverse function of the\\neigenvalues dkof the penalty matrix K, moderated by λ;λcontrols\\nthe rate at which the ρk(λ) decrease to zero. d1=d2= 0 and again\\nlinear functions are not penalized.\\n•One can reparametrize the smoothing spline using the basis vectors\\nuk(theDemmler–Reinsch basis). In this case the smoothing spline\\nsolves\\nmin\\nθ∥y−Uθ∥2+λθTDθ, (5.21)\\nwhere Uhas columns ukandDis a diagonal matrix with elements\\ndk.\\n•dfλ= trace( Sλ) =∑N\\nk=1ρk(λ). For projection smoothers, all the\\neigenvalues are 1, each one corresponding to a dimension of the pro-\\njection subspace.\\nFigure 5.8 depicts a smoothing spline matrix, with the rows ordered with\\nx. The banded nature of this representation suggests that a smoothing\\nspline is a local ﬁtting method, much like the locally weighted regression\\nprocedures in Chapter 6. The right panel shows in detail selected rows of\\nS, which we call the equivalent kernels . Asλ→0, df λ→N, andSλ→I,\\ntheN-dimensional identity matrix. As λ→ ∞, dfλ→2, and Sλ→H, the\\nhat matrix for linear regression on x.\\n5.5 Automatic Selection of the Smoothing\\nParameters\\nThe smoothing parameters for regression splines encompass the degree of\\nthe splines, and the number and placement of the knots. For smoothing'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 175}, page_content='5.5 Automatic Selection of the Smoothing Parameters 157\\n11510075502512Smoother Matrix\\n••••• • •••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•Row 115••••• ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•Row 100••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Row 75•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 50•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 25•••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 12Equivalent Kernels\\nFIGURE 5.8. The smoother matrix for a smoothing spline is nearly banded,\\nindicating an equivalent kernel with local support. The left pane l represents the\\nelements of Sas an image. The right panel shows the equivalent kernel or weigh t-\\ning function in detail for the indicated rows.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 176}, page_content='158 5. Basis Expansions and Regularization\\nsplines, we have only the penalty parameter λto select, since the knots are\\nat all the unique training X’s, and cubic degree is almost always used in\\npractice.\\nSelecting the placement and number of knots for regression splines can be\\na combinatorially complex task, unless some simpliﬁcations are enforced.\\nThe MARS procedure in Chapter 9 uses a greedy algorithm with some\\nadditional approximations to achieve a practical compromise. We will not\\ndiscuss this further here.\\n5.5.1 Fixing the Degrees of Freedom\\nSince df λ= trace( Sλ) is monotone in λfor smoothing splines, we can in-\\nvert the relationship and specify λby ﬁxing df. In practice this can be\\nachieved by simple numerical methods. So, for example, in Rone can use\\nsmooth.spline(x,y,df=6) to specify the amount of smoothing. This encour-\\nages a more traditional mode of model selection, where we might try a cou-\\nple of diﬀerent values of df, and select one based on approximate F-tests,\\nresidual plots and other more subjective criteria. Using df in this way pro-\\nvides a uniform approach to compare many diﬀerent smoothing methods.\\nIt is particularly useful in generalized additive models (Chapter 9), where\\nseveral smoothing methods can be simultaneously used in one model.\\n5.5.2 The Bias–Variance Tradeoﬀ\\nFigure 5.9 shows the eﬀect of the choice of df λwhen using a smoothing\\nspline on a simple example:\\nY=f(X) +ε,\\nf(X) =sin(12( X+ 0.2))\\nX+ 0.2,(5.22)\\nwithX∼U[0,1] and ε∼N(0,1). Our training sample consists of N= 100\\npairsxi,yidrawn independently from this model.\\nThe ﬁtted splines for three diﬀerent values of df λare shown. The yellow\\nshaded region in the ﬁgure represents the pointwise standard error of ˆfλ,\\nthat is, we have shaded the region between ˆfλ(x)±2≤se(ˆfλ(x)). Since\\nˆf=Sλy,\\nCov(ˆf) = SλCov(y)ST\\nλ\\n=SλST\\nλ. (5.23)\\nThe diagonal contains the pointwise variances at the training xi. The bias\\nis given by\\nBias(ˆf) = f−E(ˆf)\\n=f−Sλf, (5.24)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 177}, page_content='5.5 Automatic Selection of the Smoothing Parameters 159\\n6 8 10 12 140.9 1.0 1.1 1.2•••••••• ••••••••••\\n••••••••••••••••••\\ny\\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\\nO\\nOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOO\\nOOOOOOOO\\nOOO\\nOOOO\\nO\\nO\\nO\\nOOOO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nOO\\nOO\\nOOO\\nOO\\nOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOy\\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\\nO\\nOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOO\\nOOOOOOOO\\nOOO\\nOOOO\\nO\\nO\\nO\\nOOOO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nOO\\nOO\\nOOO\\nOO\\nOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOO\\nO\\nO\\ny\\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\\nO\\nOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOO\\nOOOOOOOO\\nOOO\\nOOOO\\nO\\nO\\nO\\nOOOO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nOO\\nOO\\nOOO\\nOO\\nOOOO\\nO\\nOOO\\nOOO\\nOO\\nOO\\nOO\\nO\\nOEPECV\\nX XXdfλ= 5\\ndfλ= 9 dfλ= 15dfλCross-ValidationEPE( λ) and CV( λ)\\nFIGURE 5.9. The top left panel shows the EPE( λ)andCV(λ)curves for a\\nrealization from a nonlinear additive error model (5.22). The r emaining panels\\nshow the data, the true functions (in purple), and the ﬁtted curve s (in green) with\\nyellow shaded ±2×standard error bands, for three diﬀerent values of dfλ.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 178}, page_content='160 5. Basis Expansions and Regularization\\nwherefis the (unknown) vector of evaluations of the true fat the training\\nX’s. The expectations and variances are with respect to repeated draws\\nof samples of size N= 100 from the model (5.22). In a similar fashion\\nVar(ˆfλ(x0)) and Bias( ˆfλ(x0)) can be computed at any point x0(Exer-\\ncise 5.10). The three ﬁts displayed in the ﬁgure give a visual demonstration\\nof the bias-variance tradeoﬀ associated with selecting the smoothing\\nparameter.\\ndfλ= 5:The spline under ﬁts, and clearly trims down the hills and ﬁlls in\\nthe valleys . This leads to a bias that is most dramatic in regions of\\nhigh curvature. The standard error band is very narrow, so we esti-\\nmate a badly biased version of the true function with great reliability!\\ndfλ= 9:Here the ﬁtted function is close to the true function, although a\\nslight amount of bias seems evident. The variance has not increased\\nappreciably.\\ndfλ= 15: The ﬁtted function is somewhat wiggly, but close to the true\\nfunction. The wiggliness also accounts for the increased width of the\\nstandard error bands—the curve is starting to follow some individual\\npoints too closely.\\nNote that in these ﬁgures we are seeing a single realization of data and\\nhence ﬁtted spline ˆfin each case, while the bias involves an expectation\\nE(ˆf). We leave it as an exercise (5.10) to compute similar ﬁgures where the\\nbias is shown as well. The middle curve seems “just right,” in that it has\\nachieved a good compromise between bias and variance.\\nThe integrated squared prediction error (EPE) combines both bias and\\nvariance in a single summary:\\nEPE( ˆfλ) = E( Y−ˆfλ(X))2\\n= Var( Y) + E[\\nBias2(ˆfλ(X)) + Var( ˆfλ(X))]\\n=σ2+ MSE( ˆfλ). (5.25)\\nNote that this is averaged both over the training sample (giving rise to ˆfλ),\\nand the values of the (independently chosen) prediction points ( X,Y). EPE\\nis a natural quantity of interest, and does create a tradeoﬀ between bias\\nand variance. The blue points in the top left panel of Figure 5.9 suggest\\nthat df λ= 9 is spot on!\\nSince we don’t know the true function, we do not have access to EPE, and\\nneed an estimate. This topic is discussed in some detail in Chapter 7, and\\ntechniques such as K-fold cross-validation, GCV and Cpare all in common\\nuse. In Figure 5.9 we include the N-fold (leave-one-out) cross-validation\\ncurve:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 179}, page_content='5.6 Nonparametric Logistic Regression 161\\nCV(ˆfλ) =1\\nNN∑\\ni=1(yi−ˆf(−i)\\nλ(xi))2(5.26)\\n=1\\nNN∑\\ni=1(\\nyi−ˆfλ(xi)\\n1−Sλ(i,i))2\\n, (5.27)\\nwhich can (remarkably) be computed for each value of λfrom the original\\nﬁtted values and the diagonal elements Sλ(i,i) ofSλ(Exercise 5.13).\\nThe EPE and CV curves have a similar shape, but the entire CV curve\\nis above the EPE curve. For some realizations this is reversed, and overall\\nthe CV curve is approximately unbiased as an estimate of the EPE curve.\\n5.6 Nonparametric Logistic Regression\\nThe smoothing spline problem (5.9) in Section 5.4 is posed in a regression\\nsetting. It is typically straightforward to transfer this technology t o other\\ndomains. Here we consider logistic regression with a single quantitativ e\\ninput X. The model is\\nlogPr(Y= 1|X=x)\\nPr(Y= 0|X=x)=f(x), (5.28)\\nwhich implies\\nPr(Y= 1|X=x) =ef(x)\\n1 +ef(x). (5.29)\\nFitting f(x) in a smooth fashion leads to a smooth estimate of the condi-\\ntional probability Pr( Y= 1|x), which can be used for classiﬁcation or risk\\nscoring.\\nWe construct the penalized log-likelihood criterion\\nℓ(f;λ) =N∑\\ni=1[yilogp(xi) + (1 −yi)log(1 −p(xi))]−1\\n2λ∫\\n{f′′(t)}2dt\\n=N∑\\ni=1[\\nyif(xi)−log(1 + ef(xi))]\\n−1\\n2λ∫\\n{f′′(t)}2dt, (5.30)\\nwhere we have abbreviated p(x) = Pr( Y= 1|x). The ﬁrst term in this ex-\\npression is the log-likelihood based on the binomial distribution (c.f. Chap-\\nter 4, page 120). Arguments similar to those used in Section 5.4 show that\\nthe optimal fis a ﬁnite-dimensional natural spline with knots at the unique'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 180}, page_content='162 5. Basis Expansions and Regularization\\nvalues of x. This means that we can represent f(x) =∑N\\nj=1Nj(x)θj. We\\ncompute the ﬁrst and second derivatives\\n∂ℓ(θ)\\n∂θ=NT(y−p)−λΩθ, (5.31)\\n∂2ℓ(θ)\\n∂θ∂θT=−NTWN−λΩ, (5.32)\\nwhere pis the N-vector with elements p(xi), and Wis a diagonal matrix\\nof weights p(xi)(1−p(xi)). The ﬁrst derivative (5.31) is nonlinear in θ, so\\nwe need to use an iterative algorithm as in Section 4.4.1. Using Newton–\\nRaphson as in (4.23) and (4.26) for linear logistic regression, the updat e\\nequation can be written\\nθnew= (NTWN+λΩ)−1NTW(\\nNθold+W−1(y−p))\\n= (NTWN+λΩ)−1NTWz. (5.33)\\nWe can also express this update in terms of the ﬁtted values\\nfnew=N(NTWN+λΩ)−1NTW(\\nfold+W−1(y−p))\\n=Sλ,wz. (5.34)\\nReferring back to (5.12) and (5.14), we see that the update ﬁts a weighted\\nsmoothing spline to the working response z(Exercise 5.12).\\nThe form of (5.34) is suggestive. It is tempting to replace Sλ,wby any\\nnonparametric (weighted) regression operator, and obtain general fami-\\nlies of nonparametric logistic regression models. Although here xis one-\\ndimensional, this procedure generalizes naturally to higher-dimensional x.\\nThese extensions are at the heart of generalized additive models , which we\\npursue in Chapter 9.\\n5.7 Multidimensional Splines\\nSo far we have focused on one-dimensional spline models. Each of the ap-\\nproaches have multidimensional analogs. Suppose X∈IR2, and we have\\na basis of functions h1k(X1), k= 1,... ,M 1for representing functions of\\ncoordinate X1, and likewise a set of M2functions h2k(X2) for coordinate\\nX2. Then the M1×M2dimensional tensor product basis deﬁned by\\ngjk(X) =h1j(X1)h2k(X2), j= 1,... ,M 1, k= 1,... ,M 2 (5.35)\\ncan be used for representing a two-dimensional function:\\ng(X) =M1∑\\nj=1M2∑\\nk=1θjkgjk(X). (5.36)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 181}, page_content='5.7 Multidimensional Splines 163\\nFIGURE 5.10. A tensor product basis of B-splines, showing some selected pair s.\\nEach two-dimensional function is the tensor product of the corre sponding one\\ndimensional marginals.\\nFigure 5.10 illustrates a tensor product basis using B-splines. The coeﬃ-\\ncients can be ﬁt by least squares, as before. This can be generalized to d\\ndimensions, but note that the dimension of the basis grows exponentially\\nfast—yet another manifestation of the curse of dimensionality. The MARS\\nprocedure discussed in Chapter 9 is a greedy forward algorithm for includ-\\ning only those tensor products that are deemed necessary by least squares.\\nFigure 5.11 illustrates the diﬀerence between additive and tensor product\\n(natural) splines on the simulated classiﬁcation example from Chapter 2.\\nA logistic regression model logit[Pr( T|x)] =h(x)Tθis ﬁt to the binary re-\\nsponse, and the estimated decision boundary is the contour h(x)Tˆθ= 0.\\nThe tensor product basis can achieve more ﬂexibility at the decision bound-\\nary, but introduces some spurious structure along the way.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='164 5. Basis Expansions and Regularization\\nAdditive Natural Cubic Splines - 4 df each'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='.. .. . .. . . .. . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . . .. . . .. . . .. . . .. . .. . .. . .. .. .. ...'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.23\\nTest Error:       0.28\\nBayes Error:    0.21\\nNatural Cubic Splines - Tensor Product - 4 df each'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . ... . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='. . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 182}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.230\\nTest Error:       0.282\\nBayes Error:    0.210\\nFIGURE 5.11. The simulation example of Figure 2.1. The upper panel shows the\\ndecision boundary of an additive logistic regression model, using natural splines\\nin each of the two coordinates (total df = 1 + (4 −1) + (4 −1) = 7 ). The lower\\npanel shows the results of using a tensor product of natural spline bases in each\\ncoordinate (total df = 4×4 = 16) . The broken purple boundary is the Bayes\\ndecision boundary for this problem.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 183}, page_content='5.7 Multidimensional Splines 165\\nOne-dimensional smoothing splines (via regularization) generalize to high-\\ner dimensions as well. Suppose we have pairs yi,xiwithxi∈IRd, and we\\nseek a d-dimensional regression function f(x). The idea is to set up the\\nproblem\\nmin\\nfN∑\\ni=1{yi−f(xi)}2+λJ[f], (5.37)\\nwhere Jis an appropriate penalty functional for stabilizing a function fin\\nIRd. For example, a natural generalization of the one-dimensional roughness\\npenalty (5.9) for functions on IR2is\\nJ[f] =∫ ∫\\nI R2[(∂2f(x)\\n∂x2\\n1)2\\n+2(∂2f(x)\\n∂x1∂x2)2\\n+(∂2f(x)\\n∂x2\\n2)2]\\ndx1dx2.(5.38)\\nOptimizing (5.37) with this penalty leads to a smooth two-dimensional\\nsurface, known as a thin-plate spline. It shares many properties with the\\none-dimensional cubic smoothing spline:\\n•asλ→0, the solution approaches an interpolating function [the one\\nwith smallest penalty (5.38)];\\n•asλ→ ∞, the solution approaches the least squares plane;\\n•for intermediate values of λ, the solution can be represented as a\\nlinear expansion of basis functions, whose coeﬃcients are obtained\\nby a form of generalized ridge regression.\\nThe solution has the form\\nf(x) =β0+βTx+N∑\\nj=1αjhj(x), (5.39)\\nwhere hj(x) =||x−xj||2log||x−xj||. These hjare examples of radial\\nbasis functions , which are discussed in more detail in the next section. The\\ncoeﬃcients are found by plugging (5.39) into (5.37), which reduces to a\\nﬁnite-dimensional penalized least squares problem. For the penalty to be\\nﬁnite, the coeﬃcients αjhave to satisfy a set of linear constraints; see\\nExercise 5.14.\\nThin-plate splines are deﬁned more generally for arbitrary dimension d,\\nfor which an appropriately more general Jis used.\\nThere are a number of hybrid approaches that are popular in practice,\\nboth for computational and conceptual simplicity. Unlike one-dimensional\\nsmoothing splines, the computational complexity for thin-plate splines is\\nO(N3), since there is not in general any sparse structure that can be ex-\\nploited. However, as with univariate smoothing splines, we can get away\\nwith substantially less than the Nknots prescribed by the solution (5.39).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 184}, page_content='166 5. Basis Expansions and Regularization\\n125\\n130135140145150155\\n15202530354045\\n20 30 40 50 60\\nAgeObesitySystolic Blood Pressure\\n120125130135140145150155160\\n•• ••\\n••\\n••••\\n•\\n•\\n•• ••••\\n••\\n••\\n••\\n•\\n••\\n••\\n•\\n•••••\\n•••\\n•••\\n•\\n••\\n•••\\n••\\n•\\n••\\n•••\\n••\\n••••••\\n•••\\n••\\n•\\n•\\n••\\n••\\n••\\n•\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n•\\n••\\n••••\\n••••\\n•\\n•\\n•\\n••\\n•\\n•••\\n••\\n••\\n•\\n•••\\n•••\\n•••\\n••\\n••\\n••\\n••\\n•\\n•\\n•••••\\n•••\\n•\\n• •••\\n••••\\n•••\\n••\\n••••\\n•\\n••••\\n• ••\\n•\\n•••\\n•••\\n•••••\\n•\\n••\\n••\\n•••\\n••\\n••\\n••••\\n•\\n••••\\n•••\\n••\\n••\\n••\\n••\\n••\\n•\\n•••\\n••\\n•\\n•\\n••••\\n••\\n•••\\n••••\\n•\\n•••\\n•••\\n•\\n•••••\\n•\\n•\\n•••\\n••\\n•••\\n••\\n•\\n••\\n•\\n•••\\n••••\\n• •\\n•••\\n•\\n•••\\n••\\n••\\n••\\n•\\n••\\n•\\n••••\\n•\\n••••\\n•••\\n•\\n•••\\n•••\\n••\\n••\\n•\\n••••\\n••\\n• ••\\n•••\\n•••\\n•••\\n••• •••\\n•\\n•\\n•\\n••\\n••\\n•\\n••\\n••\\n••\\n•••••\\n•••\\n•\\n••\\n•••\\n•\\n••\\n•\\n••\\n••\\n•\\n••• •\\n••\\n•\\n•••\\n••\\n••\\n••\\n•••\\n••\\n•••\\n••\\n•••\\n•••\\n•\\n•••\\n•\\n••\\n•••\\n••\\n•••\\n••••\\n•\\n••\\n••••\\n•••\\n•\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n•\\n••\\n••\\n••\\n•• • • • • •• • • • • • •• • • • • • •• • • • • •• • • •• •\\n• • • • • • • •• •••• •• • • •• • • • • •• • • • • • • •\\nFIGURE 5.12. A thin-plate spline ﬁt to the heart disease data, displayed as a\\ncontour plot. The response is systolic blood pressure , modeled as a function\\nofageandobesity . The data points are indicated, as well as the lattice of points\\nused as knots. Care should be taken to use knots from the lattice inside the convex\\nhull of the data (red), and ignore those outside (green).\\nIn practice, it is usually suﬃcient to work with a lattice of knots covering\\nthe domain. The penalty is computed for the reduced expansion just as\\nbefore. Using Kknots reduces the computations to O(NK2+K3). Fig-\\nure 5.12 shows the result of ﬁtting a thin-plate spline to some heart disease\\nrisk factors, representing the surface as a contour plot. Indicated are the\\nlocation of the input features, as well as the knots used in the ﬁt. Note that\\nλwas speciﬁed via df λ= trace( Sλ) = 15.\\nMore generally one can represent f∈IRdas an expansion in any arbi-\\ntrarily large collection of basis functions, and control the complexity by a p-\\nplying a regularizer such as (5.38). For example, we could construct a basis\\nby forming the tensor products of all pairs of univariate smoothing-spline\\nbasis functions as in (5.35), using, for example, the univariate B-splines\\nrecommended in Section 5.9.2 as ingredients. This leads to an exponential'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 185}, page_content='5.8 Regularization and Reproducing Kernel Hilbert Spaces 167\\ngrowth in basis functions as the dimension increases, and typically we have\\nto reduce the number of functions per coordinate accordingly.\\nThe additive spline models discussed in Chapter 9 are a restricted class\\nof multidimensional splines. They can be represented in this general formu-\\nlation as well; that is, there exists a penalty J[f] that guarantees that the\\nsolution has the form f(X) =α+f1(X1) +≤≤≤+fd(Xd) and that each of\\nthe functions fjare univariate splines. In this case the penalty is somewhat\\ndegenerate, and it is more natural to assume thatfis additive, and then\\nsimply impose an additional penalty on each of the component functions:\\nJ[f] = J(f1+f2+≤≤≤+fd)\\n=d∑\\nj=1∫\\nf′′\\nj(tj)2dtj. (5.40)\\nThese are naturally extended to ANOVA spline decompositions,\\nf(X) =α+∑\\njfj(Xj) +∑\\nj<kfjk(Xj,Xk) +≤≤≤, (5.41)\\nwhere each of the components are splines of the required dimension. There\\nare many choices to be made:\\n•The maximum order of interaction—we have shown up to order 2\\nabove.\\n•Which terms to include—not all main eﬀects and interactions are\\nnecessarily needed.\\n•What representation to use—some choices are:\\n–regression splines with a relatively small number of basis func-\\ntions per coordinate, and their tensor products for interactions;\\n–a complete basis as in smoothing splines, and include appropri-\\nate regularizers for each term in the expansion.\\nIn many cases when the number of potential dimensions (features) is large,\\nautomatic methods are more desirable. The MARS and MART procedures\\n(Chapters 9 and 10, respectively), both fall into this category.\\n5.8 Regularization and Reproducing Kernel\\nHilbert Spaces\\nIn this section we cast splines into the larger context of regularization meth-\\nods and reproducing kernel Hilbert spaces. This section is quite technical\\nand can be skipped by the disinterested or intimidated reader.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 186}, page_content='168 5. Basis Expansions and Regularization\\nA general class of regularization problems has the form\\nmin\\nf∈H[N∑\\ni=1L(yi,f(xi)) +λJ(f)]\\n(5.42)\\nwhere L(y,f(x)) is a loss function, J(f) is a penalty functional, and His\\na space of functions on which J(f) is deﬁned. Girosi et al. (1995) describe\\nquite general penalty functionals of the form\\nJ(f) =∫\\nI Rd|˜f(s)|2\\n˜G(s)ds, (5.43)\\nwhere ˜fdenotes the Fourier transform of f, and ˜Gis some positive function\\nthat falls oﬀ to zero as ||s|| → ∞ . The idea is that 1 /˜Gincreases the penalty\\nfor high-frequency components of f. Under some additional assumptions\\nthey show that the solutions have the form\\nf(X) =K∑\\nk=1αkφk(X) +N∑\\ni=1θiG(X−xi), (5.44)\\nwhere the φkspan the null space of the penalty functional J, and Gis the\\ninverse Fourier transform of ˜G. Smoothing splines and thin-plate splines\\nfall into this framework. The remarkable feature of this solution is tha t\\nwhile the criterion (5.42) is deﬁned over an inﬁnite-dimensional space, the\\nsolution is ﬁnite-dimensional. In the next sections we look at some speciﬁc\\nexamples.\\n5.8.1 Spaces of Functions Generated by Kernels\\nAn important subclass of problems of the form (5.42) are generated by\\na positive deﬁnite kernel K(x,y), and the corresponding space of func-\\ntionsHKis called a reproducing kernel Hilbert space (RKHS). The penalty\\nfunctional Jis deﬁned in terms of the kernel as well. We give a brief and\\nsimpliﬁed introduction to this class of models, adapted from Wahba (1990)\\nand Girosi et al. (1995), and nicely summarized in Evgeniou et al. (2000).\\nLetx,y∈IRp. We consider the space of functions generated by the linear\\nspan of {K(≤,y), y∈IRp)}; i.e arbitrary linear combinations of the form\\nf(x) =∑\\nmαmK(x,ym), where each kernel term is viewed as a function\\nof the ﬁrst argument, and indexed by the second. Suppose that Khas an\\neigen-expansion\\nK(x,y) =∞∑\\ni=1γiφi(x)φi(y) (5.45)\\nwithγi≥0,∑∞\\ni=1γ2\\ni<∞. Elements of HKhave an expansion in terms of\\nthese eigen-functions,\\nf(x) =∞∑\\ni=1ciφi(x), (5.46)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 187}, page_content='5.8 Regularization and Reproducing Kernel Hilbert Spaces 169\\nwith the constraint that\\n||f||2\\nHKdef=∞∑\\ni=1c2\\ni/γi<∞, (5.47)\\nwhere ||f||HKis the norm induced by K. The penalty functional in (5.42)\\nfor the space HKis deﬁned to be the squared norm J(f) =||f||2\\nHK. The\\nquantity J(f) can be interpreted as a generalized ridge penalty, where\\nfunctions with large eigenvalues in the expansion (5.45) get penalized less,\\nand vice versa.\\nRewriting (5.42) we have\\nmin\\nf∈HK[N∑\\ni=1L(yi,f(xi)) +λ||f||2\\nHK]\\n(5.48)\\nor equivalently\\nmin\\n{cj}∞\\n1\\uf8ee\\n\\uf8f0N∑\\ni=1L(yi,∞∑\\nj=1cjφj(xi)) +λ∞∑\\nj=1c2\\nj/γj\\uf8f9\\n\\uf8fb. (5.49)\\nIt can be shown (Wahba, 1990, see also Exercise 5.15) that the solution\\nto (5.48) is ﬁnite-dimensional, and has the form\\nf(x) =N∑\\ni=1αiK(x,xi). (5.50)\\nThe basis function hi(x) =K(x,xi) (as a function of the ﬁrst argument) is\\nknown as the representer of evaluation atxiinHK, since for f∈ H K, it is\\neasily seen that ⟨K(≤,xi),f⟩HK=f(xi). Similarly ⟨K(≤,xi),K(≤,xj)⟩HK=\\nK(xi,xj) (the reproducing property of HK), and hence\\nJ(f) =N∑\\ni=1N∑\\nj=1K(xi,xj)αiαj (5.51)\\nforf(x) =∑N\\ni=1αiK(x,xi).\\nIn light of (5.50) and (5.51), (5.48) reduces to a ﬁnite-dimensional crite-\\nrion\\nmin\\nαL(y,Kα) +λαTKα. (5.52)\\nWe are using a vector notation, in which Kis the N×Nmatrix with ijth\\nentry K(xi,xj) and so on. Simple numerical algorithms can be used to\\noptimize (5.52). This phenomenon, whereby the inﬁnite-dimensional prob-\\nlem (5.48) or (5.49) reduces to a ﬁnite dimensional optimization problem,\\nhas been dubbed the kernel property in the literature on support-vector\\nmachines (see Chapter 12).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 188}, page_content='170 5. Basis Expansions and Regularization\\nThere is a Bayesian interpretation of this class of models, in which f\\nis interpreted as a realization of a zero-mean stationary Gaussian process,\\nwith prior covariance function K. The eigen-decomposition produces a se-\\nries of orthogonal eigen-functions φj(x) with associated variances γj. The\\ntypical scenario is that “smooth” functions φjhave large prior variance,\\nwhile “rough” φjhave small prior variances. The penalty in (5.48) is the\\ncontribution of the prior to the joint likelihood, and penalizes more those\\ncomponents with smaller prior variance (compare with (5.43)).\\nFor simplicity we have dealt with the case here where all members of H\\nare penalized, as in (5.48). More generally, there may be some components\\ninHthat we wish to leave alone, such as the linear functions for cubic\\nsmoothing splines in Section 5.4. The multidimensional thin-plate splines\\nof Section 5.7 and tensor product splines fall into this category as well.\\nIn these cases there is a more convenient representation H=H0⊕ H1,\\nwith the null space H0consisting of, for example, low degree polynomi-\\nals in xthat do not get penalized. The penalty becomes J(f) =∥P1f∥,\\nwhere P1is the orthogonal projection of fontoH1. The solution has the\\nformf(x) =∑M\\nj=1βjhj(x) +∑N\\ni=1αiK(x,xi), where the ﬁrst term repre-\\nsents an expansion in H0. From a Bayesian perspective, the coeﬃcients of\\ncomponents in H0have improper priors, with inﬁnite variance.\\n5.8.2 Examples of RKHS\\nThe machinery above is driven by the choice of the kernel Kand the loss\\nfunction L. We consider ﬁrst regression using squared-error loss. In this\\ncase (5.48) specializes to penalized least squares, and the solution can be\\ncharacterized in two equivalent ways corresponding to (5.49) or (5.52):\\nmin\\n{cj}∞\\n1N∑\\ni=1\\uf8eb\\n\\uf8edyi−∞∑\\nj=1cjφj(xi)\\uf8f6\\n\\uf8f82\\n+λ∞∑\\nj=1c2\\nj\\nγj(5.53)\\nan inﬁnite-dimensional, generalized ridge regression problem, or\\nmin\\nα(y−Kα)T(y−Kα) +λαTKα. (5.54)\\nThe solution for αis obtained simply as\\nˆα= (K+λI)−1y, (5.55)\\nand\\nˆf(x) =N∑\\nj=1ˆαjK(x,xj). (5.56)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 189}, page_content='5.8 Regularization and Reproducing Kernel Hilbert Spaces 171\\nThe vector of Nﬁtted values is given by\\nˆf=Kˆα\\n=K(K+λI)−1y (5.57)\\n= (I+λK−1)−1y. (5.58)\\nThe estimate (5.57) also arises as the kriging estimate of a Gaussian ran-\\ndom ﬁeld in spatial statistics (Cressie, 1993). Compare also (5.58) w ith the\\nsmoothing spline ﬁt (5.17) on page 154.\\nPenalized Polynomial Regression\\nThe kernel K(x,y) = (⟨x,y⟩+ 1)d(Vapnik, 1996), for x,y∈IRp, has\\nM=(p+d\\nd)\\neigen-functions that span the space of polynomials in IRpof\\ntotal degree d. For example, with p= 2 and d= 2,M= 6 and\\nK(x,y) = 1 + 2 x1y1+ 2x2y2+x2\\n1y2\\n1+x2\\n2y2\\n2+ 2x1x2y1y2(5.59)\\n=M∑\\nm=1hm(x)hm(y) (5.60)\\nwith\\nh(x)T= (1,√\\n2x1,√\\n2x2,x2\\n1,x2\\n2,√\\n2x1x2). (5.61)\\nOne can represent hin terms of the Morthogonal eigen-functions and\\neigenvalues of K,\\nh(x) =VD1\\n2γφ(x), (5.62)\\nwhere Dγ= diag( γ1,γ2,... ,γ M), and VisM×Mand orthogonal.\\nSuppose we wish to solve the penalized polynomial regression problem\\nmin\\n{βm}M\\n1N∑\\ni=1(\\nyi−M∑\\nm=1βmhm(xi))2\\n+λM∑\\nm=1β2\\nm. (5.63)\\nSubstituting (5.62) into (5.63), we get an expression of the form (5.53) to\\noptimize (Exercise 5.16).\\nThe number of basis functions M=(p+d\\nd)\\ncan be very large, often much\\nlarger than N. Equation (5.55) tells us that if we use the kernel represen-\\ntation for the solution function, we have only to evaluate the kernel N2\\ntimes, and can compute the solution in O(N3) operations.\\nThis simplicity is not without implications. Each of the polynomials hm\\nin (5.61) inherits a scaling factor from the particular form of K, which has\\na bearing on the impact of the penalty in (5.63). We elaborate on this in\\nthe next section.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 190}, page_content='172 5. Basis Expansions and Regularization\\n−2 −1 0 1 2 3 40.0 0.4 0.8\\nXRadial Kernel in I R1K(≤, xm)\\nFIGURE 5.13. Radial kernels kk(x)for the mixture data, with scale parameter\\nν= 1. The kernels are centered at ﬁve points xmchosen at random from the 200.\\nGaussian Radial Basis Functions\\nIn the preceding example, the kernel is chosen because it represents an\\nexpansion of polynomials and can conveniently compute high-dimensional\\ninner products. In this example the kernel is chosen because of its functional\\nform in the representation (5.50).\\nThe Gaussian kernel K(x,y) =e−ν||x−y||2along with squared-error loss,\\nfor example, leads to a regression model that is an expansion in Gaussian\\nradial basis functions,\\nkm(x) =e−ν||x−xm||2, m= 1,... ,N, (5.64)\\neach one centered at one of the training feature vectors xm. The coeﬃcients\\nare estimated using (5.54).\\nFigure 5.13 illustrates radial kernels in IR1using the ﬁrst coordinate of\\nthe mixture example from Chapter 2. We show ﬁve of the 200 kernel basis\\nfunctions km(x) =K(x,xm).\\nFigure 5.14 illustrates the implicit feature space for the radial kernel\\nwithx∈IR1. We computed the 200 ×200 kernel matrix K, and its eigen-\\ndecomposition ΦDγΦT. We can think of the columns of Φand the corre-\\nsponding eigenvalues in Dγas empirical estimates of the eigen expansion\\n(5.45)2. Although the eigenvectors are discrete, we can represent them as\\nfunctions on IR1(Exercise 5.17). Figure 5.15 shows the largest 50 eigenval-\\nues ofK. The leading eigenfunctions are smooth, and they are successively\\nmore wiggly as the order increases. This brings to life the penalty in (5.49) ,\\nwhere we see the coeﬃcients of higher-order functions get penalized more\\nthan lower-order ones. The right panel in Figure 5.14 shows the correspond-\\n2Theℓth column of Φis an estimate of φℓ, evaluated at each of the Nobservations.\\nAlternatively, the ith row of Φis the estimated vector of basis functions φ(xi), evaluated\\nat the point xi. Although in principle, there can be inﬁnitely many element s inφ, our\\nestimate has at most Nelements.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 191}, page_content='5.8 Regularization and Reproducing Kernel Hilbert Spaces 173\\n*\\n******* ** ** ******\\n******\\n*******\\n******\\n**********\\n****\\n** *******\\n*****\\n**\\n***\\n**********\\n*****\\n*****\\n***\\n**\\n***\\n*****\\n*************\\n******\\n***\\n**\\n***\\n**\\n***********\\n**\\n****\\n**** *\\n** ******* * **\\n**************\\n********* ***\\n**\\n*******\\n***\\n**\\n****\\n***\\n***\\n**\\n***\\n*****\\n*******\\n**\\n**\\n**\\n**\\n***\\n**\\n*****\\n**\\n******\\n*\\n***\\n**\\n**\\n*********\\n**\\n**\\n****\\n**\\n**\\n**\\n****\\n****\\n***\\n**\\n**\\n*******\\n**\\n***\\n**\\n*\\n****\\n**\\n****\\n**\\n**\\n****\\n****\\n**\\n**\\n*****\\n****\\n**\\n****\\n*\\n**\\n***\\n**\\n**\\n****\\n****\\n**\\n**\\n*\\n***\\n**\\n*********\\n******\\n*\\n***\\n** **\\n****\\n*\\n***\\n***\\n***\\n***\\n*******\\n**\\n**\\n**\\n*\\n*******\\n**\\n***\\n***\\n******\\n**\\n***\\n**\\n******\\n**\\n**\\n*\\n**\\n**\\n**\\n*\\n***\\n**\\n**\\n****\\n*** **\\n*********\\n****\\n**\\n****\\n** **\\n** ****\\n****\\n***\\n***\\n**\\n** ***\\n**\\n**\\n*\\n***\\n***\\n* * * **\\n****\\n**\\n**\\n****\\n*****\\n****\\n* **** ***\\n**\\n*** **** * *\\n**\\n* *\\n**\\n**\\n******\\n***\\n***\\n****\\n***\\n***\\n*\\n**\\n**\\n**\\n**\\n***\\n***\\n***\\n****\\n***\\n********\\n*******\\n** ** **\\n**\\n***\\n***\\n**\\n*\\n*********\\n**********\\n*\\n****\\n***\\n***\\n**\\n**\\n***\\n*****\\n****\\n**\\n**\\n****\\n***\\n***\\n*\\n**\\n**\\n***\\n**\\n**\\n******\\n*******\\n***\\n***\\n*** *\\n****\\n***\\n****\\n* ***\\n* * ** *\\n* ***\\n**\\n*\\n**\\n***\\n***\\n**\\n***\\n**\\n*******\\n**\\n**\\n*******\\n**\\n*\\n* *********\\n***\\n******\\n******\\n** ***\\n*\\n**\\n*\\n**\\n**\\n***\\n* **\\n***\\n*\\n***\\n**\\n***********\\n**\\n**\\n**\\n***\\n**\\n***\\n**\\n**\\n*********\\n***\\n***\\n**\\n**\\n*\\n**\\n*\\n**\\n***\\n* * **\\n*****\\n**\\n**\\n*****\\n**\\n**\\n** ******\\n**\\n**\\n***** ****\\n***\\n****\\n* *****\\n****\\n****\\n***\\n**\\n**\\n**\\n* *******\\n****\\n****\\n***\\n**\\n****\\n*****\\n******\\n*****\\n*\\n*****\\n***\\n*\\n***\\n******\\n*****\\n***\\n***********\\n***\\n***** **\\n* *\\n***\\n*****\\n*******\\n*****\\n**\\n****\\n**\\n*\\n**\\n*\\n*****\\n***\\n*******\\n******* *\\n*****\\n*\\n**\\n***\\n*\\n*****\\n**\\n*****\\n**\\n*** *\\n***\\n***\\n******\\n**\\n***\\n********\\n***\\n**\\n**\\n***\\n***** ****\\n***\\n**\\n****\\n********\\n*******\\n***\\n**\\n***\\n**\\n***\\n**\\n*********\\n*\\n*******\\n*****\\n*\\n**\\n**\\n***\\n***\\n*\\n**\\n*\\n**\\n**\\n******\\n**\\n*\\n*****\\n*\\n***\\n** **\\n*\\n***\\n** ***\\n****\\n**\\n*******\\n****\\n**\\n****\\n*****\\n******\\n**\\n****\\n**\\n**\\n** ***\\n*****\\n*\\n**\\n**\\n***\\n**\\n****\\n**\\n*****\\n***\\n**\\n****\\n***\\n**\\n**\\n*\\n**\\n***\\n*\\n*****\\n***\\n******* ***\\n**\\n****\\n* ***\\n******\\n*****\\n*****\\n***\\n**\\n* ** ***\\n***\\n***\\n*\\n***\\n***\\n****\\n****\\n***\\n*****\\n*****\\n***\\n****\\n**\\n**\\n**\\n*\\n*****\\n**\\n******\\n********* *\\n**\\n*\\n****\\n**\\n*\\n* *\\n***\\n**\\n****\\n***\\n****\\n*** ****\\n****\\n**\\n*****\\n***\\n**\\n**\\n***\\n*****\\n*\\n***\\n*** *\\n******\\n*\\n***********\\n**\\n******\\n**\\n*******\\n**\\n****\\n*\\n**********\\n*\\n**** *\\n**\\n******\\n*\\n* **\\n***\\n*\\n**\\n*****\\n*\\n**\\n*\\n****\\n**\\n*\\n***\\n** *\\n****\\n******\\n***\\n**\\n*\\n**\\n**** ****\\n**\\n***\\n**\\n***\\n***\\n*****\\n**** * *\\n**\\n**\\n******\\n*****\\n*****\\n***\\n** *\\n****\\n*\\n****\\n** *\\n*****\\n****\\n**** *\\n*****\\n**\\n****\\n*****\\n** **\\n***\\n****\\n****\\n**\\n** *****\\n**\\n*****\\n***\\n**\\n****\\n*\\n****\\n****\\n**\\n*****\\n***\\n********\\n*\\n****\\n***\\n**\\n***\\n**\\n*****\\n**\\n***\\n*****\\n********\\n****\\n* *******\\n******* * *\\n***\\n****\\n**** *\\n**\\n**\\n***\\n****\\n**\\n**\\n**\\n******\\n**\\n**\\n**\\n****\\n*****\\n***\\n****\\n*****\\n*\\n***********\\n**\\n**\\n**\\n**\\n**\\n*******\\n******\\n***********\\n***\\n***\\n*****\\n****\\n***\\n* ***\\n*****\\n**\\n*********\\n*\\n**\\n***\\n** ****\\n** * *\\n**\\n***\\n***\\n**\\n*****\\n***\\n*****\\n****\\n**\\n**\\n***\\n*****\\n* * **\\n**\\n**\\n* **\\n***\\n**\\n*****\\n***\\n***\\n***\\n*******\\n**\\n*** *\\n****\\n*\\n* ***\\n****\\n***\\n***\\n***\\n***\\n*****\\n* ***\\n****\\n****\\n***\\n****\\n*****\\n*******\\n***\\n**\\n***\\n****\\n*\\n***\\n***\\n*\\n**\\n*\\n*****\\n**\\n***\\n**\\n**\\n***\\n**\\n*** *\\n******\\n**\\n* **\\n****\\n**\\n***\\n**\\n***\\n*******\\n**\\n***\\n**\\n****\\n***\\n****\\n* * *\\n****\\n* ***\\n***\\n**\\n**\\n******\\n***\\n**\\n*****\\n**\\n**\\n*\\n***\\n***\\n**\\n** *\\n******\\n*****\\n***\\n*\\n***\\n****\\n**\\n* **\\n*******\\n****\\n*******\\n**\\n***\\n***\\n*\\n*****\\n*\\n********\\n***\\n*\\n***\\n***\\n**\\n*******\\n**\\n*\\n*****\\n***\\n**\\n***\\n***\\n****\\n* ***\\n******\\n***\\n***\\n*****\\n*** **\\n**\\n**\\n***\\n***\\n********\\n**\\n****\\n**** *****\\n****\\n****\\n**\\n***\\n**\\n****\\n** *****\\n**\\n**\\n*****\\n**\\n**\\n**\\n***\\n****\\n*****\\n*** *\\n*******\\n****\\n********\\n**\\n** **\\n*****\\n**\\n***\\n*\\n******\\n**\\n***\\n***\\n*\\n****\\n**\\n***\\n**\\n******\\n***\\n***\\n*\\n**\\n*******\\n**\\n****\\n****\\n*\\n*******\\n**** ******\\n*\\n****\\n****\\n***\\n*\\n**\\n****\\n** **\\n** *\\n*** *\\n**\\n****\\n***************\\n***\\n***\\n*****\\n*****\\n* *****\\n***\\n****\\n*****\\n**\\n***\\n**\\n****\\n**\\n***\\n*\\n*****\\n***\\n**\\n****\\n***\\n**\\n**\\n*******\\n*****\\n***\\n*\\n***\\n*******\\n*\\n**\\n**** ***\\n****\\n*\\n****\\n****\\n*****\\n*****\\n***\\n***\\n*****\\n**\\n***\\n**\\n*\\n**'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 191}, page_content='***\\n***\\n*****\\n*****\\n* *****\\n***\\n****\\n*****\\n**\\n***\\n**\\n****\\n**\\n***\\n*\\n*****\\n***\\n**\\n****\\n***\\n**\\n**\\n*******\\n*****\\n***\\n*\\n***\\n*******\\n*\\n**\\n**** ***\\n****\\n*\\n****\\n****\\n*****\\n*****\\n***\\n***\\n*****\\n**\\n***\\n**\\n*\\n**\\n**\\n*****\\n**\\n**\\n*\\n**\\n**\\n**\\n****\\n**** *\\n*\\n**\\n******\\n****\\n**\\n****\\n***\\n*\\n*\\n****\\n****\\n****\\n***\\n***\\n******\\n***\\n****\\n*\\n***\\n**\\n*****\\n*\\n* ****\\n****\\n**\\n*\\n**\\n*\\n***\\n****\\n**\\n**\\n***\\n****\\n***\\n**\\n*\\n**\\n**\\n********* *\\n**\\n******\\n**\\n***\\n**\\n**\\n****\\n***\\n***\\n**\\n**\\n*******\\n***\\n**\\n**\\n***\\n**\\n*\\n**\\n*\\n**\\n**Orthonormal Basis Φ\\n*\\n*****\\n** *\\n* ***\\n*\\n****\\n***\\n*\\n**\\n*\\n***\\n***\\n******\\n**\\n**\\n***\\n***\\n****\\n**\\n*\\n**\\n****\\n*****\\n**\\n***\\n**\\n******\\n**\\n**\\n*\\n**\\n**\\n***\\n***\\n**\\n***\\n*****\\n*\\n***\\n*\\n***\\n*****\\n***\\n***\\n***\\n**\\n***\\n**\\n*\\n**\\n********\\n**\\n****\\n*\\n***\\n*\\n** *\\n*\\n***\\n** ***\\n**\\n**\\n**\\n**\\n***\\n***\\n***\\n***\\n***\\n***\\n**\\n***\\n****\\n***\\n**\\n****\\n***\\n***\\n**\\n***\\n****\\n*\\n**\\n**\\n***\\n**\\n**\\n**\\n**\\n**\\n*\\n**\\n****\\n*\\n**\\n******\\n*\\n***\\n**\\n**\\n*\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n****\\n*\\n*\\n**\\n**\\n**\\n**\\n****\\n***\\n**\\n**\\n***\\n***\\n*\\n**\\n***\\n**\\n*\\n****\\n**\\n*\\n***\\n**\\n**\\n****\\n****\\n**\\n**\\n****\\n*\\n****\\n**\\n****\\n*\\n**\\n***\\n**\\n**\\n***\\n*\\n****\\n**\\n**\\n*\\n***\\n**\\n***\\n**\\n*\\n**\\n*\\n******\\n*\\n**\\n*\\n***\\n*\\n***\\n*\\n*\\n***\\n***\\n***\\n*\\n**\\n****\\n**\\n*\\n**\\n**\\n**\\n*\\n***\\n***\\n*\\n**\\n***\\n**\\n*\\n****\\n**\\n**\\n***\\n**\\n*****\\n*\\n**\\n**\\n*\\n**\\n**\\n**\\n*\\n***\\n**\\n**\\n***\\n*\\n*****\\n*****\\n****\\n****\\n**\\n*\\n***\\n** **\\n***\\n***\\n***\\n*\\n**\\n*\\n***\\n**\\n** ***\\n**\\n**\\n*\\n***\\n***\\n** ***\\n****\\n**\\n**\\n***\\n*\\n**\\n***\\n*\\n***\\n*******\\n*\\n**\\n*\\n***\\n*** **\\n**\\n* *\\n**\\n**\\n***\\n***\\n***\\n***\\n****\\n***\\n***\\n*\\n**\\n**\\n**\\n*\\n*\\n***\\n**\\n*\\n*\\n**\\n**\\n**\\n***\\n***\\n*****\\n*\\n******\\n** ****\\n**\\n***\\n**\\n*\\n**\\n*\\n****\\n*\\n****\\n**\\n********\\n*\\n****\\n**\\n*\\n***\\n**\\n**\\n***\\n*****\\n****\\n**\\n**\\n****\\n***\\n*\\n**\\n*\\n**\\n**\\n***\\n**\\n**\\n******\\n*****\\n**\\n***\\n**\\n*\\n****\\n****\\n***\\n*\\n**\\n*\\n* ***\\n* *** *\\n* ***\\n**\\n*\\n**\\n***\\n***\\n**\\n*\\n**\\n**\\n*******\\n**\\n**\\n**\\n*****\\n**\\n*\\n* ****\\n***\\n**\\n***\\n*\\n*****\\n******\\n** ***\\n*\\n**\\n*\\n**\\n**\\n*\\n**\\n* **\\n***\\n*\\n***\\n**\\n*****\\n***\\n***\\n**\\n**\\n**\\n***\\n**\\n***\\n**\\n**\\n**\\n*******\\n*\\n**\\n***\\n**\\n**\\n*\\n**\\n*\\n**\\n***\\n* ***\\n*****\\n**\\n**\\n*****\\n**\\n**\\n** ***\\n*\\n**\\n**\\n**\\n***** ****\\n***\\n****\\n******\\n****\\n****\\n***\\n**\\n**\\n**\\n* *******\\n**\\n**\\n****\\n*\\n**\\n**\\n****\\n*****\\n******\\n*****\\n*\\n*****\\n***\\n*\\n***\\n******\\n*****\\n***\\n****\\n*****\\n**\\n***\\n***** **\\n* *\\n***\\n*****\\n*******\\n****\\n*\\n**\\n****\\n**\\n*\\n**\\n*\\n***\\n**\\n***\\n***\\n****\\n******* *\\n*****\\n*\\n**\\n***\\n*\\n*****\\n**\\n*****\\n**\\n*** *\\n***\\n***\\n******\\n**\\n***\\n********\\n***\\n**\\n**\\n***\\n***** ****\\n***\\n**\\n****\\n********\\n*******\\n***\\n**\\n***\\n**\\n***\\n**\\n*********\\n*\\n*******\\n*****\\n*\\n**\\n**\\n***\\n***\\n*\\n**\\n*\\n**\\n**\\n******\\n**\\n*\\n*****\\n*\\n***\\n** **\\n*\\n***\\n** ***\\n****\\n**\\n*******\\n****\\n**\\n****\\n*****\\n******\\n**\\n****\\n****\\n** ********\\n***\\n*****\\n**\\n****\\n**\\n**********\\n****\\n***\\n**\\n***\\n**\\n****\\n*****\\n***\\n******* ***\\n**\\n****\\n* ***\\n******\\n*****\\n*****\\n*****\\n* ** ***\\n***\\n***\\n*\\n***\\n***\\n****\\n*******\\n*****\\n*****\\n***\\n* *****\\n**\\n**\\n*\\n*****\\n**\\n******\\n********* ****\\n****\\n***\\n* ****\\n**\\n****\\n***\\n******* ** ********\\n********\\n**\\n**\\n********\\n****\\n*** *\\n***** ***************\\n******\\n**\\n*** ****\\n******************** ** ****** ***\\n** **\\n***\\n*\\n**********\\n*****\\n******** *\\n****\\n** * ******\\n********* * ***\\n**\\n********\\n******* ***** * *\\n****** ***** *** *********** ************ ** ***** ******* **\\n**** ************** *** ********* **** * ******************* ********** ***************** *******\\n*****\\n***\\n******** *************** ******* ** * ****\\n******* * ******* ** ** * ***** *** ** ************* ******** ** ***** ** ***** **\\n**** * ************************* ******************************** ** * ** *** *********** * ************** * ****** * *************** ***************\\n****** * **** * ** ** *** ***** ***** ************* ******** ** ***** ** ************ ************* * ********** * **** ********* ********************* ** * *** *********** ** ********** ** * * ****** * **************** ****** *** ***********'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 191}, page_content='* **** * ** ** ** * **** * *** ** ************* ******** ********* ***** ****** * ************* * ** ******** * **** ********* *********** ** ****** *** ** *** *********** * * ***** * **** ** * * ****** * * ******* ******* * ****** ** * *********** * **** * ** ** ** * **** * *** ** ************* ******** ** ***** ** ***** ** **** * **** ********* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * ***** * **** ** ** ****** * * ************** * ****** ** * **** * ****** * ** ** * ** ** ** * **** * *** ** ************* ******** ** **** *** ***** ** **** * ************* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * ***** * **** ** * * ****** * * ************** * ****** ** * **** * ****** * **** * ** ** ** * **** * *** ** **** ***** **** ******** ** ***** ** ***** ** **** * ************* * ** ******** * **** ********* *** * ******* ** ****** ** * * * *** *********** ** ***** * **** ** * * * * **** * * ************** * ****** ** * **** * ******Feature Space H\\nFIGURE 5.14. (Left panel) The ﬁrst 16normalized eigenvectors of K, the\\n200×200kernel matrix for the ﬁrst coordinate of the mixture data. These a re\\nviewed as estimates ˆφℓof the eigenfunctions in (5.45), and are represented as\\nfunctions in I R1with the observed values superimposed in color. They are arr anged\\nin rows, starting at the top left. (Right panel) Rescaled versi onshℓ=√ˆγℓˆφℓof\\nthe functions in the left panel, for which the kernel computes the “inner product.”\\n0 10 20 30 40 501e−15 1e−11 1e−07 1e−03 1e+01Eigenvalue\\nFIGURE 5.15. The largest 50eigenvalues of K; all those beyond the 30th are\\neﬀectively zero.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 192}, page_content='174 5. Basis Expansions and Regularization\\ningfeature space representation of the eigenfunctions\\nhℓ(x) =√\\nˆγℓˆφℓ(x), ℓ= 1,... ,N. (5.65)\\nNote that ⟨h(xi),h(xi′)⟩=K(xi,xi′). The scaling by the eigenvalues quickly\\nshrinks most of the functions down to zero, leaving an eﬀective dimension\\nof about 12 in this case. The corresponding optimization problem is a stan-\\ndard ridge regression, as in (5.63). So although in principle the implicit\\nfeature space is inﬁnite dimensional, the eﬀective dimension is dramat-\\nically lower because of the relative amounts of shrinkage applied to each\\nbasis function. The kernel scale parameter νplays a role here as well; larger\\nνimplies more local kmfunctions, and increases the eﬀective dimension of\\nthe feature space. See Hastie and Zhu (2006) for more details.\\nIt is also known (Girosi et al., 1995) that a thin-plate spline (Section 5.7 )\\nis an expansion in radial basis functions, generated by the kernel\\nK(x,y) =∥x−y∥2log(∥x−y∥). (5.66)\\nRadial basis functions are discussed in more detail in Section 6.7.\\nSupport Vector Classiﬁers\\nThe support vector machines of Chapter 12 for a two-class classiﬁcation\\nproblem have the form f(x) =α0+∑N\\ni=1αiK(x,xi), where the parameters\\nare chosen to minimize\\nmin\\nα0,α{N∑\\ni=1[1−yif(xi)]++λ\\n2αTKα}\\n, (5.67)\\nwhere yi∈ {−1,1}, and [ z]+denotes the positive part of z. This can be\\nviewed as a quadratic optimization problem with linear constraints, and\\nrequires a quadratic programming algorithm for its solution. The name\\nsupport vector arises from the fact that typically many of the ˆ αi= 0 [due\\nto the piecewise-zero nature of the loss function in (5.67)], and so ˆfis an\\nexpansion in a subset of the K(≤,xi). See Section 12.3.3 for more details.\\n5.9 Wavelet Smoothing\\nWe have seen two diﬀerent modes of operation with dictionaries of basis\\nfunctions. With regression splines, we select a subset of the bases, using\\neither subject-matter knowledge, or else automatically. The more adaptive\\nprocedures such as MARS (Chapter 9) can capture both smooth and non-\\nsmooth behavior. With smoothing splines, we use a complete basis, but\\nthen shrink the coeﬃcients toward smoothness.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 193}, page_content='5.9 Wavelet Smoothing 175\\nTime0.0 0.2 0.4 0.6 0.8 1.0Haar Wavelets\\nTime0.0 0.2 0.4 0.6 0.8 1.0Symmlet-8 Wavelets\\nψ1,0ψ2,1ψ2,3ψ3,2ψ3,5ψ4,4ψ4,9ψ5,1ψ5,15ψ6,15ψ6,35\\nFIGURE 5.16. Some selected wavelets at diﬀerent translations and dilations\\nfor the Haar and symmlet families. The functions have been scale d to suit the\\ndisplay.\\nWavelets typically use a complete orthonormal basis to represent func-\\ntions, but then shrink and select the coeﬃcients toward a sparse represen-\\ntation. Just as a smooth function can be represented by a few spline basis\\nfunctions, a mostly ﬂat function with a few isolated bumps can be repre-\\nsented with a few (bumpy) basis functions. Wavelets bases are very popular\\nin signal processing and compression, since they are able to represent both\\nsmooth and/or locally bumpy functions in an eﬃcient way—a phenomenon\\ndubbed time and frequency localization . In contrast, the traditional Fourier\\nbasis allows only frequency localization.\\nBefore we give details, let’s look at the Haar wavelets in the left panel\\nof Figure 5.16 to get an intuitive idea of how wavelet smoothing works.\\nThe vertical axis indicates the scale (frequency) of the wavelets, from low\\nscale at the bottom to high scale at the top. At each scale the wavelets are\\n“packed in” side-by-side to completely ﬁll the time axis: we have only shown'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 194}, page_content='176 5. Basis Expansions and Regularization\\na selected subset. Wavelet smoothing ﬁts the coeﬃcients for this basis by\\nleast squares, and then thresholds (discards, ﬁlters) the smaller coeﬃcients.\\nSince there are many basis functions at each scale, it can use bases where\\nit needs them and discard the ones it does not need, to achieve time and\\nfrequency localization. The Haar wavelets are simple to understand, but not\\nsmooth enough for most purposes. The symmlet wavelets in the right panel\\nof Figure 5.16 have the same orthonormal properties, but are smoother.\\nFigure 5.17 displays an NMR (nuclear magnetic resonance) signal, which\\nappears to be composed of smooth components and isolated spikes, plus\\nsome noise. The wavelet transform, using a symmlet basis, is shown in the\\nlower left panel. The wavelet coeﬃcients are arranged in rows, from lowest\\nscale at the bottom, to highest scale at the top. The length of each line\\nsegment indicates the size of the coeﬃcient. The bottom right panel shows\\nthe wavelet coeﬃcients after they have been thresholded. The threshold\\nprocedure, given below in equation (5.69), is the same soft-thresholding\\nrule that arises in the lasso procedure for linear regression (Section 3.4.2).\\nNotice that many of the smaller coeﬃcients have been set to zero. The\\ngreen curve in the top panel shows the back-transform of the thresholded\\ncoeﬃcients: this is the smoothed version of the original signal. In the next\\nsection we give the details of this process, including the construction of\\nwavelets and the thresholding rule.\\n5.9.1 Wavelet Bases and the Wavelet Transform\\nIn this section we give details on the construction and ﬁltering of wavelets.\\nWavelet bases are generated by translations and dilations of a single scal-\\ning function φ(x) (also known as the father ). The red curves in Figure 5.18\\nare the Haar andsymmlet-8 scaling functions. The Haar basis is particu-\\nlarly easy to understand, especially for anyone with experience in analysis\\nof variance or trees, since it produces a piecewise-constant representation.\\nThus if φ(x) =I(x∈[0,1]), then φ0,k(x) =φ(x−k),kan integer, generates\\nan orthonormal basis for functions with jumps at the integers. Call this ref-\\nerence space V0. The dilations φ1,k(x) =√\\n2φ(2x−k) form an orthonormal\\nbasis for a space V1⊃V0of functions piecewise constant on intervals of\\nlength1\\n2. In fact, more generally we have ≤≤≤ ⊃ V1⊃V0⊃V−1⊃ ≤≤≤ where\\neachVjis spanned by φj,k= 2j/2φ(2jx−k).\\nNow to the deﬁnition of wavelets. In analysis of variance, we often rep-\\nresent a pair of means θ1andθ2by their grand mean θ=1\\n2(θ1+θ2), and\\nthen a contrast α=1\\n2(θ1−θ2). A simpliﬁcation occurs if the contrast αis\\nvery small, because then we can set it to zero. In a similar manner we might\\nrepresent a function in Vj+1by a component in Vjplus the component in\\nthe orthogonal complement WjofVjtoVj+1, written as Vj+1=Vj⊕Wj.\\nThe component in Wjrepresents detail, and we might wish to set some ele-\\nments of this component to zero. It is easy to see that the functions ψ(x−k)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 195}, page_content='5.9 Wavelet Smoothing 177\\nNMR Signal\\n0 200 400 600 800 10000 20 40 60\\n0 200 400 600 800 1000Wavelet Transform - Original Signal\\n0 200 400 600 800 1000Wavelet Transform - WaveShrunk Signal\\nSignal Signal\\nW9 W9\\nW8 W8\\nW7 W7\\nW6 W6\\nW5 W5\\nW4 W4\\nV4 V4\\nFIGURE 5.17. The top panel shows an NMR signal, with the wavelet-shrunk\\nversion superimposed in green. The lower left panel represents the wavelet trans-\\nform of the original signal, down to V4, using the symmlet-8 basis. Each coeﬃ-\\ncient is represented by the height (positive or negative) of the vertical bar. The\\nlower right panel represents the wavelet coeﬃcients after being shrunken using\\nthewaveshrink function in S-PLUS, which implements the SureShrink method\\nof wavelet adaptation of Donoho and Johnstone.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 196}, page_content='178 5. Basis Expansions and Regularization\\nHaar Basis Symmlet Basis\\nφ(x) φ(x)\\nψ(x) ψ(x)\\nFIGURE 5.18. TheHaarandsymmlet father (scaling) wavelet φ(x)and mother\\nwavelet ψ(x).\\ngenerated by the mother wavelet ψ(x) =φ(2x)−φ(2x−1) form an orthonor-\\nmal basis for W0for the Haar family. Likewise ψj,k= 2j/2ψ(2jx−k) form\\na basis for Wj.\\nNowVj+1=Vj⊕Wj=Vj−1⊕Wj−1⊕Wj, so besides representing a\\nfunction by its level- jdetail and level- jrough components, the latter can\\nbe broken down to level-( j−1) detail and rough, and so on. Finally we get\\na representation of the form VJ=V0⊕W0⊕W1≤≤≤ ⊕WJ−1. Figure 5.16\\non page 175 shows particular wavelets ψj,k(x).\\nNotice that since these spaces are orthogonal, all the basis functions are\\northonormal. In fact, if the domain is discrete with N= 2J(time) points,\\nthis is as far as we can go. There are 2jbasis elements at level j, and\\nadding up, we have a total of 2J−1 elements in the Wj, and one in V0.\\nThis structured orthonormal basis allows for a multiresolution analysis ,\\nwhich we illustrate in the next section.\\nWhile helpful for understanding the construction above, the Haar basis\\nis often too coarse for practical purposes. Fortunately, many clever wavelet\\nbases have been invented. Figures 5.16 and 5.18 include the Daubechies\\nsymmlet-8 basis. This basis has smoother elements than the corresponding\\nHaar basis, but there is a tradeoﬀ:\\n•Each wavelet has a support covering 15 consecutive time intervals,\\nrather than one for the Haar basis. More generally, the symmlet- p\\nfamily has a support of 2 p−1 consecutive intervals. The wider the\\nsupport, the more time the wavelet has to die to zero, and so it can'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 197}, page_content='5.9 Wavelet Smoothing 179\\nachieve this more smoothly. Note that the eﬀective support seems to\\nbe much narrower.\\n•The symmlet- pwavelet ψ(x) has pvanishing moments; that is,\\n∫\\nψ(x)xjdx= 0, j= 0,... ,p −1.\\nOne implication is that any order- ppolynomial over the N= 2Jtimes\\npoints is reproduced exactly in V0(Exercise 5.18). In this sense V0\\nis equivalent to the null space of the smoothing-spline penalty. The\\nHaar wavelets have one vanishing moment, and V0can reproduce any\\nconstant function.\\nThe symmlet- pscaling functions are one of many families of wavelet\\ngenerators. The operations are similar to those for the Haar basis:\\n•IfV0is spanned by φ(x−k), then V1⊃V0is spanned by φ1,k(x) =√\\n2φ(2x−k) and φ(x) =∑\\nk∈Zh(k)φ1,k(x), for some ﬁlter coeﬃcients\\nh(k).\\n•W0is spanned by ψ(x) =∑\\nk∈Zg(k)φ1,k(x), with ﬁlter coeﬃcients\\ng(k) = (−1)1−kh(1−k).\\n5.9.2 Adaptive Wavelet Filtering\\nWavelets are particularly useful when the data are measured on a uniform\\nlattice, such as a discretized signal, image, or a time series. We will focus o n\\nthe one-dimensional case, and having N= 2Jlattice-points is convenient.\\nSuppose yis the response vector, and Wis the N×Northonormal wavelet\\nbasis matrix evaluated at the Nuniformly spaced observations. Then y∗=\\nWTyis called the wavelet transform ofy(and is the full least squares\\nregression coeﬃcient). A popular method for adaptive wavelet ﬁtting is\\nknown as SURE shrinkage (Stein Unbiased Risk Estimation, Donoho and\\nJohnstone (1994)). We start with the criterion\\nmin\\nθ||y−Wθ||2\\n2+ 2λ||θ||1, (5.68)\\nwhich is the same as the lasso criterion in Chapter 3. Because Wis or-\\nthonormal, this leads to the simple solution:\\nˆθj= sign( y∗\\nj)(|y∗\\nj| −λ)+. (5.69)\\nThe least squares coeﬃcients are translated toward zero, and truncated\\nat zero. The ﬁtted function (vector) is then given by the inverse wavelet\\ntransform ˆf=Wˆθ.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 198}, page_content='180 5. Basis Expansions and Regularization\\nA simple choice for λisλ=σ√2logN, where σis an estimate of the\\nstandard deviation of the noise. We can give some motivation for this cho ice.\\nSinceWis an orthonormal transformation, if the elements of yare white\\nnoise (independent Gaussian variates with mean 0 and variance σ2), then\\nso arey∗. Furthermore if random variables Z1,Z2,... ,Z Nare white noise,\\nthe expected maximum of |Zj|,j= 1,... ,N is approximately σ√2logN.\\nHence all coeﬃcients below σ√2logNare likely to be noise and are set to\\nzero.\\nThe space Wcould be any basis of orthonormal functions: polynomials,\\nnatural splines or cosinusoids. What makes wavelets special is the particular\\nform of basis functions used, which allows for a representation localized in\\ntime and in frequency .\\nLet’s look again at the NMR signal of Figure 5.17. The wavelet transfor m\\nwas computed using a symmlet −8 basis. Notice that the coeﬃcients do not\\ndescend all the way to V0, but stop at V4which has 16 basis functions.\\nAs we ascend to each level of detail, the coeﬃcients get smaller, except in\\nlocations where spiky behavior is present. The wavelet coeﬃcients represent\\ncharacteristics of the signal localized in time (the basis functions at each\\nlevel are translations of each other) and localized in frequency. Each dilation\\nincreases the detail by a factor of two, and in this sense corresponds to\\ndoubling the frequency in a traditional Fourier representation. In fact, a\\nmore mathematical understanding of wavelets reveals that the wavelets at\\na particular scale have a Fourier transform that is restricted to a limited\\nrange or octave of frequencies.\\nThe shrinking/truncation in the right panel was achieved using the SURE\\napproach described in the introduction to this section. The orthonormal\\nN×Nbasis matrix Whas columns which are the wavelet basis functions\\nevaluated at the Ntime points. In particular, in this case there will be 16\\ncolumns corresponding to the φ4,k(x), and the remainder devoted to the\\nψj,k(x), j= 4,... ,11. In practice λdepends on the noise variance, and has\\nto be estimated from the data (such as the variance of the coeﬃcients at\\nthe highest level).\\nNotice the similarity between the SURE criterion (5.68) on page 179,\\nand the smoothing spline criterion (5.21) on page 156:\\n•Both are hierarchically structured from coarse to ﬁne detail, although\\nwavelets are also localized in time within each resolution level.\\n•The splines build in a bias toward smooth functions by imposing\\ndiﬀerential shrinking constants dk. Early versions of SURE shrinkage\\ntreated all scales equally. The S+wavelets function waveshrink() has\\nmany options, some of which allow for diﬀerential shrinkage.\\n•The spline L2penalty cause pure shrinkage, while the SURE L1\\npenalty does shrinkage and selection.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 199}, page_content='Exercises 181\\nMore generally smoothing splines achieve compression of the original signal\\nby imposing smoothness, while wavelets impose sparsity. Figure 5.19 co m-\\npares a wavelet ﬁt (using SURE shrinkage) to a smoothing spline ﬁt (using\\ncross-validation) on two examples diﬀerent in nature. For the NMR data in\\nthe upper panel, the smoothing spline introduces detail everywhere in order\\nto capture the detail in the isolated spikes; the wavelet ﬁt nicely localizes\\nthe spikes. In the lower panel, the true function is smooth, and the noise is\\nrelatively high. The wavelet ﬁt has let in some additional and unnecessary\\nwiggles—a price it pays in variance for the additional adaptivity.\\nThe wavelet transform is not performed by matrix multiplication as in\\ny∗=WTy. In fact, using clever pyramidal schemes y∗can be obtained\\ninO(N) computations, which is even faster than the Nlog(N) of the fast\\nFourier transform (FFT). While the general construction is beyond the\\nscope of this book, it is easy to see for the Haar basis (Exercise 5.19).\\nLikewise, the inverse wavelet transform Wˆθis also O(N).\\nThis has been a very brief glimpse of this vast and growing ﬁeld. There is\\na very large mathematical and computational base built on wavelets. Mod-\\nern image compression is often performed using two-dimensional wavelet\\nrepresentations.\\nBibliographic Notes\\nSplines and B-splines are discussed in detail in de Boor (1978). Green\\nand Silverman (1994) and Wahba (1990) give a thorough treatment of\\nsmoothing splines and thin-plate splines; the latter also covers reproducing\\nkernel Hilbert spaces. See also Girosi et al. (1995) and Evgeniou et al.\\n(2000) for connections between many nonparametric regression techniques\\nusing RKHS approaches. Modeling functional data, as in Section 5.2.3, is\\ncovered in detail in Ramsay and Silverman (1997).\\nDaubechies (1992) is a classic and mathematical treatment of wavelets.\\nOther useful sources are Chui (1992) and Wickerhauser (1994). Donoho and\\nJohnstone (1994) developed the SURE shrinkage and selection technology\\nfrom a statistical estimation framework; see also Vidakovic (199 9). Bruce\\nand Gao (1996) is a useful applied introduction, which also describes the\\nwavelet software in S-PLUS.\\nExercises\\nEx. 5.1 Show that the truncated power basis functions in (5.3) represent a\\nbasis for a cubic spline with the two knots as indicated.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 200}, page_content='182 5. Basis Expansions and Regularization\\nNMR Signal0 200 400 600 800 10000 20 40 60spline\\nwavelet\\nSmooth Function (Simulated)n\\n0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2 4spline\\nwavelet\\ntrue•\\n••\\n•\\n•••\\n•\\n•\\n•\\n••••\\n•••\\n••••\\n••\\n••\\n••\\n••••••\\n•••\\n•\\n••\\n••\\n•••••\\n•••••••\\n••••\\n•\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n••••\\n••\\n••\\n•••\\n•••\\n•••\\n••\\n•\\n•••\\n•••\\n•\\n••••••\\n••••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\nFIGURE 5.19. Wavelet smoothing compared with smoothing splines on two\\nexamples. Each panel compares the SURE-shrunk wavelet ﬁt to the cro ss-validated\\nsmoothing spline ﬁt.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 201}, page_content='Exercises 183\\nEx. 5.2 Suppose that Bi,M(x) is an order- M B-spline deﬁned in the Ap-\\npendix on page 186 through the sequence (5.77)–(5.78).\\n(a) Show by induction that Bi,M(x) = 0 for x̸∈[τi,τi+M]. This shows, for\\nexample, that the support of cubic B-splines is at most 5 knots.\\n(b) Show by induction that Bi,M(x)>0 forx∈(τi,τi+M). The B-splines\\nare positive in the interior of their support.\\n(c) Show by induction that∑K+M\\ni=1Bi,M(x) = 1∀x∈[ξ0,ξK+1].\\n(d) Show that Bi,Mis a piecewise polynomial of order M(degree M−1)\\non [ξ0,ξK+1], with breaks only at the knots ξ1,... ,ξ K.\\n(e) Show that an order- M B-spline basis function is the density function\\nof a convolution of Muniform random variables.\\nEx. 5.3 Write a program to reproduce Figure 5.3 on page 145.\\nEx. 5.4 Consider the truncated power series representation for cubic splines\\nwithKinterior knots. Let\\nf(X) =3∑\\nj=0βjXj+K∑\\nk=1θk(X−ξk)3\\n+. (5.70)\\nProve that the natural boundary conditions for natural cubic splines (Sec-\\ntion 5.2.1) imply the following linear constraints on the coeﬃcients:\\nβ2= 0,∑K\\nk=1θk= 0,\\nβ3= 0,∑K\\nk=1ξkθk= 0.(5.71)\\nHence derive the basis (5.4) and (5.5).\\nEx. 5.5 Write a program to classify the phoneme data using a quadratic dis-\\ncriminant analysis (Section 4.3). Since there are many correlated features,\\nyou should ﬁlter them using a smooth basis of natural cubic splines (Sec-\\ntion 5.2.3). Decide beforehand on a series of ﬁve diﬀerent choices for the\\nnumber and position of the knots, and use tenfold cross-validation to make\\nthe ﬁnal selection. The phoneme data are available from the book website\\nwww-stat.stanford.edu/ElemStatLearn .\\nEx. 5.6 Suppose you wish to ﬁt a periodic function, with a known period T.\\nDescribe how you could modify the truncated power series basis to achieve\\nthis goal.\\nEx. 5.7 Derivation of smoothing splines (Green and Silverman, 1994). Sup-\\npose that N≥2, and that gis the natural cubic spline interpolant to the\\npairs{xi,zi}N\\n1, with a < x 1<≤≤≤< x N< b. This is a natural spline'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 202}, page_content='184 5. Basis Expansions and Regularization\\nwith a knot at every xi; being an N-dimensional space of functions, we can\\ndetermine the coeﬃcients such that it interpolates the sequence ziexactly.\\nLet ˜gbe any other diﬀerentiable function on [ a,b] that interpolates the N\\npairs.\\n(a) Let h(x) = ˜g(x)−g(x). Use integration by parts and the fact that gis\\na natural cubic spline to show that\\n∫b\\nag′′(x)h′′(x)dx=−N−1∑\\nj=1g′′′(x+\\nj){h(xj+1)−h(xj)}(5.72)\\n= 0.\\n(b) Hence show that∫b\\na˜g′′(t)2dt≥∫b\\nag′′(t)2dt,\\nand that equality can only hold if his identically zero in [ a,b].\\n(c) Consider the penalized least squares problem\\nmin\\nf[N∑\\ni=1(yi−f(xi))2+λ∫b\\naf′′(t)2dt]\\n.\\nUse (b) to argue that the minimizer must be a cubic spline with knots\\nat each of the xi.\\nEx. 5.8 In the appendix to this chapter we show how the smoothing spline\\ncomputations could be more eﬃciently carried out using a ( N+ 4) dimen-\\nsional basis of B-splines. Describe a slightly simpler scheme using a ( N+2)\\ndimensional B-spline basis deﬁned on the N−2 interior knots.\\nEx. 5.9 Derive the Reinsch form Sλ= (I+λK)−1for the smoothing spline.\\nEx. 5.10 Derive an expression for Var( ˆfλ(x0)) and bias( ˆfλ(x0)). Using the\\nexample (5.22), create a version of Figure 5.9 where the mean and several\\n(pointwise) quantiles of ˆfλ(x) are shown.\\nEx. 5.11 Prove that for a smoothing spline the null space of Kis spanned\\nby functions linear in X.\\nEx. 5.12 Characterize the solution to the following problem,\\nmin\\nfRSS(f,λ) =N∑\\ni=1wi{yi−f(xi)}2+λ∫\\n{f′′(t)}2dt, (5.73)\\nwhere the wi≥0 are observation weights.\\nCharacterize the solution to the smoothing spline problem (5.9) when\\nthe training data have ties in X.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 203}, page_content='Exercises 185\\nEx. 5.13 You have ﬁtted a smoothing spline ˆfλto a sample of Npairs\\n(xi,yi). Suppose you augment your original sample with the pair x0,ˆfλ(x0),\\nand reﬁt; describe the result. Use this to derive the N-fold cross-validation\\nformula (5.26).\\nEx. 5.14 Derive the constraints on the αjin the thin-plate spline expan-\\nsion (5.39) to guarantee that the penalty J(f) is ﬁnite. How else could one\\nensure that the penalty was ﬁnite?\\nEx. 5.15 This exercise derives some of the results quoted in Section 5.8.1.\\nSuppose K(x,y) satisfying the conditions (5.45) and let f(x)∈ H K. Show\\nthat\\n(a)⟨K(≤,xi),f⟩HK=f(xi).\\n(b)⟨K(≤,xi),K(≤,xj)⟩HK=K(xi,xj).\\n(c) If g(x) =∑N\\ni=1αiK(x,xi), then\\nJ(g) =N∑\\ni=1N∑\\nj=1K(xi,xj)αiαj.\\nSuppose that ˜ g(x) =g(x) +ρ(x), with ρ(x)∈ H K, and orthogonal in HK\\nto each of K(x,xi), i= 1,... ,N . Show that\\n(d)\\nN∑\\ni=1L(yi,˜g(xi)) +λJ(˜g)≥N∑\\ni=1L(yi,g(xi)) +λJ(g) (5.74)\\nwith equality iﬀ ρ(x) = 0.\\nEx. 5.16 Consider the ridge regression problem (5.53), and assume M≥N.\\nAssume you have a kernel Kthat computes the inner product K(x,y) =∑M\\nm=1hm(x)hm(y).\\n(a) Derive (5.62) on page 171 in the text. How would you compute the\\nmatrices VandDγ, given K? Hence show that (5.63) is equivalent\\nto (5.53).\\n(b) Show that\\nˆf=Hˆβ\\n=K(K+λI)−1y, (5.75)\\nwhereHis the N×Mmatrix of evaluations hm(xi), and K=HHT\\ntheN×Nmatrix of inner-products h(xi)Th(xj).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 204}, page_content='186 5. Basis Expansions and Regularization\\n(c) Show that\\nˆf(x) = h(x)Tˆβ\\n=N∑\\ni=1K(x,xi)ˆαi (5.76)\\nandˆα= (K+λI)−1y.\\n(d) How would you modify your solution if M < N ?\\nEx. 5.17 Show how to convert the discrete eigen-decomposition of Kin\\nSection 5.8.2 to estimates of the eigenfunctions of K.\\nEx. 5.18 The wavelet function ψ(x) of the symmlet- pwavelet basis has\\nvanishing moments up to order p. Show that this implies that polynomials\\nof order pare represented exactly in V0, deﬁned on page 176.\\nEx. 5.19 Show that the Haar wavelet transform of a signal of length N= 2J\\ncan be computed in O(N) computations.\\nAppendix: Computations for Splines\\nIn this Appendix, we describe the B-spline basis for representing polyno-\\nmial splines. We also discuss their use in the computations of smoothing\\nsplines.\\nB-splines\\nBefore we can get started, we need to augment the knot sequence deﬁned\\nin Section 5.2. Let ξ0< ξ1andξK< ξK+1be two boundary knots, which\\ntypically deﬁne the domain over which we wish to evaluate our spline. We\\nnow deﬁne the augmented knot sequence τsuch that\\n•τ1≤τ2≤ ≤≤≤ ≤ τM≤ξ0;\\n•τj+M=ξj, j= 1,≤≤≤,K;\\n•ξK+1≤τK+M+1≤τK+M+2≤ ≤≤≤ ≤ τK+2M.\\nThe actual values of these additional knots beyond the boundary are arbi-\\ntrary, and it is customary to make them all the same and equal to ξ0and\\nξK+1, respectively.\\nDenote by Bi,m(x) the ithB-spline basis function of order mfor the\\nknot-sequence τ,m≤M. They are deﬁned recursively in terms of divided'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 205}, page_content='Appendix: Computations for Splines 187\\ndiﬀerences as follows:\\nBi,1(x) ={\\n1 ifτi≤x < τ i+1\\n0 otherwise(5.77)\\nfori= 1,... ,K + 2M−1. These are also known as Haar basis functions.\\nBi,m(x) =x−τi\\nτi+m−1−τiBi,m−1(x) +τi+m−x\\nτi+m−τi+1Bi+1,m−1(x)\\nfori= 1,... ,K + 2M−m.\\n(5.78)\\nThus with M= 4,Bi,4, i= 1,≤≤≤,K+ 4 are the K+ 4 cubic B-spline\\nbasis functions for the knot sequence ξ. This recursion can be contin-\\nued and will generate the B-spline basis for any order spline. Figure 5.20\\nshows the sequence of B-splines up to order four with knots at the points\\n0.0,0.1,... ,1.0. Since we have created some duplicate knots, some care\\nhas to be taken to avoid division by zero. If we adopt the convention\\nthatBi,1= 0 if τi=τi+1, then by induction Bi,m= 0 if τi=τi+1=\\n...=τi+m. Note also that in the construction above, only the subset\\nBi,m, i=M−m+ 1,... ,M +Kare required for the B-spline basis\\nof order m < M with knots ξ.\\nTo fully understand the properties of these functions, and to show that\\nthey do indeed span the space of cubic splines for the knot sequence, re-\\nquires additional mathematical machinery, including the properties of di-\\nvided diﬀerences. Exercise 5.2 explores these issues.\\nThe scope of B-splines is in fact bigger than advertised here, and has to\\ndo with knot duplication. If we duplicate an interior knot in the construc-\\ntion of the τsequence above, and then generate the B-spline sequence as\\nbefore, the resulting basis spans the space of piecewise polynomials with\\none less continuous derivative at the duplicated knot. In general, if in ad-\\ndition to the repeated boundary knots, we include the interior knot ξj\\n1≤rj≤Mtimes, then the lowest-order derivative to be discontinuous\\natx=ξjwill be order M−rj. Thus for cubic splines with no repeats,\\nrj= 1, j= 1,... ,K , and at each interior knot the third derivatives (4 −1)\\nare discontinuous. Repeating the jth knot three times leads to a discontin-\\nuous 1st derivative; repeating it four times leads to a discontinuous zeroth\\nderivative, i.e., the function is discontinuous at x=ξj. This is exactly what\\nhappens at the boundary knots; we repeat the knots Mtimes, so the spline\\nbecomes discontinuous at the boundary knots (i.e., undeﬁned beyond the\\nboundary).\\nThe local support of B-splines has important computational implica-\\ntions, especially when the number of knots Kis large. Least squares com-\\nputations with Nobservations and K+Mvariables (basis functions) take\\nO(N(K+M)2+ (K+M)3) ﬂops (ﬂoating point operations.) If Kis some\\nappreciable fraction of N, this leads to O(N3) algorithms which becomes'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 206}, page_content='188 5. Basis Expansions and Regularization\\nB-splines of Order 1\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nB-splines of Order 2\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nB-splines of Order 3\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nB-splines of Order 4\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\\nFIGURE 5.20. The sequence of B-splines up to order four with ten knots evenly\\nspaced from 0to1. The B-splines have local support ; they are nonzero on an\\ninterval spanned by M+ 1knots.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 207}, page_content='Appendix: Computations for Splines 189\\nunacceptable for large N. If the Nobservations are sorted, the N×(K+M)\\nregression matrix consisting of the K+M B-spline basis functions evalu-\\nated at the Npoints has many zeros, which can be exploited to reduce the\\ncomputational complexity back to O(N). We take this up further in the\\nnext section.\\nComputations for Smoothing Splines\\nAlthough natural splines (Section 5.2.1) provide a basis for smoothing\\nsplines, it is computationally more convenient to operate in the larger space\\nof unconstrained B-splines. We write f(x) =∑N+4\\n1γjBj(x), where γjare\\ncoeﬃcients and the Bjare the cubic B-spline basis functions. The solution\\nlooks the same as before,\\nˆγ= (BTB+λΩB)−1BTy, (5.79)\\nexcept now the N×Nmatrix Nis replaced by the N×(N+ 4) matrix\\nB, and similarly the ( N+ 4)×(N+ 4) penalty matrix ΩBreplaces the\\nN×Ndimensional ΩN. Although at face value it seems that there are\\nno boundary derivative constraints, it turns out that the penalty term\\nautomatically imposes them by giving eﬀectively inﬁnite weight to any non\\nzero derivative beyond the boundary. In practice, ˆ γis restricted to a linear\\nsubspace for which the penalty is always ﬁnite.\\nSince the columns of Bare the evaluated B-splines, in order from left\\nto right and evaluated at the sorted values of X, and the cubic B-splines\\nhave local support, Bis lower 4-banded. Consequently the matrix M=\\n(BTB+λΩ) is 4-banded and hence its Cholesky decomposition M=LLT\\ncan be computed easily. One then solves LLTγ=BTyby back-substitution\\nto give γand hence the solution ˆfinO(N) operations.\\nIn practice, when Nis large, it is unnecessary to use all Ninterior knots,\\nand any reasonable thinning strategy will save in computations and have\\nnegligible eﬀect on the ﬁt. For example, the smooth.spline function in S-\\nPLUS uses an approximately logarithmic strategy: if N <50 all knots are\\nincluded, but even at N= 5,000 only 204 knots are used.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 208}, page_content='190 5. Basis Expansions and Regularization'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 209}, page_content='This is page 191\\nPrinter: Opaque this\\n6\\nKernel Smoothing Methods\\nIn this chapter we describe a class of regression techniques that achieve\\nﬂexibility in estimating the regression function f(X) over the domain IRp\\nby ﬁtting a diﬀerent but simple model separately at each query point x0.\\nThis is done by using only those observations close to the target point x0to\\nﬁt the simple model, and in such a way that the resulting estimated function\\nˆf(X) issmooth in IRp. This localization is achieved via a weighting function\\norkernel Kλ(x0,xi), which assigns a weight to xibased on its distance from\\nx0. The kernels Kλare typically indexed by a parameter λthat dictates\\nthe width of the neighborhood. These memory-based methods require in\\nprinciple little or no training; all the work gets done at evaluation time.\\nThe only parameter that needs to be determined from the training data is\\nλ. The model, however, is the entire training data set.\\nWe also discuss more general classes of kernel-based techniques , which\\ntie in with structured methods in other chapters, and are useful for density\\nestimation and classiﬁcation.\\nThe techniques in this chapter should not be confused with those asso-\\nciated with the more recent usage of the phrase “kernel methods”. In this\\nchapter kernels are mostly used as a device for localization. We discuss ker-\\nnel methods in Sections 5.8, 14.5.4, 18.5 and Chapter 12; in those contexts\\nthe kernel computes an inner product in a high-dimensional (implicit) fea-\\nture space, and is used for regularized nonlinear modeling. We make some\\nconnections to the methodology in this chapter at the end of Section 6.7.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 210}, page_content='192 6. Kernel Smoothing Methods\\nNearest-Neighbor Kernel\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOOOO\\nOO\\nOO\\nOOOO\\nOO\\nOO\\nOOOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nO\\nOO\\nOO\\nO\\nOOOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO•\\nx0ˆf(x0)Epanechnikov Kernel\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOOOO\\nOO\\nOO\\nOOOO\\nOO\\nOO\\nOOOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOO\\nO\\nO\\nOO\\nO\\nOO\\nOO\\nO\\nOO\\nO\\nOOO\\nOO\\nOOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOO\\nO\\nOO•\\nx0ˆf(x0)\\nFIGURE 6.1. In each panel 100pairs xi, yiare generated at random from the\\nblue curve with Gaussian errors: Y= sin(4 X)+ε,X∼U[0,1],ε∼N(0,1/3). In\\nthe left panel the green curve is the result of a 30-nearest-neighbor running-mean\\nsmoother. The red point is the ﬁtted constant ˆf(x0), and the red circles indicate\\nthose observations contributing to the ﬁt at x0. The solid yellow region indicates\\nthe weights assigned to observations. In the right panel, the gr een curve is the\\nkernel-weighted average, using an Epanechnikov kernel with (hal f) window width\\nλ= 0.2.\\n6.1 One-Dimensional Kernel Smoothers\\nIn Chapter 2, we motivated the k–nearest-neighbor average\\nˆf(x) = Ave( yi|xi∈Nk(x)) (6.1)\\nas an estimate of the regression function E( Y|X=x). Here Nk(x) is the set\\nofkpoints nearest to xin squared distance, and Ave denotes the average\\n(mean). The idea is to relax the deﬁnition of conditional expectation, as\\nillustrated in the left panel of Figure 6.1, and compute an average in a\\nneighborhood of the target point. In this case we have used the 30-nearest\\nneighborhood—the ﬁt at x0is the average of the 30 pairs whose xivalues\\nare closest to x0. The green curve is traced out as we apply this deﬁnition\\nat diﬀerent values x0. The green curve is bumpy, since ˆf(x) is discontinuous\\ninx. As we move x0from left to right, the k-nearest neighborhood remains\\nconstant, until a point xito the right of x0becomes closer than the furthest\\npoint xi′in the neighborhood to the left of x0, at which time xireplaces xi′.\\nThe average in (6.1) changes in a discrete way, leading to a discontinuous\\nˆf(x).\\nThis discontinuity is ugly and unnecessary. Rather than give all the\\npoints in the neighborhood equal weight, we can assign weights that die\\noﬀ smoothly with distance from the target point. The right panel shows\\nan example of this, using the so-called Nadaraya–Watson kernel-weighted'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 211}, page_content='6.1 One-Dimensional Kernel Smoothers 193\\naverage\\nˆf(x0) =∑N\\ni=1Kλ(x0,xi)yi∑N\\ni=1Kλ(x0,xi), (6.2)\\nwith the Epanechnikov quadratic kernel\\nKλ(x0,x) =D(|x−x0|\\nλ)\\n, (6.3)\\nwith\\nD(t) ={3\\n4(1−t2) if|t| ≤1;\\n0 otherwise .(6.4)\\nThe ﬁtted function is now continuous, and quite smooth in the right panel\\nof Figure 6.1. As we move the target from left to right, points enter t he\\nneighborhood initially with weight zero, and then their contribution slowly\\nincreases (see Exercise 6.1).\\nIn the right panel we used a metric window size λ= 0.2 for the kernel\\nﬁt, which does not change as we move the target point x0, while the size\\nof the 30-nearest-neighbor smoothing window adapts to the local density\\nof the xi. One can, however, also use such adaptive neighborhoods with\\nkernels, but we need to use a more general notation. Let hλ(x0) be a width\\nfunction (indexed by λ) that determines the width of the neighborhood at\\nx0. Then more generally we have\\nKλ(x0,x) =D(|x−x0|\\nhλ(x0))\\n. (6.5)\\nIn (6.3), hλ(x0) =λis constant. For k-nearest neighborhoods, the neigh-\\nborhood size kreplaces λ, and we have hk(x0) =|x0−x[k]|where x[k]is\\nthekth closest xitox0.\\nThere are a number of details that one has to attend to in practice:\\n•The smoothing parameter λ, which determines the width of the local\\nneighborhood, has to be determined. Large λimplies lower variance\\n(averages over more observations) but higher bias (we essentially as-\\nsume the true function is constant within the window).\\n•Metric window widths (constant hλ(x)) tend to keep the bias of the\\nestimate constant, but the variance is inversely proportional to the\\nlocal density. Nearest-neighbor window widths exhibit the opposite\\nbehavior; the variance stays constant and the absolute bias varies\\ninversely with local density.\\n•Issues arise with nearest-neighbors when there are ties in the xi. With\\nmost smoothing techniques one can simply reduce the data set by\\naveraging the yiat tied values of X, and supplementing these new\\nobservations at the unique values of xiwith an additional weight wi\\n(which multiples the kernel weight).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 212}, page_content='194 6. Kernel Smoothing Methods\\n-3 -2 -1 0 1 2 30.0 0.4 0.8Epanechnikov\\nTri-cube\\nGaussianKλ(x0, x)\\nFIGURE 6.2. A comparison of three popular kernels for local smoothing. Eac h\\nhas been calibrated to integrate to 1. The tri-cube kernel is compact and has two\\ncontinuous derivatives at the boundary of its support, while th e Epanechnikov ker-\\nnel has none. The Gaussian kernel is continuously diﬀerentiable, bu t has inﬁnite\\nsupport.\\n•This leaves a more general problem to deal with: observation weights\\nwi. Operationally we simply multiply them by the kernel weights be-\\nfore computing the weighted average. With nearest neighborhoods, it\\nis now natural to insist on neighborhoods with a total weight content\\nk(relative to∑wi). In the event of overﬂow (the last observation\\nneeded in a neighborhood has a weight wjwhich causes the sum of\\nweights to exceed the budget k), then fractional parts can be used.\\n•Boundary issues arise. The metric neighborhoods tend to contain less\\npoints on the boundaries, while the nearest-neighborhoods get wider.\\n•The Epanechnikov kernel has compact support (needed when used\\nwith nearest-neighbor window size). Another popular compact kernel\\nis based on the tri-cube function\\nD(t) ={\\n(1− |t|3)3if|t| ≤1;\\n0 otherwise(6.6)\\nThis is ﬂatter on the top (like the nearest-neighbor box) and is diﬀer-\\nentiable at the boundary of its support. The Gaussian density func-\\ntionD(t) =φ(t) is a popular noncompact kernel, with the standard-\\ndeviation playing the role of the window size. Figure 6.2 compares\\nthe three.\\n6.1.1 Local Linear Regression\\nWe have progressed from the raw moving average to a smoothly varying\\nlocally weighted average by using kernel weighting. The smooth kernel ﬁt\\nstill has problems, however, as exhibited in Figure 6.3 (left panel). Locally-\\nweighted averages can be badly biased on the boundaries of the domain,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 213}, page_content='6.1 One-Dimensional Kernel Smoothers 195\\nN-W Kernel at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nO\\nO\\nOO\\nOOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\n•\\nx0ˆf(x0)Local Linear Regression at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOOO\\nO\\nOO\\nOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOOOO\\nO\\nO\\nOO\\nOOO\\nOO\\nO\\nOOO\\nOO\\nO\\nOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nOO\\nOO\\nOO\\nOO\\nOOOOO\\nOOO\\nOOO\\nOOO\\nO\\nOO\\nOO\\n•\\nx0ˆf(x0)\\nFIGURE 6.3. The locally weighted average has bias problems at or near the\\nboundaries of the domain. The true function is approximately line ar here, but\\nmost of the observations in the neighborhood have a higher mean than the target\\npoint, so despite weighting, their mean will be biased upwards . By ﬁtting a locally\\nweighted linear regression (right panel), this bias is remove d to ﬁrst order\\nbecause of the asymmetry of the kernel in that region. By ﬁtting straight\\nlines rather than constants locally, we can remove this bias exactly to ﬁrst\\norder; see Figure 6.3 (right panel). Actually, this bias can be present in the\\ninterior of the domain as well, if the Xvalues are not equally spaced (for\\nthe same reasons, but usually less severe). Again locally weighted linear\\nregression will make a ﬁrst-order correction.\\nLocally weighted regression solves a separate weighted least squares prob-\\nlem at each target point x0:\\nmin\\nα(x0),β(x0)N∑\\ni=1Kλ(x0,xi)[yi−α(x0)−β(x0)xi]2. (6.7)\\nThe estimate is then ˆf(x0) = ˆα(x0) +ˆβ(x0)x0. Notice that although we ﬁt\\nan entire linear model to the data in the region, we only use it to evaluate\\nthe ﬁt at the single point x0.\\nDeﬁne the vector-valued function b(x)T= (1,x). Let Bbe the N×2\\nregression matrix with ith row b(xi)T, andW(x0) the N×Ndiagonal\\nmatrix with ith diagonal element Kλ(x0,xi). Then\\nˆf(x0) = b(x0)T(BTW(x0)B)−1BTW(x0)y (6.8)\\n=N∑\\ni=1li(x0)yi. (6.9)\\nEquation (6.8) gives an explicit expression for the local linear regression\\nestimate, and (6.9) highlights the fact that the estimate is linear in the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 214}, page_content='196 6. Kernel Smoothing Methods\\nLocal Linear Equivalent Kernel at Boundary\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOO\\nOO\\nOOO\\nOO\\nOOO\\nOOO\\nO\\nOOOOOOO\\nOO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOOOO\\nOO\\nOO\\nOOOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOO\\nOOOO\\nOO\\nO\\nOOO\\nOOOOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOO\\nOOO\\nOO\\nOO\\nOOO\\nOO\\nOOO\\nOOO\\nO\\nOOOOOO\\n•••••••••\\n•\\n••\\n••\\n•••••••••••••• • ••••• •• • • • ••• • ••• • •• ••• • •• ••••• ••• • ••• •••• •• • •• • • •• • •• •••• ••• • • • ••••••\\nx0ˆf(x0)Local Linear Equivalent Kernel in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\\nOO\\nOO\\nOOO\\nOO\\nOOO\\nOOO\\nO\\nOOOOOOO\\nOO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOOOO\\nOO\\nOO\\nOOOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOO\\nOOOO\\nOO\\nO\\nOOO\\nOOOOO\\nO\\nOO\\nOO\\nOOOO\\nOO\\nOOO\\nOO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOOOO\\nOO\\nOO\\nOOOO\\nOO\\nO\\nOO\\nO\\nOO\\nOO\\nOO\\nO\\nOO\\nOOO\\nOOO•\\n•• • ••• •••• •••• • •••••••• ••• ••••••••••••••••••••••••••••••••••••••\\n•••\\n••\\n• •• • ••• • •• • •• •••• ••• • • • ••••••\\nx0ˆf(x0)\\nFIGURE 6.4. The green points show the equivalent kernel li(x0)for local re-\\ngression. These are the weights in ˆf(x0) =PN\\ni=1li(x0)yi, plotted against their\\ncorresponding xi. For display purposes, these have been rescaled, since in fac t\\nthey sum to 1. Since the yellow shaded region is the (rescaled) equivalent ke rnel\\nfor the Nadaraya–Watson local average, we see how local regr ession automati-\\ncally modiﬁes the weighting kernel to correct for biases due to a symmetry in the\\nsmoothing window.\\nyi(theli(x0) do not involve y). These weights li(x0) combine the weight-\\ning kernel Kλ(x0,≤) and the least squares operations, and are sometimes\\nreferred to as the equivalent kernel . Figure 6.4 illustrates the eﬀect of lo-\\ncal linear regression on the equivalent kernel. Historically, the bias in the\\nNadaraya–Watson and other local average kernel methods were corrected\\nby modifying the kernel. These modiﬁcations were based on theoretical\\nasymptotic mean-square-error considerations, and besides being tedious to\\nimplement, are only approximate for ﬁnite sample sizes. Local linear re-\\ngression automatically modiﬁes the kernel to correct the bias exactly to\\nﬁrst order, a phenomenon dubbed as automatic kernel carpentry . Consider\\nthe following expansion for E ˆf(x0), using the linearity of local regression\\nand a series expansion of the true function faround x0,\\nEˆf(x0) =N∑\\ni=1li(x0)f(xi)\\n=f(x0)N∑\\ni=1li(x0) +f′(x0)N∑\\ni=1(xi−x0)li(x0)\\n+f′′(x0)\\n2N∑\\ni=1(xi−x0)2li(x0) +R, (6.10)\\nwhere the remainder term Rinvolves third- and higher-order derivatives of\\nf, and is typically small under suitable smoothness assumptions. It can be'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 215}, page_content='6.1 One-Dimensional Kernel Smoothers 197\\nLocal Linear in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOOOO\\nOOOO\\nOOOOO\\nOO\\nOO\\nOO\\nO\\nOO\\nO\\nO\\nOO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nO•ˆf(x0)Local Quadratic in Interior\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO\\nO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nOOO\\nO\\nO\\nO\\nOOO\\nOOOO\\nOOOO\\nOOOOO\\nOO\\nOO\\nOO\\nO\\nOO\\nO\\nO\\nOO\\nOO\\nOOO\\nOOO\\nO\\nOOO\\nO\\nOOO\\nOO\\nO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOOO\\nOO\\nOOO\\nOO\\nOO\\nO\\nOO\\nO\\nOOOOO\\nO•ˆf(x0)\\nFIGURE 6.5. Local linear ﬁts exhibit bias in regions of curvature of the true\\nfunction. Local quadratic ﬁts tend to eliminate this bias.\\nshown (Exercise 6.2) that for local linear regression,∑N\\ni=1li(x0) = 1 and∑N\\ni=1(xi−x0)li(x0) = 0. Hence the middle term equals f(x0), and since\\nthe bias is E ˆf(x0)−f(x0), we see that it depends only on quadratic and\\nhigher–order terms in the expansion of f.\\n6.1.2 Local Polynomial Regression\\nWhy stop at local linear ﬁts? We can ﬁt local polynomial ﬁts of any de-\\ngreed,\\nmin\\nα(x0),βj(x0), j=1,...,dN∑\\ni=1Kλ(x0,xi)\\uf8ee\\n\\uf8f0yi−α(x0)−d∑\\nj=1βj(x0)xj\\ni\\uf8f9\\n\\uf8fb2\\n(6.11)\\nwith solution ˆf(x0) = ˆα(x0)+∑d\\nj=1ˆβj(x0)xj\\n0. In fact, an expansion such as\\n(6.10) will tell us that the bias will only have components of degree d+1 and\\nhigher (Exercise 6.2). Figure 6.5 illustrates local quadratic regression. Local\\nlinear ﬁts tend to be biased in regions of curvature of the true function, a\\nphenomenon referred to as trimming the hills andﬁlling the valleys . Local\\nquadratic regression is generally able to correct this bias.\\nThere is of course a price to be paid for this bias reduction, and that is\\nincreased variance. The ﬁt in the right panel of Figure 6.5 is slightly more\\nwiggly, especially in the tails. Assuming the model yi=f(xi) +εi, with\\nεiindependent and identically distributed with mean zero and variance\\nσ2, Var( ˆf(x0)) =σ2||l(x0)||2, where l(x0) is the vector of equivalent kernel\\nweights at x0. It can be shown (Exercise 6.3) that ||l(x0)||increases with d,\\nand so there is a bias–variance tradeoﬀ in selecting the polynomial degree.\\nFigure 6.6 illustrates these variance curves for degree zero, one and two'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 216}, page_content='198 6. Kernel Smoothing Methods\\nVariance\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5Constant\\nLinear\\nQuadratic\\nFIGURE 6.6. The variances functions ||l(x)||2for local constant, linear and\\nquadratic regression, for a metric bandwidth ( λ= 0.2) tri-cube kernel.\\nlocal polynomials. To summarize some collected wisdom on this issue:\\n•Local linear ﬁts can help bias dramatically at the boundaries at a\\nmodest cost in variance. Local quadratic ﬁts do little at the bound-\\naries for bias, but increase the variance a lot.\\n•Local quadratic ﬁts tend to be most helpful in reducing bias due to\\ncurvature in the interior of the domain.\\n•Asymptotic analysis suggest that local polynomials of odd degree\\ndominate those of even degree. This is largely due to the fact that\\nasymptotically the MSE is dominated by boundary eﬀects.\\nWhile it may be helpful to tinker, and move from local linear ﬁts at the\\nboundary to local quadratic ﬁts in the interior, we do not recommend such\\nstrategies. Usually the application will dictate the degree of the ﬁt. For\\nexample, if we are interested in extrapolation, then the boundary is of\\nmore interest, and local linear ﬁts are probably more reliable.\\n6.2 Selecting the Width of the Kernel\\nIn each of the kernels Kλ,λis a parameter that controls its width:\\n•For the Epanechnikov or tri-cube kernel with metric width, λis the\\nradius of the support region.\\n•For the Gaussian kernel, λis the standard deviation.\\n•λis the number kof nearest neighbors in k-nearest neighborhoods,\\noften expressed as a fraction or spank/Nof the total training sample.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 217}, page_content='6.2 Selecting the Width of the Kernel 199\\n•••\\n••••••\\n•\\n••\\n••\\n•••••\\n••••• ••••••• •••• •• ••• ••••• • • ••••• •••• ••• • ••••• •••••••• • • • •• •• •••• • ••• • ••••••••••••••••\\n••\\n••\\n••••••\\n••\\n•• ••••••• •••• •• ••• •••••• • ••••• ••••••• • ••••• ••••••••• • • •• •• •••• • ••• • •••••• •••••• •• ••••• •• •• • •••• •• •• ••••••••••••••••••••• •• ••••• •••••\\n•\\n••••••• ••••••••• • • •• •• •••• • ••• • •••••• •••••• ••••••• •• •• • •••••• ••••••••••••••••••••••• • • •••••••••••\\n•••••••••••••••• •• •• •• •••• •••• • ••••••\\nFIGURE 6.7. Equivalent kernels for a local linear regression smoother (tri -cube\\nkernel; orange) and a smoothing spline (blue), with matching degre es of freedom.\\nThe vertical spikes indicates the target points.\\nThere is a natural bias–variance tradeoﬀ as we change the width of the\\naveraging window, which is most explicit for local averages:\\n•If the window is narrow, ˆf(x0) is an average of a small number of yi\\nclose to x0, and its variance will be relatively large—close to that of\\nan individual yi. The bias will tend to be small, again because each\\nof the E(yi) =f(xi) should be close to f(x0).\\n•If the window is wide, the variance of ˆf(x0) will be small relative to\\nthe variance of any yi, because of the eﬀects of averaging. The bias\\nwill be higher, because we are now using observations xifurther from\\nx0, and there is no guarantee that f(xi) will be close to f(x0).\\nSimilar arguments apply to local regression estimates, say local linear: a s\\nthe width goes to zero, the estimates approach a piecewise-linear function\\nthat interpolates the training data1; as the width gets inﬁnitely large, the\\nﬁt approaches the global linear least-squares ﬁt to the data.\\nThe discussion in Chapter 5 on selecting the regularization parameter for\\nsmoothing splines applies here, and will not be repeated. Local regression\\nsmoothers are linear estimators; the smoother matrix in ˆf=Sλyis built up\\nfrom the equivalent kernels (6.8), and has ijth entry {Sλ}ij=li(xj). Leave-\\none-out cross-validation is particularly simple (Exercise 6.7), as is genera l-\\nized cross-validation, Cp(Exercise 6.10), and k-fold cross-validation. The\\neﬀective degrees of freedom is again deﬁned as trace( Sλ), and can be used\\nto calibrate the amount of smoothing. Figure 6.7 compares the equivalent\\nkernels for a smoothing spline and local linear regression. The local regres-\\nsion smoother has a span of 40%, which results in df = trace( Sλ) = 5.86.\\nThe smoothing spline was calibrated to have the same df, and their equiv-\\nalent kernels are qualitatively quite similar.\\n1With uniformly spaced xi; with irregularly spaced xi, the behavior can deteriorate.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 218}, page_content='200 6. Kernel Smoothing Methods\\n6.3 Local Regression in I Rp\\nKernel smoothing and local regression generalize very naturally to two or\\nmore dimensions. The Nadaraya–Watson kernel smoother ﬁts a constant\\nlocally with weights supplied by a p-dimensional kernel. Local linear re-\\ngression will ﬁt a hyperplane locally in X, by weighted least squares, with\\nweights supplied by a p-dimensional kernel. It is simple to implement and\\nis generally preferred to the local constant ﬁt for its superior performance\\non the boundaries.\\nLetb(X) be a vector of polynomial terms in Xof maximum degree d.\\nFor example, with d= 1 and p= 2 we get b(X) = (1 ,X1,X2); with d= 2\\nwe get b(X) = (1 ,X1,X2,X2\\n1,X2\\n2,X1X2); and trivially with d= 0 we get\\nb(X) = 1. At each x0∈IRpsolve\\nmin\\nβ(x0)N∑\\ni=1Kλ(x0,xi)(yi−b(xi)Tβ(x0))2(6.12)\\nto produce the ﬁt ˆf(x0) =b(x0)Tˆβ(x0). Typically the kernel will be a radial\\nfunction, such as the radial Epanechnikov or tri-cube kernel\\nKλ(x0,x) =D(||x−x0||\\nλ)\\n, (6.13)\\nwhere ||≤||is the Euclidean norm. Since the Euclidean norm depends on the\\nunits in each coordinate, it makes most sense to standardize each predictor,\\nfor example, to unit standard deviation, prior to smoothing.\\nWhile boundary eﬀects are a problem in one-dimensional smoothing,\\nthey are a much bigger problem in two or higher dimensions, since the\\nfraction of points on the boundary is larger. In fact, one of the manifesta -\\ntions of the curse of dimensionality is that the fraction of points close to the\\nboundary increases to one as the dimension grows. Directly modifying the\\nkernel to accommodate two-dimensional boundaries becomes very messy,\\nespecially for irregular boundaries. Local polynomial regression seamless ly\\nperforms boundary correction to the desired order in any dimensions. Fig-\\nure 6.8 illustrates local linear regression on some measurements from an\\nastronomical study with an unusual predictor design (star-shaped). Here\\nthe boundary is extremely irregular, and the ﬁtted surface must also inter-\\npolate over regions of increasing data sparsity as we approach the boundary.\\nLocal regression becomes less useful in dimensions much higher than two\\nor three. We have discussed in some detail the problems of dimensional-\\nity, for example, in Chapter 2. It is impossible to simultaneously main-\\ntain localness ( ⇒low bias) and a sizable sample in the neighborhood ( ⇒\\nlow variance) as the dimension increases, without the total sample size in-\\ncreasing exponentially in p. Visualization of ˆf(X) also becomes diﬃcult in\\nhigher dimensions, and this is often one of the primary goals of smoothing.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 219}, page_content='6.4 Structured Local Regression Models in I Rp201\\nEast-WestSouth-NorthVelocity\\nEast-WestSouth-NorthVelocity\\nFIGURE 6.8. The left panel shows three-dimensional data, where the response\\nis the velocity measurements on a galaxy, and the two predictors record positions\\non the celestial sphere. The unusual “star”-shaped design ind icates the way the\\nmeasurements were made, and results in an extremely irregular b oundary. The\\nright panel shows the results of local linear regression smoot hing in I R2, using a\\nnearest-neighbor window with 15%of the data.\\nAlthough the scatter-cloud and wire-frame pictures in Figure 6.8 look at-\\ntractive, it is quite diﬃcult to interpret the results except at a gross level.\\nFrom a data analysis perspective, conditional plots are far more useful.\\nFigure 6.9 shows an analysis of some environmental data with three pre-\\ndictors. The trellis display here shows ozone as a function of radiation,\\nconditioned on the other two variables, temperature and wind speed. How-\\never, conditioning on the value of a variable really implies local to that\\nvalue (as in local regression). Above each of the panels in Figure 6.9 is an\\nindication of the range of values present in that panel for each of the condi-\\ntioning values. In the panel itself the data subsets are displayed (response\\nversus remaining variable), and a one-dimensional local linear regression is\\nﬁt to the data. Although this is not quite the same as looking at slices of\\na ﬁtted three-dimensional surface, it is probably more useful in terms of\\nunderstanding the joint behavior of the data.\\n6.4 Structured Local Regression Models in I Rp\\nWhen the dimension to sample-size ratio is unfavorable, local regression\\ndoes not help us much, unless we are willing to make some structural as-\\nsumptions about the model. Much of this book is about structured regres-\\nsion and classiﬁcation models. Here we focus on some approaches directly\\nrelated to kernel methods.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 220}, page_content='202 6. Kernel Smoothing Methods\\n12345TempWind\\n0 50 150 250TempWind\\nTempWind\\n0 50 150 250TempWindTempWind\\nTempWind\\nTempWind\\n12345TempWind12345TempWind\\nTempWind\\nTempWind\\nTempWindTempWind\\nTempWind0 50 150 250\\nTempWind\\n12345TempWind0 50 150 250\\nSolar Radiation (langleys)Cube Root Ozone (cube root ppb)\\nFIGURE 6.9. Three-dimensional smoothing example. The response is (cube-roo t\\nof) ozone concentration, and the three predictors are temperatur e, wind speed and\\nradiation. The trellis display shows ozone as a function of radiation, conditioned\\non intervals of temperature and wind speed (indicated by darker g reen or orange\\nshaded bars). Each panel contains about 40%of the range of each of the condi-\\ntioned variables. The curve in each panel is a univariate local l inear regression,\\nﬁt to the data in the panel.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 221}, page_content='6.4 Structured Local Regression Models in I Rp203\\n6.4.1 Structured Kernels\\nOne line of approach is to modify the kernel. The default spherical ker-\\nnel (6.13) gives equal weight to each coordinate, and so a natural default\\nstrategy is to standardize each variable to unit standard deviation. A more\\ngeneral approach is to use a positive semideﬁnite matrix Ato weigh the\\ndiﬀerent coordinates:\\nKλ,A(x0,x) =D((x−x0)TA(x−x0)\\nλ)\\n. (6.14)\\nEntire coordinates or directions can be downgraded or omitted by imposing\\nappropriate restrictions on A. For example, if Ais diagonal, then we can\\nincrease or decrease the inﬂuence of individual predictors Xjby increasing\\nor decreasing Ajj. Often the predictors are many and highly correlated,\\nsuch as those arising from digitized analog signals or images. The covari ance\\nfunction of the predictors can be used to tailor a metric Athat focuses less,\\nsay, on high-frequency contrasts (Exercise 6.4). Proposals have been made\\nfor learning the parameters for multidimensional kernels. For example, the\\nprojection-pursuit regression model discussed in Chapter 11 is of this ﬂavor,\\nwhere low-rank versions of Aimply ridge functions for ˆf(X). More general\\nmodels for Aare cumbersome, and we favor instead the structured forms\\nfor the regression function discussed next.\\n6.4.2 Structured Regression Functions\\nWe are trying to ﬁt a regression function E(Y|X) =f(X1,X2,... ,X p) in\\nIRp, in which every level of interaction is potentially present. It is natural\\nto consider analysis-of-variance (ANOVA) decompositions of the form\\nf(X1,X2,... ,X p) =α+∑\\njgj(Xj) +∑\\nk<ℓgkℓ(Xk,Xℓ) +≤≤≤ (6.15)\\nand then introduce structure by eliminating some of the higher-order terms.\\nAdditive models assume only main eﬀect terms: f(X) =α+∑p\\nj=1gj(Xj);\\nsecond-order models will have terms with interactions of order at most\\ntwo, and so on. In Chapter 9, we describe iterative backﬁtting algorithms\\nfor ﬁtting such low-order interaction models. In the additive model, for\\nexample, if all but the kth term is assumed known, then we can estimate gk\\nby local regression of Y−∑\\nj̸=kgj(Xj) onXk. This is done for each function\\nin turn, repeatedly, until convergence. The important detail is that at any\\nstage, one-dimensional local regression is all that is needed. The same ideas\\ncan be used to ﬁt low-dimensional ANOVA decompositions.\\nAn important special case of these structured models are the class of\\nvarying coeﬃcient models . Suppose, for example, that we divide the ppre-\\ndictors in Xinto a set ( X1,X2,... ,X q) with q < p, and the remainder of'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 222}, page_content='204 6. Kernel Smoothing Methods\\n1012141618202224DepthFemale\\n20 30 40 50 60DepthFemale\\nDepthFemale\\n20 30 40 50 60DepthFemale\\nDepthFemale\\n20 30 40 50 60DepthFemaleDepthMale\\nDepthMale20 30 40 50 60\\nDepthMale\\nDepthMale20 30 40 50 60\\nDepthMale\\n1012141618202224DepthMale20 30 40 50 60\\nAgeDiameterAortic Diameter vs Age\\nFIGURE 6.10. In each panel the aorta diameter is modeled as a linear func-\\ntion ofage. The coeﬃcients of this model vary with gender anddepth down\\ntheaorta (left is near the top, right is low down). There is a clear trend in the\\ncoeﬃcients of the linear model.\\nthe variables we collect in the vector Z. We then assume the conditionally\\nlinear model\\nf(X) =α(Z) +β1(Z)X1+≤≤≤+βq(Z)Xq. (6.16)\\nFor given Z, this is a linear model, but each of the coeﬃcients can vary\\nwithZ. It is natural to ﬁt such a model by locally weighted least squares:\\nmin\\nα(z0),β(z0)N∑\\ni=1Kλ(z0,zi)(yi−α(z0)−x1iβ1(z0)− ≤≤≤ − xqiβq(z0))2.\\n(6.17)\\nFigure 6.10 illustrates the idea on measurements of the human aorta.\\nA longstanding claim has been that the aorta thickens with age. Here we\\nmodel the diameter of the aorta as a linear function of age, but allow the\\ncoeﬃcients to vary with gender anddepth down the aorta. We used a local\\nregression model separately for males and females. While the aorta clearly\\ndoes thicken with age at the higher regions of the aorta, the relationship\\nfades with distance down the aorta. Figure 6.11 shows the intercept and\\nslope as a function of depth.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 223}, page_content='6.5 Local Likelihood and Other Models 205\\nMaleAge Intercept\\nDistance Down AortaAge Slope\\n0.0 0.2 0.4 0.6 0.8 1.0Female\\n14 16 18 20\\nDistance Down Aorta\\n0.0 0.4 0.8 1.2\\n0.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.11. The intercept and slope of ageas a function of distance down\\nthe aorta, separately for males and females. The yellow bands i ndicate one stan-\\ndard error.\\n6.5 Local Likelihood and Other Models\\nThe concept of local regression and varying coeﬃcient models is extremely\\nbroad: any parametric model can be made local if the ﬁtting method ac-\\ncommodates observation weights. Here are some examples:\\n•Associated with each observation yiis a parameter θi=θ(xi) =xT\\niβ\\nlinear in the covariate(s) xi, and inference for βis based on the log-\\nlikelihood l(β) =∑N\\ni=1l(yi,xT\\niβ). We can model θ(X) more ﬂexibly\\nby using the likelihood local to x0for inference of θ(x0) =xT\\n0β(x0):\\nl(β(x0)) =N∑\\ni=1Kλ(x0,xi)l(yi,xT\\niβ(x0)).\\nMany likelihood models, in particular the family of generalized linear\\nmodels including logistic and log-linear models, involve the covariates\\nin a linear fashion. Local likelihood allows a relaxation from a global ly\\nlinear model to one that is locally linear.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 224}, page_content='206 6. Kernel Smoothing Methods\\n•As above, except diﬀerent variables are associated with θfrom those\\nused for deﬁning the local likelihood:\\nl(θ(z0)) =N∑\\ni=1Kλ(z0,zi)l(yi,η(xi,θ(z0))).\\nFor example, η(x,θ) =xTθcould be a linear model in x. This will ﬁt\\na varying coeﬃcient model θ(z) by maximizing the local likelihood.\\n•Autoregressive time series models of order khave the form yt=\\nβ0+β1yt−1+β2yt−2+≤≤≤+βkyt−k+εt. Denoting the lag set by\\nzt= (yt−1,yt−2,... ,y t−k), the model looks like a standard linear\\nmodel yt=zT\\ntβ+εt, and is typically ﬁt by least squares. Fitting\\nby local least squares with a kernel K(z0,zt) allows the model to\\nvary according to the short-term history of the series. This is to be\\ndistinguished from the more traditional dynamic linear models that\\nvary by windowing time.\\nAs an illustration of local likelihood, we consider the local version of the\\nmulticlass linear logistic regression model (4.36) of Chapter 4. The data\\nconsist of features xiand an associated categorical response gi∈ {1,2,... ,J },\\nand the linear model has the form\\nPr(G=j|X=x) =eβj0+βT\\njx\\n1 +∑J−1\\nk=1eβk0+βT\\nkx. (6.18)\\nThe local log-likelihood for this Jclass model can be written\\nN∑\\ni=1Kλ(x0,xi){\\nβgi0(x0) +βgi(x0)T(xi−x0)\\n−log[\\n1 +J−1∑\\nk=1exp(\\nβk0(x0) +βk(x0)T(xi−x0))]}\\n.\\n(6.19)\\nNotice that\\n•we have used gias a subscript in the ﬁrst line to pick out the appro-\\npriate numerator;\\n•βJ0= 0 and βJ= 0 by the deﬁnition of the model;\\n•we have centered the local regressions at x0, so that the ﬁtted poste-\\nrior probabilities at x0are simply\\nˆPr(G=j|X=x0) =eˆβj0(x0)\\n1 +∑J−1\\nk=1eˆβk0(x0). (6.20)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 225}, page_content='6.5 Local Likelihood and Other Models 207\\nSystolic Blood PressurePrevalence CHD\\n100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0\\nObesityPrevalence CHD\\n15 25 35 450.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.12. Each plot shows the binary response CHD (coronary heart dis-\\nease) as a function of a risk factor for the South African heart d isease data.\\nFor each plot we have computed the ﬁtted prevalence of CHD using a local linear\\nlogistic regression model. The unexpected increase in the prev alence of CHD at\\nthe lower ends of the ranges is because these are retrospective data, and some of\\nthe subjects had already undergone treatment to reduce their bl ood pressure and\\nweight. The shaded region in the plot indicates an estimated p ointwise standard\\nerror band.\\nThis model can be used for ﬂexible multiclass classiﬁcation in moderately\\nlow dimensions, although successes have been reported with the high-\\ndimensional ZIP-code classiﬁcation problem. Generalized additive models\\n(Chapter 9) using kernel smoothing methods are closely related, and avoid\\ndimensionality problems by assuming an additive structure for the regres-\\nsion function.\\nAs a simple illustration we ﬁt a two-class local linear logistic model to\\nthe heart disease data of Chapter 4. Figure 6.12 shows the univariate local\\nlogistic models ﬁt to two of the risk factors (separately). This is a useful\\nscreening device for detecting nonlinearities, when the data themselves have\\nlittle visual information to oﬀer. In this case an unexpected anomaly is\\nuncovered in the data, which may have gone unnoticed with traditional\\nmethods.\\nSinceCHDis a binary indicator, we could estimate the conditional preva-\\nlence Pr( G=j|x0) by simply smoothing this binary response directly with-\\nout resorting to a likelihood formulation. This amounts to ﬁtting a locall y\\nconstant logistic regression model (Exercise 6.5). In order to enjoy the bia s-\\ncorrection of local-linear smoothing, it is more natural to operate on the\\nunrestricted logit scale.\\nTypically with logistic regression, we compute parameter estimates as\\nwell as their standard errors. This can be done locally as well, and so'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 226}, page_content='208 6. Kernel Smoothing Methods\\nSystolic Blood Pressure (for CHD group)Density Estimate\\n100 120 140 160 180 200 2200.0 0.005 0.010 0.015 0.020\\nFIGURE 6.13. A kernel density estimate for systolic blood pressure (for the\\nCHD group). The density estimate at each point is the average co ntribution from\\neach of the kernels at that point. We have scaled the kernels down by a factor of\\n10 to make the graph readable.\\nwe can produce, as shown in the plot, estimated pointwise standard-error\\nbands about our ﬁtted prevalence.\\n6.6 Kernel Density Estimation and Classiﬁcation\\nKernel density estimation is an unsupervised learning procedure, which\\nhistorically precedes kernel regression. It also leads naturally to a simple\\nfamily of procedures for nonparametric classiﬁcation.\\n6.6.1 Kernel Density Estimation\\nSuppose we have a random sample x1,... ,x Ndrawn from a probability\\ndensity fX(x), and we wish to estimate fXat a point x0. For simplicity we\\nassume for now that X∈IR. Arguing as before, a natural local estimate\\nhas the form\\nˆfX(x0) =#xi∈ N(x0)\\nNλ, (6.21)\\nwhere N(x0) is a small metric neighborhood around x0of width λ. This\\nestimate is bumpy, and the smooth Parzen estimate is preferred\\nˆfX(x0) =1\\nNλN∑\\ni=1Kλ(x0,xi), (6.22)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 227}, page_content='6.6 Kernel Density Estimation and Classiﬁcation 209\\nSystolic Blood PressureDensity Estimates\\n100 140 180 2200.0 0.010 0.020CHD\\nno CHD\\nSystolic Blood PressurePosterior Estimate\\n100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0\\nFIGURE 6.14. The left panel shows the two separate density estimates for\\nsystolic blood pressure in the CHD versus no-CHD groups, using a Gaussian\\nkernel density estimate in each. The right panel shows the estim ated posterior\\nprobabilities for CHD, using (6.25).\\nbecause it counts observations close to x0with weights that decrease with\\ndistance from x0. In this case a popular choice for Kλis the Gaussian kernel\\nKλ(x0,x) =φ(|x−x0|/λ). Figure 6.13 shows a Gaussian kernel density ﬁt\\nto the sample values for systolic blood pressure for theCHDgroup. Letting\\nφλdenote the Gaussian density with mean zero and standard-deviation λ,\\nthen (6.22) has the form\\nˆfX(x) =1\\nNN∑\\ni=1φλ(x−xi)\\n= (ˆF ⋆ φ λ)(x), (6.23)\\nthe convolution of the sample empirical distribution ˆFwithφλ. The dis-\\ntribution ˆF(x) puts mass 1 /Nat each of the observed xi, and is jumpy; in\\nˆfX(x) we have smoothed ˆFby adding independent Gaussian noise to each\\nobservation xi.\\nThe Parzen density estimate is the equivalent of the local average, and\\nimprovements have been proposed along the lines of local regression [on the\\nlog scale for densities; see Loader (1999)]. We will not pursue these here.\\nIn IRpthe natural generalization of the Gaussian density estimate amounts\\nto using the Gaussian product kernel in (6.23),\\nˆfX(x0) =1\\nN(2λ2π)p\\n2N∑\\ni=1e−1\\n2(||xi−x0||/λ)2. (6.24)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 228}, page_content='210 6. Kernel Smoothing Methods\\n0.00.51.0\\nFIGURE 6.15. The population class densities may have interesting structure\\n(left) that disappears when the posterior probabilities ar e formed (right).\\n6.6.2 Kernel Density Classiﬁcation\\nOne can use nonparametric density estimates for classiﬁcation in a straight-\\nforward fashion using Bayes’ theorem. Suppose for a Jclass problem we ﬁt\\nnonparametric density estimates ˆfj(X), j= 1,... ,J separately in each of\\nthe classes, and we also have estimates of the class priors ˆ πj(usually the\\nsample proportions). Then\\nˆPr(G=j|X=x0) =ˆπjˆfj(x0)∑J\\nk=1ˆπkˆfk(x0). (6.25)\\nFigure 6.14 uses this method to estimate the prevalence of CHD for the\\nheart risk factor study, and should be compared with the left panel of Fig-\\nure 6.12. The main diﬀerence occurs in the region of high SBP in the right\\npanel of Figure 6.14. In this region the data are sparse for both classes, a nd\\nsince the Gaussian kernel density estimates use metric kernels, the density\\nestimates are low and of poor quality (high variance) in these regions. The\\nlocal logistic regression method (6.20) uses the tri-cube kernel with k-NN\\nbandwidth; this eﬀectively widens the kernel in this region, and makes use\\nof the local linear assumption to smooth out the estimate (on the logit\\nscale).\\nIf classiﬁcation is the ultimate goal, then learning the separate class den-\\nsities well may be unnecessary, and can in fact be misleading. Figure 6.15\\nshows an example where the densities are both multimodal, but the pos-\\nterior ratio is quite smooth. In learning the separate densities from data,\\none might decide to settle for a rougher, high-variance ﬁt to capture these\\nfeatures, which are irrelevant for the purposes of estimating the posterior\\nprobabilities. In fact, if classiﬁcation is the ultimate goal, then we need onl y\\nto estimate the posterior well near the decision boundary (for two classes,\\nthis is the set {x|Pr(G= 1|X=x) =1\\n2}).\\n6.6.3 The Naive Bayes Classiﬁer\\nThis is a technique that has remained popular over the years, despite its\\nname (also known as “Idiot’s Bayes”!) It is especially appropriate when'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 229}, page_content='6.6 Kernel Density Estimation and Classiﬁcation 211\\nthe dimension pof the feature space is high, making density estimation\\nunattractive. The naive Bayes model assumes that given a class G=j, the\\nfeatures Xkare independent:\\nfj(X) =p∏\\nk=1fjk(Xk). (6.26)\\nWhile this assumption is generally not true, it does simplify the estimation\\ndramatically:\\n•The individual class-conditional marginal densities fjkcan each be\\nestimated separately using one-dimensional kernel density estimates.\\nThis is in fact a generalization of the original naive Bayes procedures,\\nwhich used univariate Gaussians to represent these marginals.\\n•If a component XjofXis discrete, then an appropriate histogram\\nestimate can be used. This provides a seamless way of mixing variable\\ntypes in a feature vector.\\nDespite these rather optimistic assumptions, naive Bayes classiﬁers often\\noutperform far more sophisticated alternatives. The reasons are related to\\nFigure 6.15: although the individual class density estimates may be biased,\\nthis bias might not hurt the posterior probabilities as much, especially\\nnear the decision regions. In fact, the problem may be able to withstand\\nconsiderable bias for the savings in variance such a “naive” assumption\\nearns.\\nStarting from (6.26) we can derive the logit-transform (using class Jas\\nthe base):\\nlogPr(G=ℓ|X)\\nPr(G=J|X)= logπℓfℓ(X)\\nπJfJ(X)\\n= logπℓ∏p\\nk=1fℓk(Xk)\\nπJ∏p\\nk=1fJk(Xk)\\n= logπℓ\\nπJ+p∑\\nk=1logfℓk(Xk)\\nfJk(Xk)\\n=αℓ+p∑\\nk=1gℓk(Xk).(6.27)\\nThis has the form of a generalized additive model , which is described in more\\ndetail in Chapter 9. The models are ﬁt in quite diﬀerent ways though; their\\ndiﬀerences are explored in Exercise 6.9. The relationship between naive\\nBayes and generalized additive models is analogous to that between linear\\ndiscriminant analysis and logistic regression (Section 4.4.5).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 230}, page_content='212 6. Kernel Smoothing Methods\\n6.7 Radial Basis Functions and Kernels\\nIn Chapter 5, functions are represented as expansions in basis functions:\\nf(x) =∑M\\nj=1βjhj(x). The art of ﬂexible modeling using basis expansions\\nconsists of picking an appropriate family of basis functions, and then con-\\ntrolling the complexity of the representation by selection, regularization, or\\nboth. Some of the families of basis functions have elements that are deﬁned\\nlocally; for example, B-splines are deﬁned locally in IR. If more ﬂexibility\\nis desired in a particular region, then that region needs to be represented\\nby more basis functions (which in the case of B-splines translates to more\\nknots). Tensor products of IR-local basis functions deliver basis functions\\nlocal in IRp. Not all basis functions are local—for example, the truncated\\npower bases for splines, or the sigmoidal basis functions σ(α0+αx) used\\nin neural-networks (see Chapter 11). The composed function f(x) can nev-\\nertheless show local behavior, because of the particular signs and values\\nof the coeﬃcients causing cancellations of global eﬀects. For example, the\\ntruncated power basis has an equivalent B-spline basis for the same space\\nof functions; the cancellation is exact in this case.\\nKernel methods achieve ﬂexibility by ﬁtting simple models in a region\\nlocal to the target point x0. Localization is achieved via a weighting kernel\\nKλ, and individual observations receive weights Kλ(x0,xi).\\nRadial basis functions combine these ideas, by treating the kernel func-\\ntionsKλ(ξ,x) as basis functions. This leads to the model\\nf(x) =M∑\\nj=1Kλj(ξj,x)βj\\n=M∑\\nj=1D(||x−ξj||\\nλj)\\nβj, (6.28)\\nwhere each basis element is indexed by a location or prototype parameter ξj\\nand a scale parameter λj. A popular choice for Dis the standard Gaussian\\ndensity function. There are several approaches to learning the parameters\\n{λj,ξj,βj}, j= 1,... ,M . For simplicity we will focus on least squares\\nmethods for regression, and use the Gaussian kernel.\\n•Optimize the sum-of-squares with respect to all the parameters:\\nmin\\n{λj,ξj,βj}M\\n1N∑\\ni=1\\uf8eb\\n\\uf8edyi−β0−M∑\\nj=1βjexp{\\n−(xi−ξj)T(xi−ξj)\\nλ2\\nj}\\uf8f6\\n\\uf8f82\\n.\\n(6.29)\\nThis model is commonly referred to as an RBF network, an alterna-\\ntive to the sigmoidal neural network discussed in Chapter 11; the ξj\\nandλjplaying the role of the weights. This criterion is nonconvex'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 231}, page_content='6.7 Radial Basis Functions and Kernels 2130 2 4 6 8 0.0 0.4 0.8 1.2\\nFIGURE 6.16. Gaussian radial basis functions in I Rwith ﬁxed width can leave\\nholes (top panel). Renormalized Gaussian radial basis functio ns avoid this prob-\\nlem, and produce basis functions similar in some respects to B-splines.\\nwith multiple local minima, and the algorithms for optimization are\\nsimilar to those used for neural networks.\\n•Estimate the {λj,ξj}separately from the βj. Given the former, the\\nestimation of the latter is a simple least squares problem. Often the\\nkernel parameters λjandξjare chosen in an unsupervised way using\\ntheXdistribution alone. One of the methods is to ﬁt a Gaussian\\nmixture density model to the training xi, which provides both the\\ncenters ξjand the scales λj. Other even more adhoc approaches use\\nclustering methods to locate the prototypes ξj, and treat λj=λ\\nas a hyper-parameter. The obvious drawback of these approaches is\\nthat the conditional distribution Pr( Y|X) and in particular E(Y|X)\\nis having no say in where the action is concentrated. On the positive\\nside, they are much simpler to implement.\\nWhile it would seem attractive to reduce the parameter set and assume\\na constant value for λj=λ, this can have an undesirable side eﬀect of\\ncreating holes—regions of IRpwhere none of the kernels has appreciable\\nsupport, as illustrated in Figure 6.16 (upper panel). Renormalized radial\\nbasis functions,\\nhj(x) =D(||x−ξj||/λ)∑M\\nk=1D(||x−ξk||/λ), (6.30)\\navoid this problem (lower panel).\\nThe Nadaraya–Watson kernel regression estimator (6.2) in IRpcan be\\nviewed as an expansion in renormalized radial basis functions,\\nˆf(x0) =∑N\\ni=1yiKλ(x0,xi)PN\\ni=1Kλ(x0,xi)\\n=∑N\\ni=1yihi(x0) (6.31)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 232}, page_content='214 6. Kernel Smoothing Methods\\nwith a basis function hilocated at every observation and coeﬃcients yi;\\nthat is, ξi=xi,ˆβi=yi, i= 1,... ,N .\\nNote the similarity between the expansion (6.31) and the solution (5.50)\\non page 169 to the regularization problem induced by the kernel K. Radial\\nbasis functions form the bridge between the modern “kernel methods” and\\nlocal ﬁtting technology.\\n6.8 Mixture Models for Density Estimation and\\nClassiﬁcation\\nThe mixture model is a useful tool for density estimation, and can be viewed\\nas a kind of kernel method. The Gaussian mixture model has the form\\nf(x) =M∑\\nm=1αmφ(x;θm,Σm) (6.32)\\nwith mixing proportions αm,∑\\nmαm= 1, and each Gaussian density has\\na mean θmand covariance matrix Σm. In general, mixture models can use\\nany component densities in place of the Gaussian in (6.32): the Gaussian\\nmixture model is by far the most popular.\\nThe parameters are usually ﬁt by maximum likelihood, using the EM\\nalgorithm as described in Chapter 8. Some special cases arise:\\n•If the covariance matrices are constrained to be scalar: Σm=σmI,\\nthen (6.32) has the form of a radial basis expansion.\\n•If in addition σm=σ >0 is ﬁxed, and M↑N, then the max-\\nimum likelihood estimate for (6.32) approaches the kernel density\\nestimate (6.22) where ˆ αm= 1/Nand ˆθm=xm.\\nUsing Bayes’ theorem, separate mixture densities in each class lead to ﬂex-\\nible models for Pr( G|X); this is taken up in some detail in Chapter 12.\\nFigure 6.17 shows an application of mixtures to the heart disease risk-\\nfactor study. In the top row are histograms of Agefor theno CHD andCHD\\ngroups separately, and then combined on the right. Using the combined\\ndata, we ﬁt a two-component mixture of the form (6.32) with the (scalars)\\nΣ1andΣ2not constrained to be equal. Fitting was done via the EM\\nalgorithm (Chapter 8): note that the procedure does not use knowledge of\\ntheCHDlabels. The resulting estimates were\\nˆθ1= 36.4, ˆΣ1= 157 .7, ˆα1= 0.7,\\nˆθ2= 58.0, ˆΣ2= 15.6, ˆα2= 0.3.\\nThe component densities φ(ˆθ1,ˆΣ1) and φ(ˆθ2,ˆΣ2) are shown in the lower-\\nleft and middle panels. The lower-right panel shows these component den-\\nsities (orange and blue) along with the estimated mixture density (green).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 233}, page_content='6.8 Mixture Models for Density Estimation and Classiﬁcation 215\\nNo CHD\\nAgeCount\\n20 30 40 50 600 5 10 15 20CHD\\nAgeCount\\n20 30 40 50 600 5 10 15Combined\\nAgeCount\\n20 30 40 50 600 5 10 15 20 25 30\\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\\nAgeMixture Estimate\\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\\nAgeMixture Estimate\\n20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\\nAgeMixture Estimate\\nFIGURE 6.17. Application of mixtures to the heart disease risk-factor stu dy.\\n(Top row:) Histograms of Agefor theno CHD andCHDgroups separately, and\\ncombined. (Bottom row:) estimated component densities from a Ga ussian mix-\\nture model, (bottom left, bottom middle); (bottom right:) E stimated component\\ndensities (blue and orange) along with the estimated mixture densi ty (green). The\\norange density has a very large standard deviation, and approximat es a uniform\\ndensity.\\nThe mixture model also provides an estimate of the probability that\\nobservation ibelongs to component m,\\nˆrim=ˆαmφ(xi; ˆθm,ˆΣm)∑M\\nk=1ˆαkφ(xi; ˆθk,ˆΣk), (6.33)\\nwhere xiisAgein our example. Suppose we threshold each value ˆ ri2and\\nhence deﬁne ˆδi=I(ˆri2>0.5). Then we can compare the classiﬁcation of\\neach observation by CHDand the mixture model:\\nMixture model\\nˆδ= 0 ˆδ= 1\\nCHD No 232 70\\nYes 76 84\\nAlthough the mixture model did not use the CHDlabels, it has done a fair\\njob in discovering the two CHDsubpopulations. Linear logistic regression,\\nusing the CHDas a response, achieves the same error rate (32%) when ﬁt to\\nthese data using maximum-likelihood (Section 4.4).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 234}, page_content='216 6. Kernel Smoothing Methods\\n6.9 Computational Considerations\\nKernel and local regression and density estimation are memory-based meth-\\nods: the model is the entire training data set, and the ﬁtting is done at\\nevaluation or prediction time. For many real-time applications, this can\\nmake this class of methods infeasible.\\nThe computational cost to ﬁt at a single observation x0isO(N) ﬂops,\\nexcept in oversimpliﬁed cases (such as square kernels). By comparison,\\nan expansion in Mbasis functions costs O(M) for one evaluation, and\\ntypically M∼O(logN). Basis function methods have an initial cost of at\\nleastO(NM2+M3).\\nThe smoothing parameter(s) λfor kernel methods are typically deter-\\nmined oﬀ-line, for example using cross-validation, at a cost of O(N2) ﬂops.\\nPopular implementations of local regression, such as the loess function in\\nS-PLUS and Rand the locfit procedure (Loader, 1999), use triangulation\\nschemes to reduce the computations. They compute the ﬁt exactly at M\\ncarefully chosen locations ( O(NM)), and then use blending techniques to\\ninterpolate the ﬁt elsewhere ( O(M) per evaluation).\\nBibliographic Notes\\nThere is a vast literature on kernel methods which we will not attempt to\\nsummarize. Rather we will point to a few good references that themselves\\nhave extensive bibliographies. Loader (1999) gives excellent coverage of lo-\\ncal regression and likelihood, and also describes state-of-the-art software\\nfor ﬁtting these models. Fan and Gijbels (1996) cover these models from\\na more theoretical aspect. Hastie and Tibshirani (1990) discuss local re-\\ngression in the context of additive modeling. Silverman (1986) gives a goo d\\noverview of density estimation, as does Scott (1992).\\nExercises\\nEx. 6.1 Show that the Nadaraya–Watson kernel smooth with ﬁxed metric\\nbandwidth λand a Gaussian kernel is diﬀerentiable. What can be said for\\nthe Epanechnikov kernel? What can be said for the Epanechnikov kernel\\nwith adaptive nearest-neighbor bandwidth λ(x0)?\\nEx. 6.2 Show that∑N\\ni=1(xi−x0)li(x0) = 0 for local linear regression. Deﬁne\\nbj(x0) =∑N\\ni=1(xi−x0)jli(x0). Show that b0(x0) = 1 for local polynomial\\nregression of any degree (including local constants). Show that bj(x0) = 0\\nfor all j∈ {1,2,... ,k }for local polynomial regression of degree k. What\\nare the implications of this on the bias?'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 235}, page_content='Exercises 217\\nEx. 6.3 Show that ||l(x)||(Section 6.1.2) increases with the degree of the\\nlocal polynomial.\\nEx. 6.4 Suppose that the ppredictors Xarise from sampling relatively\\nsmooth analog curves at puniformly spaced abscissa values. Denote by\\nCov(X|Y) =Σthe conditional covariance matrix of the predictors, and\\nassume this does not change much with Y. Discuss the nature of Maha-\\nlanobis choice A=Σ−1for the metric in (6.14). How does this compare\\nwithA=I? How might you construct a kernel Athat (a) downweights\\nhigh-frequency components in the distance metric; (b) ignores them\\ncompletely?\\nEx. 6.5 Show that ﬁtting a locally constant multinomial logit model of\\nthe form (6.19) amounts to smoothing the binary response indicators for\\neach class separately using a Nadaraya–Watson kernel smoother with kernel\\nweights Kλ(x0,xi).\\nEx. 6.6 Suppose that all you have is software for ﬁtting local regression,\\nbut you can specify exactly which monomials are included in the ﬁt. How\\ncould you use this software to ﬁt a varying-coeﬃcient model in some of the\\nvariables?\\nEx. 6.7 Derive an expression for the leave-one-out cross-validated residual\\nsum-of-squares for local polynomial regression.\\nEx. 6.8 Suppose that for continuous response Yand predictor X, we model\\nthe joint density of X,Yusing a multivariate Gaussian kernel estimator.\\nNote that the kernel in this case would be the product kernel φλ(X)φλ(Y).\\nShow that the conditional mean E(Y|X) derived from this estimate is a\\nNadaraya–Watson estimator. Extend this result to classiﬁcation by pro-\\nviding a suitable kernel for the estimation of the joint distribution of a\\ncontinuous Xand discrete Y.\\nEx. 6.9 Explore the diﬀerences between the naive Bayes model (6.27) and\\na generalized additive logistic regression model, in terms of (a) model as-\\nsumptions and (b) estimation. If all the variables Xkare discrete, what can\\nyou say about the corresponding GAM?\\nEx. 6.10 Suppose we have Nsamples generated from the model yi=f(xi)+\\nεi, with εiindependent and identically distributed with mean zero and\\nvariance σ2, thexiassumed ﬁxed (non random). We estimate fusing a\\nlinear smoother (local regression, smoothing spline, etc.) with smoothing\\nparameter λ. Thus the vector of ﬁtted values is given by ˆf=Sλy. Consider\\nthein-sample prediction error\\nPE(λ) = E1\\nNN∑\\ni=1(y∗\\ni−ˆfλ(xi))2(6.34)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 236}, page_content='218 6. Kernel Smoothing Methods\\nfor predicting new responses at the Ninput values. Show that the aver-\\nage squared residual on the training data, ASR( λ), is a biased estimate\\n(optimistic) for PE( λ), while\\nCλ= ASR( λ) +2σ2\\nNtrace(Sλ) (6.35)\\nis unbiased.\\nEx. 6.11 Show that for the Gaussian mixture model (6.32) the likelihood\\nis maximized at + ∞, and describe how.\\nEx. 6.12 Write a computer program to perform a local linear discrimi-\\nnant analysis. At each query point x0, the training data receive weights\\nKλ(x0,xi) from a weighting kernel, and the ingredients for the linear deci-\\nsion boundaries (see Section 4.3) are computed by weighted averages. Try\\nout your program on the zipcode data, and show the training and test er-\\nrors for a series of ﬁve pre-chosen values of λ. Thezipcode data are available\\nfrom the book website www-stat.stanford.edu/ElemStatLearn .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 237}, page_content='This is page 219\\nPrinter: Opaque this\\n7\\nModel Assessment and Selection\\n7.1 Introduction\\nThegeneralization performance of a learning method relates to its predic-\\ntion capability on independent test data. Assessment of this performance\\nis extremely important in practice, since it guides the choice of learning\\nmethod or model, and gives us a measure of the quality of the ultimately\\nchosen model.\\nIn this chapter we describe and illustrate the key methods for perfor-\\nmance assessment, and show how they are used to select models. We begin\\nthe chapter with a discussion of the interplay between bias, variance and\\nmodel complexity.\\n7.2 Bias, Variance and Model Complexity\\nFigure 7.1 illustrates the important issue in assessing the ability of a learn-\\ning method to generalize. Consider ﬁrst the case of a quantitative or interval\\nscale response. We have a target variable Y, a vector of inputs X, and a\\nprediction model ˆf(X) that has been estimated from a training set T.\\nThe loss function for measuring errors between Yandˆf(X) is denoted by\\nL(Y,ˆf(X)). Typical choices are\\nL(Y,ˆf(X)) ={\\n(Y−ˆf(X))2squared error\\n|Y−ˆf(X)| absolute error .(7.1)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 238}, page_content='220 7. Model Assessment and Selection\\n0 5 10 15 20 25 30 350.0 0.2 0.4 0.6 0.8 1.0 1.2\\nModel Complexity (df)Prediction ErrorHigh Bias Low Bias\\nHigh Variance Low Variance\\nFIGURE 7.1. Behavior of test sample and training sample error as the model\\ncomplexity is varied. The light blue curves show the training er rorerr, while the\\nlight red curves show the conditional test error ErrTfor100training sets of size\\n50each, as the model complexity is increased. The solid curves sh ow the expected\\ntest error Errand the expected training error E[err].\\nTest error , also referred to as generalization error , is the prediction error\\nover an independent test sample\\nErrT= E[L(Y,ˆf(X))|T] (7.2)\\nwhere both XandYare drawn randomly from their joint distribution\\n(population). Here the training set Tis ﬁxed, and test error refers to the\\nerror for this speciﬁc training set. A related quantity is the expected pre-\\ndiction error (or expected test error)\\nErr = E[ L(Y,ˆf(X))] = E[Err T]. (7.3)\\nNote that this expectation averages over everything that is random, includ-\\ning the randomness in the training set that produced ˆf.\\nFigure 7.1 shows the prediction error (light red curves) Err Tfor 100\\nsimulated training sets each of size 50. The lasso (Section 3.4.2) was used\\nto produce the sequence of ﬁts. The solid red curve is the average, and\\nhence an estimate of Err.\\nEstimation of Err Twill be our goal, although we will see that Err is\\nmore amenable to statistical analysis, and most methods eﬀectively esti-\\nmate the expected error. It does not seem possible to estimate conditional'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 239}, page_content='7.2 Bias, Variance and Model Complexity 221\\nerror eﬀectively, given only the information in the same training set. Some\\ndiscussion of this point is given in Section 7.12.\\nTraining error is the average loss over the training sample\\nerr =1\\nNN∑\\ni=1L(yi,ˆf(xi)). (7.4)\\nWe would like to know the expected test error of our estimated model\\nˆf. As the model becomes more and more complex, it uses the training\\ndata more and is able to adapt to more complicated underlying structures.\\nHence there is a decrease in bias but an increase in variance. There is some\\nintermediate model complexity that gives minimum expected test error.\\nUnfortunately training error is not a good estimate of the test error,\\nas seen in Figure 7.1. Training error consistently decreases with model\\ncomplexity, typically dropping to zero if we increase the model complexity\\nenough. However, a model with zero training error is overﬁt to the training\\ndata and will typically generalize poorly.\\nThe story is similar for a qualitative or categorical response Gtaking\\none of Kvalues in a set G, labeled for convenience as 1 ,2,... ,K . Typically\\nwe model the probabilities pk(X) = Pr( G=k|X) (or some monotone\\ntransformations fk(X)), and then ˆG(X) = arg max kˆpk(X). In some cases,\\nsuch as 1-nearest neighbor classiﬁcation (Chapters 2 and 13) we produce\\nˆG(X) directly. Typical loss functions are\\nL(G,ˆG(X)) = I(G̸=ˆG(X)) (0–1 loss) , (7.5)\\nL(G,ˆp(X)) = −2K∑\\nk=1I(G=k)log ˆpk(X)\\n=−2log ˆpG(X) (−2×log-likelihood) .(7.6)\\nThe quantity −2×the log-likelihood is sometimes referred to as the\\ndeviance .\\nAgain, test error here is Err T= E[L(G,ˆG(X))|T], the population mis-\\nclassiﬁcation error of the classiﬁer trained on T, and Err is the expected\\nmisclassiﬁcation error.\\nTraining error is the sample analogue, for example,\\nerr =−2\\nNN∑\\ni=1log ˆpgi(xi), (7.7)\\nthe sample log-likelihood for the model.\\nThe log-likelihood can be used as a loss-function for general response\\ndensities, such as the Poisson, gamma, exponential, log-normal and others.\\nIf Pr θ(X)(Y) is the density of Y, indexed by a parameter θ(X) that depends\\non the predictor X, then\\nL(Y,θ(X)) =−2≤log Pr θ(X)(Y). (7.8)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 240}, page_content='222 7. Model Assessment and Selection\\nThe “−2” in the deﬁnition makes the log-likelihood loss for the Gaussian\\ndistribution match squared-error loss.\\nFor ease of exposition, for the remainder of this chapter we will use Yand\\nf(X) to represent all of the above situations, since we focus mainly on the\\nquantitative response (squared-error loss) setting. For the other situati ons,\\nthe appropriate translations are obvious.\\nIn this chapter we describe a number of methods for estimating the\\nexpected test error for a model. Typically our model will have a tuning\\nparameter or parameters αand so we can write our predictions as ˆfα(x).\\nThe tuning parameter varies the complexity of our model, and we wish to\\nﬁnd the value of αthat minimizes error, that is, produces the minimum of\\nthe average test error curve in Figure 7.1. Having said this, for brevity w e\\nwill often suppress the dependence of ˆf(x) onα.\\nIt is important to note that there are in fact two separate goals that we\\nmight have in mind:\\nModel selection: estimating the performance of diﬀerent models in order\\nto choose the best one.\\nModel assessment: having chosen a ﬁnal model, estimating its predic-\\ntion error (generalization error) on new data.\\nIf we are in a data-rich situation, the best approach for both problems is\\nto randomly divide the dataset into three parts: a training set, a validation\\nset, and a test set. The training set is used to ﬁt the models; the validation\\nset is used to estimate prediction error for model selection; the test set is\\nused for assessment of the generalization error of the ﬁnal chosen model.\\nIdeally, the test set should be kept in a “vault,” and be brought out only\\nat the end of the data analysis. Suppose instead that we use the test-set\\nrepeatedly, choosing the model with smallest test-set error. Then the test\\nset error of the ﬁnal chosen model will underestimate the true test error,\\nsometimes substantially.\\nIt is diﬃcult to give a general rule on how to choose the number of\\nobservations in each of the three parts, as this depends on the signal-to-\\nnoise ratio in the data and the training sample size. A typical split might\\nbe 50% for training, and 25% each for validation and testing:\\nTest Train Validation TestTrain Validation Test Validation Train Validation TestTrain\\nThe methods in this chapter are designed for situations where there is\\ninsuﬃcient data to split it into three parts. Again it is too diﬃcult to give\\na general rule on how much training data is enough; among other things,\\nthis depends on the signal-to-noise ratio of the underlying function, and\\nthe complexity of the models being ﬁt to the data.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 241}, page_content='7.3 The Bias–Variance Decomposition 223\\nThe methods of this chapter approximate the validation step either an-\\nalytically (AIC, BIC, MDL, SRM) or by eﬃcient sample re-use (cross-\\nvalidation and the bootstrap). Besides their use in model selection, we also\\nexamine to what extent each method provides a reliable estimate of test\\nerror of the ﬁnal chosen model.\\nBefore jumping into these topics, we ﬁrst explore in more detail the\\nnature of test error and the bias–variance tradeoﬀ.\\n7.3 The Bias–Variance Decomposition\\nAs in Chapter 2, if we assume that Y=f(X) +εwhere E( ε) = 0 and\\nVar(ε) =σ2\\nε, we can derive an expression for the expected prediction error\\nof a regression ﬁt ˆf(X) at an input point X=x0, using squared-error loss:\\nErr(x0) = E[(Y−ˆf(x0))2|X=x0]\\n=σ2\\nε+ [Eˆf(x0)−f(x0)]2+E[ˆf(x0)−Eˆf(x0)]2\\n=σ2\\nε+ Bias2(ˆf(x0)) + Var( ˆf(x0))\\n= Irreducible Error + Bias2+ Variance . (7.9)\\nThe ﬁrst term is the variance of the target around its true mean f(x0), and\\ncannot be avoided no matter how well we estimate f(x0), unless σ2\\nε= 0.\\nThe second term is the squared bias, the amount by which the average of\\nour estimate diﬀers from the true mean; the last term is the variance; the\\nexpected squared deviation of ˆf(x0) around its mean. Typically the more\\ncomplex we make the model ˆf, the lower the (squared) bias but the higher\\nthe variance.\\nFor the k-nearest-neighbor regression ﬁt, these expressions have the sim-\\nple form\\nErr(x0) = E[(Y−ˆfk(x0))2|X=x0]\\n=σ2\\nε+[\\nf(x0)−1\\nkk∑\\nℓ=1f(x(ℓ))]2\\n+σ2\\nε\\nk. (7.10)\\nHere we assume for simplicity that training inputs xiare ﬁxed, and the ran-\\ndomness arises from the yi. The number of neighbors kis inversely related\\nto the model complexity. For small k, the estimate ˆfk(x) can potentially\\nadapt itself better to the underlying f(x). As we increase k, the bias—the\\nsquared diﬀerence between f(x0) and the average of f(x) at the k-nearest\\nneighbors—will typically increase, while the variance decreases.\\nFor a linear model ﬁt ˆfp(x) =xTˆβ, where the parameter vector βwith\\npcomponents is ﬁt by least squares, we have\\nErr(x0) = E[(Y−ˆfp(x0))2|X=x0]'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 242}, page_content='224 7. Model Assessment and Selection\\n=σ2\\nε+ [f(x0)−Eˆfp(x0)]2+||h(x0)||2σ2\\nε.(7.11)\\nHereh(x0) =X(XTX)−1x0, theN-vector of linear weights that produce\\nthe ﬁt ˆfp(x0) =x0T(XTX)−1XTy, and hence Var[ ˆfp(x0)] =||h(x0)||2σ2\\nε.\\nWhile this variance changes with x0, its average (with x0taken to be each\\nof the sample values xi) is (p/N)σ2\\nε, and hence\\n1\\nNN∑\\ni=1Err(xi) =σ2\\nε+1\\nNN∑\\ni=1[f(xi)−Eˆf(xi)]2+p\\nNσ2\\nε, (7.12)\\nthein-sample error. Here model complexity is directly related to the num-\\nber of parameters p.\\nThe test error Err( x0) for a ridge regression ﬁt ˆfα(x0) is identical in\\nform to (7.11), except the linear weights in the variance term are diﬀerent:\\nh(x0) =X(XTX+αI)Tx0. The bias term will also be diﬀerent.\\nFor a linear model family such as ridge regression, we can break down\\nthe bias more ﬁnely. Let β∗denote the parameters of the best-ﬁtting linear\\napproximation to f:\\nβ∗= arg min\\nβE(\\nf(X)−XTβ)2. (7.13)\\nHere the expectation is taken with respect to the distribution of the input\\nvariables X. Then we can write the average squared bias as\\nEx0[\\nf(x0)−Eˆfα(x0)]2\\n= E x0[\\nf(x0)−xT\\n0β∗]2+ Ex0[\\nxT\\n0β∗−ExT\\n0ˆβα]2\\n= Ave[Model Bias]2+ Ave[Estimation Bias]2\\n(7.14)\\nThe ﬁrst term on the right-hand side is the average squared model bias , the\\nerror between the best-ﬁtting linear approximation and the true function.\\nThe second term is the average squared estimation bias , the error between\\nthe average estimate E( xT\\n0ˆβ) and the best-ﬁtting linear approximation.\\nFor linear models ﬁt by ordinary least squares, the estimation bias is zero.\\nFor restricted ﬁts, such as ridge regression, it is positive, and we trade i t oﬀ\\nwith the beneﬁts of a reduced variance. The model bias can only be reduced\\nby enlarging the class of linear models to a richer collection of models, by\\nincluding interactions and transformations of the variables in the model.\\nFigure 7.2 shows the bias–variance tradeoﬀ schematically. In the case\\nof linear models, the model space is the set of all linear predictions from\\npinputs and the black dot labeled “closest ﬁt” is xTβ∗. The blue-shaded\\nregion indicates the error σεwith which we see the truth in the training\\nsample.\\nAlso shown is the variance of the least squares ﬁt, indicated by the large\\nyellow circle centered at the black dot labeled “closest ﬁt in population,’'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 243}, page_content='7.3 The Bias–Variance Decomposition 225\\nRealizationClosest fit in population\\nEstimation BiasSPACE\\nVarianceEstimationClosest fit\\nTruth\\nModel bias\\nRESTRICTEDShrunken fit\\nMODELSPACEMODEL\\nFIGURE 7.2. Schematic of the behavior of bias and variance. The model space\\nis the set of all possible predictions from the model, with the “closest ﬁt” labeled\\nwith a black dot. The model bias from the truth is shown, along wi th the variance,\\nindicated by the large yellow circle centered at the black dot l abeled “closest ﬁt\\nin population.” A shrunken or regularized ﬁt is also shown, having additional\\nestimation bias, but smaller prediction error due to its dec reased variance.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 244}, page_content='226 7. Model Assessment and Selection\\nNow if we were to ﬁt a model with fewer predictors, or regularize the coef-\\nﬁcients by shrinking them toward zero (say), we would get the “shrunken\\nﬁt” shown in the ﬁgure. This ﬁt has an additional estimation bias, due to\\nthe fact that it is not the closest ﬁt in the model space. On the other hand,\\nit has smaller variance. If the decrease in variance exceeds the increase in\\n(squared) bias, then this is worthwhile.\\n7.3.1 Example: Bias–Variance Tradeoﬀ\\nFigure 7.3 shows the bias–variance tradeoﬀ for two simulated examples.\\nThere are 80 observations and 20 predictors, uniformly distributed in the\\nhypercube [0 ,1]20. The situations are as follows:\\nLeft panels: Yis 0 if X1≤1/2 and 1 if X1>1/2, and we apply k-nearest\\nneighbors.\\nRight panels: Yis 1 if∑10\\nj=1Xjis greater than 5 and 0 otherwise, and we\\nuse best subset linear regression of size p.\\nThe top row is regression with squared error loss; the bottom row is cla ssi-\\nﬁcation with 0–1 loss. The ﬁgures show the prediction error (red), squared\\nbias (green) and variance (blue), all computed for a large test sample.\\nIn the regression problems, bias and variance add to produce the predic-\\ntion error curves, with minima at about k= 5 for k-nearest neighbors, and\\np≥10 for the linear model. For classiﬁcation loss (bottom ﬁgures), some\\ninteresting phenomena can be seen. The bias and variance curves are the\\nsame as in the top ﬁgures, and prediction error now refers to misclassiﬁ-\\ncation rate. We see that prediction error is no longer the sum of squared\\nbias and variance. For the k-nearest neighbor classiﬁer, prediction error\\ndecreases or stays the same as the number of neighbors is increased to 20,\\ndespite the fact that the squared bias is rising. For the linear model classi-\\nﬁer the minimum occurs for p≥10 as in regression, but the improvement\\nover the p= 1 model is more dramatic. We see that bias and variance seem\\nto interact in determining prediction error.\\nWhy does this happen? There is a simple explanation for the ﬁrst phe-\\nnomenon. Suppose at a given input point, the true probability of class 1 is\\n0.9 while the expected value of our estimate is 0 .6. Then the squared bias—\\n(0.6−0.9)2—is considerable, but the prediction error is zero since we make\\nthe correct decision. In other words, estimation errors that leave us on the\\nright side of the decision boundary don’t hurt. Exercise 7.2 demonstrates\\nthis phenomenon analytically, and also shows the interaction eﬀect between\\nbias and variance.\\nThe overall point is that the bias–variance tradeoﬀ behaves diﬀerently\\nfor 0–1 loss than it does for squared error loss. This in turn means that\\nthe best choices of tuning parameters may diﬀer substantially in the two'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 245}, page_content='7.3 The Bias–Variance Decomposition 2270.0 0.1 0.2 0.3 0.4\\nNumber of Neighbors k50 40 30 20 10 0k−NN − Regression\\n5 10 15 200.0 0.1 0.2 0.3 0.4\\nSubset Size pLinear Model − Regression0.0 0.1 0.2 0.3 0.4\\nNumber of Neighbors k50 40 30 20 10 0k−NN − Classification\\n5 10 15 200.0 0.1 0.2 0.3 0.4\\nSubset Size pLinear Model − Classification\\nFIGURE 7.3. Expected prediction error (orange), squared bias (green) and va ri-\\nance (blue) for a simulated example. The top row is regression w ith squared error\\nloss; the bottom row is classiﬁcation with 0–1loss. The models are k-nearest\\nneighbors (left) and best subset regression of size p(right). The variance and bias\\ncurves are the same in regression and classiﬁcation, but the pre diction error curve\\nis diﬀerent.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 246}, page_content='228 7. Model Assessment and Selection\\nsettings. One should base the choice of tuning parameter on an estimate of\\nprediction error, as described in the following sections.\\n7.4 Optimism of the Training Error Rate\\nDiscussions of error rate estimation can be confusing, because we have\\nto make clear which quantities are ﬁxed and which are random1. Before\\nwe continue, we need a few deﬁnitions, elaborating on the material of Sec-\\ntion 7.2. Given a training set T={(x1,y1),(x2,y2),...(xN,yN)}the gen-\\neralization error of a model ˆfis\\nErrT= EX0,Y0[L(Y0,ˆf(X0))|T]; (7.15)\\nNote that the training set Tis ﬁxed in expression (7.15). The point ( X0,Y0)\\nis a new test data point, drawn from F, the joint distribution of the data.\\nAveraging over training sets Tyields the expected error\\nErr = E TEX0,Y0[L(Y0,ˆf(X0))|T], (7.16)\\nwhich is more amenable to statistical analysis. As mentioned earlier, it\\nturns out that most methods eﬀectively estimate the expected error rather\\nthan E T; see Section 7.12 for more on this point.\\nNow typically, the training error\\nerr =1\\nNN∑\\ni=1L(yi,ˆf(xi)) (7.17)\\nwill be less than the true error Err T, because the same data is being used\\nto ﬁt the method and assess its error (see Exercise 2.9). A ﬁtting method\\ntypically adapts to the training data, and hence the apparent or training\\nerror err will be an overly optimistic estimate of the generalization error\\nErrT.\\nPart of the discrepancy is due to where the evaluation points occur. The\\nquantity Err Tcan be thought of as extra-sample error, since the test input\\nvectors don’t need to coincide with the training input vectors. The nature\\nof the optimism in err is easiest to understand when we focus instead on\\nthein-sample error\\nErrin=1\\nNN∑\\ni=1EY0[L(Y0\\ni,ˆf(xi))|T] (7.18)\\nTheY0notation indicates that we observe Nnew response values at\\neach of the training points xi, i= 1,2,... ,N . We deﬁne the optimism as\\n1Indeed, in the ﬁrst edition of our book, this section wasn’t s uﬃciently clear.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 247}, page_content='7.4 Optimism of the Training Error Rate 229\\nthe diﬀerence between Err inand the training error err:\\nop≡Errin−err. (7.19)\\nThis is typically positive since err is usually biased downward as an estimate\\nof prediction error. Finally, the average optimism is the expectation of the\\noptimism over training sets\\nω≡Ey(op). (7.20)\\nHere the predictors in the training set are ﬁxed, and the expectation is\\nover the training set outcome values; hence we have used the notation E y\\ninstead of E T. We can usually estimate only the expected error ωrather\\nthan op, in the same way that we can estimate the expected error Err\\nrather than the conditional error Err T.\\nFor squared error, 0–1, and other loss functions, one can show quite\\ngenerally that\\nω=2\\nNN∑\\ni=1Cov(ˆyi,yi), (7.21)\\nwhere Cov indicates covariance. Thus the amount by which err underesti-\\nmates the true error depends on how strongly yiaﬀects its own prediction.\\nThe harder we ﬁt the data, the greater Cov(ˆ yi,yi) will be, thereby increas-\\ning the optimism. Exercise 7.4 proves this result for squared error loss where\\nˆyiis the ﬁtted value from the regression. For 0–1 loss, ˆ yi∈ {0,1}is the\\nclassiﬁcation at xi, and for entropy loss, ˆ yi∈[0,1] is the ﬁtted probability\\nof class 1 at xi.\\nIn summary, we have the important relation\\nEy(Errin) = E y(err) +2\\nNN∑\\ni=1Cov(ˆyi,yi). (7.22)\\nThis expression simpliﬁes if ˆ yiis obtained by a linear ﬁt with dinputs\\nor basis functions. For example,\\nN∑\\ni=1Cov(ˆyi,yi) =dσ2\\nε (7.23)\\nfor the additive error model Y=f(X) +ε, and so\\nEy(Errin) = E y(err) + 2 ≤d\\nNσ2\\nε. (7.24)\\nExpression (7.23) is the basis for the deﬁnition of the eﬀective number of\\nparameters discussed in Section 7.6 The optimism increases linearly with'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 248}, page_content='230 7. Model Assessment and Selection\\nthe number dof inputs or basis functions we use, but decreases as the\\ntraining sample size increases. Versions of (7.24) hold approximately for\\nother error models, such as binary data and entropy loss.\\nAn obvious way to estimate prediction error is to estimate the optimism\\nand then add it to the training error err. The methods described in the\\nnext section— Cp, AIC, BIC and others—work in this way, for a special\\nclass of estimates that are linear in their parameters.\\nIn contrast, cross-validation and bootstrap methods, described later in\\nthe chapter, are direct estimates of the extra-sample error Err. These gen-\\neral tools can be used with any loss function, and with nonlinear, adaptive\\nﬁtting techniques.\\nIn-sample error is not usually of direct interest since future values of the\\nfeatures are not likely to coincide with their training set values. But for\\ncomparison between models, in-sample error is convenient and often leads\\nto eﬀective model selection. The reason is that the relative (rather than\\nabsolute) size of the error is what matters.\\n7.5 Estimates of In-Sample Prediction Error\\nThe general form of the in-sample estimates is\\nˆErrin=err + ˆω, (7.25)\\nwhere ˆ ωis an estimate of the average optimism.\\nUsing expression (7.24), applicable when dparameters are ﬁt under\\nsquared error loss, leads to a version of the so-called Cpstatistic,\\nCp=err + 2 ≤d\\nNˆσε2. (7.26)\\nHere ˆσε2is an estimate of the noise variance, obtained from the mean-\\nsquared error of a low-bias model. Using this criterion we adjust the training\\nerror by a factor proportional to the number of basis functions used.\\nTheAkaike information criterion is a similar but more generally appli-\\ncable estimate of Err inwhen a log-likelihood loss function is used. It relies\\non a relationship similar to (7.24) that holds asymptotically as N→ ∞:\\n−2≤E[log Pr ˆθ(Y)]≈ −2\\nN≤E[loglik] + 2 ≤d\\nN. (7.27)\\nHere Pr θ(Y) is a family of densities for Y(containing the “true” density),\\nˆθis the maximum-likelihood estimate of θ, and “loglik” is the maximized\\nlog-likelihood:\\nloglik =N∑\\ni=1log Pr ˆθ(yi). (7.28)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 249}, page_content='7.5 Estimates of In-Sample Prediction Error 231\\nFor example, for the logistic regression model, using the binomial log-\\nlikelihood, we have\\nAIC = −2\\nN≤loglik + 2 ≤d\\nN. (7.29)\\nFor the Gaussian model (with variance σ2\\nε= ˆσε2assumed known), the AIC\\nstatistic is equivalent to Cp, and so we refer to them collectively as AIC.\\nTo use AIC for model selection, we simply choose the model giving small-\\nest AIC over the set of models considered. For nonlinear and other complex\\nmodels, we need to replace dby some measure of model complexity. We\\ndiscuss this in Section 7.6.\\nGiven a set of models fα(x) indexed by a tuning parameter α, denote\\nbyerr(α) and d(α) the training error and number of parameters for each\\nmodel. Then for this set of models we deﬁne\\nAIC(α) =err(α) + 2≤d(α)\\nNˆσε2. (7.30)\\nThe function AIC( α) provides an estimate of the test error curve, and we\\nﬁnd the tuning parameter ˆ αthat minimizes it. Our ﬁnal chosen model\\nisfˆα(x). Note that if the basis functions are chosen adaptively, (7.23) no\\nlonger holds. For example, if we have a total of pinputs, and we choose\\nthe best-ﬁtting linear model with d < p inputs, the optimism will exceed\\n(2d/N)σ2\\nε. Put another way, by choosing the best-ﬁtting model with d\\ninputs, the eﬀective number of parameters ﬁt is more than d.\\nFigure 7.4 shows AIC in action for the phoneme recognition example\\nof Section 5.2.3 on page 148. The input vector is the log-periodogram of\\nthe spoken vowel, quantized to 256 uniformly spaced frequencies. A lin-\\near logistic regression model is used to predict the phoneme class, with\\ncoeﬃcient function β(f) =∑M\\nm=1hm(f)θm, an expansion in Mspline ba-\\nsis functions. For any given M, a basis of natural cubic splines is used\\nfor the hm, with knots chosen uniformly over the range of frequencies (so\\nd(α) =d(M) =M). Using AIC to select the number of basis functions will\\napproximately minimize Err( M) for both entropy and 0–1 loss.\\nThe simple formula\\n(2/N)N∑\\ni=1Cov(ˆyi,yi) = (2 d/N)σ2\\nε\\nholds exactly for linear models with additive errors and squared error loss,\\nand approximately for linear models and log-likelihoods. In particular, the\\nformula does not hold in general for 0–1 loss (Efron, 1986), although many\\nauthors nevertheless use it in that context (right panel of Figure 7.4).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 250}, page_content='232 7. Model Assessment and Selection\\nNumber of Basis FunctionsLog-likelihood\\n0.5 1.0 1.5 2.0 2.5Log-likelihood Loss\\n2 4 8 16 32 64 128O\\nO\\nO\\nOOO\\nO\\nOO\\nO\\nO\\nOOOOO\\nO\\nO\\nO\\nOOOOOTrain\\nTest\\nAIC\\nNumber of Basis FunctionsMisclassification Error\\n0.10 0.15 0.20 0.25 0.30 0.350-1 Loss\\n2 4 8 16 32 64 128O\\nO\\nO\\nOO\\nO\\nO\\nOO\\nO\\nO\\nOOOOOO\\nO\\nO\\nOOOOO\\nFIGURE 7.4. AIC used for model selection for the phoneme recogni-\\ntion example of Section 5.2.3. The logistic regression coeﬃci ent function\\nβ(f) =PM\\nm=1hm(f)θmis modeled as an expansion in Mspline basis functions.\\nIn the left panel we see the AIC statistic used to estimate Errinusing log-likeli-\\nhood loss. Included is an estimate of Errbased on an independent test sample. It\\ndoes well except for the extremely over-parametrized case ( M= 256 parameters\\nforN= 1000 observations). In the right panel the same is done for 0–1 loss.\\nAlthough the AIC formula does not strictly apply here, it does a reasonable job in\\nthis case.\\n7.6 The Eﬀective Number of Parameters\\nThe concept of “number of parameters” can be generalized, especially to\\nmodels where regularization is used in the ﬁtting. Suppose we stack the\\noutcomes y1,y2,... ,y Ninto a vector y, and similarly for the predictions\\nˆy. Then a linear ﬁtting method is one for which we can write\\nˆy=Sy, (7.31)\\nwhereSis anN×Nmatrix depending on the input vectors xibut not on\\ntheyi. Linear ﬁtting methods include linear regression on the original fea-\\ntures or on a derived basis set, and smoothing methods that use quadratic\\nshrinkage, such as ridge regression and cubic smoothing splines. Then the\\neﬀective number of parameters is deﬁned as\\ndf(S) = trace( S), (7.32)\\nthe sum of the diagonal elements of S(also known as the eﬀective degrees-\\nof-freedom ). Note that if Sis an orthogonal-projection matrix onto a basis'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 251}, page_content='7.7 The Bayesian Approach and BIC 233\\nset spanned by Mfeatures, then trace( S) =M. It turns out that trace( S) is\\nexactly the correct quantity to replace das the number of parameters in the\\nCpstatistic (7.26). If yarises from an additive-error model Y=f(X) +ε\\nwith Var( ε) =σ2\\nε, then one can show that∑N\\ni=1Cov(ˆyi,yi) = trace( S)σ2\\nε,\\nwhich motivates the more general deﬁnition\\ndf(ˆy) =∑N\\ni=1Cov(ˆyi,yi)\\nσ2ε(7.33)\\n(Exercises 7.4 and 7.5). Section 5.4.1 on page 153 gives some more intuit ion\\nfor the deﬁnition df = trace( S) in the context of smoothing splines.\\nFor models like neural networks, in which we minimize an error function\\nR(w) with weight decay penalty (regularization) α∑\\nmw2\\nm, the eﬀective\\nnumber of parameters has the form\\ndf(α) =M∑\\nm=1θm\\nθm+α, (7.34)\\nwhere the θmare the eigenvalues of the Hessian matrix ∂2R(w)/∂w∂wT.\\nExpression (7.34) follows from (7.32) if we make a quadratic approx imation\\nto the error function at the solution (Bishop, 1995).\\n7.7 The Bayesian Approach and BIC\\nThe Bayesian information criterion (BIC), like AIC, is applicable in s ettings\\nwhere the ﬁtting is carried out by maximization of a log-likelihood. The\\ngeneric form of BIC is\\nBIC = −2≤loglik + (log N)≤d. (7.35)\\nThe BIC statistic (times 1/2) is also known as the Schwarz criterion ( Schwarz,\\n1978).\\nUnder the Gaussian model, assuming the variance σ2\\nεis known, −2≤loglik\\nequals (up to a constant)∑\\ni(yi−ˆf(xi))2/σ2\\nε, which is N≤err/σ2\\nεfor squared\\nerror loss. Hence we can write\\nBIC =N\\nσ2ε[\\nerr + (log N)≤d\\nNσ2\\nε]\\n. (7.36)\\nTherefore BIC is proportional to AIC ( Cp), with the factor 2 replaced\\nby log N. Assuming N > e2≈7.4, BIC tends to penalize complex models\\nmore heavily, giving preference to simpler models in selection. As with AIC,\\nσ2\\nεis typically estimated by the mean squared error of a low-bias model.\\nFor classiﬁcation problems, use of the multinomial log-likelihood leads to a\\nsimilar relationship with the AIC, using cross-entropy as the error measur e.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 252}, page_content='234 7. Model Assessment and Selection\\nNote however that the misclassiﬁcation error measure does not arise in the\\nBIC context, since it does not correspond to the log-likelihood of the data\\nunder any probability model.\\nDespite its similarity with AIC, BIC is motivated in quite a diﬀerent\\nway. It arises in the Bayesian approach to model selection, which we now\\ndescribe.\\nSuppose we have a set of candidate models Mm,m= 1,... ,M and\\ncorresponding model parameters θm, and we wish to choose a best model\\nfrom among them. Assuming we have a prior distribution Pr( θm|Mm) for\\nthe parameters of each model Mm, the posterior probability of a given\\nmodel is\\nPr(Mm|Z)∝Pr(Mm)≤Pr(Z|Mm) (7.37)\\n∝Pr(Mm)≤∫\\nPr(Z|θm,Mm)Pr(θm|Mm)dθm,\\nwhere Zrepresents the training data {xi,yi}N\\n1. To compare two models\\nMmandMℓ, we form the posterior odds\\nPr(Mm|Z)\\nPr(Mℓ|Z)=Pr(Mm)\\nPr(Mℓ)≤Pr(Z|Mm)\\nPr(Z|Mℓ). (7.38)\\nIf the odds are greater than one we choose model m, otherwise we choose\\nmodel ℓ. The rightmost quantity\\nBF(Z) =Pr(Z|Mm)\\nPr(Z|Mℓ)(7.39)\\nis called the Bayes factor , the contribution of the data toward the posterior\\nodds.\\nTypically we assume that the prior over models is uniform, so that\\nPr(Mm) is constant. We need some way of approximating Pr( Z|Mm).\\nA so-called Laplace approximation to the integral followed by some other\\nsimpliﬁcations (Ripley, 1996, page 64) to (7.37) gives\\nlog Pr( Z|Mm) = log Pr( Z|ˆθm,Mm)−dm\\n2≤logN+O(1).(7.40)\\nHere ˆθmis a maximum likelihood estimate and dmis the number of free\\nparameters in model Mm. If we deﬁne our loss function to be\\n−2log Pr( Z|ˆθm,Mm),\\nthis is equivalent to the BIC criterion of equation (7.35).\\nTherefore, choosing the model with minimum BIC is equivalent to choos-\\ning the model with largest (approximate) posterior probability. But this\\nframework gives us more. If we compute the BIC criterion for a set of M,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 253}, page_content='7.8 Minimum Description Length 235\\nmodels, giving BIC m,m= 1,2,... ,M , then we can estimate the posterior\\nprobability of each model Mmas\\ne−1\\n2≤BIC m\\n∑M\\nℓ=1e−1\\n2≤BIC ℓ. (7.41)\\nThus we can estimate not only the best model, but also assess the relative\\nmerits of the models considered.\\nFor model selection purposes, there is no clear choice between AIC and\\nBIC. BIC is asymptotically consistent as a selection criterion. What this\\nmeans is that given a family of models, including the true model, the prob-\\nability that BIC will select the correct model approaches one as the sample\\nsizeN→ ∞. This is not the case for AIC, which tends to choose models\\nwhich are too complex as N→ ∞. On the other hand, for ﬁnite samples,\\nBIC often chooses models that are too simple, because of its heavy penalty\\non complexity.\\n7.8 Minimum Description Length\\nThe minimum description length (MDL) approach gives a selection cri-\\nterion formally identical to the BIC approach, but is motivated from an\\noptimal coding viewpoint. We ﬁrst review the theory of coding for data\\ncompression, and then apply it to model selection.\\nWe think of our datum zas a message that we want to encode and\\nsend to someone else (the “receiver”). We think of our model as a way of\\nencoding the datum, and will choose the most parsimonious model, that is\\nthe shortest code, for the transmission.\\nSuppose ﬁrst that the possible messages we might want to transmit are\\nz1,z2,... ,z m. Our code uses a ﬁnite alphabet of length A: for example, we\\nmight use a binary code {0,1}of length A= 2. Here is an example with\\nfour possible messages and a binary coding:\\nMessage z1z2z3z4\\nCode 010110 111(7.42)\\nThis code is known as an instantaneous preﬁx code: no code is the pre-\\nﬁx of any other, and the receiver (who knows all of the possible codes),\\nknows exactly when the message has been completely sent. We restrict our\\ndiscussion to such instantaneous preﬁx codes.\\nOne could use the coding in (7.42) or we could permute the codes, for\\nexample use codes 110 ,10,111,0 forz1,z2,z3,z4. How do we decide which\\nto use? It depends on how often we will be sending each of the messages.\\nIf, for example, we will be sending z1most often, it makes sense to use the\\nshortest code 0 for z1. Using this kind of strategy—shorter codes for more\\nfrequent messages—the average message length will be shorter.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 254}, page_content='236 7. Model Assessment and Selection\\nIn general, if messages are sent with probabilities Pr( zi),i= 1,2,... ,4,\\na famous theorem due to Shannon says we should use code lengths li=\\n−log2Pr(zi) and the average message length satisﬁes\\nE(length) ≥ −∑\\nPr(zi)log2(Pr(zi)). (7.43)\\nThe right-hand side above is also called the entropy of the distribution\\nPr(zi). The inequality is an equality when the probabilities satisfy pi=\\nA−li. In our example, if Pr( zi) = 1/2,1/4,1/8,1/8,respectively, then the\\ncoding shown in (7.42) is optimal and achieves the entropy lower bound.\\nIn general the lower bound cannot be achieved, but procedures like the\\nHuﬀmann coding scheme can get close to the bound. Note that with an\\ninﬁnite set of messages, the entropy is replaced by −∫\\nPr(z)log2Pr(z)dz.\\nFrom this result we glean the following:\\nTo transmit a random variable zhaving probability density func-\\ntionPr(z), we require about −log2Pr(z)bits of information.\\nWe henceforth change notation from log2Pr(z) to log Pr( z) = logePr(z);\\nthis is for convenience, and just introduces an unimportant multiplicative\\nconstant.\\nNow we apply this result to the problem of model selection. We have\\na model Mwith parameters θ, and data Z= (X,y) consisting of both\\ninputs and outputs. Let the (conditional) probability of the outputs under\\nthe model be Pr( y|θ,M,X), assume the receiver knows all of the inputs,\\nand we wish to transmit the outputs. Then the message length required to\\ntransmit the outputs is\\nlength = −log Pr( y|θ,M,X)−log Pr( θ|M), (7.44)\\nthe log-probability of the target values given the inputs. The second term\\nis the average code length for transmitting the model parameters θ, while\\nthe ﬁrst term is the average code length for transmitting the discrepancy\\nbetween the model and actual target values. For example suppose we have\\na single target ywithy∼N(θ,σ2), parameter θ∼N(0,1) and no input\\n(for simplicity). Then the message length is\\nlength = constant + log σ+(y−θ)2\\nσ2+θ2\\n2. (7.45)\\nNote that the smaller σis, the shorter on average is the message length,\\nsinceyis more concentrated around θ.\\nThe MDL principle says that we should choose the model that mini-\\nmizes (7.44). We recognize (7.44) as the (negative) log-posterior distribu-\\ntion, and hence minimizing description length is equivalent to maximizing\\nposterior probability. Hence the BIC criterion, derived as approximation to\\nlog-posterior probability, can also be viewed as a device for (approximate)\\nmodel choice by minimum description length.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 255}, page_content='7.9 Vapnik–Chervonenkis Dimension 237\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 0.0 1.0\\nxsin(50 ≤x)\\nFIGURE 7.5. The solid curve is the function sin(50 x)forx∈[0,1]. The green\\n(solid) and blue (hollow) points illustrate how the associate d indicator function\\nI(sin(αx)>0)can shatter (separate) an arbitrarily large number of points b y\\nchoosing an appropriately high frequency α.\\nNote that we have ignored the precision with which a random variable\\nzis coded. With a ﬁnite code length we cannot code a continuous variable\\nexactly. However, if we code zwithin a tolerance δz, the message length\\nneeded is the log of the probability in the interval [ z,z+δz] which is well ap-\\nproximated by δzPr(z) ifδzis small. Since log δzPr(z) = log δz+log Pr( z),\\nthis means we can just ignore the constant log δzand use log Pr( z) as our\\nmeasure of message length, as we did above.\\nThe preceding view of MDL for model selection says that we should\\nchoose the model with highest posterior probability. However, many Bayes-\\nians would instead do inference by sampling from the posterior distribution.\\n7.9 Vapnik–Chervonenkis Dimension\\nA diﬃculty in using estimates of in-sample error is the need to specify the\\nnumber of parameters (or the complexity) dused in the ﬁt. Although the\\neﬀective number of parameters introduced in Section 7.6 is useful for some\\nnonlinear models, it is not fully general. The Vapnik–Chervonenkis (VC)\\ntheory provides such a general measure of complexity, and gives associated\\nbounds on the optimism. Here we give a brief review of this theory.\\nSuppose we have a class of functions {f(x,α)}indexed by a parameter\\nvector α, with x∈IRp. Assume for now that fis an indicator function,\\nthat is, takes the values 0 or 1. If α= (α0,α1) and fis the linear indi-\\ncator function I(α0+αT\\n1x >0), then it seems reasonable to say that the\\ncomplexity of the class fis the number of parameters p+ 1. But what\\nabout f(x,α) =I(sinα≤x) where αis any real number and x∈IR? The\\nfunction sin(50 ≤x) is shown in Figure 7.5. This is a very wiggly function\\nthat gets even rougher as the frequency αincreases, but it has only one\\nparameter: despite this, it doesn’t seem reasonable to conclude that it has\\nless complexity than the linear indicator function I(α0+α1x) inp= 1\\ndimension.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 256}, page_content='238 7. Model Assessment and Selection\\nFIGURE 7.6. The ﬁrst three panels show that the class of lines in the plane\\ncan shatter three points. The last panel shows that this class c annot shatter four\\npoints, as no line will put the hollow points on one side and the solid points on\\nthe other. Hence the VC dimension of the class of straight lines i n the plane is\\nthree. Note that a class of nonlinear curves could shatter four p oints, and hence\\nhas VC dimension greater than three.\\nThe Vapnik–Chervonenkis dimension is a way of measuring the com-\\nplexity of a class of functions by assessing how wiggly its members can\\nbe.\\nTheVC dimension of the class {f(x,α)}is deﬁned to be the\\nlargest number of points (in some conﬁguration) that can be\\nshattered by members of {f(x,α)}.\\nA set of points is said to be shattered by a class of functions if, no matter\\nhow we assign a binary label to each point, a member of the class can\\nperfectly separate them.\\nFigure 7.6 shows that the VC dimension of linear indicator functions\\nin the plane is 3 but not 4, since no four points can be shattered by a\\nset of lines. In general, a linear indicator function in pdimensions has VC\\ndimension p+1, which is also the number of free parameters. On the other\\nhand, it can be shown that the family sin( αx) has inﬁnite VC dimension,\\nas Figure 7.5 suggests. By appropriate choice of α, any set of points can be\\nshattered by this class (Exercise 7.8).\\nSo far we have discussed the VC dimension only of indicator functions,\\nbut this can be extended to real-valued functions. The VC dimension of a\\nclass of real-valued functions {g(x,α)}is deﬁned to be the VC dimension\\nof the indicator class {I(g(x,α)−β >0)}, where βtakes values over the\\nrange of g.\\nOne can use the VC dimension in constructing an estimate of (extra-\\nsample) prediction error; diﬀerent types of results are available. Using the\\nconcept of VC dimension, one can prove results about the optimism of the\\ntraining error when using a class of functions. An example of such a result is\\nthe following. If we ﬁt Ntraining points using a class of functions {f(x,α)}\\nhaving VC dimension h, then with probability at least 1 −ηover training'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 257}, page_content='7.9 Vapnik–Chervonenkis Dimension 239\\nsets:\\nErrT≤err +ǫ\\n2(\\n1 +√\\n1 +4≤err\\nǫ)\\n(binary classiﬁcation)\\nErrT≤err\\n(1−c√ǫ)+(regression) (7.46)\\nwhere ǫ=a1h[log (a2N/h) + 1]−log (η/4)\\nN,\\nand 0 < a1≤4,0< a2≤2\\nThese bounds hold simultaneously for all members f(x,α), and are taken\\nfrom Cherkassky and Mulier (2007, pages 116–118). They recommend the\\nvalue c= 1. For regression they suggest a1=a2= 1, and for classiﬁcation\\nthey make no recommendation, with a1= 4 and a2= 2 corresponding\\nto worst-case scenarios. They also give an alternative practical bound for\\nregression\\nErrT≤err(\\n1−√\\nρ−ρlogρ+logN\\n2N)−1\\n+(7.47)\\nwithρ=h\\nN, which is free of tuning constants. The bounds suggest that the\\noptimism increases with hand decreases with Nin qualitative agreement\\nwith the AIC correction d/Ngiven is (7.24). However, the results in (7.46)\\nare stronger: rather than giving the expected optimism for each ﬁxed func-\\ntionf(x,α), they give probabilistic upper bounds for all functions f(x,α),\\nand hence allow for searching over the class.\\nVapnik’s structural risk minimization (SRM) approach ﬁts a nested se-\\nquence of models of increasing VC dimensions h1< h2<≤≤≤, and then\\nchooses the model with the smallest value of the upper bound.\\nWe note that upper bounds like the ones in (7.46) are often very loose,\\nbut that doesn’t rule them out as good criteria for model selection, where\\nthe relative (not absolute) size of the test error is important. The main\\ndrawback of this approach is the diﬃculty in calculating the VC dimension\\nof a class of functions. Often only a crude upper bound for VC dimension\\nis obtainable, and this may not be adequate. An example in which the\\nstructural risk minimization program can be successfully carried out is the\\nsupport vector classiﬁer, discussed in Section 12.2.\\n7.9.1 Example (Continued)\\nFigure 7.7 shows the results when AIC, BIC and SRM are used to select\\nthe model size for the examples of Figure 7.3. For the examples labeled KNN,\\nthe model index αrefers to neighborhood size, while for those labeled REGα\\nrefers to subset size. Using each selection method (e.g., AIC) we estimated\\nthe best model ˆ αand found its true prediction error Err T(ˆα) on a test\\nset. For the same training set we computed the prediction error of the best'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 258}, page_content='240 7. Model Assessment and Selection\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestAIC\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBIC\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestSRM\\nFIGURE 7.7. Boxplots show the distribution of the relative error\\n100×[Err T(ˆα)−min αErrT(α)]/[max αErrT(α)−min αErrT(α)]over the four\\nscenarios of Figure 7.3. This is the error in using the chosen mo del relative to\\nthe best model. There are 100training sets each of size 80represented in each\\nboxplot, with the errors computed on test sets of size 10,000.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 259}, page_content='7.10 Cross-Validation 241\\nand worst possible model choices: min αErrT(α) and max αErrT(α). The\\nboxplots show the distribution of the quantity\\n100×ErrT(ˆα)−minαErrT(α)\\nmax αErrT(α)−minαErrT(α),\\nwhich represents the error in using the chosen model relative to the best\\nmodel. For linear regression the model complexity was measured by the\\nnumber of features; as mentioned in Section 7.5, this underestimates the\\ndf, since it does not charge for the search for the best model of that size.\\nThis was also used for the VC dimension of the linear classiﬁer. For k-\\nnearest neighbors, we used the quantity N/k. Under an additive-error re-\\ngression model, this can be justiﬁed as the exact eﬀective degrees of free-\\ndom (Exercise 7.6); we do not know if it corresponds to the VC dimen-\\nsion. We used a1=a2= 1 for the constants in (7.46); the results for SRM\\nchanged with diﬀerent constants, and this choice gave the most favorable re-\\nsults. We repeated the SRM selection using the alternative practical bound\\n(7.47), and got almost identical results. For misclassiﬁcation error w e used\\nˆσε2= [N/(N−d)]≤err(α) for the least restrictive model ( k= 5 for KNN,\\nsincek= 1 results in zero training error). The AIC criterion seems to work\\nwell in all four scenarios, despite the lack of theoretical support with 0–1\\nloss. BIC does nearly as well, while the performance of SRM is mixed.\\n7.10 Cross-Validation\\nProbably the simplest and most widely used method for estimating predic-\\ntion error is cross-validation. This method directly estimates the expected\\nextra-sample error Err = E[ L(Y,ˆf(X))], the average generalization error\\nwhen the method ˆf(X) is applied to an independent test sample from the\\njoint distribution of XandY. As mentioned earlier, we might hope that\\ncross-validation estimates the conditional error, with the training set T\\nheld ﬁxed. But as we will see in Section 7.12, cross-validation typically\\nestimates well only the expected prediction error.\\n7.10.1 K-Fold Cross-Validation\\nIdeally, if we had enough data, we would set aside a validation set and use\\nit to assess the performance of our prediction model. Since data are often\\nscarce, this is usually not possible. To ﬁnesse the problem, K-fold cross-\\nvalidation uses part of the available data to ﬁt the model, and a diﬀerent\\npart to test it. We split the data into Kroughly equal-sized parts; for\\nexample, when K= 5, the scenario looks like this:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 260}, page_content='242 7. Model Assessment and Selection\\nValidation Train1 2 3 4 5\\nTrain Train Train\\nFor the kth part (third above), we ﬁt the model to the other K−1 parts\\nof the data, and calculate the prediction error of the ﬁtted model when\\npredicting the kth part of the data. We do this for k= 1,2,... ,K and\\ncombine the Kestimates of prediction error.\\nHere are more details. Let κ:{1,... ,N } ↦→ { 1,... ,K }be an indexing\\nfunction that indicates the partition to which observation iis allocated by\\nthe randomization. Denote by ˆf−k(x) the ﬁtted function, computed with\\nthekth part of the data removed. Then the cross-validation estimate of\\nprediction error is\\nCV(ˆf) =1\\nNN∑\\ni=1L(yi,ˆf−κ(i)(xi)). (7.48)\\nTypical choices of Kare 5 or 10 (see below). The case K=Nis known\\nasleave-one-out cross-validation. In this case κ(i) =i, and for the ith\\nobservation the ﬁt is computed using all the data except the ith.\\nGiven a set of models f(x,α) indexed by a tuning parameter α, denote\\nbyˆf−k(x,α) theαth model ﬁt with the kth part of the data removed. Then\\nfor this set of models we deﬁne\\nCV(ˆf,α) =1\\nNN∑\\ni=1L(yi,ˆf−κ(i)(xi,α)). (7.49)\\nThe function CV( ˆf,α) provides an estimate of the test error curve, and we\\nﬁnd the tuning parameter ˆ αthat minimizes it. Our ﬁnal chosen model is\\nf(x,ˆα), which we then ﬁt to all the data.\\nIt is interesting to wonder about what quantity K-fold cross-validation\\nestimates. With K= 5 or 10, we might guess that it estimates the ex-\\npected error Err, since the training sets in each fold are quite diﬀerent\\nfrom the original training set. On the other hand, if K=Nwe might\\nguess that cross-validation estimates the conditional error Err T. It turns\\nout that cross-validation only estimates eﬀectively the average error Err,\\nas discussed in Section 7.12.\\nWhat value should we choose for K? With K=N, the cross-validation\\nestimator is approximately unbiased for the true (expected) prediction er-\\nror, but can have high variance because the N“training sets” are so similar\\nto one another. The computational burden is also considerable, requiring\\nNapplications of the learning method. In certain special problems, this\\ncomputation can be done quickly—see Exercises 7.3 and 5.13.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 261}, page_content='7.10 Cross-Validation 243\\nSize of Training Set1-Err\\n0 50 100 150 2000.0 0.2 0.4 0.6 0.8\\nFIGURE 7.8. Hypothetical learning curve for a classiﬁer on a given task: a\\nplot of 1−Errversus the size of the training set N. With a dataset of 200\\nobservations, 5-fold cross-validation would use training sets of size 160, which\\nwould behave much like the full set. However, with a dataset o f50observations\\nﬁvefold cross-validation would use training sets of size 40, and this would result\\nin a considerable overestimate of prediction error.\\nOn the other hand, with K= 5 say, cross-validation has lower variance.\\nBut bias could be a problem, depending on how the performance of the\\nlearning method varies with the size of the training set. Figure 7.8 shows\\na hypothetical “learning curve” for a classiﬁer on a given task, a plot of\\n1−Err versus the size of the training set N. The performance of the\\nclassiﬁer improves as the training set size increases to 100 observations;\\nincreasing the number further to 200 brings only a small beneﬁt. If our\\ntraining set had 200 observations, ﬁvefold cross-validation would estimat e\\nthe performance of our classiﬁer over training sets of size 160, which from\\nFigure 7.8 is virtually the same as the performance for training set size\\n200. Thus cross-validation would not suﬀer from much bias. However if the\\ntraining set had 50 observations, ﬁvefold cross-validation would estimate\\nthe performance of our classiﬁer over training sets of size 40, and from the\\nﬁgure that would be an underestimate of 1 −Err. Hence as an estimate of\\nErr, cross-validation would be biased upward.\\nTo summarize, if the learning curve has a considerable slope at the given\\ntraining set size, ﬁve- or tenfold cross-validation will overestimate the tr ue\\nprediction error. Whether this bias is a drawback in practice depends on\\nthe objective. On the other hand, leave-one-out cross-validation has low\\nbias but can have high variance. Overall, ﬁve- or tenfold cross-validation\\nare recommended as a good compromise: see Breiman and Spector (1992)\\nand Kohavi (1995).\\nFigure 7.9 shows the prediction error and tenfold cross-validation curve\\nestimated from a single training set, from the scenario in the bottom righ t\\npanel of Figure 7.3. This is a two-class classiﬁcation problem, using a lin-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 262}, page_content='244 7. Model Assessment and Selection\\nSubset Size pMisclassification Error\\n5 10 15 200.0 0.1 0.2 0.3 0.4 0.5 0.6•••\\n••\\n••\\n•\\n•\\n•••••• • •••••\\n•\\n••\\n•\\n•\\n••\\n••• • •• ••• • • •\\nFIGURE 7.9. Prediction error (orange) and tenfold cross-validation curve\\n(blue) estimated from a single training set, from the scenario in the bottom right\\npanel of Figure 7.3.\\near model with best subsets regression of subset size p. Standard error bars\\nare shown, which are the standard errors of the individual misclassiﬁcation\\nerror rates for each of the ten parts. Both curves have minima at p= 10,\\nalthough the CV curve is rather ﬂat beyond 10. Often a “one-standard\\nerror” rule is used with cross-validation, in which we choose the most par-\\nsimonious model whose error is no more than one standard error above\\nthe error of the best model. Here it looks like a model with about p= 9\\npredictors would be chosen, while the true model uses p= 10.\\nGeneralized cross-validation provides a convenient approximation to leave-\\none out cross-validation, for linear ﬁtting under squared-error loss. As de-\\nﬁned in Section 7.6, a linear ﬁtting method is one for which we can write\\nˆy=Sy. (7.50)\\nNow for many linear ﬁtting methods,\\n1\\nNN∑\\ni=1[yi−ˆf−i(xi)]2=1\\nNN∑\\ni=1[yi−ˆf(xi)\\n1−Sii]2\\n, (7.51)\\nwhere Siiis the ith diagonal element of S(see Exercise 7.3). The GCV\\napproximation is\\nGCV( ˆf) =1\\nNN∑\\ni=1[\\nyi−ˆf(xi)\\n1−trace(S)/N]2\\n. (7.52)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 263}, page_content='7.10 Cross-Validation 245\\nThe quantity trace( S) is the eﬀective number of parameters, as deﬁned in\\nSection 7.6.\\nGCV can have a computational advantage in some settings, where the\\ntrace of Scan be computed more easily than the individual elements Sii.\\nIn smoothing problems, GCV can also alleviate the tendency of cross-\\nvalidation to undersmooth. The similarity between GCV and AIC can be\\nseen from the approximation 1 /(1−x)2≈1 + 2x(Exercise 7.7).\\n7.10.2 The Wrong and Right Way to Do Cross-validation\\nConsider a classiﬁcation problem with a large number of predictors, as may\\narise, for example, in genomic or proteomic applications. A typical strategy\\nfor analysis might be as follows:\\n1. Screen the predictors: ﬁnd a subset of “good” predictors that show\\nfairly strong (univariate) correlation with the class labels\\n2. Using just this subset of predictors, build a multivariate classiﬁer.\\n3. Use cross-validation to estimate the unknown tuning parameters and\\nto estimate the prediction error of the ﬁnal model.\\nIs this a correct application of cross-validation? Consider a scenario with\\nN= 50 samples in two equal-sized classes, and p= 5000 quantitative\\npredictors (standard Gaussian) that are independent of the class labels.\\nThe true (test) error rate of any classiﬁer is 50%. We carried out the above\\nrecipe, choosing in step (1) the 100 predictors having highest correlation\\nwith the class labels, and then using a 1-nearest neighbor classiﬁer, based\\non just these 100 predictors, in step (2). Over 50 simulations from this\\nsetting, the average CV error rate was 3%. This is far lower than the true\\nerror rate of 50%.\\nWhat has happened? The problem is that the predictors have an unfair\\nadvantage, as they were chosen in step (1) on the basis of all of the samples .\\nLeaving samples out afterthe variables have been selected does not cor-\\nrectly mimic the application of the classiﬁer to a completely independent\\ntest set, since these predictors “have already seen” the left out samples.\\nFigure 7.10 (top panel) illustrates the problem. We selected the 100 pre-\\ndictors having largest correlation with the class labels over all 50 sampl es.\\nThen we chose a random set of 10 samples, as we would do in ﬁve-fold cross-\\nvalidation, and computed the correlations of the pre-selected 100 predictors\\nwith the class labels over just these 10 samples (top panel). We see that\\nthe correlations average about 0.28, rather than 0, as one might expect.\\nHere is the correct way to carry out cross-validation in this example:\\n1. Divide the samples into Kcross-validation folds (groups) at random.\\n2. For each fold k= 1,2,... ,K'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 264}, page_content='246 7. Model Assessment and Selection\\nCorrelations of Selected Predictors with OutcomeFrequency\\n−1.0 −0.5 0.0 0.5 1.00 10 20 30Wrong way\\nCorrelations of Selected Predictors with OutcomeFrequency\\n−1.0 −0.5 0.0 0.5 1.00 10 20 30Right way\\nFIGURE 7.10. Cross-validation the wrong and right way: histograms shows th e\\ncorrelation of class labels, in 10randomly chosen samples, with the 100predic-\\ntors chosen using the incorrect (upper red) and correct (lower g reen) versions of\\ncross-validation.\\n(a) Find a subset of “good” predictors that show fairly strong (uni-\\nvariate) correlation with the class labels, using all of the samples\\nexcept those in fold k.\\n(b) Using just this subset of predictors, build a multivariate classi-\\nﬁer, using all of the samples except those in fold k.\\n(c) Use the classiﬁer to predict the class labels for the samples in\\nfoldk.\\nThe error estimates from step 2(c) are then accumulated over all Kfolds, to\\nproduce the cross-validation estimate of prediction error. The lower panel\\nof Figure 7.10 shows the correlations of class labels with the 100 predictor s\\nchosen in step 2(a) of the correct procedure, over the samples in a typical\\nfoldk. We see that they average about zero, as they should.\\nIn general, with a multistep modeling procedure, cross-validation must\\nbe applied to the entire sequence of modeling steps. In particular, samples\\nmust be “left out” before any selection or ﬁltering steps are applied. There\\nis one qualiﬁcation: initial unsupervised screening steps can be done be-\\nfore samples are left out. For example, we could select the 1000 predictors'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 265}, page_content='7.10 Cross-Validation 247\\nwith highest variance across all 50 samples, before starting cross-valida tion.\\nSince this ﬁltering does not involve the class labels, it does not give the\\npredictors an unfair advantage.\\nWhile this point may seem obvious to the reader, we have seen this\\nblunder committed many times in published papers in top rank journals.\\nWith the large numbers of predictors that are so common in genomic and\\nother areas, the potential consequences of this error have also increased\\ndramatically; see Ambroise and McLachlan (2002) for a detailed discussion\\nof this issue.\\n7.10.3 Does Cross-Validation Really Work?\\nWe once again examine the behavior of cross-validation in a high-dimensional\\nclassiﬁcation problem. Consider a scenario with N= 20 samples in two\\nequal-sized classes, and p= 500 quantitative predictors that are indepen-\\ndent of the class labels. Once again, the true error rate of any classiﬁer is\\n50%. Consider a simple univariate classiﬁer: a single split that minimizes\\nthe misclassiﬁcation error (a “stump”). Stumps are trees with a single split ,\\nand are used in boosting methods (Chapter 10). A simple argument sug-\\ngests that cross-validation will not work properly in this setting2:\\nFitting to the entire training set, we will ﬁnd a predictor th at\\nsplits the data very well If we do 5-fold cross-validation, thi s\\nsame predictor should split any 4/5ths and 1/5th of the data\\nwell too, and hence its cross-validation error will be small ( much\\nless than 50%) Thus CV does not give an accurate estimate of\\nerror.\\nTo investigate whether this argument is correct, Figure 7.11 shows the\\nresult of a simulation from this setting. There are 500 predictors and 20\\nsamples, in each of two equal-sized classes, with all predictors having a\\nstandard Gaussian distribution. The panel in the top left shows the number\\nof training errors for each of the 500 stumps ﬁt to the training data. We\\nhave marked in color the six predictors yielding the fewest errors. In the top\\nright panel, the training errors are shown for stumps ﬁt to a random 4 /5ths\\npartition of the data (16 samples), and tested on the remaining 1 /5th (four\\nsamples). The colored points indicate the same predictors marked in the\\ntop left panel. We see that the stump for the blue predictor (whose stump\\nwas the best in the top left panel), makes two out of four test errors (50%),\\nand is no better than random.\\nWhat has happened? The preceding argument has ignored the fact that\\nin cross-validation, the model must be completely retrained for each fold\\n2This argument was made to us by a scientist at a proteomics lab meeting, and led\\nto material in this section.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 266}, page_content='248 7. Model Assessment and Selection\\n0 100 200 300 400 5002 3 4 5 6 7 8 9\\nPredictorError on Full Training Set\\n1 2 3 4 5 6 7 80 1 2 3 4\\nError on 4/5Error on 1/5\\n−1 0 1 2\\nPredictor 436 (blue)Class Label\\n0 1\\nfull\\n4/5\\n0.0 0.2 0.4 0.6 0.8 1.0\\nCV Errors\\nFIGURE 7.11. Simulation study to investigate the performance of cross vali-\\ndation in a high-dimensional problem where the predictors are independent of the\\nclass labels. The top-left panel shows the number of errors mad e by individual\\nstump classiﬁers on the full training set ( 20observations). The top right panel\\nshows the errors made by individual stumps trained on a random sp lit of the\\ndataset into 4/5ths (16observations) and tested on the remaining 1/5th (4ob-\\nservations). The best performers are depicted by colored dot s in each panel. The\\nbottom left panel shows the eﬀect of re-estimating the split po int in each fold: the\\ncolored points correspond to the four samples in the 4/5ths validation set. The\\nsplit point derived from the full dataset classiﬁes all four sa mples correctly, but\\nwhen the split point is re-estimated on the 4/5ths data (as it should be), it com-\\nmits two errors on the four validation samples. In the bottom right we see the\\noverall result of ﬁve-fold cross-validation applied to 50simulated datasets. The\\naverage error rate is about 50%, as it should be.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 267}, page_content='7.11 Bootstrap Methods 249\\nof the process. In the present example, this means that the best predictor\\nand corresponding split point are found from 4 /5ths of the data. The eﬀect\\nof predictor choice is seen in the top right panel. Since the class labels are\\nindependent of the predictors, the performance of a stump on the 4 /5ths\\ntraining data contains no information about its performance in the remain-\\ning 1/5th. The eﬀect of the choice of split point is shown in the bottom left\\npanel. Here we see the data for predictor 436, corresponding to the blue\\ndot in the top left plot. The colored points indicate the 1 /5th data, while\\nthe remaining points belong to the 4 /5ths. The optimal split points for this\\npredictor based on both the full training set and 4 /5ths data are indicated.\\nThe split based on the full data makes no errors on the 1 /5ths data. But\\ncross-validation must base its split on the 4 /5ths data, and this incurs two\\nerrors out of four samples.\\nThe results of applying ﬁve-fold cross-validation to each of 50 simulated\\ndatasets is shown in the bottom right panel. As we would hope, the average\\ncross-validation error is around 50%, which is the true expected prediction\\nerror for this classiﬁer. Hence cross-validation has behaved as it should.\\nOn the other hand, there is considerable variability in the error, underscor-\\ning the importance of reporting the estimated standard error of the CV\\nestimate. See Exercise 7.10 for another variation of this problem.\\n7.11 Bootstrap Methods\\nThe bootstrap is a general tool for assessing statistical accuracy. Firs t we\\ndescribe the bootstrap in general, and then show how it can be used to\\nestimate extra-sample prediction error. As with cross-validation, the boo t-\\nstrap seeks to estimate the conditional error Err T, but typically estimates\\nwell only the expected prediction error Err.\\nSuppose we have a model ﬁt to a set of training data. We denote the\\ntraining set by Z= (z1,z2,... ,z N) where zi= (xi,yi). The basic idea is\\nto randomly draw datasets with replacement from the training data, each\\nsample the same size as the original training set. This is done Btimes\\n(B= 100 say), producing Bbootstrap datasets, as shown in Figure 7.12.\\nThen we reﬁt the model to each of the bootstrap datasets, and examine\\nthe behavior of the ﬁts over the Breplications.\\nIn the ﬁgure, S(Z) is any quantity computed from the data Z, for ex-\\nample, the prediction at some input point. From the bootstrap sampling\\nwe can estimate any aspect of the distribution of S(Z), for example, its\\nvariance,\\nˆVar[S(Z)] =1\\nB−1B∑\\nb=1(S(Z∗b)−¯S∗)2, (7.53)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 268}, page_content='250 7. Model Assessment and Selection\\n  Bootstrap\\nBootstrapreplications\\nsamples\\nsampleTrainingZ= (z1,z2,... ,z N)Z∗1Z∗2Z∗BS(Z∗1) S(Z∗2) S(Z∗B)\\nFIGURE 7.12. Schematic of the bootstrap process. We wish to assess the sta-\\ntistical accuracy of a quantity S(Z)computed from our dataset. Btraining sets\\nZ∗b, b= 1, . . . , B each of size Nare drawn with replacement from the original\\ndataset. The quantity of interest S(Z)is computed from each bootstrap training\\nset, and the values S(Z∗1), . . . , S (Z∗B)are used to assess the statistical accuracy\\nofS(Z).\\nwhere ¯S∗=∑\\nbS(Z∗b)/B. Note that ˆVar[S(Z)] can be thought of as a\\nMonte-Carlo estimate of the variance of S(Z) under sampling from the\\nempirical distribution function ˆFfor the data ( z1,z2,... ,z N).\\nHow can we apply the bootstrap to estimate prediction error? One ap-\\nproach would be to ﬁt the model in question on a set of bootstrap samples,\\nand then keep track of how well it predicts the original training set. If\\nˆf∗b(xi) is the predicted value at xi, from the model ﬁtted to the bth boot-\\nstrap dataset, our estimate is\\nˆErrboot=1\\nB1\\nNB∑\\nb=1N∑\\ni=1L(yi,ˆf∗b(xi)). (7.54)\\nHowever, it is easy to see that ˆErrbootdoes not provide a good estimate in\\ngeneral. The reason is that the bootstrap datasets are acting as the training\\nsamples, while the original training set is acting as the test sample, and\\nthese two samples have observations in common. This overlap can make\\noverﬁt predictions look unrealistically good, and is the reason that cross-\\nvalidation explicitly uses non-overlapping data for the training and test\\nsamples. Consider for example a 1-nearest neighbor classiﬁer applied to a\\ntwo-class classiﬁcation problem with the same number of observations in'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 269}, page_content='7.11 Bootstrap Methods 251\\neach class, in which the predictors and class labels are in fact independent.\\nThen the true error rate is 0 .5. But the contributions to the bootstrap\\nestimate ˆErrbootwill be zero unless the observation idoes not appear in the\\nbootstrap sample b. In this latter case it will have the correct expectation\\n0.5. Now\\nPr{observation i∈bootstrap sample b}= 1−(\\n1−1\\nN)N\\n≈1−e−1\\n= 0.632. (7.55)\\nHence the expectation of ˆErrbootis about 0 .5×0.368 = 0 .184, far below\\nthe correct error rate 0 .5.\\nBy mimicking cross-validation, a better bootstrap estimate can be ob-\\ntained. For each observation, we only keep track of predictions from boot-\\nstrap samples not containing that observation. The leave-one-out bootstrap\\nestimate of prediction error is deﬁned by\\nˆErr(1)=1\\nNN∑\\ni=11\\n|C−i|∑\\nb∈C−iL(yi,ˆf∗b(xi)). (7.56)\\nHereC−iis the set of indices of the bootstrap samples bthat do notcontain\\nobservation i, and|C−i|is the number of such samples. In computing ˆErr(1),\\nwe either have to choose Blarge enough to ensure that all of the |C−i|are\\ngreater than zero, or we can just leave out the terms in (7.56) corresponding\\nto|C−i|’s that are zero.\\nThe leave-one out bootstrap solves the overﬁtting problem suﬀered by\\nˆErrboot, but has the training-set-size bias mentioned in the discussion of\\ncross-validation. The average number of distinct observations in each boot-\\nstrap sample is about 0 .632≤N, so its bias will roughly behave like that of\\ntwofold cross-validation. Thus if the learning curve has considerable slope\\nat sample size N/2, the leave-one out bootstrap will be biased upward as\\nan estimate of the true error.\\nThe “ .632 estimator” is designed to alleviate this bias. It is deﬁned by\\nˆErr(.632)=.368≤err +.632≤ˆErr(1). (7.57)\\nThe derivation of the .632 estimator is complex; intuitively it pulls the\\nleave-one out bootstrap estimate down toward the training error rate, and\\nhence reduces its upward bias. The use of the constant .632 relates to (7.55).\\nThe.632 estimator works well in “light ﬁtting” situations, but can break\\ndown in overﬁt ones. Here is an example due to Breiman et al. (1984).\\nSuppose we have two equal-size classes, with the targets independent of\\nthe class labels, and we apply a one-nearest neighbor rule. Then err = 0,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 270}, page_content='252 7. Model Assessment and Selection\\nˆErr(1)= 0.5 and so ˆErr(.632)=.632×0.5 =.316. However, the true error\\nrate is 0.5.\\nOne can improve the .632 estimator by taking into account the amount\\nof overﬁtting. First we deﬁne γto be the no-information error rate : this\\nis the error rate of our prediction rule if the inputs and class labels were\\nindependent. An estimate of γis obtained by evaluating the prediction rule\\non all possible combinations of targets yiand predictors xi′\\nˆγ=1\\nN2N∑\\ni=1N∑\\ni′=1L(yi,ˆf(xi′)). (7.58)\\nFor example, consider the dichotomous classiﬁcation problem: let ˆ p1be\\nthe observed proportion of responses yiequaling 1, and let ˆ q1be the ob-\\nserved proportion of predictions ˆf(xi′) equaling 1. Then\\nˆγ= ˆp1(1−ˆq1) + (1 −ˆp1)ˆq1. (7.59)\\nWith a rule like 1-nearest neighbors for which ˆ q1= ˆp1the value of ˆ γis\\n2ˆp1(1−ˆp1). The multi-category generalization of (7.59) is ˆ γ=∑\\nℓˆpℓ(1−ˆqℓ).\\nUsing this, the relative overﬁtting rate is deﬁned to be\\nˆR=ˆErr(1)−err\\nˆγ−err, (7.60)\\na quantity that ranges from 0 if there is no overﬁtting ( ˆErr(1)=err) to 1\\nif the overﬁtting equals the no-information value ˆ γ−err. Finally, we deﬁne\\nthe “.632+” estimator by\\nˆErr(.632+)= (1 −ˆw)≤err + ˆw≤ˆErr(1)(7.61)\\nwith ˆw=.632\\n1−.368ˆR.\\nThe weight wranges from .632 if ˆR= 0 to 1 if ˆR= 1, so ˆErr(.632+)\\nranges from ˆErr(.632)toˆErr(1). Again, the derivation of (7.61) is compli-\\ncated: roughly speaking, it produces a compromise between the leave-one-\\nout bootstrap and the training error rate that depends on the amount of\\noverﬁtting. For the 1-nearest-neighbor problem with class labels indepen-\\ndent of the inputs, ˆ w=ˆR= 1, so ˆErr(.632+)=ˆErr(1), which has the correct\\nexpectation of 0.5. In other problems with less overﬁtting, ˆErr(.632+)will\\nlie somewhere between err and ˆErr(1).\\n7.11.1 Example (Continued)\\nFigure 7.13 shows the results of tenfold cross-validation and the .632+ bo ot-\\nstrap estimate in the same four problems of Figures 7.7. As in that ﬁgure,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 271}, page_content='7.11 Bootstrap Methods 253\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestCross−validation\\nreg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBootstrap\\nFIGURE 7.13. Boxplots show the distribution of the relative error\\n100≤[Errˆα−min αErr(α)]/[max αErr(α)−min αErr(α)]over the four scenar-\\nios of Figure 7.3. This is the error in using the chosen model re lative to the best\\nmodel. There are 100training sets represented in each boxplot.\\nFigure 7.13 shows boxplots of 100 ≤[Errˆα−minαErr(α)]/[max αErr(α)−\\nminαErr(α)], the error in using the chosen model relative to the best model.\\nThere are 100 diﬀerent training sets represented in each boxplot. Both mea-\\nsures perform well overall, perhaps the same or slightly worse that the AI C\\nin Figure 7.7.\\nOur conclusion is that for these particular problems and ﬁtting methods,\\nminimization of either AIC, cross-validation or bootstrap yields a model\\nfairly close to the best available. Note that for the purpose of model selec-\\ntion, any of the measures could be biased and it wouldn’t aﬀect things, as\\nlong as the bias did not change the relative performance of the methods.\\nFor example, the addition of a constant to any of the measures would not\\nchange the resulting chosen model. However, for many adaptive, nonlinear\\ntechniques (like trees), estimation of the eﬀective number of parameters is\\nvery diﬃcult. This makes methods like AIC impractical and leaves us with\\ncross-validation or bootstrap as the methods of choice.\\nA diﬀerent question is: how well does each method estimate test error?\\nOn the average the AIC criterion overestimated prediction error of its cho-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 272}, page_content='254 7. Model Assessment and Selection\\nsen model by 38%, 37%, 51%, and 30%, respectively, over the four scenarios,\\nwith BIC performing similarly. In contrast, cross-validation over estimated\\nthe error by 1%, 4%, 0%, and 4%, with the bootstrap doing about the\\nsame. Hence the extra work involved in computing a cross-validation or\\nbootstrap measure is worthwhile, if an accurate estimate of test error i s\\nrequired. With other ﬁtting methods like trees, cross-validation and boot-\\nstrap can underestimate the true error by 10%, because the search for best\\ntree is strongly aﬀected by the validation set. In these situations only a\\nseparate test set will provide an unbiased estimate of test error.\\n7.12 Conditional or Expected Test Error?\\nFigures 7.14 and 7.15 examine the question of whether cross-validation does\\na good job in estimating Err T, the error conditional on a given training set\\nT(expression (7.15) on page 228), as opposed to the expected test error.\\nFor each of 100 training sets generated from the “reg/linear” setting in\\nthe top-right panel of Figure 7.3, Figure 7.14 shows the conditional error\\ncurves Err Tas a function of subset size (top left). The next two panels show\\n10-fold and N-fold cross-validation, the latter also known as leave-one-out\\n(LOO). The thick red curve in each plot is the expected error Err, while\\nthe thick black curves are the expected cross-validation curves. The lower\\nright panel shows how well cross-validation approximates the conditional\\nand expected error.\\nOne might have expected N-fold CV to approximate Err Twell, since it\\nalmost uses the full training sample to ﬁt a new test point. 10-fold CV, on\\nthe other hand, might be expected to estimate Err well, since it averages\\nover somewhat diﬀerent training sets. From the ﬁgure it appears 10-fold\\ndoes a better job than N-fold in estimating Err T, and estimates Err even\\nbetter. Indeed, the similarity of the two black curves with the red curve\\nsuggests both CV curves are approximately unbiased for Err, with 10-fold\\nhaving less variance. Similar trends were reported by Efron (1983).\\nFigure 7.15 shows scatterplots of both 10-fold and N-fold cross-validation\\nerror estimates versus the true conditional error for the 100 simulations.\\nAlthough the scatterplots do not indicate much correlation, the lower right\\npanel shows that for the most part the correlations are negative, a curi-\\nous phenomenon that has been observed before. This negative correlation\\nexplains why neither form of CV estimates Err Twell. The broken lines in\\neach plot are drawn at Err( p), the expected error for the best subset of\\nsizep. We see again that both forms of CV are approximately unbiased for\\nexpected error, but the variation in test error for diﬀerent training sets is\\nquite substantial.\\nAmong the four experimental conditions in 7.3, this “reg/linear” scenario\\nshowed the highest correlation between actual and predicted test error. This'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 273}, page_content='7.12 Conditional or Expected Test Error? 255\\n5 10 15 200.1 0.2 0.3 0.4Prediction Error\\nSubset Size pError\\n5 10 15 200.1 0.2 0.3 0.410−Fold CV Error\\nSubset Size pError\\n5 10 15 200.1 0.2 0.3 0.4Leave−One−Out CV Error\\nSubset Size pError\\n5 10 15 200.015 0.025 0.035 0.045Approximation Error\\nSubset Size pMean Absolute DeviationET|CV10−Err|\\nET|CV10−ErrT|\\nET|CVN−ErrT|\\nFIGURE 7.14. Conditional prediction-error ErrT,10-fold cross-validation, and\\nleave-one-out cross-validation curves for a 100simulations from the top-right\\npanel in Figure 7.3. The thick red curve is the expected predict ion error Err,\\nwhile the thick black curves are the expected CV curves ETCV10andETCVN.\\nThe lower-right panel shows the mean absolute deviation of th e CV curves from\\nthe conditional error, ET|CVK−ErrT|forK= 10(blue) and K=N(green),\\nas well as from the expected error ET|CV10−Err|(orange).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 274}, page_content='256 7. Model Assessment and Selection\\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 1\\nPrediction ErrorCV Error\\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 5\\nPrediction ErrorCV Error\\n0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 10\\nPrediction ErrorCV Error\\n5 10 15 20−0.6 −0.4 −0.2 0.0 0.2\\nSubset SizeCorrelation\\nLeave−one−out\\n10−Fold\\nFIGURE 7.15. Plots of the CV estimates of error versus the true conditional\\nerror for each of the 100training sets, for the simulation setup in the top right\\npanel Figure 7.3. Both 10-fold and leave-one-out CV are depicted in diﬀerent\\ncolors. The ﬁrst three panels correspond to diﬀerent subset size sp, and vertical\\nand horizontal lines are drawn at Err(p). Although there appears to be little cor-\\nrelation in these plots, we see in the lower right panel that fo r the most part the\\ncorrelation is negative .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 275}, page_content='Exercises 257\\nphenomenon also occurs for bootstrap estimates of error, and we would\\nguess, for any other estimate of conditional prediction error.\\nWe conclude that estimation of test error for a particular training set is\\nnot easy in general, given just the data from that same training set. Instead,\\ncross-validation and related methods may provide reasonable estimates of\\ntheexpected error Err.\\nBibliographic Notes\\nKey references for cross-validation are Stone (1974), Stone (1977) and\\nAllen (1974). The AIC was proposed by Akaike (1973), while the BIC\\nwas introduced by Schwarz (1978). Madigan and Raftery (1994) give an\\noverview of Bayesian model selection. The MDL criterion is due to Rissa-\\nnen (1983). Cover and Thomas (1991) contains a good description of coding\\ntheory and complexity. VC dimension is described in Vapnik (1996). Stone\\n(1977) showed that the AIC and leave-one out cross-validation are asymp-\\ntotically equivalent. Generalized cross-validation is described by Golub et\\nal. (1979) and Wahba (1980); a further discussion of the topic may be found\\nin the monograph by Wahba (1990). See also Hastie and Tibshirani (1990),\\nChapter 3. The bootstrap is due to Efron (1979); see Efron and Tibshi-\\nrani (1993) for an overview. Efron (1983) proposes a number of bootst rap\\nestimates of prediction error, including the optimism and .632 estimates.\\nEfron (1986) compares CV, GCV and bootstrap estimates of error rates.\\nThe use of cross-validation and the bootstrap for model selection is stud-\\nied by Breiman and Spector (1992), Breiman (1992), Shao (1996), Zhang\\n(1993) and Kohavi (1995). The .632+ estimator was proposed by Efron\\nand Tibshirani (1997).\\nCherkassky and Ma (2003) published a study on the performance of\\nSRM for model selection in regression, in response to our study of section\\n7.9.1. They complained that we had been unfair to SRM because had not\\napplied it properly. Our response can be found in the same issue of the\\njournal (Hastie et al. (2003)).\\nExercises\\nEx. 7.1 Derive the estimate of in-sample error (7.24).\\nEx. 7.2 For 0–1 loss with Y∈ {0,1}and Pr( Y= 1|x0) =f(x0), show that\\nErr(x0) = Pr( Y̸=ˆG(x0)|X=x0)\\n= Err B(x0) +|2f(x0)−1|Pr(ˆG(x0)̸=G(x0)|X=x0),\\n(7.62)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 276}, page_content='258 7. Model Assessment and Selection\\nwhere ˆG(x) =I(ˆf(x)>1\\n2),G(x) =I(f(x)>1\\n2) is the Bayes classiﬁer,\\nand Err B(x0) = Pr( Y̸=G(x0)|X=x0), the irreducible Bayes error atx0.\\nUsing the approximation ˆf(x0)∼N(Eˆf(x0),Var(ˆf(x0)), show that\\nPr(ˆG(x0)̸=G(x0)|X=x0)≈Φ(\\nsign(1\\n2−f(x0))(Eˆf(x0)−1\\n2)√\\nVar(ˆf(x0)))\\n.(7.63)\\nIn the above,\\nΦ(t) =1√\\n2π∫t\\n−∞exp(−t2/2)dt,\\nthe cumulative Gaussian distribution function. This is an increasing func-\\ntion, with value 0 at t=−∞and value 1 at t= +∞.\\nWe can think of sign(1\\n2−f(x0))(Eˆf(x0)−1\\n2) as a kind of boundary-\\nbiasterm, as it depends on the true f(x0) only through which side of the\\nboundary (1\\n2) that it lies. Notice also that the bias and variance combine\\nin a multiplicative rather than additive fashion. If E ˆf(x0) is on the same\\nside of1\\n2asf(x0), then the bias is negative, and decreasing the variance\\nwill decrease the misclassiﬁcation error. On the other hand, if E ˆf(x0) is\\non the opposite side of1\\n2tof(x0), then the bias is positive and it pays to\\nincrease the variance! Such an increase will improve the chance that ˆf(x0)\\nfalls on the correct side of1\\n2(Friedman, 1997).\\nEx. 7.3 Letˆf=Sybe a linear smoothing of y.\\n(a) IfSiiis the ith diagonal element of S, show that for Sarising from least\\nsquares projections and cubic smoothing splines, the cross-validated\\nresidual can be written as\\nyi−ˆf−i(xi) =yi−ˆf(xi)\\n1−Sii. (7.64)\\n(b) Use this result to show that |yi−ˆf−i(xi)| ≥ |yi−ˆf(xi)|.\\n(c) Find general conditions on any smoother Sto make result (7.64) hold.\\nEx. 7.4 Consider the in-sample prediction error (7.18) and the training\\nerrorerr in the case of squared-error loss:\\nErrin=1\\nNN∑\\ni=1EY0(Y0\\ni−ˆf(xi))2\\nerr =1\\nNN∑\\ni=1(yi−ˆf(xi))2.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 277}, page_content='Exercises 259\\nAdd and subtract f(xi) and E ˆf(xi) in each expression and expand. Hence\\nestablish that the average optimism in the training error is\\n2\\nNN∑\\ni=1Cov(ˆyi,yi),\\nas given in (7.21).\\nEx. 7.5 For a linear smoother ˆy=Sy, show that\\nN∑\\ni=1Cov(ˆyi,yi) = trace( S)σ2\\nε, (7.65)\\nwhich justiﬁes its use as the eﬀective number of parameters.\\nEx. 7.6 Show that for an additive-error model, the eﬀective degrees-of-\\nfreedom for the k-nearest-neighbors regression ﬁt is N/k.\\nEx. 7.7 Use the approximation 1 /(1−x)2≈1+2xto expose the relationship\\nbetween Cp/AIC (7.26) and GCV (7.52), the main diﬀerence being the\\nmodel used to estimate the noise variance σ2\\nε.\\nEx. 7.8 Show that the set of functions {I(sin(αx)>0)}can shatter the\\nfollowing points on the line:\\nz1= 10−1,... ,zℓ= 10−ℓ, (7.66)\\nfor any ℓ. Hence the VC dimension of the class {I(sin(αx)>0)}is inﬁnite.\\nEx. 7.9 For the prostate data of Chapter 3, carry out a best-subset linear\\nregression analysis, as in Table 3.3 (third column from left). Compute t he\\nAIC, BIC, ﬁve- and tenfold cross-validation, and bootstrap .632 estimat es\\nof prediction error. Discuss the results.\\nEx. 7.10 Referring to the example in Section 7.10.3, suppose instead that\\nall of the ppredictors are binary, and hence there is no need to estimate\\nsplit points. The predictors are independent of the class labels as before.\\nThen if pis very large, we can probably ﬁnd a predictor that splits the\\nentire training data perfectly, and hence would split the validation data\\n(one-ﬁfth of data) perfectly as well. This predictor would therefore have\\nzero cross-validation error. Does this mean that cross-validation does not\\nprovide a good estimate of test error in this situation? [This question wa s\\nsuggested by Li Ma.]'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 278}, page_content='260 7. Model Assessment and Selection'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 279}, page_content='This is page 261\\nPrinter: Opaque this\\n8\\nModel Inference and Averaging\\n8.1 Introduction\\nFor most of this book, the ﬁtting (learning) of models has been achieved by\\nminimizing a sum of squares for regression, or by minimizing cross-entropy\\nfor classiﬁcation. In fact, both of these minimizations are instances of the\\nmaximum likelihood approach to ﬁtting.\\nIn this chapter we provide a general exposition of the maximum likeli-\\nhood approach, as well as the Bayesian method for inference. The boot-\\nstrap, introduced in Chapter 7, is discussed in this context, and its relation\\nto maximum likelihood and Bayes is described. Finally, we present some\\nrelated techniques for model averaging and improvement, including com-\\nmittee methods, bagging, stacking and bumping.\\n8.2 The Bootstrap and Maximum Likelihood\\nMethods\\n8.2.1 A Smoothing Example\\nThe bootstrap method provides a direct computational way of assessing\\nuncertainty, by sampling from the training data. Here we illustrate the\\nbootstrap in a simple one-dimensional smoothing problem, and show its\\nconnection to maximum likelihood.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 280}, page_content='262 8. Model Inference and Averaging\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.00.0 0.2 0.4 0.6 0.8 1.0\\nxB-spline Basis\\nFIGURE 8.1. (Left panel): Data for smoothing example. (Right panel:) Set of\\nseven B-spline basis functions. The broken vertical lines indicate the p lacement\\nof the three knots.\\nDenote the training data by Z={z1,z2,... ,z N}, with zi= (xi,yi),\\ni= 1,2,... ,N . Here xiis a one-dimensional input, and yithe outcome,\\neither continuous or categorical. As an example, consider the N= 50 data\\npoints shown in the left panel of Figure 8.1.\\nSuppose we decide to ﬁt a cubic spline to the data, with three knots\\nplaced at the quartiles of the Xvalues. This is a seven-dimensional lin-\\near space of functions, and can be represented, for example, by a linear\\nexpansion of B-spline basis functions (see Section 5.9.2):\\nθ(x) =7∑\\nj=1βjhj(x). (8.1)\\nHere the hj(x),j= 1,2,... ,7 are the seven functions shown in the right\\npanel of Figure 8.1. We can think of θ(x) as representing the conditional\\nmean E( Y|X=x).\\nLetHbe the N×7 matrix with ijth element hj(xi). The usual estimate\\nofβ, obtained by minimizing the squared error over the training set, is\\ngiven by\\nˆβ= (HTH)−1HTy. (8.2)\\nThe corresponding ﬁt ˆ θ(x) =∑7\\nj=1ˆβjhj(x) is shown in the top left panel\\nof Figure 8.2.\\nThe estimated covariance matrix of ˆβis\\nˆVar(ˆβ) = (HTH)−1ˆσ2, (8.3)\\nwhere we have estimated the noise variance by ˆ σ2=∑N\\ni=1(yi−ˆθ(xi))2/N.\\nLetting h(x)T= (h1(x),h2(x),... ,h 7(x)), the standard error of a predic-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 281}, page_content='8.2 The Bootstrap and Maximum Likelihood Methods 263\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\nxy\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\\nxy\\n••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\nFIGURE 8.2. (Top left:) B-spline smooth of data. (Top right:) B-spline smooth\\nplus and minus 1.96×standard error bands. (Bottom left:) Ten bootstrap repli-\\ncates of the B-spline smooth. (Bottom right:) B-spline smooth with 95% standard\\nerror bands computed from the bootstrap distribution.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 282}, page_content='264 8. Model Inference and Averaging\\ntion ˆθ(x) =h(x)Tˆβis\\nˆse[ˆθ(x)] = [h(x)T(HTH)−1h(x)]1\\n2ˆσ. (8.4)\\nIn the top right panel of Figure 8.2 we have plotted ˆ θ(x)±1.96≤ˆse[ˆθ(x)].\\nSince 1.96 is the 97.5% point of the standard normal distribution, these\\nrepresent approximate 100 −2×2.5% = 95% pointwise conﬁdence bands\\nforθ(x).\\nHere is how we could apply the bootstrap in this example. We draw B\\ndatasets each of size N= 50 with replacement from our training data, the\\nsampling unit being the pair zi= (xi,yi). To each bootstrap dataset Z∗\\nwe ﬁt a cubic spline ˆ θ∗(x); the ﬁts from ten such samples are shown in the\\nbottom left panel of Figure 8.2. Using B= 200 bootstrap samples, we can\\nform a 95% pointwise conﬁdence band from the percentiles at each x: we\\nﬁnd the 2 .5%×200 = ﬁfth largest and smallest values at each x. These are\\nplotted in the bottom right panel of Figure 8.2. The bands look similar to\\nthose in the top right, being a little wider at the endpoints.\\nThere is actually a close connection between the least squares estimates\\n(8.2) and (8.3), the bootstrap, and maximum likelihood. Suppose we further\\nassume that the model errors are Gaussian,\\nY=θ(X) +ε;ε∼N(0,σ2),\\nθ(x) =7∑\\nj=1βjhj(x). (8.5)\\nThe bootstrap method described above, in which we sample with re-\\nplacement from the training data, is called the nonparametric bootstrap .\\nThis really means that the method is “model-free,” since it uses the raw\\ndata, not a speciﬁc parametric model, to generate new datasets. Consider\\na variation of the bootstrap, called the parametric bootstrap , in which we\\nsimulate new responses by adding Gaussian noise to the predicted values:\\ny∗\\ni= ˆθ(xi) +ε∗\\ni;ε∗\\ni∼N(0,ˆσ2);i= 1,2,... ,N. (8.6)\\nThis process is repeated Btimes, where B= 200 say. The resulting boot-\\nstrap datasets have the form ( x1,y∗\\n1),... ,(xN,y∗\\nN) and we recompute the\\nB-spline smooth on each. The conﬁdence bands from this method will ex-\\nactly equal the least squares bands in the top right panel, as the number of\\nbootstrap samples goes to inﬁnity. A function estimated from a bootstrap\\nsample y∗is given by ˆ θ∗(x) =h(x)T(HTH)−1HTy∗, and has distribution\\nˆθ∗(x)∼N(ˆθ(x),h(x)T(HTH)−1h(x)ˆσ2). (8.7)\\nNotice that the mean of this distribution is the least squares estimate, and\\nthe standard deviation is the same as the approximate formula (8.4).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 283}, page_content='8.2 The Bootstrap and Maximum Likelihood Methods 265\\n8.2.2 Maximum Likelihood Inference\\nIt turns out that the parametric bootstrap agrees with least squares in the\\nprevious example because the model (8.5) has additive Gaussian errors. In\\ngeneral, the parametric bootstrap agrees not with least squares but with\\nmaximum likelihood, which we now review.\\nWe begin by specifying a probability density or probability mass function\\nfor our observations\\nzi∼gθ(z). (8.8)\\nIn this expression θrepresents one or more unknown parameters that gov-\\nern the distribution of Z. This is called a parametric model forZ. As an\\nexample, if Zhas a normal distribution with mean θand variance σ2, then\\nθ= (θ,σ2), (8.9)\\nand\\ngθ(z) =1√\\n2πσe−1\\n2(z−θ)2/σ2. (8.10)\\nMaximum likelihood is based on the likelihood function , given by\\nL(θ;Z) =N∏\\ni=1gθ(zi), (8.11)\\nthe probability of the observed data under the model gθ. The likelihood is\\ndeﬁned only up to a positive multiplier, which we have taken to be one.\\nWe think of L(θ;Z) as a function of θ, with our data Zﬁxed.\\nDenote the logarithm of L(θ;Z) by\\nℓ(θ;Z) =N∑\\ni=1ℓ(θ;zi)\\n=N∑\\ni=1loggθ(zi), (8.12)\\nwhich we will sometimes abbreviate as ℓ(θ). This expression is called the\\nlog-likelihood, and each value ℓ(θ;zi) = log gθ(zi) is called a log-likelihood\\ncomponent. The method of maximum likelihood chooses the value θ=ˆθ\\nto maximize ℓ(θ;Z).\\nThe likelihood function can be used to assess the precision of ˆθ. We need\\na few more deﬁnitions. The score function is deﬁned by\\n˙ℓ(θ;Z) =N∑\\ni=1˙ℓ(θ;zi), (8.13)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 284}, page_content='266 8. Model Inference and Averaging\\nwhere ˙ℓ(θ;zi) =∂ℓ(θ;zi)/∂θ. Assuming that the likelihood takes its maxi-\\nmum in the interior of the parameter space, ˙ℓ(ˆθ;Z) = 0. The information\\nmatrix is\\nI(θ) =−N∑\\ni=1∂2ℓ(θ;zi)\\n∂θ∂θT. (8.14)\\nWhenI(θ) is evaluated at θ=ˆθ, it is often called the observed information .\\nTheFisher information (or expected information) is\\ni(θ) = E θ[I(θ)]. (8.15)\\nFinally, let θ0denote the true value of θ.\\nA standard result says that the sampling distribution of the maximum\\nlikelihood estimator has a limiting normal distribution\\nˆθ→N(θ0,i(θ0)−1), (8.16)\\nasN→ ∞. Here we are independently sampling from gθ0(z). This suggests\\nthat the sampling distribution of ˆθmay be approximated by\\nN(ˆθ,i(ˆθ)−1) orN(ˆθ,I(ˆθ)−1), (8.17)\\nwhere ˆθrepresents the maximum likelihood estimate from the observed\\ndata.\\nThe corresponding estimates for the standard errors of ˆθjare obtained\\nfrom\\n√\\ni(ˆθ)−1\\njj and√\\nI(ˆθ)−1\\njj. (8.18)\\nConﬁdence points for θjcan be constructed from either approximation\\nin (8.17). Such a conﬁdence point has the form\\nˆθj−z(1−α)≤√\\ni(ˆθ)−1\\njj or ˆθj−z(1−α)≤√\\nI(ˆθ)−1\\njj,\\nrespectively, where z(1−α)is the 1 −αpercentile of the standard normal\\ndistribution. More accurate conﬁdence intervals can be derived from the\\nlikelihood function, by using the chi-squared approximation\\n2[ℓ(ˆθ)−ℓ(θ0)]∼χ2\\np, (8.19)\\nwhere pis the number of components in θ. The resulting 1 −2αconﬁ-\\ndence interval is the set of all θ0such that 2[ ℓ(ˆθ)−ℓ(θ0)]≤χ2\\np(1−2α),\\nwhere χ2\\np(1−2α)is the 1 −2αpercentile of the chi-squared distribution with\\npdegrees of freedom.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 285}, page_content='8.3 Bayesian Methods 267\\nLet’s return to our smoothing example to see what maximum likelihood\\nyields. The parameters are θ= (β,σ2). The log-likelihood is\\nℓ(θ) =−N\\n2logσ22π−1\\n2σ2N∑\\ni=1(yi−h(xi)Tβ)2. (8.20)\\nThe maximum likelihood estimate is obtained by setting ∂ℓ/∂β = 0 and\\n∂ℓ/∂σ2= 0, giving\\nˆβ= (HTH)−1HTy,\\nˆσ2=1\\nN∑\\n(yi−ˆθ(xi))2,(8.21)\\nwhich are the same as the usual estimates given in (8.2) and below (8.3).\\nThe information matrix for θ= (β,σ2) is block-diagonal, and the block\\ncorresponding to βis\\nI(β) = (HTH)/σ2, (8.22)\\nso that the estimated variance ( HTH)−1ˆσ2agrees with the least squares\\nestimate (8.3).\\n8.2.3 Bootstrap versus Maximum Likelihood\\nIn essence the bootstrap is a computer implementation of nonparametric or\\nparametric maximum likelihood. The advantage of the bootstrap over the\\nmaximum likelihood formula is that it allows us to compute maximum like-\\nlihood estimates of standard errors and other quantities in settings where\\nno formulas are available.\\nIn our example, suppose that we adaptively choose by cross-validation\\nthe number and position of the knots that deﬁne the B-splines, rather\\nthan ﬁx them in advance. Denote by λthe collection of knots and their\\npositions. Then the standard errors and conﬁdence bands should account\\nfor the adaptive choice of λ, but there is no way to do this analytically.\\nWith the bootstrap, we compute the B-spline smooth with an adaptive\\nchoice of knots for each bootstrap sample. The percentiles of the resulting\\ncurves capture the variability from both the noise in the targets as well as\\nthat from ˆλ. In this particular example the conﬁdence bands (not shown)\\ndon’t look much diﬀerent than the ﬁxed λbands. But in other problems,\\nwhere more adaptation is used, this can be an important eﬀect to capture.\\n8.3 Bayesian Methods\\nIn the Bayesian approach to inference, we specify a sampling model Pr( Z|θ)\\n(density or probability mass function) for our data given the parameters,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 286}, page_content='268 8. Model Inference and Averaging\\nand a prior distribution for the parameters Pr( θ) reﬂecting our knowledge\\nabout θbefore we see the data. We then compute the posterior distribution\\nPr(θ|Z) =Pr(Z|θ)≤Pr(θ)∫\\nPr(Z|θ)≤Pr(θ)dθ, (8.23)\\nwhich represents our updated knowledge about θafter we see the data. To\\nunderstand this posterior distribution, one might draw samples from it or\\nsummarize by computing its mean or mode. The Bayesian approach diﬀers\\nfrom the standard (“frequentist”) method for inference in its use of a prior\\ndistribution to express the uncertainty present before seeing the data, and\\nto allow the uncertainty remaining after seeing the data to be expressed in\\nthe form of a posterior distribution.\\nThe posterior distribution also provides the basis for predicting the values\\nof a future observation znew, via the predictive distribution :\\nPr(znew|Z) =∫\\nPr(znew|θ)≤Pr(θ|Z)dθ. (8.24)\\nIn contrast, the maximum likelihood approach would use Pr( znew|ˆθ),\\nthe data density evaluated at the maximum likelihood estimate, to predict\\nfuture data. Unlike the predictive distribution (8.24), this does not account\\nfor the uncertainty in estimating θ.\\nLet’s walk through the Bayesian approach in our smoothing example.\\nWe start with the parametric model given by equation (8.5), and assume\\nfor the moment that σ2is known. We assume that the observed feature\\nvalues x1,x2,... ,x Nare ﬁxed, so that the randomness in the data comes\\nsolely from yvarying around its mean θ(x).\\nThe second ingredient we need is a prior distribution. Distributions on\\nfunctions are fairly complex entities: one approach is to use a Gaussian\\nprocess prior in which we specify the prior covariance between any two\\nfunction values θ(x) and θ(x′) (Wahba, 1990; Neal, 1996).\\nHere we take a simpler route: by considering a ﬁnite B-spline basis for\\nθ(x), we can instead provide a prior for the coeﬃcients β, and this implicitly\\ndeﬁnes a prior for θ(x). We choose a Gaussian prior centered at zero\\nβ∼N(0,τΣ) (8.25)\\nwith the choices of the prior correlation matrix Σand variance τto be\\ndiscussed below. The implicit process prior for θ(x) is hence Gaussian,\\nwith covariance kernel\\nK(x,x′) = cov[ θ(x),θ(x′)]\\n=τ≤h(x)TΣh(x′). (8.26)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 287}, page_content='8.3 Bayesian Methods 269\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-3 -2 -1 0 1 2 3θ(x)\\nx\\nFIGURE 8.3. Smoothing example: Ten draws from the Gaussian prior distri-\\nbution for the function θ(x).\\nThe posterior distribution for βis also Gaussian, with mean and covariance\\nE(β|Z) =(\\nHTH+σ2\\nτΣ−1)−1\\nHTy,\\ncov(β|Z) =(\\nHTH+σ2\\nτΣ−1)−1\\nσ2,(8.27)\\nwith the corresponding posterior values for θ(x),\\nE(θ(x)|Z) =h(x)T(\\nHTH+σ2\\nτΣ−1)−1\\nHTy,\\ncov[θ(x),θ(x′)|Z] =h(x)T(\\nHTH+σ2\\nτΣ−1)−1\\nh(x′)σ2.(8.28)\\nHow do we choose the prior correlation matrix Σ? In some settings the\\nprior can be chosen from subject matter knowledge about the parameters.\\nHere we are willing to say the function θ(x) should be smooth, and have\\nguaranteed this by expressing θin a smooth low-dimensional basis of B-\\nsplines. Hence we can take the prior correlation matrix to be the identity\\nΣ=I. When the number of basis functions is large, this might not be suf-\\nﬁcient, and additional smoothness can be enforced by imposing restrictions\\nonΣ; this is exactly the case with smoothing splines (Section 5.8.1).\\nFigure 8.3 shows ten draws from the corresponding prior for θ(x). To\\ngenerate posterior values of the function θ(x), we generate values β′from its\\nposterior (8.27), giving corresponding posterior value θ′(x) =∑7\\n1β′\\njhj(x).\\nTen such posterior curves are shown in Figure 8.4. Two diﬀerent values\\nwere used for the prior variance τ, 1 and 1000. Notice how similar the\\nright panel looks to the bootstrap distribution in the bottom left panel'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 288}, page_content='270 8. Model Inference and Averaging\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\\n••••••••••\\n••••\\n•\\n•\\n•••\\n•••••••••\\n••••••••\\n••••\\n••\\n••θ(x)θ(x)\\nx xτ= 1 τ= 1000\\nFIGURE 8.4. Smoothing example: Ten draws from the posterior distribution\\nfor the function θ(x), for two diﬀerent values of the prior variance τ. The purple\\ncurves are the posterior means.\\nof Figure 8.2 on page 263. This similarity is no accident. As τ→ ∞, the\\nposterior distribution (8.27) and the bootstrap distribution (8.7) co incide.\\nOn the other hand, for τ= 1, the posterior curves θ(x) in the left panel\\nof Figure 8.4 are smoother than the bootstrap curves, because we have\\nimposed more prior weight on smoothness.\\nThe distribution (8.25) with τ→ ∞ is called a noninformative prior for\\nθ. In Gaussian models, maximum likelihood and parametric bootstrap anal -\\nyses tend to agree with Bayesian analyses that use a noninformative prior\\nfor the free parameters. These tend to agree, because with a constant prior,\\nthe posterior distribution is proportional to the likelihood. This corresp on-\\ndence also extends to the nonparametric case, where the nonparametric\\nbootstrap approximates a noninformative Bayes analysis; Section 8.4 has\\nthe details.\\nWe have, however, done some things that are not proper from a Bayesian\\npoint of view. We have used a noninformative (constant) prior for σ2and\\nreplaced it with the maximum likelihood estimate ˆ σ2in the posterior. A\\nmore standard Bayesian analysis would also put a prior on σ(typically\\ng(σ)∝1/σ), calculate a joint posterior for θ(x) and σ, and then integrate\\noutσ, rather than just extract the maximum of the posterior distribution\\n(“MAP” estimate).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 289}, page_content='8.4 Relationship Between the Bootstrap and Bayesian Inference 271\\n8.4 Relationship Between the Bootstrap and\\nBayesian Inference\\nConsider ﬁrst a very simple example, in which we observe a single obser-\\nvation zfrom a normal distribution\\nz∼N(θ,1). (8.29)\\nTo carry out a Bayesian analysis for θ, we need to specify a prior. The\\nmost convenient and common choice would be θ∼N(0,τ) giving posterior\\ndistribution\\nθ|z∼N(z\\n1 + 1/τ,1\\n1 + 1/τ)\\n. (8.30)\\nNow the larger we take τ, the more concentrated the posterior becomes\\naround the maximum likelihood estimate ˆθ=z. In the limit as τ→ ∞ we\\nobtain a noninformative (constant) prior, and the posterior distribution is\\nθ|z∼N(z,1). (8.31)\\nThis is the same as a parametric bootstrap distribution in which we gen-\\nerate bootstrap values z∗from the maximum likelihood estimate of the\\nsampling density N(z,1).\\nThere are three ingredients that make this correspondence work:\\n1. The choice of noninformative prior for θ.\\n2. The dependence of the log-likelihood ℓ(θ;Z) on the data Zonly\\nthrough the maximum likelihood estimate ˆθ. Hence we can write the\\nlog-likelihood as ℓ(θ;ˆθ).\\n3. The symmetry of the log-likelihood in θandˆθ, that is, ℓ(θ;ˆθ) =\\nℓ(ˆθ;θ) + constant.\\nProperties (2) and (3) essentially only hold for the Gaussian distribu-\\ntion. However, they also hold approximately for the multinomial distribu-\\ntion, leading to a correspondence between the nonparametric bootstrap\\nand Bayes inference, which we outline next.\\nAssume that we have a discrete sample space with Lcategories. Let wjbe\\nthe probability that a sample point falls in category j, and ˆwjthe observed\\nproportion in category j. Letw= (w1,w2,... ,w L),ˆw= ( ˆw1,ˆw2,... ,ˆwL).\\nDenote our estimator by S( ˆw); take as a prior distribution for wa sym-\\nmetric Dirichlet distribution with parameter a:\\nw∼DiL(a1), (8.32)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 290}, page_content='272 8. Model Inference and Averaging\\nthat is, the prior probability mass function is proportional to∏L\\nℓ=1wa−1\\nℓ.\\nThen the posterior density of wis\\nw∼DiL(a1 +Nˆw), (8.33)\\nwhere Nis the sample size. Letting a→0 to obtain a noninformative prior\\ngives\\nw∼DiL(Nˆw). (8.34)\\nNow the bootstrap distribution, obtained by sampling with replacement\\nfrom the data, can be expressed as sampling the category proportions from\\na multinomial distribution. Speciﬁcally,\\nNˆw∗∼Mult( N,ˆw), (8.35)\\nwhere Mult( N,ˆw) denotes a multinomial distribution, having probability\\nmass function(N\\nNˆw∗\\n1,...,N ˆw∗\\nL)∏ˆwNˆw∗\\nℓ\\nℓ. This distribution is similar to the pos-\\nterior distribution above, having the same support, same mean, and nearly\\nthe same covariance matrix. Hence the bootstrap distribution of S( ˆw∗) will\\nclosely approximate the posterior distribution of S(w).\\nIn this sense, the bootstrap distribution represents an (approximate)\\nnonparametric, noninformative posterior distribution for our parameter.\\nBut this bootstrap distribution is obtained painlessly—without having to\\nformally specify a prior and without having to sample from the posterior\\ndistribution. Hence we might think of the bootstrap distribution as a “poor\\nman’s” Bayes posterior. By perturbing the data, the bootstrap approxi-\\nmates the Bayesian eﬀect of perturbing the parameters, and is typically\\nmuch simpler to carry out.\\n8.5 The EM Algorithm\\nThe EM algorithm is a popular tool for simplifying diﬃcult maximum\\nlikelihood problems. We ﬁrst describe it in the context of a simple mixture\\nmodel.\\n8.5.1 Two-Component Mixture Model\\nIn this section we describe a simple mixture model for density estimation,\\nand the associated EM algorithm for carrying out maximum likelihood\\nestimation. This has a natural connection to Gibbs sampling methods for\\nBayesian inference. Mixture models are discussed and demonstrated in sev-\\neral other parts of the book, in particular Sections 6.8, 12.7 and 13.2.3.\\nThe left panel of Figure 8.5 shows a histogram of the 20 ﬁctitious data\\npoints in Table 8.1.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 291}, page_content='8.5 The EM Algorithm 273\\n0 2 4 60.0 0.2 0.4 0.6 0.8 1.0\\ny ydensity\\n0 2 4 60.0 0.2 0.4 0.6 0.8 1.0• •• •••••••\\n•\\n•\\n••••• •• •\\nFIGURE 8.5. Mixture example. (Left panel:) Histogram of data. (Right panel: )\\nMaximum likelihood ﬁt of Gaussian densities (solid red) and resp onsibility (dotted\\ngreen) of the left component density for observation y, as a function of y.\\nTABLE 8.1. Twenty ﬁctitious data points used in the two-component mixture\\nexample in Figure 8.5.\\n-0.39 0.12 0.94 1.67 1.76 2.44 3.72 4.28 4.92 5.53\\n0.06 0.48 1.01 1.68 1.80 3.25 4.12 4.60 5.28 6.22\\nWe would like to model the density of the data points, and due to the\\napparent bi-modality, a Gaussian distribution would not be appropriate.\\nThere seems to be two separate underlying regimes, so instead we model\\nYas a mixture of two normal distributions:\\nY1∼N(θ1,σ2\\n1),\\nY2∼N(θ2,σ2\\n2), (8.36)\\nY= (1 −∆)≤Y1+ ∆≤Y2,\\nwhere ∆ ∈ {0,1}with Pr(∆ = 1) = π. This generative representation is\\nexplicit: generate a ∆ ∈ {0,1}with probability π, and then depending on\\nthe outcome, deliver either Y1orY2. Letφθ(x) denote the normal density\\nwith parameters θ= (θ,σ2). Then the density of Yis\\ngY(y) = (1 −π)φθ1(y) +πφθ2(y). (8.37)\\nNow suppose we wish to ﬁt this model to the data in Figure 8.5 by maxi-\\nmum likelihood. The parameters are\\nθ= (π,θ1,θ2) = (π,θ1,σ2\\n1,θ2,σ2\\n2). (8.38)\\nThe log-likelihood based on the Ntraining cases is\\nℓ(θ;Z) =N∑\\ni=1log[(1 −π)φθ1(yi) +πφθ2(yi)]. (8.39)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 292}, page_content='274 8. Model Inference and Averaging\\nDirect maximization of ℓ(θ;Z) is quite diﬃcult numerically, because of\\nthe sum of terms inside the logarithm. There is, however, a simpler ap-\\nproach. We consider unobserved latent variables ∆ itaking values 0 or 1 as\\nin (8.36): if ∆ i= 1 then Yicomes from model 2, otherwise it comes from\\nmodel 1. Suppose we knew the values of the ∆ i’s. Then the log-likelihood\\nwould be\\nℓ0(θ;Z,∆) =N∑\\ni=1[(1−∆i)logφθ1(yi) + ∆ ilogφθ2(yi)]\\n+N∑\\ni=1[(1−∆i)log(1 −π) + ∆ ilogπ],(8.40)\\nand the maximum likelihood estimates of θ1andσ2\\n1would be the sample\\nmean and variance for those data with ∆ i= 0, and similarly those for θ2\\nandσ2\\n2would be the sample mean and variance of the data with ∆ i= 1.\\nThe estimate of πwould be the proportion of ∆ i= 1.\\nSince the values of the ∆ i’s are actually unknown, we proceed in an\\niterative fashion, substituting for each ∆ iin (8.40) its expected value\\nγi(θ) = E(∆ i|θ,Z) = Pr(∆ i= 1|θ,Z), (8.41)\\nalso called the responsibility of model 2 for observation i. We use a proce-\\ndure called the EM algorithm, given in Algorithm 8.1 for the special case of\\nGaussian mixtures. In the expectation step, we do a soft assignment of each\\nobservation to each model: the current estimates of the parameters are used\\nto assign responsibilities according to the relative density of the training\\npoints under each model. In the maximization step, these responsibilities\\nare used in weighted maximum-likelihood ﬁts to update the estimates of\\nthe parameters.\\nA good way to construct initial guesses for ˆ θ1and ˆθ2is simply to choose\\ntwo of the yiat random. Both ˆ σ2\\n1and ˆσ2\\n2can be set equal to the overall\\nsample variance∑N\\ni=1(yi−¯y)2/N. The mixing proportion ˆ πcan be started\\nat the value 0 .5.\\nNote that the actual maximizer of the likelihood occurs when we put a\\nspike of inﬁnite height at any one data point, that is, ˆ θ1=yifor some\\niand ˆσ2\\n1= 0. This gives inﬁnite likelihood, but is not a useful solution.\\nHence we are actually looking for a good local maximum of the likelihood,\\none for which ˆ σ2\\n1,ˆσ2\\n2>0. To further complicate matters, there can be\\nmore than one local maximum having ˆ σ2\\n1,ˆσ2\\n2>0. In our example, we\\nran the EM algorithm with a number of diﬀerent initial guesses for the\\nparameters, all having ˆ σ2\\nk>0.5, and chose the run that gave us the highest\\nmaximized likelihood. Figure 8.6 shows the progress of the EM algorithm in\\nmaximizing the log-likelihood. Table 8.2 shows ˆ π=∑\\niˆγi/N, the maximum\\nlikelihood estimate of the proportion of observations in class 2, at sel ected\\niterations of the EM procedure.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 293}, page_content='8.5 The EM Algorithm 275\\nAlgorithm 8.1 EM Algorithm for Two-component Gaussian Mixture.\\n1. Take initial guesses for the parameters ˆ θ1,ˆσ2\\n1,ˆθ2,ˆσ2\\n2,ˆπ(see text).\\n2.Expectation Step : compute the responsibilities\\nˆγi=ˆπφˆθ2(yi)\\n(1−ˆπ)φˆθ1(yi) + ˆπφˆθ2(yi), i= 1,2,... ,N. (8.42)\\n3.Maximization Step : compute the weighted means and variances:\\nˆθ1=∑N\\ni=1(1−ˆγi)yi∑N\\ni=1(1−ˆγi), ˆσ2\\n1=∑N\\ni=1(1−ˆγi)(yi−ˆθ1)2\\n∑N\\ni=1(1−ˆγi),\\nˆθ2=∑N\\ni=1ˆγiyi∑N\\ni=1ˆγi, ˆσ2\\n2=∑N\\ni=1ˆγi(yi−ˆθ2)2\\n∑N\\ni=1ˆγi,\\nand the mixing probability ˆ π=∑N\\ni=1ˆγi/N.\\n4. Iterate steps 2 and 3 until convergence.\\nTABLE 8.2. Selected iterations of the EM algorithm for mixture example.\\nIteration ˆ π\\n1 0.485\\n5 0.493\\n10 0.523\\n15 0.544\\n20 0.546\\nThe ﬁnal maximum likelihood estimates are\\nˆθ1= 4.62, ˆσ2\\n1= 0.87,\\nˆθ2= 1.06, ˆσ2\\n2= 0.77,\\nˆπ= 0.546.\\nThe right panel of Figure 8.5 shows the estimated Gaussian mixture density\\nfrom this procedure (solid red curve), along with the responsibilities (dotted\\ngreen curve). Note that mixtures are also useful for supervised learning; in\\nSection 6.7 we show how the Gaussian mixture model leads to a version of\\nradial basis functions.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 294}, page_content='276 8. Model Inference and Averaging\\nIterationObserved Data Log-likelihood\\n5 10 15 20-44 -43 -42 -41 -40 -39\\nooooooooooooooo o o o o o\\nFIGURE 8.6. EM algorithm: observed data log-likelihood as a function of t he\\niteration number.\\n8.5.2 The EM Algorithm in General\\nThe above procedure is an example of the EM (or Baum–Welch) algorithm\\nfor maximizing likelihoods in certain classes of problems. These problems\\nare ones for which maximization of the likelihood is diﬃcult, but made\\neasier by enlarging the sample with latent (unobserved) data. This is called\\ndata augmentation . Here the latent data are the model memberships ∆ i.\\nIn other problems, the latent data are actual data that should have been\\nobserved but are missing.\\nAlgorithm 8.2 gives the general formulation of the EM algorithm. Our\\nobserved data is Z, having log-likelihood ℓ(θ;Z) depending on parameters\\nθ. The latent or missing data is Zm, so that the complete data is T=\\n(Z,Zm) with log-likelihood ℓ0(θ;T),ℓ0based on the complete density. In\\nthe mixture problem ( Z,Zm) = (y,∆), and ℓ0(θ;T) is given in (8.40).\\nIn our mixture example, E( ℓ0(θ′;T)|Z,ˆθ(j)) is simply (8.40) with the ∆ i\\nreplaced by the responsibilities ˆ γi(ˆθ), and the maximizers in step 3 are just\\nweighted means and variances.\\nWe now give an explanation of why the EM algorithm works in general.\\nSince\\nPr(Zm|Z,θ′) =Pr(Zm,Z|θ′)\\nPr(Z|θ′), (8.44)\\nwe can write\\nPr(Z|θ′) =Pr(T|θ′)\\nPr(Zm|Z,θ′). (8.45)\\nIn terms of log-likelihoods, we have ℓ(θ′;Z) =ℓ0(θ′;T)−ℓ1(θ′;Zm|Z), where\\nℓ1is based on the conditional density Pr( Zm|Z,θ′). Taking conditional\\nexpectations with respect to the distribution of T|Zgoverned by parameter\\nθgives\\nℓ(θ′;Z) = E[ ℓ0(θ′;T)|Z,θ]−E[ℓ1(θ′;Zm|Z)|Z,θ]'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 295}, page_content='8.5 The EM Algorithm 277\\nAlgorithm 8.2 The EM Algorithm.\\n1. Start with initial guesses for the parameters ˆθ(0).\\n2.Expectation Step : at the jth step, compute\\nQ(θ′,ˆθ(j)) = E( ℓ0(θ′;T)|Z,ˆθ(j)) (8.43)\\nas a function of the dummy argument θ′.\\n3.Maximization Step : determine the new estimate ˆθ(j+1)as the maxi-\\nmizer of Q(θ′,ˆθ(j)) over θ′.\\n4. Iterate steps 2 and 3 until convergence.\\n≡Q(θ′,θ)−R(θ′,θ). (8.46)\\nIn the Mstep, the EM algorithm maximizes Q(θ′,θ) over θ′, rather than\\nthe actual objective function ℓ(θ′;Z). Why does it succeed in maximizing\\nℓ(θ′;Z)? Note that R(θ∗,θ) is the expectation of a log-likelihood of a density\\n(indexed by θ∗), with respect to the same density indexed by θ, and hence\\n(by Jensen’s inequality) is maximized as a function of θ∗, when θ∗=θ(see\\nExercise 8.1). So if θ′maximizes Q(θ′,θ), we see that\\nℓ(θ′;Z)−ℓ(θ;Z) = [ Q(θ′,θ)−Q(θ,θ)]−[R(θ′,θ)−R(θ,θ)]\\n≥0. (8.47)\\nHence the EM iteration never decreases the log-likelihood.\\nThis argument also makes it clear that a full maximization in the M\\nstep is not necessary: we need only to ﬁnd a value ˆθ(j+1)so that Q(θ′,ˆθ(j))\\nincreases as a function of the ﬁrst argument, that is, Q(ˆθ(j+1),ˆθ(j))>\\nQ(ˆθ(j),ˆθ(j)). Such procedures are called GEM (generalized EM) algorithms.\\nThe EM algorithm can also be viewed as a minorization procedure: see\\nExercise 8.7.\\n8.5.3 EM as a Maximization–Maximization Procedure\\nHere is a diﬀerent view of the EM procedure, as a joint maximization\\nalgorithm. Consider the function\\nF(θ′,˜P) = E ˜P[ℓ0(θ′;T)]−E˜P[log˜P(Zm)]. (8.48)\\nHere ˜P(Zm) is any distribution over the latent data Zm. In the mixture\\nexample, ˜P(Zm) comprises the set of probabilities γi= Pr(∆ i= 1|θ,Z).\\nNote that Fevaluated at ˜P(Zm) = Pr( Zm|Z,θ′), is the log-likelihood of'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 296}, page_content='278 8. Model Inference and Averaging\\n1 2 3 4 50 1 2 3 40.10.3\\n0.50.7\\n0.9Model Parameters\\nLatent Data ParametersEMEM\\nFIGURE 8.7. Maximization–maximization view of the EM algorithm. Shown\\nare the contours of the (augmented) observed data log-likelih oodF(θ′,˜P). The\\nEstep is equivalent to maximizing the log-likelihood over the pa rameters of the\\nlatent data distribution. The Mstep maximizes it over the parameters of the\\nlog-likelihood. The red curve corresponds to the observed da ta log-likelihood, a\\nproﬁle obtained by maximizing F(θ′,˜P)for each value of θ′.\\nthe observed data, from (8.46)1. The function Fexpands the domain of\\nthe log-likelihood, to facilitate its maximization.\\nThe EM algorithm can be viewed as a joint maximization method for F\\noverθ′and˜P(Zm), by ﬁxing one argument and maximizing over the other.\\nThe maximizer over ˜P(Zm) for ﬁxed θ′can be shown to be\\n˜P(Zm) = Pr( Zm|Z,θ′) (8.49)\\n(Exercise 8.2). This is the distribution computed by the Estep, for example,\\n(8.42) in the mixture example. In the Mstep, we maximize F(θ′,˜P) over θ′\\nwith˜Pﬁxed: this is the same as maximizing the ﬁrst term E ˜P[ℓ0(θ′;T)|Z,θ]\\nsince the second term does not involve θ′.\\nFinally, since F(θ′,˜P) and the observed data log-likelihood agree when\\n˜P(Zm) = Pr( Zm|Z,θ′), maximization of the former accomplishes maxi-\\nmization of the latter. Figure 8.7 shows a schematic view of this process.\\nThis view of the EM algorithm leads to alternative maximization proce-\\n1(8.46) holds for all θ, including θ=θ′.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 297}, page_content='8.6 MCMC for Sampling from the Posterior 279\\nAlgorithm 8.3 Gibbs Sampler.\\n1. Take some initial values U(0)\\nk,k= 1,2,... ,K .\\n2. Repeat for t= 1,2,... ,. :\\nFork= 1,2,... ,K generate U(t)\\nkfrom\\nPr(U(t)\\nk|U(t)\\n1,... ,U(t)\\nk−1,U(t−1)\\nk+1,... ,U(t−1)\\nK).\\n3. Continue step 2 until the joint distribution of ( U(t)\\n1,U(t)\\n2,... ,U(t)\\nK)\\ndoes not change.\\ndures. For example, one does not need to maximize with respect to all of\\nthe latent data parameters at once, but could instead maximize over one\\nof them at a time, alternating with the Mstep.\\n8.6 MCMC for Sampling from the Posterior\\nHaving deﬁned a Bayesian model, one would like to draw samples from\\nthe resulting posterior distribution, in order to make inferences about the\\nparameters. Except for simple models, this is often a diﬃcult computa-\\ntional problem. In this section we discuss the Markov chain Monte Carlo\\n(MCMC) approach to posterior sampling. We will see that Gibbs sampling,\\nan MCMC procedure, is closely related to the EM algorithm: the main dif-\\nference is that it samples from the conditional distributions rather than\\nmaximizing over them.\\nConsider ﬁrst the following abstract problem. We have random variables\\nU1,U2,... ,U Kand we wish to draw a sample from their joint distribution.\\nSuppose this is diﬃcult to do, but it is easy to simulate from the conditional\\ndistributions Pr( Uj|U1,U2,... ,U j−1,Uj+1,... ,U K), j= 1,2,... ,K . The\\nGibbs sampling procedure alternatively simulates from each of these distri-\\nbutions and when the process stabilizes, provides a sample from the desired\\njoint distribution. The procedure is deﬁned in Algorithm 8.3.\\nUnder regularity conditions it can be shown that this procedure even-\\ntually stabilizes, and the resulting random variables are indeed a sample\\nfrom the joint distribution of U1,U2,... ,U K. This occurs despite the fact\\nthat the samples ( U(t)\\n1,U(t)\\n2,... ,U(t)\\nK) are clearly not independent for dif-\\nferent t. More formally, Gibbs sampling produces a Markov chain whose\\nstationary distribution is the true joint distribution, and hence the term\\n“Markov chain Monte Carlo.” It is not surprising that the true joint dis -\\ntribution is stationary under this process, as the successive steps leave the\\nmarginal distributions of the Uk’s unchanged.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 298}, page_content='280 8. Model Inference and Averaging\\nNote that we don’t need to know the explicit form of the conditional\\ndensities, but just need to be able to sample from them. After the procedure\\nreaches stationarity, the marginal density of any subset of the variables\\ncan be approximated by a density estimate applied to the sample values.\\nHowever if the explicit form of the conditional density Pr( Uk,|Uℓ,ℓ̸=k)\\nis available, a better estimate of say the marginal density of Ukcan be\\nobtained from (Exercise 8.3):\\nˆPrUk(u) =1\\n(M−m+ 1)M∑\\nt=mPr(u|U(t)\\nℓ,ℓ̸=k). (8.50)\\nHere we have averaged over the last M−m+ 1 members of the sequence,\\nto allow for an initial “burn-in” period before stationarity is reached.\\nNow getting back to Bayesian inference, our goal is to draw a sample from\\nthe joint posterior of the parameters given the data Z. Gibbs sampling will\\nbe helpful if it is easy to sample from the conditional distribution of each\\nparameter given the other parameters and Z. An example—the Gaussian\\nmixture problem—is detailed next.\\nThere is a close connection between Gibbs sampling from a posterior and\\nthe EM algorithm in exponential family models. The key is to consider the\\nlatent data Zmfrom the EM procedure to be another parameter for the\\nGibbs sampler. To make this explicit for the Gaussian mixture problem,\\nwe take our parameters to be ( θ,Zm). For simplicity we ﬁx the variances\\nσ2\\n1,σ2\\n2and mixing proportion πat their maximum likelihood values so that\\nthe only unknown parameters in θare the means θ1andθ2. The Gibbs\\nsampler for the mixture problem is given in Algorithm 8.4. We see that\\nsteps 2(a) and 2(b) are the same as the EandMsteps of the EM pro-\\ncedure, except that we sample rather than maximize. In step 2(a), rather\\nthan compute the maximum likelihood responsibilities γi= E(∆ i|θ,Z),\\nthe Gibbs sampling procedure simulates the latent data ∆ ifrom the distri-\\nbutions Pr(∆ i|θ,Z). In step 2(b), rather than compute the maximizers of\\nthe posterior Pr( θ1,θ2,∆|Z) we simulate from the conditional distribution\\nPr(θ1,θ2|∆,Z).\\nFigure 8.8 shows 200 iterations of Gibbs sampling, with the mean param-\\netersθ1(lower) and θ2(upper) shown in the left panel, and the proportion\\nof class 2 observations∑\\ni∆i/Non the right. Horizontal broken lines have\\nbeen drawn at the maximum likelihood estimate values ˆ θ1,ˆθ2and∑\\niˆγi/N\\nin each case. The values seem to stabilize quite quickly, and are distributed\\nevenly around the maximum likelihood values.\\nThe above mixture model was simpliﬁed, in order to make the clear\\nconnection between Gibbs sampling and the EM algorithm. More realisti-\\ncally, one would put a prior distribution on the variances σ2\\n1,σ2\\n2and mixing\\nproportion π, and include separate Gibbs sampling steps in which we sam-\\nple from their posterior distributions, conditional on the other parameters.\\nOne can also incorporate proper (informative) priors for the mean param-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 299}, page_content='8.6 MCMC for Sampling from the Posterior 281\\nAlgorithm 8.4 Gibbs sampling for mixtures.\\n1. Take some initial values θ(0)= (θ(0)\\n1,θ(0)\\n2).\\n2. Repeat for t= 1,2,... ,.\\n(a) For i= 1,2,... ,N generate ∆(t)\\ni∈ {0,1}with Pr(∆(t)\\ni= 1) =\\nˆγi(θ(t)), from equation (8.42).\\n(b) Set\\nˆθ1=∑N\\ni=1(1−∆(t)\\ni)≤yi∑N\\ni=1(1−∆(t)\\ni),\\nˆθ2=∑N\\ni=1∆(t)\\ni≤yi∑N\\ni=1∆(t)\\ni,\\nand generate θ(t)\\n1∼N(ˆθ1,ˆσ2\\n1) and θ(t)\\n2∼N(ˆθ2,ˆσ2\\n2).\\n3. Continue step 2 until the joint distribution of ( ∆(t),θ(t)\\n1,θ(t)\\n2) doesn’t\\nchange\\nGibbs IterationMean Parameters\\n0 50 100 150 2000 2 4 6 8\\nGibbs IterationMixing Proportion\\n0 50 100 150 2000.3 0.4 0.5 0.6 0.7\\nFIGURE 8.8. Mixture example. (Left panel:) 200values of the two mean param-\\neters from Gibbs sampling; horizontal lines are drawn at the maxi mum likelihood\\nestimates ˆθ1,ˆθ2. (Right panel:) Proportion of values with ∆i= 1, for each of the\\n200Gibbs sampling iterations; a horizontal line is drawn atP\\niˆγi/N.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 300}, page_content='282 8. Model Inference and Averaging\\neters. These priors must not be improper as this will lead to a degenerate\\nposterior, with all the mixing weight on one component.\\nGibbs sampling is just one of a number of recently developed procedures\\nfor sampling from posterior distributions. It uses conditional sampling of\\neach parameter given the rest, and is useful when the structure of the prob-\\nlem makes this sampling easy to carry out. Other methods do not require\\nsuch structure, for example the Metropolis–Hastings algorithm. These and\\nother computational Bayesian methods have been applied to sophisticated\\nlearning algorithms such as Gaussian process models and neural networks.\\nDetails may be found in the references given in the Bibliographic Notes at\\nthe end of this chapter.\\n8.7 Bagging\\nEarlier we introduced the bootstrap as a way of assessing the accuracy of a\\nparameter estimate or a prediction. Here we show how to use the bootstrap\\nto improve the estimate or prediction itself. In Section 8.4 we investigat ed\\nthe relationship between the bootstrap and Bayes approaches, and found\\nthat the bootstrap mean is approximately a posterior average. Bagging\\nfurther exploits this connection.\\nConsider ﬁrst the regression problem. Suppose we ﬁt a model to our\\ntraining data Z={(x1,y1),(x2,y2),... ,(xN,yN)}, obtaining the predic-\\ntionˆf(x) at input x. Bootstrap aggregation or bagging averages this predic-\\ntion over a collection of bootstrap samples, thereby reducing its variance.\\nFor each bootstrap sample Z∗b,b= 1,2,... ,B , we ﬁt our model, giving\\nprediction ˆf∗b(x). The bagging estimate is deﬁned by\\nˆfbag(x) =1\\nBB∑\\nb=1ˆf∗b(x). (8.51)\\nDenote by ˆPthe empirical distribution putting equal probability 1 /Non\\neach of the data points ( xi,yi). In fact the “true” bagging estimate is\\ndeﬁned by E ˆPˆf∗(x), where Z∗= (x∗\\n1,y∗\\n1),(x∗\\n2,y∗\\n2),... ,(x∗\\nN,y∗\\nN) and each\\n(x∗\\ni,y∗\\ni)∼ˆP. Expression (8.51) is a Monte Carlo estimate of the true\\nbagging estimate, approaching it as B→ ∞.\\nThe bagged estimate (8.51) will diﬀer from the original estimate ˆf(x)\\nonly when the latter is a nonlinear or adaptive function of the data. For\\nexample, to bag the B-spline smooth of Section 8.2.1, we average the curves\\nin the bottom left panel of Figure 8.2 at each value of x. The B-spline\\nsmoother is linear in the data if we ﬁx the inputs; hence if we sample using\\nthe parametric bootstrap in equation (8.6), then ˆfbag(x)→ˆf(x) asB→ ∞\\n(Exercise 8.4). Hence bagging just reproduces the original smooth in the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 301}, page_content='8.7 Bagging 283\\ntop left panel of Figure 8.2. The same is approximately true if we were to\\nbag using the nonparametric bootstrap.\\nA more interesting example is a regression tree, where ˆf(x) denotes the\\ntree’s prediction at input vector x(regression trees are described in Chap-\\nter 9). Each bootstrap tree will typically involve diﬀerent features tha n the\\noriginal, and might have a diﬀerent number of terminal nodes. The bagged\\nestimate is the average prediction at xfrom these Btrees.\\nNow suppose our tree produces a classiﬁer ˆG(x) for a K-class response.\\nHere it is useful to consider an underlying indicator-vector function ˆf(x),\\nwith value a single one and K−1 zeroes, such that ˆG(x) = arg max kˆf(x).\\nThen the bagged estimate ˆfbag(x) (8.51) is a K-vector [ p1(x),p2(x),... ,\\npK(x)], with pk(x) equal to the proportion of trees predicting class katx.\\nThe bagged classiﬁer selects the class with the most “votes” from the B\\ntrees, ˆGbag(x) = arg max kˆfbag(x).\\nOften we require the class-probability estimates at x, rather than the\\nclassiﬁcations themselves. It is tempting to treat the voting proportions\\npk(x) as estimates of these probabilities. A simple two-class example shows\\nthat they fail in this regard. Suppose the true probability of class 1 at xis\\n0.75, and each of the bagged classiﬁers accurately predict a 1. Then p1(x) =\\n1, which is incorrect. For many classiﬁers ˆG(x), however, there is already\\nan underlying function ˆf(x) that estimates the class probabilities at x(for\\ntrees, the class proportions in the terminal node). An alternative bagging\\nstrategy is to average these instead, rather than the vote indicator vectors.\\nNot only does this produce improved estimates of the class probabilities,\\nbut it also tends to produce bagged classiﬁers with lower variance, especially\\nfor small B(see Figure 8.10 in the next example).\\n8.7.1 Example: Trees with Simulated Data\\nWe generated a sample of size N= 30, with two classes and p= 5 features,\\neach having a standard Gaussian distribution with pairwise correlation\\n0.95. The response Ywas generated according to Pr( Y= 1|x1≤0.5) = 0 .2,\\nPr(Y= 1|x1>0.5) = 0 .8. The Bayes error is 0 .2. A test sample of size 2000\\nwas also generated from the same population. We ﬁt classiﬁcation trees to\\nthe training sample and to each of 200 bootstrap samples (classiﬁcation\\ntrees are described in Chapter 9). No pruning was used. Figure 8.9 shows\\nthe original tree and eleven bootstrap trees. Notice how the trees are all\\ndiﬀerent, with diﬀerent splitting features and cutpoints. The test error for\\nthe original tree and the bagged tree is shown in Figure 8.10. In this ex-\\nample the trees have high variance due to the correlation in the predictors.\\nBagging succeeds in smoothing out this variance and hence reducing the\\ntest error.\\nBagging can dramatically reduce the variance of unstable procedures\\nlike trees, leading to improved prediction. A simple argument shows why'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 302}, page_content='284 8. Model Inference and Averaging\\n|x.1 < 0.395\\n010\\n101\\n10Original Tree\\n|x.1 < 0.555\\n0\\n1001b = 1\\n|x.2 < 0.205\\n0101\\n01b = 2\\n|x.2 < 0.285\\n1 1010b = 3\\n|x.3 < 0.985\\n0\\n1\\n011 1b = 4\\n|x.4 < −1.36\\n0\\n1\\n1010\\n10b = 5\\n|x.1 < 0.395\\n1 10 01b = 6\\n|x.1 < 0.395\\n01011b = 7\\n|x.3 < 0.985\\n010 010b = 8\\n|x.1 < 0.395\\n0\\n1\\n0110b = 9\\n|x.1 < 0.555\\n101\\n01b = 10\\n|x.1 < 0.555\\n0 101b = 11\\nFIGURE 8.9. Bagging trees on simulated dataset. The top left panel shows th e\\noriginal tree. Eleven trees grown on bootstrap samples are sh own. For each tree,\\nthe top split is annotated.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 303}, page_content='8.7 Bagging 285\\n0 50 100 150 2000.20 0.25 0.30 0.35 0.40 0.45 0.50\\nNumber of Bootstrap SamplesTest ErrorBagged TreesOriginal Tree\\nBayesConsensus\\nProbability\\nFIGURE 8.10. Error curves for the bagging example of Figure 8.9. Shown is\\nthe test error of the original tree and bagged trees as a function of the number of\\nbootstrap samples. The orange points correspond to the consensus vote, while the\\ngreen points average the probabilities.\\nbagging helps under squared-error loss, in short because averaging reduces\\nvariance and leaves bias unchanged.\\nAssume our training observations ( xi,yi), i= 1,... ,N are indepen-\\ndently drawn from a distribution P, and consider the ideal aggregate es-\\ntimator fag(x) = E Pˆf∗(x). Here xis ﬁxed and the bootstrap dataset Z∗\\nconsists of observations x∗\\ni,y∗\\ni,i= 1,2,... ,N sampled from P. Note that\\nfag(x) is a bagging estimate, drawing bootstrap samples from the actual\\npopulation Prather than the data. It is not an estimate that we can use\\nin practice, but is convenient for analysis. We can write\\nEP[Y−ˆf∗(x)]2= E P[Y−fag(x) +fag(x)−ˆf∗(x)]2\\n= E P[Y−fag(x)]2+ EP[ˆf∗(x)−fag(x)]2\\n≥EP[Y−fag(x)]2. (8.52)\\nThe extra error on the right-hand side comes from the variance of ˆf∗(x)\\naround its mean fag(x). Therefore true population aggregation never in-\\ncreases mean squared error. This suggests that bagging—drawing samples\\nfrom the training data— will often decrease mean-squared error.\\nThe above argument does not hold for classiﬁcation under 0-1 loss, be-\\ncause of the nonadditivity of bias and variance. In that setting, bagging a'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 304}, page_content='286 8. Model Inference and Averaging\\ngood classiﬁer can make it better, but bagging a bad classiﬁer can make it\\nworse. Here is a simple example, using a randomized rule. Suppose Y= 1\\nfor all x, and the classiﬁer ˆG(x) predicts Y= 1 (for all x) with proba-\\nbility 0.4 and predicts Y= 0 (for all x) with probability 0.6. Then the\\nmisclassiﬁcation error of ˆG(x) is 0.6 but that of the bagged classiﬁer is 1.0.\\nFor classiﬁcation we can understand the bagging eﬀect in terms of a\\nconsensus of independent weak learners (Dietterich, 2000a). Let the Bayes\\noptimal decision at xbeG(x) = 1 in a two-class example. Suppose each\\nof the weak learners G∗\\nbhave an error-rate eb=e <0.5, and let S1(x) =∑B\\nb=1I(G∗\\nb(x) = 1) be the consensus vote for class 1. Since the weak learn-\\ners are assumed to be independent, S1(x)∼Bin(B,1−e), and Pr( S1>\\nB/2)→1 asBgets large. This concept has been popularized outside of\\nstatistics as the “Wisdom of Crowds” (Surowiecki, 2004) — the collective\\nknowledge of a diverse and independent body of people typically exceeds\\nthe knowledge of any single individual, and can be harnessed by voting.\\nOf course, the main caveat here is “independent,” and bagged trees are\\nnot. Figure 8.11 illustrates the power of a consensus vote in a simulated\\nexample, where only 30% of the voters have some knowledge.\\nIn Chapter 15 we see how random forests improve on bagging by reducing\\nthe correlation between the sampled trees.\\nNote that when we bag a model, any simple structure in the model is\\nlost. As an example, a bagged tree is no longer a tree. For interpretation\\nof the model this is clearly a drawback. More stable procedures like near-\\nest neighbors are typically not aﬀected much by bagging. Unfortunately,\\nthe unstable models most helped by bagging are unstable because of the\\nemphasis on interpretability, and this is lost in the bagging process.\\nFigure 8.12 shows an example where bagging doesn’t help. The 100 data\\npoints shown have two features and two classes, separated by the gray\\nlinear boundary x1+x2= 1. We choose as our classiﬁer ˆG(x) a single\\naxis-oriented split, choosing the split along either x1orx2that produces\\nthe largest decrease in training misclassiﬁcation error.\\nThe decision boundary obtained from bagging the 0-1 decision rule over\\nB= 50 bootstrap samples is shown by the blue curve in the left panel.\\nIt does a poor job of capturing the true boundary. The single split rule,\\nderived from the training data, splits near 0 (the middle of the range of x1\\norx2), and hence has little contribution away from the center. Averaging\\nthe probabilities rather than the classiﬁcations does not help here. Bagging\\nestimates the expected class probabilities from the single split rule, that is,\\naveraged over many replications. Note that the expected class probabilities\\ncomputed by bagging cannot be realized on any single replication, in the\\nsame way that a woman cannot have 2.4 children. In this sense, bagging\\nincreases somewhat the space of models of the individual base classiﬁer.\\nHowever, it doesn’t help in this and many other examples where a greater\\nenlargement of the model class is needed. “Boosting” is a way of doing this'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 305}, page_content='8.7 Bagging 2870 2 4 6 8 10\\nP −  Probability of Informed Person Being CorrectExpected Correct out of 10Wisdom of Crowds\\nConsensus\\nIndividual\\n0.25 0.50 0.75 1.00\\nFIGURE 8.11. Simulated academy awards voting. 50members vote in 10 cat-\\negories, each with 4nominations. For any category, only 15voters have some\\nknowledge, represented by their probability of selecting the “ correct” candidate in\\nthat category (so P= 0.25means they have no knowledge). For each category, the\\n15experts are chosen at random from the 50. Results show the expected correct\\n(based on 50simulations) for the consensus, as well as for the individuals. T he\\nerror bars indicate one standard deviation. We see, for example, t hat if the 15\\ninformed for a category have a 50% chance of selecting the correct candidate, the\\nconsensus doubles the expected performance of an individual.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 306}, page_content='288 8. Model Inference and Averaging\\n•••\\n••••\\n•••\\n••\\n••\\n•\\n••••\\n•••\\n•••\\n•••\\n•\\n••\\n•••\\n•\\n•• •• •••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n••\\n•\\n•\\n••••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•• • •••\\n••\\n•••\\n•••\\n•\\n••••\\n•\\n• ••\\n••\\n••\\n••• •••\\n••••\\n••\\n••\\n•••\\n••\\n••\\n•\\n••\\n•\\n•\\n••\\n•\\n••\\n••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•••\\n••\\n•••••\\n•\\n••\\n•••••\\n••\\n•••\\n•••••\\n••••\\n••\\n•••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n•\\n•Bagged Decision Rule\\n•••\\n••••\\n•••\\n••\\n••\\n•\\n••••\\n•••\\n•••\\n•••\\n•\\n••\\n•••\\n•\\n•• •• •••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n••\\n•\\n•\\n••••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•• • •••\\n••\\n•••\\n•••\\n•\\n••••\\n•\\n• ••\\n••\\n••\\n••• •••\\n••••\\n••\\n••\\n•••\\n••\\n••\\n•\\n••\\n•\\n•\\n••\\n•\\n••\\n••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•••\\n••\\n•••••\\n•\\n••\\n•••••\\n••\\n•••\\n•••••\\n••••\\n••\\n•••\\n•••\\n••\\n•\\n•\\n••\\n•••\\n•\\n•Boosted Decision Rule\\nFIGURE 8.12. Data with two features and two classes, separated by a linear\\nboundary. (Left panel:) Decision boundary estimated from bagg ing the decision\\nrule from a single split, axis-oriented classiﬁer. (Right panel: ) Decision boundary\\nfrom boosting the decision rule of the same classiﬁer. The test error rates are\\n0.166, and 0.065, respectively. Boosting is described in Chapter 10.\\nand is described in Chapter 10. The decision boundary in the right panel is\\nthe result of the boosting procedure, and it roughly captures the diagonal\\nboundary.\\n8.8 Model Averaging and Stacking\\nIn Section 8.4 we viewed bootstrap values of an estimator as approximate\\nposterior values of a corresponding parameter, from a kind of nonparamet-\\nric Bayesian analysis. Viewed in this way, the bagged estimate (8.51) i s\\nan approximate posterior Bayesian mean. In contrast, the training sampl e\\nestimate ˆf(x) corresponds to the mode of the posterior. Since the posterior\\nmean (not mode) minimizes squared-error loss, it is not surprising that\\nbagging can often reduce mean squared-error.\\nHere we discuss Bayesian model averaging more generally. We have a\\nset of candidate models Mm, m= 1,... ,M for our training set Z. These\\nmodels may be of the same type with diﬀerent parameter values (e.g.,\\nsubsets in linear regression), or diﬀerent models for the same task (e.g.,\\nneural networks and regression trees).\\nSuppose ζis some quantity of interest, for example, a prediction f(x) at\\nsome ﬁxed feature value x. The posterior distribution of ζis\\nPr(ζ|Z) =M∑\\nm=1Pr(ζ|Mm,Z)Pr(Mm|Z), (8.53)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 307}, page_content='8.8 Model Averaging and Stacking 289\\nwith posterior mean\\nE(ζ|Z) =M∑\\nm=1E(ζ|Mm,Z)Pr(Mm|Z). (8.54)\\nThis Bayesian prediction is a weighted average of the individual predictions,\\nwith weights proportional to the posterior probability of each model.\\nThis formulation leads to a number of diﬀerent model-averaging strate-\\ngies.Committee methods take a simple unweighted average of the predic-\\ntions from each model, essentially giving equal probability to each model.\\nMore ambitiously, the development in Section 7.7 shows the BIC criterion\\ncan be used to estimate posterior model probabilities. This is applicable\\nin cases where the diﬀerent models arise from the same parametric model,\\nwith diﬀerent parameter values. The BIC gives weight to each model de-\\npending on how well it ﬁts and how many parameters it uses. One can also\\ncarry out the Bayesian recipe in full. If each model Mmhas parameters\\nθm, we write\\nPr(Mm|Z)∝Pr(Mm)≤Pr(Z|Mm)\\n∝Pr(Mm)≤∫\\nPr(Z|θm,Mm)Pr(θm|Mm)dθm.\\n(8.55)\\nIn principle one can specify priors Pr( θm|Mm) and numerically com-\\npute the posterior probabilities from (8.55), to be used as model-averaging\\nweights. However, we have seen no real evidence that this is worth all of\\nthe eﬀort, relative to the much simpler BIC approximation.\\nHow can we approach model averaging from a frequentist viewpoint?\\nGiven predictions ˆf1(x),ˆf2(x),... ,ˆfM(x), under squared-error loss, we can\\nseek the weights w= (w1,w2,... ,w M) such that\\nˆw= argmin\\nwEP[\\nY−M∑\\nm=1wmˆfm(x)]2\\n. (8.56)\\nHere the input value xis ﬁxed and the Nobservations in the dataset Z(and\\nthe target Y) are distributed according to P. The solution is the population\\nlinear regression of YonˆF(x)T≡[ˆf1(x),ˆf2(x),... ,ˆfM(x)]:\\nˆw= EP[ˆF(x)ˆF(x)T]−1EP[ˆF(x)Y]. (8.57)\\nNow the full regression has smaller error than any single model\\nEP[\\nY−M∑\\nm=1ˆwmˆfm(x)]2\\n≤EP[\\nY−ˆfm(x)]2\\n∀m (8.58)\\nso combining models never makes things worse, at the population level.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 308}, page_content='290 8. Model Inference and Averaging\\nOf course the population linear regression (8.57) is not available, and it\\nis natural to replace it with the linear regression over the training set. But\\nthere are simple examples where this does not work well. For example, if\\nˆfm(x), m= 1,2,... ,M represent the prediction from the best subset of\\ninputs of size mamong Mtotal inputs, then linear regression would put all\\nof the weight on the largest model, that is, ˆ wM= 1,ˆwm= 0, m < M . The\\nproblem is that we have not put each of the models on the same footing\\nby taking into account their complexity (the number of inputs min this\\nexample).\\nStacked generalization , orstacking , is a way of doing this. Let ˆf−i\\nm(x)\\nbe the prediction at x, using model m, applied to the dataset with the\\nith training observation removed. The stacking estimate of the weights is\\nobtained from the least squares linear regression of yionˆf−i\\nm(xi), m=\\n1,2,... ,M . In detail the stacking weights are given by\\nˆwst= argmin\\nwN∑\\ni=1[\\nyi−M∑\\nm=1wmˆf−i\\nm(xi)]2\\n. (8.59)\\nThe ﬁnal prediction is∑\\nmˆwst\\nmˆfm(x). By using the cross-validated pre-\\ndictions ˆf−i\\nm(x), stacking avoids giving unfairly high weight to models with\\nhigher complexity. Better results can be obtained by restricting the weights\\nto be nonnegative, and to sum to 1. This seems like a reasonable restriction\\nif we interpret the weights as posterior model probabilities as in equation\\n(8.54), and it leads to a tractable quadratic programming problem.\\nThere is a close connection between stacking and model selection via\\nleave-one-out cross-validation (Section 7.10). If we restrict the minimizatio n\\nin (8.59) to weight vectors wthat have one unit weight and the rest zero,\\nthis leads to a model choice ˆ mwith smallest leave-one-out cross-validation\\nerror. Rather than choose a single model, stacking combines them with\\nestimated optimal weights. This will often lead to better prediction, but\\nless interpretability than the choice of only one of the Mmodels.\\nThe stacking idea is actually more general than described above. One\\ncan use any learning method, not just linear regression, to combine the\\nmodels as in (8.59); the weights could also depend on the input location\\nx. In this way, learning methods are “stacked” on top of one another, to\\nimprove prediction performance.\\n8.9 Stochastic Search: Bumping\\nThe ﬁnal method described in this chapter does not involve averaging or\\ncombining models, but rather is a technique for ﬁnding a better single\\nmodel. Bumping uses bootstrap sampling to move randomly through model\\nspace. For problems where ﬁtting method ﬁnds many local minima, bump-\\ning can help the method to avoid getting stuck in poor solutions.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 309}, page_content='8.9 Stochastic Search: Bumping 291\\nRegular 4-Node Tree\\n••\\n•••\\n••\\n•\\n•••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n••\\n•\\n•\\n•••\\n••\\n•\\n••\\n••\\n••\\n••\\n••••••\\n••••\\n•\\n•\\n• ••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n• •••\\n••\\n•\\n••\\n•••\\n••\\n•••\\n•\\n••••\\n•\\n•••\\n•\\n•\\n•••\\n••\\n•\\n•••••\\n•••\\n••\\n•\\n•\\n••\\n•\\n••\\n••••• •\\n••\\n•\\n•••\\n••\\n••\\n••\\n•••••\\n•\\n••••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n••\\n••\\n••••\\n•\\n•• •••\\n••\\n•\\n•••\\n••\\n••\\n•\\n•••\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n••\\n••\\n••\\n•\\n•\\n••\\n••\\n•••\\n••\\n•••\\n•• •\\n•••\\n••\\n••\\n•\\n•••\\n••\\n•\\n•••\\n••\\n••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n•••\\n•\\n•••\\n•••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n••••••\\n•\\n••\\n••\\n••\\n•\\n•\\n••\\n••••\\n•••\\n••\\n•••\\n•••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n•••\\n•••\\n••••\\n•••••\\n•\\n• •• ••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n•\\n•••\\n••\\n••\\n•\\n•\\n••\\n••\\n••Bumped 4-Node Tree\\n••\\n•••\\n••\\n•\\n•••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n••\\n•\\n•\\n•••\\n••\\n•\\n••\\n••\\n••\\n••\\n••••••\\n••••\\n•\\n•\\n• ••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n• •••\\n••\\n•\\n••\\n•••\\n••\\n•••\\n•\\n••••\\n•\\n•••\\n•\\n•\\n•••\\n••\\n•\\n•••••\\n•••\\n••\\n•\\n•\\n••\\n•\\n••\\n••••• •\\n••\\n•\\n•••\\n••\\n••\\n••\\n•••••\\n•\\n••••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n••\\n••\\n••••\\n•\\n•• •••\\n••\\n•\\n•••\\n••\\n••\\n•\\n•••\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n••\\n•\\n••\\n••\\n•••\\n••\\n••\\n••\\n••\\n•\\n•\\n••\\n••\\n•••\\n••\\n•••\\n•• •\\n•••\\n••\\n••\\n•\\n•••\\n••\\n•\\n•••\\n••\\n••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n•••\\n•\\n•••\\n•••\\n•••\\n••••\\n••\\n•\\n••\\n•\\n••••••\\n•\\n••\\n••\\n••\\n•\\n•\\n••\\n••••\\n•••\\n••\\n•••\\n•••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•\\n••\\n•\\n••\\n••\\n•••\\n•••\\n••••\\n•••••\\n•\\n• •• ••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n•\\n•••\\n••\\n••\\n•\\n•\\n••\\n••\\n••\\nFIGURE 8.13. Data with two features and two classes (blue and orange), dis-\\nplaying a pure interaction. The left panel shows the partition fo und by three splits\\nof a standard, greedy, tree-growing algorithm. The vertical g rey line near the left\\nedge is the ﬁrst split, and the broken lines are the two subsequent splits. The al-\\ngorithm has no idea where to make a good initial split, and makes a poor choice.\\nThe right panel shows the near-optimal splits found by bumping th e tree-growing\\nalgorithm 20times.\\nAs in bagging, we draw bootstrap samples and ﬁt a model to each. But\\nrather than average the predictions, we choose the model estimated from a\\nbootstrap sample that best ﬁts the training data. In detail, we draw boot-\\nstrap samples Z∗1,... ,Z∗Band ﬁt our model to each, giving predictions\\nˆf∗b(x), b= 1,2,... ,B at input point x. We then choose the model that\\nproduces the smallest prediction error, averaged over the original training\\nset. For squared error, for example, we choose the model obtained from\\nbootstrap sample ˆb, where\\nˆb= arg min\\nbN∑\\ni=1[yi−ˆf∗b(xi)]2. (8.60)\\nThe corresponding model predictions are ˆf∗ˆb(x). By convention we also\\ninclude the original training sample in the set of bootstrap samples, so that\\nthe method is free to pick the original model if it has the lowest training\\nerror.\\nBy perturbing the data, bumping tries to move the ﬁtting procedure\\naround to good areas of model space. For example, if a few data points are\\ncausing the procedure to ﬁnd a poor solution, any bootstrap sample that\\nomits those data points should procedure a better solution.\\nFor another example, consider the classiﬁcation data in Figure 8.13, the\\nnotorious exclusive or (XOR) problem. There are two classes (blue and\\norange) and two input features, with the features exhibiting a pure inter-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 310}, page_content='292 8. Model Inference and Averaging\\naction. By splitting the data at x1= 0 and then splitting each resulting\\nstrata at x2= 0, (or vice versa) a tree-based classiﬁer could achieve per-\\nfect discrimination. However, the greedy, short-sighted CART algorithm\\n(Section 9.2) tries to ﬁnd the best split on either feature, and then splits\\nthe resulting strata. Because of the balanced nature of the data, all initial\\nsplits on x1orx2appear to be useless, and the procedure essentially gener-\\nates a random split at the top level. The actual split found for these data is\\nshown in the left panel of Figure 8.13. By bootstrap sampling from the data ,\\nbumping breaks the balance in the classes, and with a reasonable number\\nof bootstrap samples (here 20), it will by chance produce at least one tree\\nwith initial split near either x1= 0 or x2= 0. Using just 20 bootstrap\\nsamples, bumping found the near optimal splits shown in the right panel\\nof Figure 8.13. This shortcoming of the greedy tree-growing algorithm is\\nexacerbated if we add a number of noise features that are independent of\\nthe class label. Then the tree-growing algorithm cannot distinguish x1or\\nx2from the others, and gets seriously lost.\\nSince bumping compares diﬀerent models on the training data, one must\\nensure that the models have roughly the same complexity. In the case of\\ntrees, this would mean growing trees with the same number of terminal\\nnodes on each bootstrap sample. Bumping can also help in problems where\\nit is diﬃcult to optimize the ﬁtting criterion, perhaps because of a lack of\\nsmoothness. The trick is to optimize a diﬀerent, more convenient criterion\\nover the bootstrap samples, and then choose the model producing the best\\nresults for the desired criterion on the training sample.\\nBibliographic Notes\\nThere are many books on classical statistical inference: Cox and Hink-\\nley (1974) and Silvey (1975) give nontechnical accounts. The bootstrap\\nis due to Efron (1979) and is described more fully in Efron and Tibshi-\\nrani (1993) and Hall (1992). A good modern book on Bayesian inference\\nis Gelman et al. (1995). A lucid account of the application of Bayesian\\nmethods to neural networks is given in Neal (1996). The statistical appli-\\ncation of Gibbs sampling is due to Geman and Geman (1984), and Gelfand\\nand Smith (1990), with related work by Tanner and Wong (1987). Markov\\nchain Monte Carlo methods, including Gibbs sampling and the Metropolis–\\nHastings algorithm, are discussed in Spiegelhalter et al. (1996). The EM\\nalgorithm is due to Dempster et al. (1977); as the discussants in that pa-\\nper make clear, there was much related, earlier work. The view of EM as\\na joint maximization scheme for a penalized complete-data log-likelihood\\nwas elucidated by Neal and Hinton (1998); they credit Csiszar and Tusn´ ady\\n(1984) and Hathaway (1986) as having noticed this connection earlier. Bag-\\nging was proposed by Breiman (1996a). Stacking is due to Wolpert (1992) ;'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 311}, page_content='Exercises 293\\nBreiman (1996b) contains an accessible discussion for statisticians. Lebla nc\\nand Tibshirani (1996) describe variations on stacking based on the boot-\\nstrap. Model averaging in the Bayesian framework has been recently advo-\\ncated by Madigan and Raftery (1994). Bumping was proposed by Tibshi-\\nrani and Knight (1999).\\nExercises\\nEx. 8.1 Letr(y) and q(y) be probability density functions. Jensen’s in-\\nequality states that for a random variable Xand a convex function φ(x),\\nE[φ(X)]≥φ[E(X)]. Use Jensen’s inequality to show that\\nEqlog[r(Y)/q(Y)] (8.61)\\nis maximized as a function of r(y) when r(y) =q(y). Hence show that\\nR(θ,θ)≥R(θ′,θ) as stated below equation (8.46).\\nEx. 8.2 Consider the maximization of the log-likelihood (8.48), over dis-\\ntributions ˜P(Zm) such that ˜P(Zm)≥0 and∑\\nZm˜P(Zm) = 1. Use La-\\ngrange multipliers to show that the solution is the conditional distribution\\n˜P(Zm) = Pr( Zm|Z,θ′), as in (8.49).\\nEx. 8.3 Justify the estimate (8.50), using the relationship\\nPr(A) =∫\\nPr(A|B)d(Pr(B)).\\nEx. 8.4 Consider the bagging method of Section 8.7. Let our estimate ˆf(x)\\nbe the B-spline smoother ˆ θ(x) of Section 8.2.1. Consider the parametric\\nbootstrap of equation (8.6), applied to this estimator. Show that if we ba g\\nˆf(x), using the parametric bootstrap to generate the bootstrap samples,\\nthe bagging estimate ˆfbag(x) converges to the original estimate ˆf(x) as\\nB→ ∞.\\nEx. 8.5 Suggest generalizations of each of the loss functions in Figure 10.4\\nto more than two classes, and design an appropriate plot to compare them.\\nEx. 8.6 Consider the bone mineral density data of Figure 5.6.\\n(a) Fit a cubic smooth spline to the relative change in spinal BMD, as a\\nfunction of age. Use cross-validation to estimate the optimal amount\\nof smoothing. Construct pointwise 90% conﬁdence bands for the un-\\nderlying function.\\n(b) Compute the posterior mean and covariance for the true function via\\n(8.28), and compare the posterior bands to those obtained in (a).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 312}, page_content='294 8. Model Inference and Averaging\\n(c) Compute 100 bootstrap replicates of the ﬁtted curves, as in the bottom\\nleft panel of Figure 8.2. Compare the results to those obtained in (a)\\nand (b).\\nEx. 8.7 EM as a minorization algorithm (Hunter and Lange, 2004; Wu and\\nLange, 2007). A function g(x,y) to said to minorize a function f(x) if\\ng(x,y)≤f(x), g(x,x) =f(x) (8.62)\\nfor all x,yin the domain. This is useful for maximizing f(x) since is easy\\nto show that f(x) is non-decreasing under the update\\nxs+1= argmaxxg(x,xs) (8.63)\\nThere are analogous deﬁnitions for majorization , for minimizing a function\\nf(x). The resulting algorithms are known as MMalgorithms, for “Minorize-\\nMaximize” or “Majorize-Minimize.”\\nShow that the EM algorithm (Section 8.5.2) is an example of an MM al-\\ngorithm, using Q(θ′,θ)+log Pr( Z|θ)−Q(θ,θ) to minorize the observed data\\nlog-likelihood ℓ(θ′;Z). (Note that only the ﬁrst term involves the relevant\\nparameter θ′).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 313}, page_content='This is page 295\\nPrinter: Opaque this\\n9\\nAdditive Models, Trees, and Related\\nMethods\\nIn this chapter we begin our discussion of some speciﬁc methods for super-\\nvised learning. These techniques each assume a (diﬀerent) structured form\\nfor the unknown regression function, and by doing so they ﬁnesse the curse\\nof dimensionality. Of course, they pay the possible price of misspecifying\\nthe model, and so in each case there is a tradeoﬀ that has to be made. They\\ntake oﬀ where Chapters 3–6 left oﬀ. We describe ﬁve related techniques:\\ngeneralized additive models, trees, multivariate adaptive regression splines,\\nthe patient rule induction method, and hierarchical mixtures of experts.\\n9.1 Generalized Additive Models\\nRegression models play an important role in many data analyses, providi ng\\nprediction and classiﬁcation rules, and data analytic tools for understand-\\ning the importance of diﬀerent inputs.\\nAlthough attractively simple, the traditional linear model often fails in\\nthese situations: in real life, eﬀects are often not linear. In earlier chapters\\nwe described techniques that used predeﬁned basis functions to achieve\\nnonlinearities. This section describes more automatic ﬂexible statistical\\nmethods that may be used to identify and characterize nonlinear regression\\neﬀects. These methods are called “generalized additive models.”\\nIn the regression setting, a generalized additive model has the form\\nE(Y|X1,X2,... ,X p) =α+f1(X1) +f2(X2) +≤≤≤+fp(Xp).(9.1)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 314}, page_content='296 9. Additive Models, Trees, and Related Methods\\nAs usual X1,X2,... ,X prepresent predictors and Yis the outcome; the fj’s\\nare unspeciﬁed smooth (“nonparametric”) functions. If we were to model\\neach function using an expansion of basis functions (as in Chapter 5), the\\nresulting model could then be ﬁt by simple least squares. Our approach\\nhere is diﬀerent: we ﬁt each function using a scatterplot smoother (e.g., a\\ncubic smoothing spline or kernel smoother), and provide an algorithm for\\nsimultaneously estimating all pfunctions (Section 9.1.1).\\nFor two-class classiﬁcation, recall the logistic regression model for binar y\\ndata discussed in Section 4.4. We relate the mean of the binary response\\nθ(X) = Pr( Y= 1|X) to the predictors via a linear regression model and\\nthelogitlink function:\\nlog(θ(X)\\n1−θ(X))\\n=α+β1X1+≤≤≤+βpXp. (9.2)\\nTheadditive logistic regression model replaces each linear term by a more\\ngeneral functional form\\nlog(θ(X)\\n1−θ(X))\\n=α+f1(X1) +≤≤≤+fp(Xp), (9.3)\\nwhere again each fjis an unspeciﬁed smooth function. While the non-\\nparametric form for the functions fjmakes the model more ﬂexible, the\\nadditivity is retained and allows us to interpret the model in much the\\nsame way as before. The additive logistic regression model is an example\\nof a generalized additive model. In general, the conditional mean θ(X) of\\na response Yis related to an additive function of the predictors via a link\\nfunction g:\\ng[θ(X)] =α+f1(X1) +≤≤≤+fp(Xp). (9.4)\\nExamples of classical link functions are the following:\\n•g(θ) =θis the identity link, used for linear and additive models for\\nGaussian response data.\\n•g(θ) = logit( θ) as above, or g(θ) = probit( θ), theprobit link function,\\nfor modeling binomial probabilities. The probit function is the inverse\\nGaussian cumulative distribution function: probit( θ) = Φ−1(θ).\\n•g(θ) = log( θ) for log-linear or log-additive models for Poisson count\\ndata.\\nAll three of these arise from exponential family sampling models, which\\nin addition include the gamma and negative-binomial distributions. These\\nfamilies generate the well-known class of generalized linear models, which\\nare all extended in the same way to generalized additive models.\\nThe functions fjare estimated in a ﬂexible manner, using an algorithm\\nwhose basic building block is a scatterplot smoother. The estimated func-\\ntionˆfjcan then reveal possible nonlinearities in the eﬀect of Xj. Not all'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 315}, page_content='9.1 Generalized Additive Models 297\\nof the functions fjneed to be nonlinear. We can easily mix in linear and\\nother parametric forms with the nonlinear terms, a necessity when some of\\nthe inputs are qualitative variables (factors). The nonlinear terms are not\\nrestricted to main eﬀects either; we can have nonlinear components in two\\nor more variables, or separate curves in Xjfor each level of the factor Xk.\\nThus each of the following would qualify:\\n•g(θ) =XTβ+αk+f(Z)—asemiparametric model, where Xis a\\nvector of predictors to be modeled linearly, αkthe eﬀect for the kth\\nlevel of a qualitative input V, and the eﬀect of predictor Zis modeled\\nnonparametrically.\\n•g(θ) =f(X) +gk(Z)—again kindexes the levels of a qualitative\\ninput V, and thus creates an interaction term g(V,Z) =gk(Z) for\\nthe eﬀect of VandZ.\\n•g(θ) =f(X) +g(Z,W) where gis a nonparametric function in two\\nfeatures.\\nAdditive models can replace linear models in a wide variety of settings,\\nfor example an additive decomposition of time series,\\nYt=St+Tt+εt, (9.5)\\nwhere Stis a seasonal component, Ttis a trend and εis an error term.\\n9.1.1 Fitting Additive Models\\nIn this section we describe a modular algorithm for ﬁtting additive models\\nand their generalizations. The building block is the scatterplot smoother\\nfor ﬁtting nonlinear eﬀects in a ﬂexible way. For concreteness we use as our\\nscatterplot smoother the cubic smoothing spline described in Chapter 5.\\nThe additive model has the form\\nY=α+p∑\\nj=1fj(Xj) +ε, (9.6)\\nwhere the error term εhas mean zero. Given observations xi,yi, a criterion\\nlike the penalized sum of squares (5.9) of Section 5.4 can be speciﬁed for\\nthis problem,\\nPRSS( α,f1,f2,... ,f p) =N∑\\ni=1(\\nyi−α−p∑\\nj=1fj(xij))2\\n+p∑\\nj=1λj∫\\nf′′\\nj(tj)2dtj,\\n(9.7)\\nwhere the λj≥0 are tuning parameters. It can be shown that the minimizer\\nof (9.7) is an additive cubic spline model; each of the functions fjis a'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 316}, page_content='298 9. Additive Models, Trees, and Related Methods\\nAlgorithm 9.1 The Backﬁtting Algorithm for Additive Models.\\n1. Initialize: ˆ α=1\\nN∑N\\n1yi,ˆfj≡0,∀i,j.\\n2. Cycle: j= 1,2,... ,p,... , 1,2,... ,p,... ,\\nˆfj← S j[\\n{yi−ˆα−∑\\nk̸=jˆfk(xik)}N\\n1]\\n,\\nˆfj←ˆfj−1\\nNN∑\\ni=1ˆfj(xij).\\nuntil the functions ˆfjchange less than a prespeciﬁed threshold.\\ncubic spline in the component Xj, with knots at each of the unique values\\nofxij, i= 1,... ,N . However, without further restrictions on the model,\\nthe solution is not unique. The constant αis not identiﬁable, since we\\ncan add or subtract any constants to each of the functions fj, and adjust\\nαaccordingly. The standard convention is to assume that∑N\\n1fj(xij) =\\n0∀j—the functions average zero over the data. It is easily seen that ˆ α=\\nave(yi) in this case. If in addition to this restriction, the matrix of input\\nvalues (having ijth entry xij) has full column rank, then (9.7) is a strictly\\nconvex criterion and the minimizer is unique. If the matrix is singular, then\\nthelinear part of the components fjcannot be uniquely determined (while\\nthe nonlinear parts can!)(Buja et al., 1989).\\nFurthermore, a simple iterative procedure exists for ﬁnding the solution.\\nWe set ˆ α= ave( yi), and it never changes. We apply a cubic smoothing\\nspline Sjto the targets {yi−ˆα−∑\\nk̸=jˆfk(xik)}N\\n1, as a function of xij,\\nto obtain a new estimate ˆfj. This is done for each predictor in turn, using\\nthe current estimates of the other functions ˆfkwhen computing yi−ˆα−∑\\nk̸=jˆfk(xik). The process is continued until the estimates ˆfjstabilize. This\\nprocedure, given in detail in Algorithm 9.1, is known as “backﬁtting” and\\nthe resulting ﬁt is analogous to a multiple regression for linear models.\\nIn principle, the second step in (2) of Algorithm 9.1 is not needed, since\\nthe smoothing spline ﬁt to a mean-zero response has mean zero (Exer-\\ncise 9.1). In practice, machine rounding can cause slippage, and the ad-\\njustment is advised.\\nThis same algorithm can accommodate other ﬁtting methods in exactly\\nthe same way, by specifying appropriate smoothing operators Sj:\\n•other univariate regression smoothers such as local polynomial re-\\ngression and kernel methods;'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 317}, page_content='9.1 Generalized Additive Models 299\\n•linear regression operators yielding polynomial ﬁts, piecewise con-\\nstant ﬁts, parametric spline ﬁts, series and Fourier ﬁts;\\n•more complicated operators such as surface smoothers for second or\\nhigher-order interactions or periodic smoothers for seasonal eﬀects.\\nIf we consider the operation of smoother Sjonly at the training points, it\\ncan be represented by an N×Noperator matrix Sj(see Section 5.4.1).\\nThen the degrees of freedom for the jth term are (approximately) computed\\nas df j= trace[ Sj]−1, by analogy with degrees of freedom for smoothers\\ndiscussed in Chapters 5 and 6.\\nFor a large class of linear smoothers Sj, backﬁtting is equivalent to a\\nGauss–Seidel algorithm for solving a certain linear system of equations.\\nDetails are given in Exercise 9.2.\\nFor the logistic regression model and other generalized additive models,\\nthe appropriate criterion is a penalized log-likelihood. To maximize it, the\\nbackﬁtting procedure is used in conjunction with a likelihood maximizer.\\nThe usual Newton–Raphson routine for maximizing log-likelihoods in gen-\\neralized linear models can be recast as an IRLS (iteratively reweighted\\nleast squares) algorithm. This involves repeatedly ﬁtting a weighted linear\\nregression of a working response variable on the covariates; each regress ion\\nyields a new value of the parameter estimates, which in turn give new work-\\ning responses and weights, and the process is iterated (see Section 4.4.1).\\nIn the generalized additive model, the weighted linear regression is simply\\nreplaced by a weighted backﬁtting algorithm. We describe the algorithm in\\nmore detail for logistic regression below, and more generally in Chapter 6\\nof Hastie and Tibshirani (1990).\\n9.1.2 Example: Additive Logistic Regression\\nProbably the most widely used model in medical research is the logistic\\nmodel for binary data. In this model the outcome Ycan be coded as 0\\nor 1, with 1 indicating an event (like death or relapse of a disease) and\\n0 indicating no event. We wish to model Pr( Y= 1|X), the probability of\\nan event given values of the prognostic factors XT= (X1,... ,X p). The\\ngoal is usually to understand the roles of the prognostic factors, rather\\nthan to classify new individuals. Logistic models are also used in applica-\\ntions where one is interested in estimating the class probabilities, for use\\nin risk screening. Apart from medical applications, credit risk screening is\\na popular application.\\nThe generalized additive logistic model has the form\\nlogPr(Y= 1|X)\\nPr(Y= 0|X)=α+f1(X1) +≤≤≤+fp(Xp). (9.8)\\nThe functions f1,f2,... ,f pare estimated by a backﬁtting algorithm\\nwithin a Newton–Raphson procedure, shown in Algorithm 9.2.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 318}, page_content='300 9. Additive Models, Trees, and Related Methods\\nAlgorithm 9.2 Local Scoring Algorithm for the Additive Logistic Regres-\\nsion Model.\\n1. Compute starting values: ˆ α= log[¯ y/(1−¯y)], where ¯ y= ave( yi), the\\nsample proportion of ones, and set ˆfj≡0∀j.\\n2. Deﬁne ˆ ηi= ˆα+∑\\njˆfj(xij) and ˆ pi= 1/[1 + exp( −ˆηi)].\\nIterate:\\n(a) Construct the working target variable\\nzi= ˆηi+(yi−ˆpi)\\nˆpi(1−ˆpi).\\n(b) Construct weights wi= ˆpi(1−ˆpi)\\n(c) Fit an additive model to the targets ziwith weights wi, us-\\ning a weighted backﬁtting algorithm. This gives new estimates\\nˆα,ˆfj,∀j\\n3. Continue step 2. until the change in the functions falls below a pre-\\nspeciﬁed threshold.\\nThe additive model ﬁtting in step (2) of Algorithm 9.2 requires a weighted\\nscatterplot smoother. Most smoothing procedures can accept observation\\nweights (Exercise 5.12); see Chapter 3 of Hastie and Tibshirani (1990) fo r\\nfurther details.\\nThe additive logistic regression model can be generalized further to han-\\ndle more than two classes, using the multilogit formulation as outlined in\\nSection 4.4. While the formulation is a straightforward extension of ( 9.8),\\nthe algorithms for ﬁtting such models are more complex. See Yee and Wild\\n(1996) for details, and the VGAMsoftware currently available from:\\nhttp://www.stat.auckland.ac.nz/ ∼yee.\\nExample: Predicting Email Spam\\nWe apply a generalized additive model to the spam data introduced in\\nChapter 1. The data consists of information from 4601 email messages, in\\na study to screen email for “spam” (i.e., junk email). The data is publicly\\navailable at ftp.ics.uci.edu , and was donated by George Forman from\\nHewlett-Packard laboratories, Palo Alto, California.\\nThe response variable is binary, with values email orspam, and there are\\n57 predictors as described below:\\n•48 quantitative predictors—the percentage of words in the email that\\nmatch a given word. Examples include business ,address ,internet ,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 319}, page_content='9.1 Generalized Additive Models 301\\nTABLE 9.1. Test data confusion matrix for the additive logistic regress ion model\\nﬁt to the spam training data. The overall test error rate is 5.5%.\\nPredicted Class\\nTrue Class email (0)spam(1)\\nemail (0) 58.3% 2.5%\\nspam(1) 3.0% 36.3%\\nfree, andgeorge . The idea was that these could be customized for\\nindividual users.\\n•6 quantitative predictors—the percentage of characters in the email\\nthat match a given character. The characters are ch;,ch(,ch[,ch!,\\nch$, andch#.\\n•The average length of uninterrupted sequences of capital letters:\\nCAPAVE .\\n•The length of the longest uninterrupted sequence of capital letters:\\nCAPMAX .\\n•The sum of the length of uninterrupted sequences of capital letters:\\nCAPTOT .\\nWe coded spamas 1 and email as zero. A test set of size 1536 was randomly\\nchosen, leaving 3065 observations in the training set. A generalized additive\\nmodel was ﬁt, using a cubic smoothing spline with a nominal four degrees of\\nfreedom for each predictor. What this means is that for each predictor Xj,\\nthe smoothing-spline parameter λjwas chosen so that trace[ Sj(λj)]−1 = 4,\\nwhereSj(λ) is the smoothing spline operator matrix constructed using the\\nobserved values xij, i= 1,... ,N . This is a convenient way of specifying\\nthe amount of smoothing in such a complex model.\\nMost of the spampredictors have a very long-tailed distribution. Before\\nﬁtting the GAM model, we log-transformed each variable (actually log( x+\\n0.1)), but the plots in Figure 9.1 are shown as a function of the original\\nvariables.\\nThe test error rates are shown in Table 9.1; the overall error rate is 5.3 %.\\nBy comparison, a linear logistic regression has a test error rate of 7.6% .\\nTable 9.2 shows the predictors that are highly signiﬁcant in the additive\\nmodel.\\nFor ease of interpretation, in Table 9.2 the contribution for each variabl e\\nis decomposed into a linear component and the remaining nonlinear com-\\nponent. The top block of predictors are positively correlated with spam,\\nwhile the bottom block is negatively correlated. The linear component is a\\nweighted least squares linear ﬁt of the ﬁtted curve on the predictor, while\\nthe nonlinear part is the residual. The linear component of an estimated'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 320}, page_content='302 9. Additive Models, Trees, and Related Methods\\nTABLE 9.2. Signiﬁcant predictors from the additive model ﬁt to the spam train-\\ning data. The coeﬃcients represent the linear part of ˆfj, along with their standard\\nerrors and Z-score. The nonlinear P-value is for a test of nonlineari ty of ˆfj.\\nName Num. df Coeﬃcient Std. Error ZScore Nonlinear\\nP-value\\nPositive eﬀects\\nour 5 3.9 0.566 0.114 4.970 0.052\\nover 6 3.9 0.244 0.195 1.249 0.004\\nremove 7 4.0 0.949 0.183 5.201 0.093\\ninternet 8 4.0 0.524 0.176 2.974 0.028\\nfree 16 3.9 0.507 0.127 4.010 0.065\\nbusiness 17 3.8 0.779 0.186 4.179 0.194\\nhpl 26 3.8 0.045 0.250 0.181 0.002\\nch! 52 4.0 0.674 0.128 5.283 0.164\\nch$ 53 3.9 1.419 0.280 5.062 0.354\\nCAPMAX 56 3.8 0.247 0.228 1.080 0.000\\nCAPTOT 57 4.0 0.755 0.165 4.566 0.063\\nNegative eﬀects\\nhp 25 3.9 −1.404 0.224 −6.262 0.140\\ngeorge 27 3.7 −5.003 0.744 −6.722 0.045\\n1999 37 3.8 −0.672 0.191 −3.512 0.011\\nre 45 3.9 −0.620 0.133 −4.649 0.597\\nedu 46 4.0 −1.183 0.209 −5.647 0.000\\nfunction is summarized by the coeﬃcient, standard error and Z-score; the\\nlatter is the coeﬃcient divided by its standard error, and is considered\\nsigniﬁcant if it exceeds the appropriate quantile of a standard normal dis-\\ntribution. The column labeled nonlinear P-value is a test of nonlinearity\\nof the estimated function. Note, however, that the eﬀect of each predictor\\nis fully adjusted for the entire eﬀects of the other predictors, not just for\\ntheir linear parts. The predictors shown in the table were judged signiﬁ-\\ncant by at least one of the tests (linear or nonlinear) at the p= 0.01 level\\n(two-sided).\\nFigure 9.1 shows the estimated functions for the signiﬁcant predictors\\nappearing in Table 9.2. Many of the nonlinear eﬀects appear to account for\\na strong discontinuity at zero. For example, the probability of spamdrops\\nsigniﬁcantly as the frequency of george increases from zero, but then does\\nnot change much after that. This suggests that one might replace each of\\nthe frequency predictors by an indicator variable for a zero count, and resort\\nto a linear logistic model. This gave a test error rate of 7 .4%; including the\\nlinear eﬀects of the frequencies as well dropped the test error to 6 .6%. It\\nappears that the nonlinearities in the additive model have an additional\\npredictive power.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 321}, page_content='9.1 Generalized Additive Models 303\\n0 2 4 6 8-5 0 5\\n0 1 2 3-5 0 5\\n0 2 4 6-5 0 5 10\\n0 2 4 6 8 10-5 0 5 10\\n0 2 4 6 8 10-5 0 5 10\\n0 2 4 6-5 0 5 10\\n0 5 10 15 20-10 -5 0\\n0 5 10-10 -5 0\\n0 10 20 30-10 -5 0 5\\n0 2 4 6-5 0 5\\n0 5 10 15 20-10 -5 0 5\\n0 5 10 15-10 -5 0\\n0 10 20 30-5 0 5 10\\n0 1 2 3 4 5 6-5 0 5 10\\n0 2000 6000 10000-5 0 5\\n0 5000 10000 15000-5 0 5our over remove internet\\nfree business hp hpl\\ngeorge 1999 re edu\\nch! ch$ CAPMAX CAPTOTˆf(our)\\nˆf(over)\\nˆf(remove )\\nˆf(internet )ˆf(free)\\nˆf(business )\\nˆf(hp)\\nˆf(hpl)ˆf(george )\\nˆf(1999)\\nˆf(re)\\nˆf(edu)ˆf(ch!)\\nˆf(ch$)\\nˆf(CAPMAX )\\nˆf(CAPTOT )\\nFIGURE 9.1. Spam analysis: estimated functions for signiﬁcant predictors. The\\nrug plot along the bottom of each frame indicates the observed values of the cor-\\nresponding predictor. For many of the predictors the nonlinearity picks up the\\ndiscontinuity at zero.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 322}, page_content='304 9. Additive Models, Trees, and Related Methods\\nIt is more serious to classify a genuine email message as spam, since then\\na good email would be ﬁltered out and would not reach the user. We can\\nalter the balance between the class error rates by changing the losses (see\\nSection 2.4). If we assign a loss L01for predicting a true class 0 as class 1,\\nandL10for predicting a true class 1 as class 0, then the estimated Bayes\\nrule predicts class 1 if its probability is greater than L01/(L01+L10). For\\nexample, if we take L01= 10,L10= 1 then the (true) class 0 and class 1\\nerror rates change to 0.8% and 8.7%.\\nMore ambitiously, we can encourage the model to ﬁt better data in the\\nclass 0 by using weights L01for the class 0 observations and L10for the\\nclass 1 observations. As above, we then use the estimated Bayes rule to\\npredict. This gave error rates of 1.2% and 8.0% in (true) class 0 and class 1,\\nrespectively. We discuss below the issue of unequal losses further, in the\\ncontext of tree-based models.\\nAfter ﬁtting an additive model, one should check whether the inclusion\\nof some interactions can signiﬁcantly improve the ﬁt. This can be done\\n“manually,” by inserting products of some or all of the signiﬁcant inputs,\\nor automatically via the MARS procedure (Section 9.4).\\nThis example uses the additive model in an automatic fashion. As a data\\nanalysis tool, additive models are often used in a more interactive fashi on,\\nadding and dropping terms to determine their eﬀect. By calibrating the\\namount of smoothing in terms of df j, one can move seamlessly between\\nlinear models (df j= 1) and partially linear models, where some terms are\\nmodeled more ﬂexibly. See Hastie and Tibshirani (1990) for more details.\\n9.1.3 Summary\\nAdditive models provide a useful extension of linear models, making them\\nmore ﬂexible while still retaining much of their interpretability. The famil iar\\ntools for modeling and inference in linear models are also available for\\nadditive models, seen for example in Table 9.2. The backﬁtting procedure\\nfor ﬁtting these models is simple and modular, allowing one to choose a\\nﬁtting method appropriate for each input variable. As a result they have\\nbecome widely used in the statistical community.\\nHowever additive models can have limitations for large data-mining ap-\\nplications. The backﬁtting algorithm ﬁts all predictors, which is not feasi-\\nble or desirable when a large number are available. The BRUTO procedure\\n(Hastie and Tibshirani, 1990, Chapter 9) combines backﬁtting with selec-\\ntion of inputs, but is not designed for large data-mining problems. There\\nhas also been recent work using lasso-type penalties to estimate sparse ad-\\nditive models, for example the COSSO procedure of Lin and Zhang (2006)\\nand the SpAM proposal of Ravikumar et al. (2008). For large problems a\\nforward stagewise approach such as boosting (Chapter 10) is more eﬀectiv e,\\nand also allows for interactions to be included in the model.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 323}, page_content='9.2 Tree-Based Methods 305\\n9.2 Tree-Based Methods\\n9.2.1 Background\\nTree-based methods partition the feature space into a set of rectangles, and\\nthen ﬁt a simple model (like a constant) in each one. They are conceptually\\nsimple yet powerful. We ﬁrst describe a popular method for tree-based\\nregression and classiﬁcation called CART, and later contrast it with C4. 5,\\na major competitor.\\nLet’s consider a regression problem with continuous response Yand in-\\nputsX1andX2, each taking values in the unit interval. The top left panel\\nof Figure 9.2 shows a partition of the feature space by lines that are parall el\\nto the coordinate axes. In each partition element we can model Ywith a\\ndiﬀerent constant. However, there is a problem: although each partitioning\\nline has a simple description like X1=c, some of the resulting regions are\\ncomplicated to describe.\\nTo simplify matters, we restrict attention to recursive binary partitio ns\\nlike that in the top right panel of Figure 9.2. We ﬁrst split the space into\\ntwo regions, and model the response by the mean of Yin each region.\\nWe choose the variable and split-point to achieve the best ﬁt. Then one\\nor both of these regions are split into two more regions, and this process\\nis continued, until some stopping rule is applied. For example, in the top\\nright panel of Figure 9.2, we ﬁrst split at X1=t1. Then the region X1≤t1\\nis split at X2=t2and the region X1> t1is split at X1=t3. Finally, the\\nregion X1> t3is split at X2=t4. The result of this process is a partition\\ninto the ﬁve regions R1,R2,... ,R 5shown in the ﬁgure. The corresponding\\nregression model predicts Ywith a constant cmin region Rm, that is,\\nˆf(X) =5∑\\nm=1cmI{(X1,X2)∈Rm}. (9.9)\\nThis same model can be represented by the binary tree in the bottom left\\npanel of Figure 9.2. The full dataset sits at the top of the tree. Observations\\nsatisfying the condition at each junction are assigned to the left branch,\\nand the others to the right branch. The terminal nodes or leaves of the\\ntree correspond to the regions R1,R2,... ,R 5. The bottom right panel of\\nFigure 9.2 is a perspective plot of the regression surface from this model.\\nFor illustration, we chose the node means c1=−5,c2=−7,c3= 0,c4=\\n2,c5= 4 to make this plot.\\nA key advantage of the recursive binary tree is its interpretability. The\\nfeature space partition is fully described by a single tree. With more than\\ntwo inputs, partitions like that in the top right panel of Figure 9.2 are\\ndiﬃcult to draw, but the binary tree representation works in the same\\nway. This representation is also popular among medical scientists, perhaps\\nbecause it mimics the way that a doctor thinks. The tree stratiﬁes the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 324}, page_content='306 9. Additive Models, Trees, and Related Methods\\n|t1t2\\nt3t4\\nR1R1\\nR2R2\\nR3R3\\nR4R4\\nR5R5\\nX1X1 X1\\nX2X2X2\\nX1≤t1\\nX2≤t2 X1≤t3\\nX2≤t4\\nFIGURE 9.2. Partitions and CART. Top right panel shows a partition of a\\ntwo-dimensional feature space by recursive binary splitting, a s used in CART,\\napplied to some fake data. Top left panel shows a general partit ion that cannot\\nbe obtained from recursive binary splitting. Bottom left panel s hows the tree cor-\\nresponding to the partition in the top right panel, and a perspect ive plot of the\\nprediction surface appears in the bottom right panel.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 325}, page_content='9.2 Tree-Based Methods 307\\npopulation into strata of high and low outcome, on the basis of patient\\ncharacteristics.\\n9.2.2 Regression Trees\\nWe now turn to the question of how to grow a regression tree. Our data\\nconsists of pinputs and a response, for each of Nobservations: that is,\\n(xi,yi) for i= 1,2,... ,N , with xi= (xi1,xi2,... ,x ip). The algorithm\\nneeds to automatically decide on the splitting variables and split points,\\nand also what topology (shape) the tree should have. Suppose ﬁrst that we\\nhave a partition into Mregions R1,R2,... ,R M, and we model the response\\nas a constant cmin each region:\\nf(x) =M∑\\nm=1cmI(x∈Rm). (9.10)\\nIf we adopt as our criterion minimization of the sum of squares∑(yi−\\nf(xi))2, it is easy to see that the best ˆ cmis just the average of yiin region\\nRm:\\nˆcm= ave( yi|xi∈Rm). (9.11)\\nNow ﬁnding the best binary partition in terms of minimum sum of squares\\nis generally computationally infeasible. Hence we proceed with a greedy\\nalgorithm. Starting with all of the data, consider a splitting variable jand\\nsplit point s, and deﬁne the pair of half-planes\\nR1(j,s) ={X|Xj≤s}andR2(j,s) ={X|Xj> s}. (9.12)\\nThen we seek the splitting variable jand split point sthat solve\\nmin\\nj, s[\\nmin\\nc1∑\\nxi∈R1(j,s)(yi−c1)2+ min\\nc2∑\\nxi∈R2(j,s)(yi−c2)2]\\n. (9.13)\\nFor any choice jands, the inner minimization is solved by\\nˆc1= ave( yi|xi∈R1(j,s)) and ˆ c2= ave( yi|xi∈R2(j,s)). (9.14)\\nFor each splitting variable, the determination of the split point scan\\nbe done very quickly and hence by scanning through all of the inputs,\\ndetermination of the best pair ( j,s) is feasible.\\nHaving found the best split, we partition the data into the two resulting\\nregions and repeat the splitting process on each of the two regions. Then\\nthis process is repeated on all of the resulting regions.\\nHow large should we grow the tree? Clearly a very large tree might overﬁt\\nthe data, while a small tree might not capture the important structure.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 326}, page_content='308 9. Additive Models, Trees, and Related Methods\\nTree size is a tuning parameter governing the model’s complexity, and the\\noptimal tree size should be adaptively chosen from the data. One approach\\nwould be to split tree nodes only if the decrease in sum-of-squares due to the\\nsplit exceeds some threshold. This strategy is too short-sighted, however,\\nsince a seemingly worthless split might lead to a very good split below it.\\nThe preferred strategy is to grow a large tree T0, stopping the splitting\\nprocess only when some minimum node size (say 5) is reached. Then this\\nlarge tree is pruned using cost-complexity pruning , which we now describe.\\nWe deﬁne a subtree T⊂T0to be any tree that can be obtained by\\npruning T0, that is, collapsing any number of its internal (non-terminal)\\nnodes. We index terminal nodes by m, with node mrepresenting region\\nRm. Let|T|denote the number of terminal nodes in T. Letting\\nNm= #{xi∈Rm},\\nˆcm=1\\nNm∑\\nxi∈Rmyi,\\nQm(T) =1\\nNm∑\\nxi∈Rm(yi−ˆcm)2,(9.15)\\nwe deﬁne the cost complexity criterion\\nCα(T) =|T|∑\\nm=1NmQm(T) +α|T|. (9.16)\\nThe idea is to ﬁnd, for each α, the subtree Tα⊆T0to minimize Cα(T).\\nThe tuning parameter α≥0 governs the tradeoﬀ between tree size and its\\ngoodness of ﬁt to the data. Large values of αresult in smaller trees Tα, and\\nconversely for smaller values of α. As the notation suggests, with α= 0 the\\nsolution is the full tree T0. We discuss how to adaptively choose αbelow.\\nFor each αone can show that there is a unique smallest subtree Tαthat\\nminimizes Cα(T). To ﬁnd Tαwe use weakest link pruning : we successively\\ncollapse the internal node that produces the smallest per-node increase in∑\\nmNmQm(T), and continue until we produce the single-node (root) tree.\\nThis gives a (ﬁnite) sequence of subtrees, and one can show this sequence\\nmust contain Tα. See Breiman et al. (1984) or Ripley (1996) for details.\\nEstimation of αis achieved by ﬁve- or tenfold cross-validation: we choose\\nthe value ˆ αto minimize the cross-validated sum of squares. Our ﬁnal tree\\nisTˆα.\\n9.2.3 Classiﬁcation Trees\\nIf the target is a classiﬁcation outcome taking values 1 ,2,... ,K , the only\\nchanges needed in the tree algorithm pertain to the criteria for splitting\\nnodes and pruning the tree. For regression we used the squared-error node'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 327}, page_content='9.2 Tree-Based Methods 309\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5\\npEntropy\\nGini index\\nMisclassification error\\nFIGURE 9.3. Node impurity measures for two-class classiﬁcation, as a funct ion\\nof the proportion pin class 2. Cross-entropy has been scaled to pass through\\n(0.5,0.5).\\nimpurity measure Qm(T) deﬁned in (9.15), but this is not suitable for\\nclassiﬁcation. In a node m, representing a region RmwithNmobservations,\\nlet\\nˆpmk=1\\nNm∑\\nxi∈RmI(yi=k),\\nthe proportion of class kobservations in node m. We classify the obser-\\nvations in node mto class k(m) = arg max kˆpmk, the majority class in\\nnodem. Diﬀerent measures Qm(T) of node impurity include the following:\\nMisclassiﬁcation error:1\\nNm∑\\ni∈RmI(yi̸=k(m)) = 1 −ˆpmk(m).\\nGini index:∑\\nk̸=k′ˆpmkˆpmk′=∑K\\nk=1ˆpmk(1−ˆpmk).\\nCross-entropy or deviance: −∑K\\nk=1ˆpmklog ˆpmk.\\n(9.17)\\nFor two classes, if pis the proportion in the second class, these three mea-\\nsures are 1 −max(p,1−p), 2p(1−p) and −plogp−(1−p)log (1 −p),\\nrespectively. They are shown in Figure 9.3. All three are similar, but cross -\\nentropy and the Gini index are diﬀerentiable, and hence more amenable to\\nnumerical optimization. Comparing (9.13) and (9.15), we see that we need\\nto weight the node impurity measures by the number NmLandNmRof\\nobservations in the two child nodes created by splitting node m.\\nIn addition, cross-entropy and the Gini index are more sensitive to changes\\nin the node probabilities than the misclassiﬁcation rate. For example, in\\na two-class problem with 400 observations in each class (denote this by\\n(400,400)), suppose one split created nodes (300 ,100) and (100 ,300), while'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 328}, page_content='310 9. Additive Models, Trees, and Related Methods\\nthe other created nodes (200 ,400) and (200 ,0). Both splits produce a mis-\\nclassiﬁcation rate of 0.25, but the second split produces a pure node and is\\nprobably preferable. Both the Gini index and cross-entropy are lower for the\\nsecond split. For this reason, either the Gini index or cross-entropy should\\nbe used when growing the tree. To guide cost-complexity pruning, any of\\nthe three measures can be used, but typically it is the misclassiﬁcation rate.\\nThe Gini index can be interpreted in two interesting ways. Rather than\\nclassify observations to the majority class in the node, we could classify\\nthem to class kwith probability ˆ pmk. Then the training error rate of this\\nrule in the node is∑\\nk̸=k′ˆpmkˆpmk′—the Gini index. Similarly, if we code\\neach observation as 1 for class kand zero otherwise, the variance over the\\nnode of this 0-1 response is ˆ pmk(1−ˆpmk). Summing over classes kagain\\ngives the Gini index.\\n9.2.4 Other Issues\\nCategorical Predictors\\nWhen splitting a predictor having qpossible unordered values, there are\\n2q−1−1 possible partitions of the qvalues into two groups, and the com-\\nputations become prohibitive for large q. However, with a 0 −1 outcome,\\nthis computation simpliﬁes. We order the predictor classes according to the\\nproportion falling in outcome class 1. Then we split this predictor as if it\\nwere an ordered predictor. One can show this gives the optimal split, in\\nterms of cross-entropy or Gini index, among all possible 2q−1−1 splits. This\\nresult also holds for a quantitative outcome and square error loss—the cat-\\negories are ordered by increasing mean of the outcome. Although intuitive,\\nthe proofs of these assertions are not trivial. The proof for binary outcomes\\nis given in Breiman et al. (1984) and Ripley (1996); the proof for quanti ta-\\ntive outcomes can be found in Fisher (1958). For multicategory outcomes,\\nno such simpliﬁcations are possible, although various approximations have\\nbeen proposed (Loh and Vanichsetakul, 1988).\\nThe partitioning algorithm tends to favor categorical predictors with\\nmany levels q; the number of partitions grows exponentially in q, and the\\nmore choices we have, the more likely we can ﬁnd a good one for the data\\nat hand. This can lead to severe overﬁtting if qis large, and such variables\\nshould be avoided.\\nThe Loss Matrix\\nIn classiﬁcation problems, the consequences of misclassifying observations\\nare more serious in some classes than others. For example, it is probably\\nworse to predict that a person will not have a heart attack when he/she\\nactually will, than vice versa. To account for this, we deﬁne a K×Kloss\\nmatrix L, with Lkk′being the loss incurred for classifying a class kobser-\\nvation as class k′. Typically no loss is incurred for correct classiﬁcations,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 329}, page_content='9.2 Tree-Based Methods 311\\nthat is, Lkk= 0∀k. To incorporate the losses into the modeling process,\\nwe could modify the Gini index to∑\\nk̸=k′Lkk′ˆpmkˆpmk′; this would be the\\nexpected loss incurred by the randomized rule. This works for the multi-\\nclass case, but in the two-class case has no eﬀect, since the coeﬃcient of\\nˆpmkˆpmk′isLkk′+Lk′k. For two classes a better approach is to weight the\\nobservations in class kbyLkk′. This can be used in the multiclass case only\\nif, as a function of k,Lkk′doesn’t depend on k′. Observation weighting can\\nbe used with the deviance as well. The eﬀect of observation weighting is to\\nalter the prior probability on the classes. In a terminal node, the empirical\\nBayes rule implies that we classify to class k(m) = arg min k∑\\nℓLℓkˆpmℓ.\\nMissing Predictor Values\\nSuppose our data has some missing predictor values in some or all of the\\nvariables. We might discard any observation with some missing values, but\\nthis could lead to serious depletion of the training set. Alternatively we\\nmight try to ﬁll in (impute) the missing values, with say the mean of that\\npredictor over the nonmissing observations. For tree-based models, there\\nare two better approaches. The ﬁrst is applicable to categorical predictors:\\nwe simply make a new category for “missing.” From this we might dis-\\ncover that observations with missing values for some measurement behave\\ndiﬀerently than those with nonmissing values. The second more general\\napproach is the construction of surrogate variables. When considering a\\npredictor for a split, we use only the observations for which that predictor\\nis not missing. Having chosen the best (primary) predictor and split point,\\nwe form a list of surrogate predictors and split points. The ﬁrst surroga te\\nis the predictor and corresponding split point that best mimics the split of\\nthe training data achieved by the primary split. The second surrogate is\\nthe predictor and corresponding split point that does second best, and so\\non. When sending observations down the tree either in the training phase\\nor during prediction, we use the surrogate splits in order, if the primary\\nsplitting predictor is missing. Surrogate splits exploit correlations between\\npredictors to try and alleviate the eﬀect of missing data. The higher the cor-\\nrelation between the missing predictor and the other predictors, the smaller\\nthe loss of information due to the missing value. The general problem of\\nmissing data is discussed in Section 9.6.\\nWhy Binary Splits?\\nRather than splitting each node into just two groups at each stage (as\\nabove), we might consider multiway splits into more than two groups. Whil e\\nthis can sometimes be useful, it is not a good general strategy. The problem\\nis that multiway splits fragment the data too quickly, leaving insuﬃcient\\ndata at the next level down. Hence we would want to use such splits only\\nwhen needed. Since multiway splits can be achieved by a series of binary\\nsplits, the latter are preferred.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 330}, page_content='312 9. Additive Models, Trees, and Related Methods\\nOther Tree-Building Procedures\\nThe discussion above focuses on the CART (classiﬁcation and regression\\ntree) implementation of trees. The other popular methodology is ID3 and\\nits later versions, C4.5 and C5.0 (Quinlan, 1993). Early versions of the\\nprogram were limited to categorical predictors, and used a top-down rule\\nwith no pruning. With more recent developments, C5.0 has become quite\\nsimilar to CART. The most signiﬁcant feature unique to C5.0 is a scheme\\nfor deriving rule sets. After a tree is grown, the splitting rules that deﬁne the\\nterminal nodes can sometimes be simpliﬁed: that is, one or more condition\\ncan be dropped without changing the subset of observations that fall in\\nthe node. We end up with a simpliﬁed set of rules deﬁning each terminal\\nnode; these no longer follow a tree structure, but their simplicity might\\nmake them more attractive to the user.\\nLinear Combination Splits\\nRather than restricting splits to be of the form Xj≤s, one can allow splits\\nalong linear combinations of the form∑ajXj≤s. The weights ajand\\nsplit point sare optimized to minimize the relevant criterion (such as the\\nGini index). While this can improve the predictive power of the tree, it can\\nhurt interpretability. Computationally, the discreteness of the split point\\nsearch precludes the use of a smooth optimization for the weights. A better\\nway to incorporate linear combination splits is in the hierarchical mixtures\\nof experts (HME) model, the topic of Section 9.5.\\nInstability of Trees\\nOne major problem with trees is their high variance. Often a small change\\nin the data can result in a very diﬀerent series of splits, making interpre-\\ntation somewhat precarious. The major reason for this instability is the\\nhierarchical nature of the process: the eﬀect of an error in the top split\\nis propagated down to all of the splits below it. One can alleviate this to\\nsome degree by trying to use a more stable split criterion, but the inherent\\ninstability is not removed. It is the price to be paid for estimating a simple,\\ntree-based structure from the data. Bagging (Section 8.7) averages many\\ntrees to reduce this variance.\\nLack of Smoothness\\nAnother limitation of trees is the lack of smoothness of the prediction sur-\\nface, as can be seen in the bottom right panel of Figure 9.2. In classiﬁcation\\nwith 0/1 loss, this doesn’t hurt much, since bias in estimation of the class\\nprobabilities has a limited eﬀect. However, this can degrade performance\\nin the regression setting, where we would normally expect the underlying\\nfunction to be smooth. The MARS procedure, described in Section 9.4,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 331}, page_content='9.2 Tree-Based Methods 313\\nTABLE 9.3. Spam data: confusion rates for the 17-node tree (chosen by cross–\\nvalidation) on the test data. Overall error rate is 9.3%.\\nPredicted\\nTrueemail spam\\nemail 57.3% 4.0%\\nspam 5.3% 33.4%\\ncan be viewed as a modiﬁcation of CART designed to alleviate this lack of\\nsmoothness.\\nDiﬃculty in Capturing Additive Structure\\nAnother problem with trees is their diﬃculty in modeling additive struc-\\nture. In regression, suppose, for example, that Y=c1I(X1< t1)+c2I(X2<\\nt2) +εwhere εis zero-mean noise. Then a binary tree might make its ﬁrst\\nsplit on X1neart1. At the next level down it would have to split both nodes\\nonX2att2in order to capture the additive structure. This might happen\\nwith suﬃcient data, but the model is given no special encouragement to ﬁnd\\nsuch structure. If there were ten rather than two additive eﬀects, it would\\ntake many fortuitous splits to recreate the structure, and the data analyst\\nwould be hard pressed to recognize it in the estimated tree. The “blame”\\nhere can again be attributed to the binary tree structure, which has both\\nadvantages and drawbacks. Again the MARS method (Section 9.4) gives\\nup this tree structure in order to capture additive structure.\\n9.2.5 Spam Example (Continued)\\nWe applied the classiﬁcation tree methodology to the spamexample intro-\\nduced earlier. We used the deviance measure to grow the tree and mis-\\nclassiﬁcation rate to prune it. Figure 9.4 shows the 10-fold cross-validat ion\\nerror rate as a function of the size of the pruned tree, along with ±2 stan-\\ndard errors of the mean, from the ten replications. The test error curve is\\nshown in orange. Note that the cross-validation error rates are indexed by\\na sequence of values of αandnottree size; for trees grown in diﬀerent folds,\\na value of αmight imply diﬀerent sizes. The sizes shown at the base of the\\nplot refer to |Tα|, the sizes of the pruned original tree.\\nThe error ﬂattens out at around 17 terminal nodes, giving the pruned tree\\nin Figure 9.5. Of the 13 distinct features chosen by the tree, 11 overlap with\\nthe 16 signiﬁcant features in the additive model (Table 9.2). The overall\\nerror rate shown in Table 9.3 is about 50% higher than for the additive\\nmodel in Table 9.1.\\nConsider the rightmost branches of the tree. We branch to the right\\nwith aspamwarning if more than 5.5% of the characters are the $ sign.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 332}, page_content='314 9. Additive Models, Trees, and Related Methods\\n0 10 20 30 400.0 0.1 0.2 0.3 0.4\\nTree SizeMisclassification Rate176 21 7 5 3 2 0α\\nFIGURE 9.4. Results for spamexample. The blue curve is the 10-fold cross-val-\\nidation estimate of misclassiﬁcation rate as a function of tre e size, with standard\\nerror bars. The minimum occurs at a tree size with about 17terminal nodes (using\\nthe “one-standard-error” rule). The orange curve is the test er ror, which tracks\\nthe CV error quite closely. The cross-validation is indexed by values of α, shown\\nabove. The tree sizes shown below refer to |Tα|, the size of the original tree indexed\\nbyα.\\nHowever, if in addition the phrase hpoccurs frequently, then this is likely\\nto be company business and we classify as email. All of the 22 cases in\\nthe test set satisfying these criteria were correctly classiﬁed. If the second\\ncondition is not met, and in addition the average length of repeated capital\\nlettersCAPAVE is larger than 2.9, then we classify as spam. Of the 227 test\\ncases, only seven were misclassiﬁed.\\nIn medical classiﬁcation problems, the terms sensitivity andspeciﬁcity\\nare used to characterize a rule. They are deﬁned as follows:\\nSensitivity: probability of predicting disease given true state is disease.\\nSpeciﬁcity: probability of predicting non-disease given true state is non-\\ndisease.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 333}, page_content='9.2 Tree-Based Methods 315\\n600/1536\\n280/1177\\n180/1065\\n 80/861\\n 80/652\\n 77/423\\n 20/238\\n 19/236   1/2 57/185\\n 48/113\\n 37/101   1/12  9/72  3/229  0/209100/204\\n 36/123\\n 16/94\\n 14/89   3/5  9/29 16/81  9/112\\n  6/109   0/3 48/359\\n 26/337\\n 19/110\\n 18/109   0/1  7/227  0/22\\nspam\\nspamspamspamspam\\nspamspam\\nspam\\nspam\\nspam\\nspamspamemail\\nemailemail\\nemailemailemailemail\\nemail\\nemail\\nemail\\nemailemailemail\\nemailemailemailemailemailemailemailemail\\nch$<0.0555\\nremove<0.06\\nch!<0.191\\ngeorge<0.005\\nhp<0.03\\nCAPMAX<10.5\\nreceive<0.125 edu<0.045\\nour<1.2CAPAVE<2.7505\\nfree<0.065\\nbusiness<0.145george<0.15hp<0.405\\nCAPAVE<2.907\\n1999<0.58ch$>0.0555\\nremove>0.06\\nch!>0.191\\ngeorge>0.005\\nhp>0.03\\nCAPMAX>10.5\\nreceive>0.125 edu>0.045\\nour>1.2CAPAVE>2.7505\\nfree>0.065\\nbusiness>0.145george>0.15hp>0.405\\nCAPAVE>2.907\\n1999>0.58\\nFIGURE 9.5. The pruned tree for the spamexample. The split variables are\\nshown in blue on the branches, and the classiﬁcation is shown in e very node.The\\nnumbers under the terminal nodes indicate misclassiﬁcation rates on the test data.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 334}, page_content='316 9. Additive Models, Trees, and Related Methods\\nSpecificitySensitivity\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0• •\\n•• •\\n•\\n•\\n•\\n••\\n•\\n•••• ••••••••••••••• ••••• ••••••••• •••••••••••••\\n••\\n•\\n•\\n••\\n••\\n•••\\n•\\n•\\n•\\n•\\n•Tree (0.95)\\nGAM (0.98)\\nWeighted Tree (0.90)\\nFIGURE 9.6. ROC curves for the classiﬁcation rules ﬁt to the spamdata. Curves\\nthat are closer to the northeast corner represent better classi ﬁers. In this case the\\nGAM classiﬁer dominates the trees. The weighted tree achieves better sensitivity\\nfor higher speciﬁcity than the unweighted tree. The numbers in t he legend repre-\\nsent the area under the curve.\\nIf we think of spamandemail as the presence and absence of disease, re-\\nspectively, then from Table 9.3 we have\\nSensitivity = 100 ×33.4\\n33.4 + 5.3= 86.3%,\\nSpeciﬁcity = 100 ×57.3\\n57.3 + 4.0= 93.4%.\\nIn this analysis we have used equal losses. As before let Lkk′be the\\nloss associated with predicting a class kobject as class k′. By varying the\\nrelative sizes of the losses L01andL10, we increase the sensitivity and\\ndecrease the speciﬁcity of the rule, or vice versa. In this example, we want\\nto avoid marking good email asspam, and thus we want the speciﬁcity to\\nbe very high. We can achieve this by setting L01>1 say, with L10= 1.\\nThe Bayes’ rule in each terminal node classiﬁes to class 1 ( spam) if the\\nproportion of spamis≥L01/(L10+L01), and class zero otherwise. The'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 335}, page_content='9.3 PRIM: Bump Hunting 317\\nreceiver operating characteristic curve (ROC) is a commonly used summary\\nfor assessing the tradeoﬀ between sensitivity and speciﬁcity. It is a plot of\\nthe sensitivity versus speciﬁcity as we vary the parameters of a classiﬁcation\\nrule. Varying the loss L01between 0.1 and 10, and applying Bayes’ rule to\\nthe 17-node tree selected in Figure 9.4, produced the ROC curve shown\\nin Figure 9.6. The standard error of each curve near 0.9 is approximately√\\n0.9(1−0.9)/1536 = 0 .008, and hence the standard error of the diﬀerence\\nis about 0 .01. We see that in order to achieve a speciﬁcity of close to 100%,\\nthe sensitivity has to drop to about 50%. The area under the curve is a\\ncommonly used quantitative summary; extending the curve linearly in each\\ndirection so that it is deﬁned over [0 ,100], the area is approximately 0 .95.\\nFor comparison, we have included the ROC curve for the GAM model ﬁt\\nto these data in Section 9.2; it gives a better classiﬁcation rule for any los s,\\nwith an area of 0 .98.\\nRather than just modifying the Bayes rule in the nodes, it is better to\\ntake full account of the unequal losses in growing the tree, as was done\\nin Section 9.2. With just two classes 0 and 1, losses may be incorporated\\ninto the tree-growing process by using weight Lk,1−kfor an observation in\\nclassk. Here we chose L01= 5,L10= 1 and ﬁt the same size tree as before\\n(|Tα|= 17). This tree has higher sensitivity at high values of the speciﬁcity\\nthan the original tree, but does more poorly at the other extreme. Its top\\nfew splits are the same as the original tree, and then it departs from it.\\nFor this application the tree grown using L01= 5 is clearly better than the\\noriginal tree.\\nThe area under the ROC curve, used above, is sometimes called the c-\\nstatistic . Interestingly, it can be shown that the area under the ROC curve\\nis equivalent to the Mann-Whitney U statistic (or Wilcoxon rank-sum test),\\nfor the median diﬀerence between the prediction scores in the two groups\\n(Hanley and McNeil, 1982). For evaluating the contribution of an additional\\npredictor when added to a standard model, the c-statistic may not be an\\ninformative measure. The new predictor can be very signiﬁcant in terms\\nof the change in model deviance, but show only a small increase in the c-\\nstatistic. For example, removal of the highly signiﬁcant term george from\\nthe model of Table 9.2 results in a decrease in the c-statistic of less than\\n0.01. Instead, it is useful to examine how the additional predictor changes\\nthe classiﬁcation on an individual sample basis. A good discussion of this\\npoint appears in Cook (2007).\\n9.3 PRIM: Bump Hunting\\nTree-based methods (for regression) partition the feature space into box-\\nshaped regions, to try to make the response averages in each box as diﬀer-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 336}, page_content='318 9. Additive Models, Trees, and Related Methods\\nent as possible. The splitting rules deﬁning the boxes are related to each\\nthrough a binary tree, facilitating their interpretation.\\nThe patient rule induction method (PRIM) also ﬁnds boxes in the feature\\nspace, but seeks boxes in which the response average is high. Hence it looks\\nfor maxima in the target function, an exercise known as bump hunting . (If\\nminima rather than maxima are desired, one simply works with the negative\\nresponse values.)\\nPRIM also diﬀers from tree-based partitioning methods in that the box\\ndeﬁnitions are not described by a binary tree. This makes interpretation of\\nthe collection of rules more diﬃcult; however, by removing the binary tree\\nconstraint, the individual rules are often simpler.\\nThe main box construction method in PRIM works from the top down,\\nstarting with a box containing all of the data. The box is compressed along\\none face by a small amount, and the observations then falling outside the\\nbox are peeled oﬀ. The face chosen for compression is the one resulting in\\nthe largest box mean, after the compression is performed. Then the process\\nis repeated, stopping when the current box contains some minimum number\\nof data points.\\nThis process is illustrated in Figure 9.7. There are 200 data points uni-\\nformly distributed over the unit square. The color-coded plot indicates the\\nresponse Ytaking the value 1 (red) when 0 .5< X1<0.8 and 0 .4< X2<\\n0.6. and zero (blue) otherwise. The panels shows the successive boxes found\\nby the top-down peeling procedure, peeling oﬀ a proportion α= 0.1 of the\\nremaining data points at each stage.\\nFigure 9.8 shows the mean of the response values in the box, as the box\\nis compressed.\\nAfter the top-down sequence is computed, PRIM reverses the process,\\nexpanding along any edge, if such an expansion increases the box mean.\\nThis is called pasting . Since the top-down procedure is greedy at each step,\\nsuch an expansion is often possible.\\nThe result of these steps is a sequence of boxes, with diﬀerent numbers\\nof observation in each box. Cross-validation, combined with the judgment\\nof the data analyst, is used to choose the optimal box size.\\nDenote by B1the indices of the observations in the box found in step 1.\\nThe PRIM procedure then removes the observations in B1from the training\\nset, and the two-step process—top down peeling, followed by bottom-up\\npasting—is repeated on the remaining dataset. This entire process is re-\\npeated several times, producing a sequence of boxes B1,B2,... ,B k. Each\\nbox is deﬁned by a set of rules involving a subset of predictors like\\n(a1≤X1≤b1) and ( b1≤X3≤b2).\\nA summary of the PRIM procedure is given Algorithm 9.3.\\nPRIM can handle a categorical predictor by considering all partitions of\\nthe predictor, as in CART. Missing values are also handled in a manner\\nsimilar to CART. PRIM is designed for regression (quantitative respo nse'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 337}, page_content='9.3 PRIM: Bump Hunting 319\\n1\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no2\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no3\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no4\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no\\n5\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no6\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no7\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no8\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no\\n12\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no17\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no22\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no27\\nooo\\noo\\noooo\\nooo\\noo\\noo\\noo\\noo\\no\\noooo\\noo\\noooo\\no oo\\noo\\noo\\no\\noo\\no\\no\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooo\\no\\nooooo\\no\\nooo o\\noo\\noo\\nooo\\nooo\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noooo\\noo\\nooo\\no\\noo\\noo oo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noo\\no\\nooo\\noooo\\nooo\\nooo\\noo\\noo\\nooo\\nooo\\noo\\noo\\noo oo\\no\\nooo\\noo\\no\\noooo\\noo\\no\\nFIGURE 9.7. Illustration of PRIM algorithm. There are two classes, indica ted\\nby the blue (class 0) and red (class 1) points. The procedure starts with a rectangle\\n(broken black lines) surrounding all of the data, and then peels a way points along\\none edge by a prespeciﬁed amount in order to maximize the mean of th e points\\nremaining in the box. Starting at the top left panel, the sequence of p eelings is\\nshown, until a pure red region is isolated in the bottom right pa nel. The iteration\\nnumber is indicated at the top of each panel.\\nNumber of Observations in BoxBox Mean\\n50 100 1500.2 0.4 0.6 0.8 1.0\\n•••••••••••••••••••••••••••\\nFIGURE 9.8. Box mean as a function of number of observations in the box.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 338}, page_content='320 9. Additive Models, Trees, and Related Methods\\nAlgorithm 9.3 Patient Rule Induction Method.\\n1. Start with all of the training data, and a maximal box containing all\\nof the data.\\n2. Consider shrinking the box by compressing one face, so as to peel oﬀ\\nthe proportion αof observations having either the highest values of\\na predictor Xj, or the lowest. Choose the peeling that produces the\\nhighest response mean in the remaining box. (Typically α= 0.05 or\\n0.10.)\\n3. Repeat step 2 until some minimal number of observations (say 10)\\nremain in the box.\\n4. Expand the box along any face, as long as the resulting box mean\\nincreases.\\n5. Steps 1–4 give a sequence of boxes, with diﬀerent numbers of obser-\\nvations in each box. Use cross-validation to choose a member of the\\nsequence. Call the box B1.\\n6. Remove the data in box B1from the dataset and repeat steps 2–5 to\\nobtain a second box, and continue to get as many boxes as desired.\\nvariable); a two-class outcome can be handled simply by coding it as 0 and\\n1. There is no simple way to deal with k >2 classes simultaneously: one\\napproach is to run PRIM separately for each class versus a baseline class.\\nAn advantage of PRIM over CART is its patience. Because of its bi-\\nnary splits, CART fragments the data quite quickly. Assuming splits of\\nequal size, with Nobservations it can only make log2(N)−1 splits before\\nrunning out of data. If PRIM peels oﬀ a proportion αof training points\\nat each stage, it can perform approximately −log(N)/log(1−α) peeling\\nsteps before running out of data. For example, if N= 128 and α= 0.10,\\nthen log2(N)−1 = 6 while −log(N)/log(1−α)≈46. Taking into account\\nthat there must be an integer number of observations at each stage, PRIM\\nin fact can peel only 29 times. In any case, the ability of PRIM to be more\\npatient should help the top-down greedy algorithm ﬁnd a better solution.\\n9.3.1 Spam Example (Continued)\\nWe applied PRIM to the spamdata, with the response coded as 1 for spam\\nand 0 for email.\\nThe ﬁrst two boxes found by PRIM are summarized below:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 339}, page_content='9.4 MARS: Multivariate Adaptive Regression Splines 321\\nRule 1 Global Mean Box Mean Box Support\\nTraining 0.3931 0.9607 0.1413\\nTest 0.3958 1.0000 0.1536\\nRule 1\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3ch!>0.029\\nCAPAVE >2.331\\nyour >0.705\\n1999 <0.040\\nCAPTOT >79.50\\nedu <0.070\\nre<0.535\\nch;<0.030\\nRule 2 Remain Mean Box Mean Box Support\\nTraining 0.2998 0.9560 0.1043\\nTest 0.2862 0.9264 0.1061\\nRule 2{\\nremove >0.010\\ngeorge <0.110\\nThe box support is the proportion of observations falling in the box.\\nThe ﬁrst box is purely spam, and contains about 15% of the test data.\\nThe second box contains 10.6% of the test observations, 92.6% of which\\narespam. Together the two boxes contain 26% of the data and are about\\n97%spam. The next few boxes (not shown) are quite small, containing only\\nabout 3% of the data.\\nThe predictors are listed in order of importance. Interestingly the top\\nsplitting variables in the CART tree (Figure 9.5) do not appear in PRIM’s\\nﬁrst box.\\n9.4 MARS: Multivariate Adaptive Regression\\nSplines\\nMARS is an adaptive procedure for regression, and is well suited for high-\\ndimensional problems (i.e., a large number of inputs). It can be viewed as a\\ngeneralization of stepwise linear regression or a modiﬁcation of the CART\\nmethod to improve the latter’s performance in the regression setting. We\\nintroduce MARS from the ﬁrst point of view, and later make the connection\\nto CART.\\nMARS uses expansions in piecewise linear basis functions of the form\\n(x−t)+and (t−x)+. The “+” means positive part, so\\n(x−t)+={\\nx−t,ifx > t,\\n0,otherwise,and ( t−x)+={\\nt−x,ifx < t,\\n0,otherwise .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 340}, page_content='322 9. Additive Models, Trees, and Related Methods\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5(x−t)+ (t−x)+\\nxtBasis Function\\nFIGURE 9.9. The basis functions (x−t)+(solid orange) and (t−x)+(broken\\nblue) used by MARS.\\nAs an example, the functions ( x−0.5)+and (0 .5−x)+are shown in Fig-\\nure 9.9.\\nEach function is piecewise linear, with a knotat the value t. In the\\nterminology of Chapter 5, these are linear splines. We call the two functions\\nareﬂected pair in the discussion below. The idea is to form reﬂected pairs\\nfor each input Xjwith knots at each observed value xijof that input.\\nTherefore, the collection of basis functions is\\nC={(Xj−t)+,(t−Xj)+}t∈ {x1j, x2j, . . . , x Nj}\\nj= 1,2, . . . , p.(9.18)\\nIf all of the input values are distinct, there are 2 Npbasis functions alto-\\ngether. Note that although each basis function depends only on a single\\nXj, for example, h(X) = (Xj−t)+, it is considered as a function over the\\nentire input space IRp.\\nThe model-building strategy is like a forward stepwise linear regression,\\nbut instead of using the original inputs, we are allowed to use functions\\nfrom the set Cand their products. Thus the model has the form\\nf(X) =β0+M∑\\nm=1βmhm(X), (9.19)\\nwhere each hm(X) is a function in C, or a product of two or more such\\nfunctions.\\nGiven a choice for the hm, the coeﬃcients βmare estimated by minimiz-\\ning the residual sum-of-squares, that is, by standard linear regression. The\\nreal art, however, is in the construction of the functions hm(x). We start\\nwith only the constant function h0(X) = 1 in our model, and all functions\\nin the set Care candidate functions. This is depicted in Figure 9.10.\\nAt each stage we consider as a new basis function pair all products of a\\nfunction hmin the model set Mwith one of the reﬂected pairs in C. We\\nadd to the model Mthe term of the form\\nˆβM+1hℓ(X)≤(Xj−t)++ˆβM+2hℓ(X)≤(t−Xj)+, hℓ∈ M,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 341}, page_content='9.4 MARS: Multivariate Adaptive Regression Splines 323\\nX1\\nX1X1\\nX1\\nX2X2\\nX2X2\\nX2\\nXpXpXpConstant\\nFIGURE 9.10. Schematic of the MARS forward model-building procedure. On\\nthe left are the basis functions currently in the model: initiall y, this is the constant\\nfunction h(X) = 1. On the right are all candidate basis functions to be considered\\nin building the model. These are pairs of piecewise linear basi s functions as in\\nFigure 9.9, with knots tat all unique observed values xijof each predictor Xj.\\nAt each stage we consider all products of a candidate pair with a basis function\\nin the model. The product that decreases the residual error t he most is added into\\nthe current model. Above we illustrate the ﬁrst three steps of t he procedure, with\\nthe selected functions shown in red.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 342}, page_content='324 9. Additive Models, Trees, and Related Methods\\nX1X2h(X1, X2)\\nFIGURE 9.11. The function h(X1, X2) = (X1−x51)+≤(x72−X2)+, resulting\\nfrom multiplication of two piecewise linear MARS basis functio ns.\\nthat produces the largest decrease in training error. Here ˆβM+1andˆβM+2\\nare coeﬃcients estimated by least squares, along with all the other M+ 1\\ncoeﬃcients in the model. Then the winning products are added to the\\nmodel and the process is continued until the model set Mcontains some\\npreset maximum number of terms.\\nFor example, at the ﬁrst stage we consider adding to the model a function\\nof the form β1(Xj−t)++β2(t−Xj)+;t∈ {xij}, since multiplication by\\nthe constant function just produces the function itself. Suppose the best\\nchoice is ˆβ1(X2−x72)++ˆβ2(x72−X2)+. Then this pair of basis functions\\nis added to the set M, and at the next stage we consider including a pair\\nof products the form\\nhm(X)≤(Xj−t)+and hm(X)≤(t−Xj)+, t∈ {xij},\\nwhere for hmwe have the choices\\nh0(X) = 1 ,\\nh1(X) = ( X2−x72)+,or\\nh2(X) = ( x72−X2)+.\\nThe third choice produces functions such as ( X1−x51)+≤(x72−X2)+,\\ndepicted in Figure 9.11.\\nAt the end of this process we have a large model of the form (9.19). This\\nmodel typically overﬁts the data, and so a backward deletion procedure\\nis applied. The term whose removal causes the smallest increase in resid-\\nual squared error is deleted from the model at each stage, producing an\\nestimated best model ˆfλof each size (number of terms) λ. One could use\\ncross-validation to estimate the optimal value of λ, but for computational'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 343}, page_content='9.4 MARS: Multivariate Adaptive Regression Splines 325\\nsavings the MARS procedure instead uses generalized cross-validation. This\\ncriterion is deﬁned as\\nGCV( λ) =∑N\\ni=1(yi−ˆfλ(xi))2\\n(1−M(λ)/N)2. (9.20)\\nThe value M(λ) is the eﬀective number of parameters in the model: this\\naccounts both for the number of terms in the models, plus the number\\nof parameters used in selecting the optimal positions of the knots. Some\\nmathematical and simulation results suggest that one should pay a price\\nof three parameters for selecting a knot in a piecewise linear regression.\\nThus if there are rlinearly independent basis functions in the model, and\\nKknots were selected in the forward process, the formula is M(λ) =r+cK,\\nwhere c= 3. (When the model is restricted to be additive—details below—\\na penalty of c= 2 is used). Using this, we choose the model along the\\nbackward sequence that minimizes GCV( λ).\\nWhy these piecewise linear basis functions, and why this particular model\\nstrategy? A key property of the functions of Figure 9.9 is their ability to\\noperate locally; they are zero over part of their range. When they are mul-\\ntiplied together, as in Figure 9.11, the result is nonzero only over the small\\npart of the feature space where both component functions are nonzero. As\\na result, the regression surface is built up parsimoniously, using nonzero\\ncomponents locally—only where they are needed. This is important, since\\none should “spend” parameters carefully in high dimensions, as they can\\nrun out quickly. The use of other basis functions such as polynomials, would\\nproduce a nonzero product everywhere, and would not work as well.\\nThe second important advantage of the piecewise linear basis function\\nconcerns computation. Consider the product of a function in Mwith each\\nof the Nreﬂected pairs for an input Xj. This appears to require the ﬁtting\\nofNsingle-input linear regression models, each of which uses O(N) oper-\\nations, making a total of O(N2) operations. However, we can exploit the\\nsimple form of the piecewise linear function. We ﬁrst ﬁt the reﬂected pair\\nwith rightmost knot. As the knot is moved successively one position at a\\ntime to the left, the basis functions diﬀer by zero over the left part of the\\ndomain, and by a constant over the right part. Hence after each such move\\nwe can update the ﬁt in O(1) operations. This allows us to try every knot\\nin only O(N) operations.\\nThe forward modeling strategy in MARS is hierarchical, in the sense that\\nmultiway products are built up from products involving terms already in\\nthe model. For example, a four-way product can only be added to the model\\nif one of its three-way components is already in the model. The philosophy\\nhere is that a high-order interaction will likely only exist if some of its lo wer-\\norder “footprints” exist as well. This need not be true, but is a reasonable\\nworking assumption and avoids the search over an exponentially growing\\nspace of alternatives.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 344}, page_content='326 9. Additive Models, Trees, and Related Methods\\nRank of ModelTest Misclassification Error\\n0 20 40 60 80 1000.1 0.2 0.3 0.4\\n• • • • • • • • • • •••••• •• •• • • •••• • ••• • • •• • ••••• ••••••• ••••••••••••••• ••• ••••• ••••••••••••••••••••••••••••••\\n0.055GCV choice\\nFIGURE 9.12. Spam data: test error misclassiﬁcation rate for the MARS pro-\\ncedure, as a function of the rank (number of independent basis functi ons) in the\\nmodel.\\nThere is one restriction put on the formation of model terms: each input\\ncan appear at most once in a product. This prevents the formation of\\nhigher-order powers of an input, which increase or decrease too sharply\\nnear the boundaries of the feature space. Such powers can be approximated\\nin a more stable way with piecewise linear functions.\\nA useful option in the MARS procedure is to set an upper limit on\\nthe order of interaction. For example, one can set a limit of two, allowing\\npairwise products of piecewise linear functions, but not three- or higher-\\nway products. This can aid in the interpretation of the ﬁnal model. An\\nupper limit of one results in an additive model.\\n9.4.1 Spam Example (Continued)\\nWe applied MARS to the “spam” data analyzed earlier in this chapter. To\\nenhance interpretability, we restricted MARS to second-degree interactions.\\nAlthough the target is a two-class variable, we used the squared-error loss\\nfunction nonetheless (see Section 9.4.3). Figure 9.12 shows the test error\\nmisclassiﬁcation rate as a function of the rank (number of independent ba-\\nsis functions) in the model. The error rate levels oﬀ at about 5 .5%, which is\\nslightly higher than that of the generalized additive model (5 .3%) discussed\\nearlier. GCV chose a model size of 60, which is roughly the smallest model\\ngiving optimal performance. The leading interactions found by MARS in-\\nvolved inputs ( ch$, remove ), (ch$, free ) and (hp, CAPTOT ). However, these\\ninteractions give no improvement in performance over the generalized ad-\\nditive model.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 345}, page_content='9.4 MARS: Multivariate Adaptive Regression Splines 327\\n9.4.2 Example (Simulated Data)\\nHere we examine the performance of MARS in three contrasting scenarios.\\nThere are N= 100 observations, and the predictors X1,X2,... ,X pand\\nerrors εhave independent standard normal distributions.\\nScenario 1: The data generation model is\\nY= (X1−1)++ (X1−1)+≤(X2−.8)++ 0.12≤ε. (9.21)\\nThe noise standard deviation 0.12 was chosen so that the signal-to-\\nnoise ratio was about 5. We call this the tensor-product scenario; the\\nproduct term gives a surface that looks like that of Figure 9.11.\\nScenario 2: This is the same as scenario 1, but with p= 20 total predictors;\\nthat is, there are 18 inputs that are independent of the response.\\nScenario 3: This has the structure of a neural network:\\nℓ1=X1+X2+X3+X4+X5,\\nℓ2=X6−X7+X8−X9+X10,\\nσ(t) = 1 /(1 +e−t),\\nY=σ(ℓ1) +σ(ℓ2) + 0.12≤ε.(9.22)\\nScenarios 1 and 2 are ideally suited for MARS, while scenario 3 contains\\nhigh-order interactions and may be diﬃcult for MARS to approximate. We\\nran ﬁve simulations from each model, and recorded the results.\\nIn scenario 1, MARS typically uncovered the correct model almost per-\\nfectly. In scenario 2, it found the correct structure but also found a few\\nextraneous terms involving other predictors.\\nLetθ(x) be the true mean of Y, and let\\nMSE 0= ave x∈Test(¯y−θ(x))2,\\nMSE = ave x∈Test(ˆf(x)−θ(x))2.(9.23)\\nThese represent the mean-square error of the constant model and the ﬁtted\\nMARS model, estimated by averaging at the 1000 test values of x. Table 9.4\\nshows the proportional decrease in model error or R2for each scenario:\\nR2=MSE 0−MSE\\nMSE 0. (9.24)\\nThe values shown are means and standard error over the ﬁve simulations.\\nThe performance of MARS is degraded only slightly by the inclusion of the\\nuseless inputs in scenario 2; it performs substantially worse in scenario 3.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 346}, page_content='328 9. Additive Models, Trees, and Related Methods\\nTABLE 9.4. Proportional decrease in model error ( R2) when MARS is applied\\nto three diﬀerent scenarios.\\nScenario Mean (S.E.)\\n1: Tensor product p= 2 0.97 (0.01)\\n2: Tensor product p= 20 0.96 (0.01)\\n3: Neural network 0.79 (0.01)\\n9.4.3 Other Issues\\nMARS for Classiﬁcation\\nThe MARS method and algorithm can be extended to handle classiﬁcation\\nproblems. Several strategies have been suggested.\\nFor two classes, one can code the output as 0/1 and treat the problem as\\na regression; we did this for the spamexample. For more than two classes,\\none can use the indicator response approach described in Section 4.2. One\\ncodes the Kresponse classes via 0/1 indicator variables, and then per-\\nforms a multi-response MARS regression. For the latter we use a common\\nset of basis functions for all response variables. Classiﬁcation is made to\\nthe class with the largest predicted response value. There are, however, po-\\ntential masking problems with this approach, as described in Section 4.2.\\nA generally superior approach is the “optimal scoring” method discussed\\nin Section 12.5.\\nStone et al. (1997) developed a hybrid of MARS called PolyMARS specif-\\nically designed to handle classiﬁcation problems. It uses the multiple logistic\\nframework described in Section 4.4. It grows the model in a forward stage-\\nwise fashion like MARS, but at each stage uses a quadratic approximation\\nto the multinomial log-likelihood to search for the next basis-function pair.\\nOnce found, the enlarged model is ﬁt by maximum likelihood, and the\\nprocess is repeated.\\nRelationship of MARS to CART\\nAlthough they might seem quite diﬀerent, the MARS and CART strategies\\nactually have strong similarities. Suppose we take the MARS procedure and\\nmake the following changes:\\n•Replace the piecewise linear basis functions by step functions I(x−t >\\n0) and I(x−t≤0).\\n•When a model term is involved in a multiplication by a candidate\\nterm, it gets replaced by the interaction, and hence is not available\\nfor further interactions.\\nWith these changes, the MARS forward procedure is the same as the CART\\ntree-growing algorithm. Multiplying a step function by a pair of reﬂected'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 347}, page_content='9.5 Hierarchical Mixtures of Experts 329\\nstep functions is equivalent to splitting a node at the step. The second\\nrestriction implies that a node may not be split more than once, and leads\\nto the attractive binary-tree representation of the CART model. On the\\nother hand, it is this restriction that makes it diﬃcult for CART to model\\nadditive structures. MARS forgoes the tree structure and gains the ability\\nto capture additive eﬀects.\\nMixed Inputs\\nMars can handle “mixed” predictors—quantitative and qualitative—in a\\nnatural way, much like CART does. MARS considers all possible binary\\npartitions of the categories for a qualitative predictor into two groups.\\nEach such partition generates a pair of piecewise constant basis functions—\\nindicator functions for the two sets of categories. This basis pair is now\\ntreated as any other, and is used in forming tensor products with other\\nbasis functions already in the model.\\n9.5 Hierarchical Mixtures of Experts\\nThe hierarchical mixtures of experts (HME) procedure can be viewed as a\\nvariant of tree-based methods. The main diﬀerence is that the tree splits\\nare not hard decisions but rather soft probabilistic ones. At each node an\\nobservation goes left or right with probabilities depending on its input val-\\nues. This has some computational advantages since the resulting parameter\\noptimization problem is smooth, unlike the discrete split point search in the\\ntree-based approach. The soft splits might also help in prediction accuracy\\nand provide a useful alternative description of the data.\\nThere are other diﬀerences between HMEs and the CART implementa-\\ntion of trees. In an HME, a linear (or logistic regression) model is ﬁt in\\neach terminal node, instead of a constant as in CART. The splits can be\\nmultiway, not just binary, and the splits are probabilistic functions of a\\nlinear combination of inputs, rather than a single input as in the standard\\nuse of CART. However, the relative merits of these choices are not clear,\\nand most were discussed at the end of Section 9.2.\\nA simple two-level HME model in shown in Figure 9.13. It can be thought\\nof as a tree with soft splits at each non-terminal node. However, the inven-\\ntors of this methodology use a diﬀerent terminology. The terminal nodes\\nare called experts , and the non-terminal nodes are called gating networks .\\nThe idea is that each expert provides an opinion (prediction) about the\\nresponse, and these are combined together by the gating networks. As we\\nwill see, the model is formally a mixture model, and the two-level model\\nin the ﬁgure can be extend to multiple levels, hence the name hierarchical\\nmixtures of experts .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 348}, page_content='330 9. Additive Models, Trees, and Related Methods\\ng1 g2\\ng1|1 g2|1g1|2 g2|2Gating GatingGatingGating\\nGating GatingGating GatingGating\\nNetwork Network NetworkNetwork\\nNetwork\\nNetworkNetwork\\nNetwork Network NetworkNetwork\\nNetworkNetwork NetworkNetwork Network\\nNetwork Network NetworkNetwork\\nExpert Expert Expert Expert Expert Expert Expert Expert Expert Expert Expert\\nPr(y|x, θ11) Pr( y|x, θ21) Pr( y|x, θ12) Pr( y|x, θ22)\\nFIGURE 9.13. A two-level hierarchical mixture of experts (HME) model.\\nConsider the regression or classiﬁcation problem, as described earlier in\\nthe chapter. The data is ( xi,yi),i= 1,2,... ,N , with yieither a continuous\\nor binary-valued response, and xia vector-valued input. For ease of nota-\\ntion we assume that the ﬁrst element of xiis one, to account for intercepts.\\nHere is how an HME is deﬁned. The top gating network has the output\\ngj(x,γj) =eγT\\njx\\n∑K\\nk=1eγT\\nkx, j= 1,2,... ,K, (9.25)\\nwhere each γjis a vector of unknown parameters. This represents a soft\\nK-way split ( K= 2 in Figure 9.13.) Each gj(x,γj) is the probability of\\nassigning an observation with feature vector xto the jth branch. Notice\\nthat with K= 2 groups, if we take the coeﬃcient of one of the elements of\\nxto be + ∞, then we get a logistic curve with inﬁnite slope. In this case,\\nthe gating probabilities are either 0 or 1, corresponding to a hard split on\\nthat input.\\nAt the second level, the gating networks have a similar form:\\ngℓ|j(x,γjℓ) =eγT\\njℓx\\n∑K\\nk=1eγT\\njkx, ℓ= 1,2,... ,K. (9.26)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 349}, page_content='9.5 Hierarchical Mixtures of Experts 331\\nThis is the probability of assignment to the ℓth branch, given assignment\\nto the jth branch at the level above.\\nAt each expert (terminal node), we have a model for the response variable\\nof the form\\nY∼Pr(y|x,θjℓ). (9.27)\\nThis diﬀers according to the problem.\\nRegression: The Gaussian linear regression model is used, with θjℓ=\\n(βjℓ,σ2\\njℓ):\\nY=βT\\njℓx+εandε∼N(0,σ2\\njℓ). (9.28)\\nClassiﬁcation: The linear logistic regression model is used:\\nPr(Y= 1|x,θjℓ) =1\\n1 +e−θT\\njℓx. (9.29)\\nDenoting the collection of all parameters by Ψ = {γj,γjℓ,θjℓ}, the total\\nprobability that Y=yis\\nPr(y|x,Ψ) =K∑\\nj=1gj(x,γj)K∑\\nℓ=1gℓ|j(x,γjℓ)Pr(y|x,θjℓ). (9.30)\\nThis is a mixture model, with the mixture probabilities determined by the\\ngating network models.\\nTo estimate the parameters, we maximize the log-likelihood of the data,∑\\nilog Pr( yi|xi,Ψ), over the parameters in Ψ. The most convenient method\\nfor doing this is the EM algorithm, which we describe for mixtures in\\nSection 8.5. We deﬁne latent variables ∆ j, all of which are zero except for\\na single one. We interpret these as the branching decisions made by the top\\nlevel gating network. Similarly we deﬁne latent variables ∆ ℓ|jto describe\\nthe gating decisions at the second level.\\nIn the E-step, the EM algorithm computes the expectations of the ∆ j\\nand ∆ ℓ|jgiven the current values of the parameters. These expectations\\nare then used as observation weights in the M-step of the procedure, to\\nestimate the parameters in the expert networks. The parameters in the\\ninternal nodes are estimated by a version of multiple logistic regression.\\nThe expectations of the ∆ jor ∆ ℓ|jare probability proﬁles, and these are\\nused as the response vectors for these logistic regressions.\\nThe hierarchical mixtures of experts approach is a promising competitor\\nto CART trees. By using soft splits rather than hard decision rules it can\\ncapture situations where the transition from low to high response is gradual .\\nThe log-likelihood is a smooth function of the unknown weights and hence\\nis amenable to numerical optimization. The model is similar to CART with\\nlinear combination splits, but the latter is more diﬃcult to optimize. On'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 350}, page_content='332 9. Additive Models, Trees, and Related Methods\\nthe other hand, to our knowledge there are no methods for ﬁnding a good\\ntree topology for the HME model, as there are in CART. Typically one uses\\na ﬁxed tree of some depth, possibly the output of the CART procedure.\\nThe emphasis in the research on HMEs has been on prediction rather than\\ninterpretation of the ﬁnal model. A close cousin of the HME is the latent\\nclass model (Lin et al., 2000), which typically has only one layer; here\\nthe nodes or latent classes are interpreted as groups of subjects that show\\nsimilar response behavior.\\n9.6 Missing Data\\nIt is quite common to have observations with missing values for one or mor e\\ninput features. The usual approach is to impute (ﬁll-in) the missing values\\nin some way.\\nHowever, the ﬁrst issue in dealing with the problem is determining wheth-\\ner the missing data mechanism has distorted the observed data. Roughly\\nspeaking, data are missing at random if the mechanism resulting in its\\nomission is independent of its (unobserved) value. A more precise deﬁnition\\nis given in Little and Rubin (2002). Suppose yis the response vector and X\\nis the N×pmatrix of inputs (some of which are missing). Denote by Xobs\\nthe observed entries in Xand let Z= (y,X),Zobs= (y,Xobs). Finally, if R\\nis an indicator matrix with ijth entry 1 if xijis missing and zero otherwise,\\nthen the data is said to be missing at random (MAR) if the distribution of\\nRdepends on the data Zonly through Zobs:\\nPr(R|Z,θ) = Pr( R|Zobs,θ). (9.31)\\nHereθare any parameters in the distribution of R. Data are said to be\\nmissing completely at random (MCAR) if the distribution of Rdoesn’t\\ndepend on the observed or missing data:\\nPr(R|Z,θ) = Pr( R|θ). (9.32)\\nMCAR is a stronger assumption than MAR: most imputation methods rely\\non MCAR for their validity.\\nFor example, if a patient’s measurement was not taken because the doctor\\nfelt he was too sick, that observation would not be MAR or MCAR. In this\\ncase the missing data mechanism causes our observed training data to give a\\ndistorted picture of the true population, and data imputation is dangerous\\nin this instance. Often the determination of whether features are MCAR\\nmust be made from information about the data collection process. For\\ncategorical features, one way to diagnose this problem is to code “missing”\\nas an additional class. Then we ﬁt our model to the training data and see\\nif class “missing” is predictive of the response.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 351}, page_content='9.6 Missing Data 333\\nAssuming the features are missing completely at random, there are a\\nnumber of ways of proceeding:\\n1. Discard observations with any missing values.\\n2. Rely on the learning algorithm to deal with missing values in its\\ntraining phase.\\n3. Impute all missing values before training.\\nApproach (1) can be used if the relative amount of missing data is small,\\nbut otherwise should be avoided. Regarding (2), CART is one learning\\nalgorithm that deals eﬀectively with missing values, through surrogate splits\\n(Section 9.2.4). MARS and PRIM use similar approaches. In generalized\\nadditive modeling, all observations missing for a given input feature are\\nomitted when the partial residuals are smoothed against that feature in\\nthe backﬁtting algorithm, and their ﬁtted values are set to zero. Since the\\nﬁtted curves have mean zero (when the model includes an intercept), this\\namounts to assigning the average ﬁtted value to the missing observations.\\nFor most learning methods, the imputation approach (3) is necessary.\\nThe simplest tactic is to impute the missing value with the mean or median\\nof the nonmissing values for that feature. (Note that the above procedure\\nfor generalized additive models is analogous to this.)\\nIf the features have at least some moderate degree of dependence, one\\ncan do better by estimating a predictive model for each feature given the\\nother features and then imputing each missing value by its prediction from\\nthe model. In choosing the learning method for imputation of the features,\\none must remember that this choice is distinct from the method used for\\npredicting yfromX. Thus a ﬂexible, adaptive method will often be pre-\\nferred, even for the eventual purpose of carrying out a linear regression of y\\nonX. In addition, if there are many missing feature values in the training\\nset, the learning method must itself be able to deal with missing feature\\nvalues. CART therefore is an ideal choice for this imputation “engine.”\\nAfter imputation, missing values are typically treated as if they were ac-\\ntually observed. This ignores the uncertainty due to the imputation, which\\nwill itself introduce additional uncertainty into estimates and predictions\\nfrom the response model. One can measure this additional uncertainty by\\ndoing multiple imputations and hence creating many diﬀerent training sets.\\nThe predictive model for ycan be ﬁt to each training set, and the variation\\nacross training sets can be assessed. If CART was used for the imputation\\nengine, the multiple imputations could be done by sampling from the values\\nin the corresponding terminal nodes.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 352}, page_content='334 9. Additive Models, Trees, and Related Methods\\n9.7 Computational Considerations\\nWith Nobservations and ppredictors, additive model ﬁtting requires some\\nnumber mpof applications of a one-dimensional smoother or regression\\nmethod. The required number of cycles mof the backﬁtting algorithm is\\nusually less than 20 and often less than 10, and depends on the amount\\nof correlation in the inputs. With cubic smoothing splines, for example,\\nNlogNoperations are needed for an initial sort and Noperations for the\\nspline ﬁt. Hence the total operations for an additive model ﬁt is pNlogN+\\nmpN.\\nTrees require pNlogNoperations for an initial sort for each predictor,\\nand typically another pNlogNoperations for the split computations. If the\\nsplits occurred near the edges of the predictor ranges, this number could\\nincrease to N2p.\\nMARS requires Nm2+pmN operations to add a basis function to a\\nmodel with mterms already present, from a pool of ppredictors. Hence to\\nbuild an M-term model requires NM3+pM2Ncomputations, which can\\nbe quite prohibitive if Mis a reasonable fraction of N.\\nEach of the components of an HME are typically inexpensive to ﬁt at\\neach M-step: Np2for the regressions, and Np2K2for a K-class logistic\\nregression. The EM algorithm, however, can take a long time to converge,\\nand so sizable HME models are considered costly to ﬁt.\\nBibliographic Notes\\nThe most comprehensive source for generalized additive models is the text\\nof that name by Hastie and Tibshirani (1990). Diﬀerent applications of\\nthis work in medical problems are discussed in Hastie et al. (1989) and\\nHastie and Herman (1990), and the software implementation in Splus is\\ndescribed in Chambers and Hastie (1991). Green and Silverman (1994)\\ndiscuss penalization and spline models in a variety of settings. Efron and\\nTibshirani (1991) give an exposition of modern developments in statisti cs\\n(including generalized additive models), for a nonmathematical audience.\\nClassiﬁcation and regression trees date back at least as far as Morgan and\\nSonquist (1963). We have followed the modern approaches of Breiman et\\nal. (1984) and Quinlan (1993). The PRIM method is due to Friedman\\nand Fisher (1999), while MARS is introduced in Friedman (1991), with an\\nadditive precursor in Friedman and Silverman (1989). Hierarchical mixtures\\nof experts were proposed in Jordan and Jacobs (1994); see also Jacobs et\\nal. (1991).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 353}, page_content='Exercises 335\\nExercises\\nEx. 9.1 Show that a smoothing spline ﬁt of yitoxipreserves the linear\\npartof the ﬁt. In other words, if yi= ˆyi+ri, where ˆ yirepresents the\\nlinear regression ﬁts, and Sis the smoothing matrix, then Sy=ˆy+Sr.\\nShow that the same is true for local linear regression (Section 6.1.1). Hence\\nargue that the adjustment step in the second line of (2) in Algorithm 9.1\\nis unnecessary.\\nEx. 9.2 LetAbe a known k×kmatrix, bbe a known k-vector, and z\\nbe an unknown k-vector. A Gauss–Seidel algorithm for solving the linear\\nsystem of equations Az=bworks by successively solving for element zjin\\nthejth equation, ﬁxing all other zj’s at their current guesses. This process\\nis repeated for j= 1,2,... ,k, 1,2,... ,k,... , until convergence (Golub and\\nVan Loan, 1983).\\n(a) Consider an additive model with Nobservations and pterms, with\\nthejth term to be ﬁt by a linear smoother Sj. Consider the following\\nsystem of equations:\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edI S 1S1≤≤≤S1\\nS2I S 2≤≤≤S2\\n...............\\nSpSpSp≤≤≤I\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edf1\\nf2\\n...\\nfp\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edS1y\\nS2y\\n...\\nSpy\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8. (9.33)\\nHere each fjis an N-vector of evaluations of the jth function at\\nthe data points, and yis anN-vector of the response values. Show\\nthat backﬁtting is a blockwise Gauss–Seidel algorithm for solving this\\nsystem of equations.\\n(b) Let S1andS2be symmetric smoothing operators (matrices) with\\neigenvalues in [0 ,1). Consider a backﬁtting algorithm with response\\nvector yand smoothers S1,S2. Show that with any starting values,\\nthe algorithm converges and give a formula for the ﬁnal iterates.\\nEx. 9.3 Backﬁtting equations. Consider a backﬁtting procedure with orthog-\\nonal projections, and let Dbe the overall regression matrix whose columns\\nspanV=Lcol(S1)⊕ Lcol(S2)⊕ ≤≤≤ ⊕ L col(Sp), where Lcol(S) denotes the\\ncolumn space of a matrix S. Show that the estimating equations\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edI S 1S1≤≤≤S1\\nS2I S 2≤≤≤S2\\n...............\\nSpSpSp≤≤≤I\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edf1\\nf2\\n...\\nfp\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8edS1y\\nS2y\\n...\\nSpy\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f8\\nare equivalent to the least squares normal equations DTDβ=DTywhere\\nβis the vector of coeﬃcients.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 354}, page_content='336 9. Additive Models, Trees, and Related Methods\\nEx. 9.4 Suppose the same smoother Sis used to estimate both terms in a\\ntwo-term additive model (i.e., both variables are identical). Assume that S\\nis symmetric with eigenvalues in [0 ,1). Show that the backﬁtting residual\\nconverges to ( I+S)−1(I−S)y, and that the residual sum of squares con-\\nverges upward. Can the residual sum of squares converge upward in less\\nstructured situations? How does this ﬁt compare to the ﬁt with a single\\nterm ﬁt by S? [Hint: Use the eigen-decomposition of Sto help with this\\ncomparison.]\\nEx. 9.5 Degrees of freedom of a tree . Given data yiwith mean f(xi) and\\nvariance σ2, and a ﬁtting operation y→ˆy, let’s deﬁne the degrees of\\nfreedom of a ﬁt by∑\\nicov(yi,ˆyi)/σ2.\\nConsider a ﬁt ˆyestimated by a regression tree, ﬁt to a set of predictors\\nX1,X2,... ,X p.\\n(a) In terms of the number of terminal nodes m, give a rough formula for\\nthe degrees of freedom of the ﬁt.\\n(b) Generate 100 observations with predictors X1,X2,... ,X 10as inde-\\npendent standard Gaussian variates and ﬁx these values.\\n(c) Generate response values also as standard Gaussian ( σ2= 1), indepen-\\ndent of the predictors. Fit regression trees to the data of ﬁxed size 1,5\\nand 10 terminal nodes and hence estimate the degrees of freedom of\\neach ﬁt. [Do ten simulations of the response and average the results,\\nto get a good estimate of degrees of freedom.]\\n(d) Compare your estimates of degrees of freedom in (a) and (c) and\\ndiscuss.\\n(e) If the regression tree ﬁt were a linear operation, we could write ˆy=Sy\\nfor some matrix S. Then the degrees of freedom would be tr( S).\\nSuggest a way to compute an approximate Smatrix for a regression\\ntree, compute it and compare the resulting degrees of freedom to\\nthose in (a) and (c).\\nEx. 9.6 Consider the ozone data of Figure 6.9.\\n(a) Fit an additive model to the cube root of ozone concentration. as a\\nfunction of temperature, wind speed, and radiation. Compare your\\nresults to those obtained via the trellis display in Figure 6.9.\\n(b) Fit trees, MARS, and PRIM to the same data, and compare the results\\nto those found in (a) and in Figure 6.9.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 355}, page_content='This is page 337\\nPrinter: Opaque this\\n10\\nBoosting and Additive Trees\\n10.1 Boosting Methods\\nBoosting is one of the most powerful learning ideas introduced in the last\\ntwenty years. It was originally designed for classiﬁcation problems, but as\\nwill be seen in this chapter, it can proﬁtably be extended to regression\\nas well. The motivation for boosting was a procedure that combines the\\noutputs of many “weak” classiﬁers to produce a powerful “committee.”\\nFrom this perspective boosting bears a resemblance to bagging and other\\ncommittee-based approaches (Section 8.8). However we shall see that the\\nconnection is at best superﬁcial and that boosting is fundamentally diﬀer-\\nent.\\nWe begin by describing the most popular boosting algorithm due to\\nFreund and Schapire (1997) called “AdaBoost.M1.” Consider a two-class\\nproblem, with the output variable coded as Y∈ {−1,1}. Given a vector of\\npredictor variables X, a classiﬁer G(X) produces a prediction taking one\\nof the two values {−1,1}. The error rate on the training sample is\\nerr =1\\nNN∑\\ni=1I(yi̸=G(xi)),\\nand the expected error rate on future predictions is E XYI(Y̸=G(X)).\\nA weak classiﬁer is one whose error rate is only slightly better than\\nrandom guessing. The purpose of boosting is to sequentially apply the\\nweak classiﬁcation algorithm to repeatedly modiﬁed versions of the data,\\nthereby producing a sequence of weak classiﬁers Gm(x),m= 1,2,... ,M .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 356}, page_content='338 10. Boosting and Additive Trees\\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  Sample\\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  SampleWeighted  Sample\\nTraining  SampleWeighted  Sample\\nTraining  SampleWeighted  SampleWeighted  SampleWeighted  Sample\\nWeighted  SampleWeighted  SampleWeighted  Sample\\nTraining  SampleWeighted  SampleG(x) = sign[∑M\\nm=1αmGm(x)]\\nGM(x)\\nG3(x)\\nG2(x)\\nG1(x)Final Classifier\\nFIGURE 10.1. Schematic of AdaBoost. Classiﬁers are trained on weighted ver-\\nsions of the dataset, and then combined to produce a ﬁnal predictio n.\\nThe predictions from all of them are then combined through a weighted\\nmajority vote to produce the ﬁnal prediction:\\nG(x) = sign(M∑\\nm=1αmGm(x))\\n. (10.1)\\nHereα1,α2,... ,α Mare computed by the boosting algorithm, and weight\\nthe contribution of each respective Gm(x). Their eﬀect is to give higher\\ninﬂuence to the more accurate classiﬁers in the sequence. Figure 10.1 shows\\na schematic of the AdaBoost procedure.\\nThe data modiﬁcations at each boosting step consist of applying weights\\nw1,w2,... ,w Nto each of the training observations ( xi,yi), i= 1,2,... ,N .\\nInitially all of the weights are set to wi= 1/N, so that the ﬁrst step simply\\ntrains the classiﬁer on the data in the usual manner. For each successive\\niteration m= 2,3,... ,M the observation weights are individually modi-\\nﬁed and the classiﬁcation algorithm is reapplied to the weighted observa-\\ntions. At step m, those observations that were misclassiﬁed by the classiﬁer\\nGm−1(x) induced at the previous step have their weights increased, whereas\\nthe weights are decreased for those that were classiﬁed correctly. Thus as\\niterations proceed, observations that are diﬃcult to classify correctly re-\\nceive ever-increasing inﬂuence. Each successive classiﬁer is thereby forced'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 357}, page_content='10.1 Boosting Methods 339\\nAlgorithm 10.1 AdaBoost.M1.\\n1. Initialize the observation weights wi= 1/N, i = 1,2,... ,N .\\n2. For m= 1 to M:\\n(a) Fit a classiﬁer Gm(x) to the training data using weights wi.\\n(b) Compute\\nerrm=∑N\\ni=1wiI(yi̸=Gm(xi))\\n∑N\\ni=1wi.\\n(c) Compute αm= log((1 −errm)/errm).\\n(d) Set wi←wi≤exp[αm≤I(yi̸=Gm(xi))], i= 1,2,... ,N .\\n3. Output G(x) = sign[∑M\\nm=1αmGm(x)]\\n.\\nto concentrate on those training observations that are missed by previous\\nones in the sequence.\\nAlgorithm 10.1 shows the details of the AdaBoost.M1 algorithm. The\\ncurrent classiﬁer Gm(x) is induced on the weighted observations at line 2a.\\nThe resulting weighted error rate is computed at line 2b. Line 2c calculates\\nthe weight αmgiven to Gm(x) in producing the ﬁnal classiﬁer G(x) (line\\n3). The individual weights of each of the observations are updated for the\\nnext iteration at line 2d. Observations misclassiﬁed by Gm(x) have their\\nweights scaled by a factor exp( αm), increasing their relative inﬂuence for\\ninducing the next classiﬁer Gm+1(x) in the sequence.\\nThe AdaBoost.M1 algorithm is known as “Discrete AdaBoost” in Fried-\\nman et al. (2000), because the base classiﬁer Gm(x) returns a discrete class\\nlabel. If the base classiﬁer instead returns a real-valued prediction (e.g.,\\na probability mapped to the interval [ −1,1]), AdaBoost can be modiﬁed\\nappropriately (see “Real AdaBoost” in Friedman et al. (2000)).\\nThe power of AdaBoost to dramatically increase the performance of even\\na very weak classiﬁer is illustrated in Figure 10.2. The features X1,... ,X 10\\nare standard independent Gaussian, and the deterministic target Yis de-\\nﬁned by\\nY={\\n1 if∑10\\nj=1X2\\nj> χ2\\n10(0.5),\\n−1 otherwise .(10.2)\\nHereχ2\\n10(0.5) = 9 .34 is the median of a chi-squared random variable with\\n10 degrees of freedom (sum of squares of 10 standard Gaussians). There are\\n2000 training cases, with approximately 1000 cases in each class, and 10,0 00\\ntest observations. Here the weak classiﬁer is just a “stump”: a two terminal-\\nnode classiﬁcation tree. Applying this classiﬁer alone to the training data\\nset yields a very poor test set error rate of 45.8%, compared to 50% for'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 358}, page_content='340 10. Boosting and Additive Trees\\n0 100 200 300 4000.0 0.1 0.2 0.3 0.4 0.5\\nBoosting IterationsTest ErrorSingle Stump\\n244 Node Tree\\nFIGURE 10.2. Simulated data (10.2): test error rate for boosting with stump s,\\nas a function of the number of iterations. Also shown are the test error rate for\\na single stump, and a 244-node classiﬁcation tree.\\nrandom guessing. However, as boosting iterations proceed the error rate\\nsteadily decreases, reaching 5.8% after 400 iterations. Thus, boosting this\\nsimple very weak classiﬁer reduces its prediction error rate by almost a\\nfactor of four. It also outperforms a single large classiﬁcation tree ( error\\nrate 24 .7%). Since its introduction, much has been written to explain the\\nsuccess of AdaBoost in producing accurate classiﬁers. Most of this work\\nhas centered on using classiﬁcation trees as the “base learner” G(x), where\\nimprovements are often most dramatic. In fact, Breiman (NIPS Workshop,\\n1996) referred to AdaBoost with trees as the “best oﬀ-the-shelf classiﬁer in\\nthe world” (see also Breiman (1998)). This is especially the case for data -\\nmining applications, as discussed more fully in Section 10.7 later in this\\nchapter.\\n10.1.1 Outline of This Chapter\\nHere is an outline of the developments in this chapter:\\n•We show that AdaBoost ﬁts an additive model in a base learner,\\noptimizing a novel exponential loss function. This loss function is'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 359}, page_content='10.2 Boosting Fits an Additive Model 341\\nvery similar to the (negative) binomial log-likelihood (Sections 10.2–\\n10.4).\\n•The population minimizer of the exponential loss function is shown\\nto be the log-odds of the class probabilities (Section 10.5).\\n•We describe loss functions for regression and classiﬁcation that are\\nmore robust than squared error or exponential loss (Section 10.6).\\n•It is argued that decision trees are an ideal base learner for data\\nmining applications of boosting (Sections 10.7 and 10.9).\\n•We develop a class of gradient boosted models (GBMs), for boosting\\ntrees with any loss function (Section 10.10).\\n•The importance of “slow learning” is emphasized, and implemented\\nby shrinkage of each new term that enters the model (Section 10.12),\\nas well as randomization (Section 10.12.2).\\n•Tools for interpretation of the ﬁtted model are described (Section 10.13).\\n10.2 Boosting Fits an Additive Model\\nThe success of boosting is really not very mysterious. The key lies in ex-\\npression (10.1). Boosting is a way of ﬁtting an additive expansion in a set\\nof elementary “basis” functions. Here the basis functions are the individual\\nclassiﬁers Gm(x)∈ {−1,1}. More generally, basis function expansions take\\nthe form\\nf(x) =M∑\\nm=1βmb(x;γm), (10.3)\\nwhere βm,m= 1,2,... ,M are the expansion coeﬃcients, and b(x;γ)∈IR\\nare usually simple functions of the multivariate argument x, characterized\\nby a set of parameters γ. We discuss basis expansions in some detail in\\nChapter 5.\\nAdditive expansions like this are at the heart of many of the learning\\ntechniques covered in this book:\\n•In single-hidden-layer neural networks (Chapter 11), b(x;γ) =σ(γ0+\\nγT\\n1x), where σ(t) = 1/(1+e−t) is the sigmoid function, and γparam-\\neterizes a linear combination of the input variables.\\n•In signal processing, wavelets (Section 5.9.1) are a popular choice with\\nγparameterizing the location and scale shifts of a “mother” wavelet.\\n•Multivariate adaptive regression splines (Section 9.4) uses truncated-\\npower spline basis functions where γparameterizes the variables and\\nvalues for the knots.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 360}, page_content='342 10. Boosting and Additive Trees\\nAlgorithm 10.2 Forward Stagewise Additive Modeling.\\n1. Initialize f0(x) = 0.\\n2. For m= 1 to M:\\n(a) Compute\\n(βm,γm) = arg min\\nβ,γN∑\\ni=1L(yi,fm−1(xi) +βb(xi;γ)).\\n(b) Set fm(x) =fm−1(x) +βmb(x;γm).\\n•For trees, γparameterizes the split variables and split points at the\\ninternal nodes, and the predictions at the terminal nodes.\\nTypically these models are ﬁt by minimizing a loss function averaged\\nover the training data, such as the squared-error or a likelihood-based loss\\nfunction,\\nmin\\n{βm,γm}M\\n1N∑\\ni=1L(\\nyi,M∑\\nm=1βmb(xi;γm))\\n. (10.4)\\nFor many loss functions L(y,f(x)) and/or basis functions b(x;γ), this re-\\nquires computationally intensive numerical optimization techniques. How-\\never, a simple alternative often can be found when it is feasible to rapidly\\nsolve the subproblem of ﬁtting just a single basis function,\\nmin\\nβ,γN∑\\ni=1L(yi,βb(xi;γ)). (10.5)\\n10.3 Forward Stagewise Additive Modeling\\nForward stagewise modeling approximates the solution to (10.4) by sequen-\\ntially adding new basis functions to the expansion without adjusting the\\nparameters and coeﬃcients of those that have already been added. This is\\noutlined in Algorithm 10.2. At each iteration m, one solves for the optimal\\nbasis function b(x;γm) and corresponding coeﬃcient βmto add to the cur-\\nrent expansion fm−1(x). This produces fm(x), and the process is repeated.\\nPreviously added terms are not modiﬁed.\\nFor squared-error loss\\nL(y,f(x)) = ( y−f(x))2, (10.6)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 361}, page_content='10.4 Exponential Loss and AdaBoost 343\\none has\\nL(yi,fm−1(xi) +βb(xi;γ)) = ( yi−fm−1(xi)−βb(xi;γ))2\\n= (rim−βb(xi;γ))2, (10.7)\\nwhere rim=yi−fm−1(xi) is simply the residual of the current model\\non the ith observation. Thus, for squared-error loss, the term βmb(x;γm)\\nthat best ﬁts the current residuals is added to the expansion at each step.\\nThis idea is the basis for “least squares” regression boosting discussed in\\nSection 10.10.2. However, as we show near the end of the next section,\\nsquared-error loss is generally not a good choice for classiﬁcation; hence\\nthe need to consider other loss criteria.\\n10.4 Exponential Loss and AdaBoost\\nWe now show that AdaBoost.M1 (Algorithm 10.1) is equivalent to forwar d\\nstagewise additive modeling (Algorithm 10.2) using the loss function\\nL(y,f(x)) = exp( −y f(x)). (10.8)\\nThe appropriateness of this criterion is addressed in the next section.\\nFor AdaBoost the basis functions are the individual classiﬁers Gm(x)∈\\n{−1,1}. Using the exponential loss function, one must solve\\n(βm,Gm) = arg min\\nβ,GN∑\\ni=1exp[−yi(fm−1(xi) +β G(xi))]\\nfor the classiﬁer Gmand corresponding coeﬃcient βmto be added at each\\nstep. This can be expressed as\\n(βm,Gm) = arg min\\nβ,GN∑\\ni=1w(m)\\niexp(−β yiG(xi)) (10.9)\\nwithw(m)\\ni= exp( −yifm−1(xi)). Since each w(m)\\nidepends neither on β\\nnorG(x), it can be regarded as a weight that is applied to each observa-\\ntion. This weight depends on fm−1(xi), and so the individual weight values\\nchange with each iteration m.\\nThe solution to (10.9) can be obtained in two steps. First, for any value\\nofβ >0, the solution to (10.9) for Gm(x) is\\nGm= arg min\\nGN∑\\ni=1w(m)\\niI(yi̸=G(xi)), (10.10)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 362}, page_content='344 10. Boosting and Additive Trees\\nwhich is the classiﬁer that minimizes the weighted error rate in predicting\\ny. This can be easily seen by expressing the criterion in (10.9) as\\ne−β≤∑\\nyi=G(xi)w(m)\\ni+eβ≤∑\\nyi̸=G(xi)w(m)\\ni,\\nwhich in turn can be written as\\n(\\neβ−e−β)\\n≤N∑\\ni=1w(m)\\niI(yi̸=G(xi)) +e−β≤N∑\\ni=1w(m)\\ni. (10.11)\\nPlugging this Gminto (10.9) and solving for βone obtains\\nβm=1\\n2log1−errm\\nerrm, (10.12)\\nwhere err mis the minimized weighted error rate\\nerrm=∑N\\ni=1w(m)\\niI(yi̸=Gm(xi))\\n∑N\\ni=1w(m)\\ni. (10.13)\\nThe approximation is then updated\\nfm(x) =fm−1(x) +βmGm(x),\\nwhich causes the weights for the next iteration to be\\nw(m+1)\\ni =w(m)\\ni≤e−βmyiGm(xi). (10.14)\\nUsing the fact that −yiGm(xi) = 2≤I(yi̸=Gm(xi))−1, (10.14) becomes\\nw(m+1)\\ni =w(m)\\ni≤eαmI(yi̸=Gm(xi))≤e−βm, (10.15)\\nwhere αm= 2βmis the quantity deﬁned at line 2c of AdaBoost.M1 (Al-\\ngorithm 10.1). The factor e−βmin (10.15) multiplies all weights by the\\nsame value, so it has no eﬀect. Thus (10.15) is equivalent to line 2(d) of\\nAlgorithm 10.1.\\nOne can view line 2(a) of the Adaboost.M1 algorithm as a method for\\napproximately solving the minimization in (10.11) and hence (10.10). Hence\\nwe conclude that AdaBoost.M1 minimizes the exponential loss criterion\\n(10.8) via a forward-stagewise additive modeling approach.\\nFigure 10.3 shows the training-set misclassiﬁcation error rate and aver-\\nage exponential loss for the simulated data problem (10.2) of Figure 10.2 .\\nThe training-set misclassiﬁcation error decreases to zero at around 250 it-\\nerations (and remains there), but the exponential loss keeps decreasing.\\nNotice also in Figure 10.2 that the test-set misclassiﬁcation error conti nues\\nto improve after iteration 250. Clearly Adaboost is not optimizing tra ining-\\nset misclassiﬁcation error; the exponential loss is more sensitive to cha nges\\nin the estimated class probabilities.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 363}, page_content='10.5 Why Exponential Loss? 345\\n0 100 200 300 4000.0 0.2 0.4 0.6 0.8 1.0\\nBoosting IterationsTraining Error\\nMisclassification RateExponential Loss\\nFIGURE 10.3. Simulated data, boosting with stumps: misclassiﬁcation error\\nrate on the training set, and average exponential loss: (1/N)PN\\ni=1exp(−yif(xi)).\\nAfter about 250iterations, the misclassiﬁcation error is zero, while the expo nential\\nloss continues to decrease.\\n10.5 Why Exponential Loss?\\nThe AdaBoost.M1 algorithm was originally motivated from a very diﬀer -\\nent perspective than presented in the previous section. Its equivalence to\\nforward stagewise additive modeling based on exponential loss was only\\ndiscovered ﬁve years after its inception. By studying the properties of the\\nexponential loss criterion, one can gain insight into the procedure and dis-\\ncover ways it might be improved.\\nThe principal attraction of exponential loss in the context of additive\\nmodeling is computational; it leads to the simple modular reweighting Ad-\\naBoost algorithm. However, it is of interest to inquire about its stat istical\\nproperties. What does it estimate and how well is it being estimated? The\\nﬁrst question is answered by seeking its population minimizer.\\nIt is easy to show (Friedman et al., 2000) that\\nf∗(x) = arg min\\nf(x)EY|x(e−Y f(x)) =1\\n2logPr(Y= 1|x)\\nPr(Y=−1|x), (10.16)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 364}, page_content='346 10. Boosting and Additive Trees\\nor equivalently\\nPr(Y= 1|x) =1\\n1 +e−2f∗(x).\\nThus, the additive expansion produced by AdaBoost is estimating one-\\nhalf the log-odds of P(Y= 1|x). This justiﬁes using its sign as the classiﬁ-\\ncation rule in (10.1).\\nAnother loss criterion with the same population minimizer is the bi-\\nnomial negative log-likelihood or deviance (also known as cross-entropy),\\ninterpreting fas the logit transform. Let\\np(x) = Pr( Y= 1|x) =ef(x)\\ne−f(x)+ef(x)=1\\n1 +e−2f(x)(10.17)\\nand deﬁne Y′= (Y+ 1)/2∈ {0,1}. Then the binomial log-likelihood loss\\nfunction is\\nl(Y,p(x)) =Y′logp(x) + (1 −Y′)log(1 −p(x)),\\nor equivalently the deviance is\\n−l(Y,f(x)) = log(\\n1 +e−2Y f(x))\\n. (10.18)\\nSince the population maximizer of log-likelihood is at the true probabilities\\np(x) = Pr( Y= 1|x), we see from (10.17) that the population minimizers of\\nthe deviance E Y|x[−l(Y,f(x))] and E Y|x[e−Y f(x)] are the same. Thus, using\\neither criterion leads to the same solution at the population level. Note that\\ne−Y fitself is not a proper log-likelihood, since it is not the logarithm of\\nany probability mass function for a binary random variable Y∈ {−1,1}.\\n10.6 Loss Functions and Robustness\\nIn this section we examine the diﬀerent loss functions for classiﬁcation and\\nregression more closely, and characterize them in terms of their robustness\\nto extreme data.\\nRobust Loss Functions for Classiﬁcation\\nAlthough both the exponential (10.8) and binomial deviance (10.18) yield\\nthe same solution when applied to the population joint distribution, the\\nsame is not true for ﬁnite data sets. Both criteria are monotone decreasing\\nfunctions of the “margin” yf(x). In classiﬁcation (with a −1/1 response)\\nthe margin plays a role analogous to the residuals y−f(x) in regression. The\\nclassiﬁcation rule G(x) = sign[ f(x)] implies that observations with positive\\nmargin yif(xi)>0 are classiﬁed correctly whereas those with negative\\nmargin yif(xi)<0 are misclassiﬁed. The decision boundary is deﬁned by'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 365}, page_content='10.6 Loss Functions and Robustness 347\\n−2 −1 0 1 20.0 0.5 1.0 1.5 2.0 2.5 3.0Misclassification\\nExponential\\nBinomial Deviance\\nSquared Error\\nSupport VectorLoss\\ny≤f\\nFIGURE 10.4. Loss functions for two-class classiﬁcation. The response is\\ny=±1; the prediction is f, with class prediction sign(f). The losses are\\nmisclassiﬁcation: I(sign( f)̸=y); exponential: exp(−yf); binomial deviance:\\nlog(1 + exp( −2yf)); squared error: (y−f)2; and support vector: (1−yf)+(see\\nSection 12.3). Each function has been scaled so that it passes t hrough the point\\n(0,1).\\nf(x) = 0. The goal of the classiﬁcation algorithm is to produce positive\\nmargins as frequently as possible. Any loss criterion used for classiﬁcati on\\nshould penalize negative margins more heavily than positive ones since\\npositive margin observations are already correctly classiﬁed.\\nFigure 10.4 shows both the exponential (10.8) and binomial deviance\\ncriteria as a function of the margin y≤f(x). Also shown is misclassiﬁcation\\nlossL(y,f(x)) =I(y≤f(x)<0), which gives unit penalty for negative mar-\\ngin values, and no penalty at all for positive ones. Both the exponential\\nand deviance loss can be viewed as monotone continuous approximations\\nto misclassiﬁcation loss. They continuously penalize increasingly negative\\nmargin values more heavily than they reward increasingly positive ones.\\nThe diﬀerence between them is in degree. The penalty associated with bi-\\nnomial deviance increases linearly for large increasingly negative margin,\\nwhereas the exponential criterion increases the inﬂuence of such observa-\\ntions exponentially.\\nAt any point in the training process the exponential criterion concen-\\ntrates much more inﬂuence on observations with large negative margins.\\nBinomial deviance concentrates relatively less inﬂuence on such observa-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 366}, page_content='348 10. Boosting and Additive Trees\\ntions, more evenly spreading the inﬂuence among all of the data. It is\\ntherefore far more robust in noisy settings where the Bayes error rate is\\nnot close to zero, and especially in situations where there is misspeciﬁcation\\nof the class labels in the training data. The performance of AdaBoost has\\nbeen empirically observed to dramatically degrade in such situations.\\nAlso shown in the ﬁgure is squared-error loss. The minimizer of the cor-\\nresponding risk on the population is\\nf∗(x) = arg min\\nf(x)EY|x(Y−f(x))2= E(Y|x) = 2≤Pr(Y= 1|x)−1.(10.19)\\nAs before the classiﬁcation rule is G(x) = sign[ f(x)]. Squared-error loss\\nis not a good surrogate for misclassiﬁcation error. As seen in Figure 10 .4, it\\nis not a monotone decreasing function of increasing margin yf(x). For mar-\\ngin values yif(xi)>1 it increases quadratically, thereby placing increasing\\ninﬂuence (error) on observations that are correctly classiﬁed with increas-\\ning certainty, thereby reducing the relative inﬂuence of those incorrectly\\nclassiﬁed yif(xi)<0. Thus, if class assignment is the goal, a monotone de-\\ncreasing criterion serves as a better surrogate loss function. Figure 12.4 on\\npage 426 in Chapter 12 includes a modiﬁcation of quadratic loss, the “Hu-\\nberized” square hinge loss (Rosset et al., 2004b), which enjoys the favorable\\nproperties of the binomial deviance, quadratic loss and the SVM hinge loss.\\nIt has the same population minimizer as the quadratic (10.19), is zero for\\ny≤f(x)>1, and becomes linear for y≤f(x)<−1. Since quadratic functions\\nare easier to compute with than exponentials, our experience suggests this\\nto be a useful alternative to the binomial deviance.\\nWith K-class classiﬁcation, the response Ytakes values in the unordered\\nsetG={G1,... ,Gk}(see Sections 2.4 and 4.4). We now seek a classiﬁer\\nG(x) taking values in G. It is suﬃcient to know the class conditional proba-\\nbilities pk(x) = Pr( Y=Gk|x),k= 1,2,... ,K , for then the Bayes classiﬁer\\nis\\nG(x) =Gkwhere k= arg max\\nℓpℓ(x). (10.20)\\nIn principal, though, we need not learn the pk(x), but simply which one is\\nlargest. However, in data mining applications the interest is often more in\\nthe class probabilities pℓ(x), ℓ= 1,... ,K themselves, rather than in per-\\nforming a class assignment. As in Section 4.4, the logistic model generalizes\\nnaturally to Kclasses,\\npk(x) =efk(x)\\n∑K\\nl=1efl(x), (10.21)\\nwhich ensures that 0 ≤pk(x)≤1 and that they sum to one. Note that\\nhere we have Kdiﬀerent functions, one per class. There is a redundancy\\nin the functions fk(x), since adding an arbitrary h(x) to each leaves the\\nmodel unchanged. Traditionally one of them is set to zero: for example,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 367}, page_content='10.6 Loss Functions and Robustness 349\\nfK(x) = 0, as in (4.17). Here we prefer to retain the symmetry, and impose\\nthe constraint∑K\\nk=1fk(x) = 0. The binomial deviance extends naturally\\nto the K-class multinomial deviance loss function:\\nL(y,p(x)) = −K∑\\nk=1I(y=Gk)logpk(x)\\n=−K∑\\nk=1I(y=Gk)fk(x) + log(K∑\\nℓ=1efℓ(x))\\n.(10.22)\\nAs in the two-class case, the criterion (10.22) penalizes incorrect predictions\\nonly linearly in their degree of incorrectness.\\nZhu et al. (2005) generalize the exponential loss for K-class problems.\\nSee Exercise 10.5 for details.\\nRobust Loss Functions for Regression\\nIn the regression setting, analogous to the relationship between exponential\\nloss and binomial log-likelihood is the relationship between squared-error\\nlossL(y,f(x)) = ( y−f(x))2and absolute loss L(y,f(x)) =|y−f(x)|. The\\npopulation solutions are f(x) = E( Y|x) for squared-error loss, and f(x) =\\nmedian( Y|x) for absolute loss; for symmetric error distributions these are\\nthe same. However, on ﬁnite samples squared-error loss places much more\\nemphasis on observations with large absolute residuals |yi−f(xi)|during\\nthe ﬁtting process. It is thus far less robust, and its performance severely\\ndegrades for long-tailed error distributions and especially for grossly mis-\\nmeasured y-values (“outliers”). Other more robust criteria, such as abso-\\nlute loss, perform much better in these situations. In the statistical ro -\\nbustness literature, a variety of regression loss criteria have been proposed\\nthat provide strong resistance (if not absolute immunity) to gross outliers\\nwhile being nearly as eﬃcient as least squares for Gaussian errors. They\\nare often better than either for error distributions with moderately heavy\\ntails. One such criterion is the Huber loss criterion used for M-regression\\n(Huber, 1964)\\nL(y,f(x)) ={\\n[y−f(x)]2for|y−f(x)| ≤δ,\\n2δ|y−f(x)| −δ2otherwise.(10.23)\\nFigure 10.5 compares these three loss functions.\\nThese considerations suggest than when robustness is a concern, as is\\nespecially the case in data mining applications (see Section 10.7), squared-\\nerror loss for regression and exponential loss for classiﬁcation are not the\\nbest criteria from a statistical perspective. However, they both lead to the\\nelegant modular boosting algorithms in the context of forward stagewis e\\nadditive modeling. For squared-error loss one simply ﬁts the base learner\\nto the residuals from the current model yi−fm−1(xi) at each step. For'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 368}, page_content='350 10. Boosting and Additive Trees\\n−3 −2 −1 0 1 2 30 2 4 6 8Squared Error\\nAbsolute Error\\nHuberLoss\\ny−f\\nFIGURE 10.5. A comparison of three loss functions for regression, plotted as a\\nfunction of the margin y−f. The Huber loss function combines the good properties\\nof squared-error loss near zero and absolute error loss when |y−f|is large.\\nexponential loss one performs a weighted ﬁt of the base learner to the\\noutput values yi, with weights wi= exp( −yifm−1(xi)). Using other more\\nrobust criteria directly in their place does not give rise to such simple\\nfeasible boosting algorithms. However, in Section 10.10.2 we show how one\\ncan derive simple elegant boosting algorithms based on any diﬀerentiable\\nloss criterion, thereby producing highly robust boosting procedures for data\\nmining.\\n10.7 “Oﬀ-the-Shelf” Procedures for Data Mining\\nPredictive learning is an important aspect of data mining. As can be seen\\nfrom this book, a wide variety of methods have been developed for predic-\\ntive learning from data. For each particular method there are situations\\nfor which it is particularly well suited, and others where it performs badly\\ncompared to the best that can be done with that data. We have attempted\\nto characterize appropriate situations in our discussions of each of the re-\\nspective methods. However, it is seldom known in advance which procedure\\nwill perform best or even well for any given problem. Table 10.1 summarizes\\nsome of the characteristics of a number of learning methods.\\nIndustrial and commercial data mining applications tend to be especially\\nchallenging in terms of the requirements placed on learning procedures.\\nData sets are often very large in terms of number of observations and\\nnumber of variables measured on each of them. Thus, computational con-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 369}, page_content='10.7 “Oﬀ-the-Shelf” Procedures for Data Mining 351\\nTABLE 10.1. Some characteristics of diﬀerent learning methods. Key: ▲= good,\\n◆=fair, and ▼=poor.\\nCharacteristic Neural SVM Trees MARS k-NN,\\nNets Kernels\\nNatural handling of data\\nof “mixed” type▼ ▼ ▲ ▲ ▼\\nHandling of missing values ▼ ▼ ▲ ▲ ▲\\nRobustness to outliers in\\ninput space▼ ▼ ▲▼ ▲\\nInsensitive to monotone\\ntransformations of inputs▼ ▼ ▲▼ ▼\\nComputational scalability\\n(large N)▼ ▼ ▲ ▲ ▼\\nAbility to deal with irrel-\\nevant inputs▼ ▼ ▲ ▲ ▼\\nAbility to extract linear\\ncombinations of features▲ ▲ ▼ ▼ ◆\\nInterpretability ▼ ▼ ◆▲ ▼\\nPredictive power ▲ ▲ ▼◆ ▲\\nsiderations play an important role. Also, the data are usually messy : the\\ninputs tend to be mixtures of quantitative, binary, and categorical vari-\\nables, the latter often with many levels. There are generally many missing\\nvalues, complete observations being rare. Distributions of numeric predic-\\ntor and response variables are often long-tailed and highly skewed. This\\nis the case for the spam data (Section 9.1.2); when ﬁtting a generalized\\nadditive model, we ﬁrst log-transformed each of the predictors in order to\\nget a reasonable ﬁt. In addition they usually contain a substantial fraction\\nof gross mis-measurements (outliers). The predictor variables are generally\\nmeasured on very diﬀerent scales.\\nIn data mining applications, usually only a small fraction of the large\\nnumber of predictor variables that have been included in the analysis are\\nactually relevant to prediction. Also, unlike many applications such as pat-\\ntern recognition, there is seldom reliable domain knowledge to help create\\nespecially relevant features and/or ﬁlter out the irrelevant ones, the inclu-\\nsion of which dramatically degrades the performance of many methods.\\nIn addition, data mining applications generally require interpretable mod-\\nels. It is not enough to simply produce predictions. It is also desirable to\\nhave information providing qualitative understanding of the relationship'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 370}, page_content='352 10. Boosting and Additive Trees\\nbetween joint values of the input variables and the resulting predicted re-\\nsponse value. Thus, black box methods such as neural networks, which can\\nbe quite useful in purely predictive settings such as pattern recognition,\\nare far less useful for data mining.\\nThese requirements of speed, interpretability and the messy nature of\\nthe data sharply limit the usefulness of most learning procedures as oﬀ-\\nthe-shelf methods for data mining. An “oﬀ-the-shelf” method is one that\\ncan be directly applied to the data without requiring a great deal of time-\\nconsuming data preprocessing or careful tuning of the learning procedure.\\nOf all the well-known learning methods, decision trees come closest to\\nmeeting the requirements for serving as an oﬀ-the-shelf procedure for data\\nmining. They are relatively fast to construct and they produce interpretable\\nmodels (if the trees are small). As discussed in Section 9.2, they naturally\\nincorporate mixtures of numeric and categorical predictor variables and\\nmissing values. They are invariant under (strictly monotone) transforma-\\ntions of the individual predictors. As a result, scaling and/or more general\\ntransformations are not an issue, and they are immune to the eﬀects of pre-\\ndictor outliers. They perform internal feature selection as an integral part\\nof the procedure. They are thereby resistant, if not completely immune,\\nto the inclusion of many irrelevant predictor variables. These properties of\\ndecision trees are largely the reason that they have emerged as the most\\npopular learning method for data mining.\\nTrees have one aspect that prevents them from being the ideal tool for\\npredictive learning, namely inaccuracy. They seldom provide predictive ac-\\ncuracy comparable to the best that can be achieved with the data at hand.\\nAs seen in Section 10.1, boosting decision trees improves their accuracy,\\noften dramatically. At the same time it maintains most of their desirabl e\\nproperties for data mining. Some advantages of trees that are sacriﬁced by\\nboosting are speed, interpretability, and, for AdaBoost, robustness against\\noverlapping class distributions and especially mislabeling of the training\\ndata. A gradient boosted model (GBM) is a generalization of tree boosting\\nthat attempts to mitigate these problems, so as to produce an accurate and\\neﬀective oﬀ-the-shelf procedure for data mining.\\n10.8 Example: Spam Data\\nBefore we go into the details of gradient boosting, we demonstrate its abi li-\\nties on a two-class classiﬁcation problem. The spam data are introduced in\\nChapter 1, and used as an example for many of the procedures in Chapter 9\\n(Sections 9.1.2, 9.2.5, 9.3.1 and 9.4.1).\\nApplying gradient boosting to these data resulted in a test error rate of\\n4.5%, using the same test set as was used in Section 9.1.2. By comparison,\\nan additive logistic regression achieved 5.5%, a CART tree fully grown and'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 371}, page_content='10.9 Boosting Trees 353\\npruned by cross-validation 8.7%, and MARS 5.5%. The standard error of\\nthese estimates is around 0.6%, although gradient boosting is signiﬁcantly\\nbetter than all of them using the McNemar test (Exercise 10.6).\\nIn Section 10.13 below we develop a relative importance measure for\\neach predictor, as well as a partial dependence plot describing a predictor’s\\ncontribution to the ﬁtted model. We now illustrate these for the spam data.\\nFigure 10.6 displays the relative importance spectrum for all 57 predictor\\nvariables. Clearly some predictors are more important than others in sep-\\naratingspamfromemail. The frequencies of the character strings !,$,hp,\\nandremove are estimated to be the four most relevant predictor variables.\\nAt the other end of the spectrum, the character strings 857,415,table, and\\n3dhave virtually no relevance.\\nThe quantity being modeled here is the log-odds of spamversusemail\\nf(x) = logPr(spam|x)\\nPr(email|x)(10.24)\\n(see Section 10.13 below). Figure 10.7 shows the partial dependence of the\\nlog-odds on selected important predictors, two positively associated with\\nspam(!andremove ), and two negatively associated ( eduandhp). These\\nparticular dependencies are seen to be essentially monotonic. There is a\\ngeneral agreement with the corresponding functions found by the additive\\nlogistic regression model; see Figure 9.1 on page 303.\\nRunning a gradient boosted model on these data with J= 2 terminal-\\nnode trees produces a purely additive (main eﬀects) model for the log-\\nodds, with a corresponding error rate of 4.7%, as compared to 4.5% for the\\nfull gradient boosted model (with J= 5 terminal-node trees). Although\\nnot signiﬁcant, this slightly higher error rate suggests that there may be\\ninteractions among some of the important predictor variables. This can\\nbe diagnosed through two-variable partial dependence plots. Figure 10.8\\nshows one of the several such plots displaying strong interaction eﬀects.\\nOne sees that for very low frequencies of hp, the log-odds of spamare\\ngreatly increased. For high frequencies of hp, the log-odds of spamtend to\\nbe much lower and roughly constant as a function of !. As the frequency\\nofhpdecreases, the functional relationship with !strengthens.\\n10.9 Boosting Trees\\nRegression and classiﬁcation trees are discussed in detail in Section 9.2.\\nThey partition the space of all joint predictor variable values into disjoin t\\nregions Rj,j= 1,2,... ,J , as represented by the terminal nodes of the tree.\\nA constant γjis assigned to each such region and the predictive rule is\\nx∈Rj⇒f(x) =γj.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 372}, page_content='354 10. Boosting and Additive Trees\\n!$hpremovefreeCAPAVEyourCAPMAXgeorgeCAPTOTeduyouourmoneywill1999businessre(receiveinternet000emailmeeting;650overmailpmpeopletechnologyhplallorderaddressmakefontprojectdataoriginalreportconferencelab[creditparts#85tablecsdirect415857telnetlabsaddresses3d\\n0 20 40 60 80 100\\nRelative Importance\\nFIGURE 10.6. Predictor variable importance spectrum for the spamdata. The\\nvariable names are written on the vertical axis.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 373}, page_content='10.9 Boosting Trees 355\\n!Partial Dependence\\n0.0 0.2 0.4 0.6 0.8 1.0-0.2 0.0 0.2 0.4 0.6 0.8 1.0\\nremovePartial Dependence\\n0.0 0.2 0.4 0.6-0.2 0.0 0.2 0.4 0.6 0.8 1.0\\neduPartial Dependence\\n0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.6 -0.2 0.0 0.2\\nhpPartial Dependence\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0-1.0 -0.6 -0.2 0.0 0.2\\nFIGURE 10.7. Partial dependence of log-odds of spamon four important pre-\\ndictors. The red ticks at the base of the plots are deciles of t he input variable.\\n0.51.01.52.02.53.00.20.40.60.81.0-1.0-0.5 0.0 0.5 1.0\\nhp!\\nFIGURE 10.8. Partial dependence of the log-odds of spamvs.email as a func-\\ntion of joint frequencies of hpand the character !.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 374}, page_content='356 10. Boosting and Additive Trees\\nThus a tree can be formally expressed as\\nT(x;Θ) =J∑\\nj=1γjI(x∈Rj), (10.25)\\nwith parameters Θ = {Rj,γj}J\\n1.Jis usually treated as a meta-parameter.\\nThe parameters are found by minimizing the empirical risk\\nˆΘ = arg min\\nΘJ∑\\nj=1∑\\nxi∈RjL(yi,γj). (10.26)\\nThis is a formidable combinatorial optimization problem, and we usually\\nsettle for approximate suboptimal solutions. It is useful to divide the opti -\\nmization problem into two parts:\\nFinding γjgiven Rj:Given the Rj, estimating the γjis typically trivial,\\nand often ˆ γj= ¯yj, the mean of the yifalling in region Rj. For mis-\\nclassiﬁcation loss, ˆ γjis the modal class of the observations falling in\\nregion Rj.\\nFinding Rj:This is the diﬃcult part, for which approximate solutions are\\nfound. Note also that ﬁnding the Rjentails estimating the γjas well.\\nA typical strategy is to use a greedy, top-down recursive partitioning\\nalgorithm to ﬁnd the Rj. In addition, it is sometimes necessary to\\napproximate (10.26) by a smoother and more convenient criterion for\\noptimizing the Rj:\\n˜Θ = arg min\\nΘN∑\\ni=1˜L(yi,T(xi,Θ)). (10.27)\\nThen given the ˆRj=˜Rj, the γjcan be estimated more precisely\\nusing the original criterion.\\nIn Section 9.2 we described such a strategy for classiﬁcation trees. The Gini\\nindex replaced misclassiﬁcation loss in the growing of the tree (identifying\\ntheRj).\\nThe boosted tree model is a sum of such trees,\\nfM(x) =M∑\\nm=1T(x;Θm), (10.28)\\ninduced in a forward stagewise manner (Algorithm 10.2). At each step in\\nthe forward stagewise procedure one must solve\\nˆΘm= arg min\\nΘmN∑\\ni=1L(yi,fm−1(xi) +T(xi;Θm)) (10.29)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 375}, page_content='10.9 Boosting Trees 357\\nfor the region set and constants Θ m={Rjm,γjm}Jm\\n1of the next tree, given\\nthe current model fm−1(x).\\nGiven the regions Rjm, ﬁnding the optimal constants γjmin each region\\nis typically straightforward:\\nˆγjm= arg min\\nγjm∑\\nxi∈RjmL(yi,fm−1(xi) +γjm). (10.30)\\nFinding the regions is diﬃcult, and even more diﬃcult than for a single\\ntree. For a few special cases, the problem simpliﬁes.\\nFor squared-error loss, the solution to (10.29) is no harder than for a\\nsingle tree. It is simply the regression tree that best predicts the current\\nresiduals yi−fm−1(xi), and ˆ γjmis the mean of these residuals in each\\ncorresponding region.\\nFor two-class classiﬁcation and exponential loss, this stagewise approac h\\ngives rise to the AdaBoost method for boosting classiﬁcation trees (Algo -\\nrithm 10.1). In particular, if the trees T(x;Θm) are restricted to be scaled\\nclassiﬁcation trees, then we showed in Section 10.4 that the solution to\\n(10.29) is the tree that minimizes the weighted error rate∑N\\ni=1w(m)\\niI(yi̸=\\nT(xi;Θm)) with weights w(m)\\ni=e−yifm−1(xi). By a scaled classiﬁcation\\ntree, we mean βmT(x;Θm), with the restriction that γjm∈ {−1,1}).\\nWithout this restriction, (10.29) still simpliﬁes for exponential loss t o a\\nweighted exponential criterion for the new tree:\\nˆΘm= arg min\\nΘmN∑\\ni=1w(m)\\niexp[−yiT(xi;Θm)]. (10.31)\\nIt is straightforward to implement a greedy recursive-partitioning algori thm\\nusing this weighted exponential loss as a splitting criterion. Given the Rjm,\\none can show (Exercise 10.7) that the solution to (10.30) is the weighted\\nlog-odds in each corresponding region\\nˆγjm= log∑\\nxi∈Rjmw(m)\\niI(yi= 1)\\n∑\\nxi∈Rjmw(m)\\niI(yi=−1). (10.32)\\nThis requires a specialized tree-growing algorithm; in practice, we prefer\\nthe approximation presented below that uses a weighted least squares re-\\ngression tree.\\nUsing loss criteria such as the absolute error or the Huber loss (10.23) in\\nplace of squared-error loss for regression, and the deviance (10.22) in place\\nof exponential loss for classiﬁcation, will serve to robustify boosting trees.\\nUnfortunately, unlike their nonrobust counterparts, these robust criteria\\ndo not give rise to simple fast boosting algorithms.\\nFor more general loss criteria the solution to (10.30), given the Rjm,\\nis typically straightforward since it is a simple “location” estimat e. For'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 376}, page_content='358 10. Boosting and Additive Trees\\nabsolute loss it is just the median of the residuals in each respective region.\\nFor the other criteria fast iterative algorithms exist for solving (10 .30),\\nand usually their faster “single-step” approximations are adequate. The\\nproblem is tree induction. Simple fast algorithms do not exist for solving\\n(10.29) for these more general loss criteria, and approximations like (1 0.27)\\nbecome essential.\\n10.10 Numerical Optimization via Gradient\\nBoosting\\nFast approximate algorithms for solving (10.29) with any diﬀerenti able loss\\ncriterion can be derived by analogy to numerical optimization. The loss in\\nusing f(x) to predict yon the training data is\\nL(f) =N∑\\ni=1L(yi,f(xi)). (10.33)\\nThe goal is to minimize L(f) with respect to f, where here f(x) is con-\\nstrained to be a sum of trees (10.28). Ignoring this constraint, minimizing\\n(10.33) can be viewed as a numerical optimization\\nˆf= arg min\\nfL(f), (10.34)\\nwhere the “parameters” f∈IRNare the values of the approximating func-\\ntionf(xi) at each of the Ndata points xi:\\nf={f(x1),f(x2)),... ,f (xN)}.\\nNumerical optimization procedures solve (10.34) as a sum of component\\nvectors\\nfM=M∑\\nm=0hm,hm∈IRN,\\nwhere f0=h0is an initial guess, and each successive fmis induced based\\non the current parameter vector fm−1, which is the sum of the previously\\ninduced updates. Numerical optimization methods diﬀer in their prescrip-\\ntions for computing each increment vector hm(“step”).\\n10.10.1 Steepest Descent\\nSteepest descent chooses hm=−ρmgmwhere ρmis a scalar and gm∈IRN\\nis the gradient of L(f) evaluated at f=fm−1. The components of the\\ngradient gmare\\ngim=[∂L(yi,f(xi))\\n∂f(xi)]\\nf(xi)=fm−1(xi)(10.35)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 377}, page_content='10.10 Numerical Optimization via Gradient Boosting 359\\nThestep length ρmis the solution to\\nρm= arg min\\nρL(fm−1−ρgm). (10.36)\\nThe current solution is then updated\\nfm=fm−1−ρmgm\\nand the process repeated at the next iteration. Steepest descent can be\\nviewed as a very greedy strategy, since −gmis the local direction in IRN\\nfor which L(f) is most rapidly decreasing at f=fm−1.\\n10.10.2 Gradient Boosting\\nForward stagewise boosting (Algorithm 10.2) is also a very greedy st rategy.\\nAt each step the solution tree is the one that maximally reduces (10.29),\\ngiven the current model fm−1and its ﬁts fm−1(xi). Thus, the tree predic-\\ntionsT(xi;Θm) are analogous to the components of the negative gradient\\n(10.35). The principal diﬀerence between them is that the tree compo-\\nnentstm= (T(x1;Θm),... ,T (xN;Θm) are not independent. They are con-\\nstrained to be the predictions of a Jm-terminal node decision tree, whereas\\nthe negative gradient is the unconstrained maximal descent direction.\\nThe solution to (10.30) in the stagewise approach is analogous to the li ne\\nsearch (10.36) in steepest descent. The diﬀerence is that (10.30) performs\\na separate line search for those components of tmthat correspond to each\\nseparate terminal region {T(xi;Θm)}xi∈Rjm.\\nIf minimizing loss on the training data (10.33) were the only goal, steep-\\nest descent would be the preferred strategy. The gradient (10.35) is trivial\\nto calculate for any diﬀerentiable loss function L(y,f(x)), whereas solving\\n(10.29) is diﬃcult for the robust criteria discussed in Section 10.6. Unfor-\\ntunately the gradient (10.35) is deﬁned only at the training data points xi,\\nwhereas the ultimate goal is to generalize fM(x) to new data not repre-\\nsented in the training set.\\nA possible resolution to this dilemma is to induce a tree T(x;Θm) at the\\nmth iteration whose predictions tmare as close as possible to the negative\\ngradient. Using squared error to measure closeness, this leads us to\\n˜Θm= arg min\\nΘN∑\\ni=1(−gim−T(xi;Θ))2. (10.37)\\nThat is, one ﬁts the tree Tto the negative gradient values (10.35) by least\\nsquares. As noted in Section 10.9 fast algorithms exist for least squares\\ndecision tree induction. Although the solution regions ˜Rjmto (10.37) will\\nnot be identical to the regions Rjmthat solve (10.29), it is generally sim-\\nilar enough to serve the same purpose. In any case, the forward stagewise'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 378}, page_content='360 10. Boosting and Additive Trees\\nTABLE 10.2. Gradients for commonly used loss functions.\\nSetting Loss Function −∂L(yi,f(xi))/∂f(xi)\\nRegression1\\n2[yi−f(xi)]2yi−f(xi)\\nRegression |yi−f(xi)| sign[yi−f(xi)]\\nRegression Huber yi−f(xi) for|yi−f(xi)| ≤δm\\nδmsign[yi−f(xi)] for|yi−f(xi)|> δm\\nwhere δm=αth-quantile {|yi−f(xi)|}\\nClassiﬁcation Deviance kth component: I(yi=Gk)−pk(xi)\\nboosting procedure, and top-down decision tree induction, are themselves\\napproximation procedures. After constructing the tree (10.37), the corre-\\nsponding constants in each region are given by (10.30).\\nTable 10.2 summarizes the gradients for commonly used loss functions.\\nFor squared error loss, the negative gradient is just the ordinary residual\\n−gim=yi−fm−1(xi), so that (10.37) on its own is equivalent standard\\nleast squares boosting. With absolute error loss, the negative gradient i s\\nthesignof the residual, so at each iteration (10.37) ﬁts the tree to the\\nsign of the current residuals by least squares. For Huber M-regression, the\\nnegative gradient is a compromise between these two (see the table).\\nFor classiﬁcation the loss function is the multinomial deviance (10.22),\\nandKleast squares trees are constructed at each iteration. Each tree Tkm\\nis ﬁt to its respective negative gradient vector gkm,\\n−gikm=∂L(yi,f1m(xi),... ,f 1m(xi))\\n∂fkm(xi)\\n=I(yi=Gk)−pk(xi), (10.38)\\nwithpk(x) given by (10.21). Although Kseparate trees are built at each\\niteration, they are related through (10.21). For binary classiﬁcation ( K=\\n2), only one tree is needed (exercise 10.10).\\n10.10.3 Implementations of Gradient Boosting\\nAlgorithm 10.3 presents the generic gradient tree-boosting algorithm for\\nregression. Speciﬁc algorithms are obtained by inserting diﬀerent loss cri-\\nteriaL(y,f(x)). The ﬁrst line of the algorithm initializes to the optimal\\nconstant model, which is just a single terminal node tree. The components\\nof the negative gradient computed at line 2(a) are referred to as general-\\nized or pseudo residuals, r. Gradients for commonly used loss functions are\\nsummarized in Table 10.2.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 379}, page_content='10.11 Right-Sized Trees for Boosting 361\\nAlgorithm 10.3 Gradient Tree Boosting Algorithm.\\n1. Initialize f0(x) = arg min γ∑N\\ni=1L(yi,γ).\\n2. For m= 1 to M:\\n(a) For i= 1,2,... ,N compute\\nrim=−[∂L(yi,f(xi))\\n∂f(xi)]\\nf=fm−1.\\n(b) Fit a regression tree to the targets rimgiving terminal regions\\nRjm, j= 1,2,... ,J m.\\n(c) For j= 1,2,... ,J mcompute\\nγjm= arg min\\nγ∑\\nxi∈RjmL(yi,fm−1(xi) +γ).\\n(d) Update fm(x) =fm−1(x) +∑Jm\\nj=1γjmI(x∈Rjm).\\n3. Output ˆf(x) =fM(x).\\nThe algorithm for classiﬁcation is similar. Lines 2(a)–(d) are repeated\\nKtimes at each iteration m, once for each class using (10.38). The result\\nat line 3 is Kdiﬀerent (coupled) tree expansions fkM(x),k= 1,2,... ,K .\\nThese produce probabilities via (10.21) or do classiﬁcation as in (10.20).\\nDetails are given in Exercise 10.9. Two basic tuning parameters are the\\nnumber of iterations Mand the sizes of each of the constituent trees\\nJm, m= 1,2,... ,M .\\nThe original implementation of this algorithm was called MART for\\n“multiple additive regression trees,” and was referred to in the ﬁrst edi-\\ntion of this book. Many of the ﬁgures in this chapter were produced by\\nMART. Gradient boosting as described here is implemented in the R gbm\\npackage (Ridgeway, 1999, “Gradient Boosted Models”), and is freely avai l-\\nable. The gbmpackage is used in Section 10.14.2, and extensively in Chap-\\nters 16 and 15. Another R implementation of boosting is mboost (Hothorn\\nand B¨ uhlmann, 2006). A commercial implementation of gradient boost-\\ning/MART called TreeNet\\uf8e8is available from Salford Systems, Inc.\\n10.11 Right-Sized Trees for Boosting\\nHistorically, boosting was considered to be a technique for combining mod-\\nels, here trees. As such, the tree building algorithm was regarded as a'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 380}, page_content='362 10. Boosting and Additive Trees\\nprimitive that produced models to be combined by the boosting proce-\\ndure. In this scenario, the optimal size of each tree is estimated separately\\nin the usual manner when it is built (Section 9.2). A very large (oversized)\\ntree is ﬁrst induced, and then a bottom-up procedure is employed to prune\\nit to the estimated optimal number of terminal nodes. This approach as-\\nsumes implicitly that each tree is the last one in the expansion (10.28).\\nExcept perhaps for the very last tree, this is clearly a very poor assump-\\ntion. The result is that trees tend to be much too large, especially during\\nthe early iterations. This substantially degrades performance and increases\\ncomputation.\\nThe simplest strategy for avoiding this problem is to restrict all trees\\nto be the same size, Jm=J∀m. At each iteration a J-terminal node\\nregression tree is induced. Thus Jbecomes a meta-parameter of the entire\\nboosting procedure, to be adjusted to maximize estimated performance for\\nthe data at hand.\\nOne can get an idea of useful values for Jby considering the properties\\nof the “target” function\\nη= arg min\\nfEXYL(Y,f(X)). (10.39)\\nHere the expected value is over the population joint distribution of ( X,Y).\\nThe target function η(x) is the one with minimum prediction risk on future\\ndata. This is the function we are trying to approximate.\\nOne relevant property of η(X) is the degree to which the coordinate vari-\\nables XT= (X1,X2,... ,X p) interact with one another. This is captured\\nby its ANOVA (analysis of variance) expansion\\nη(X) =∑\\njηj(Xj)+∑\\njkηjk(Xj,Xk)+∑\\njklηjkl(Xj,Xk,Xl)+≤≤≤.(10.40)\\nThe ﬁrst sum in (10.40) is over functions of only a single predictor variable\\nXj. The particular functions ηj(Xj) are those that jointly best approximate\\nη(X) under the loss criterion being used. Each such ηj(Xj) is called the\\n“main eﬀect” of Xj. The second sum is over those two-variable functions\\nthat when added to the main eﬀects best ﬁt η(X). These are called the\\nsecond-order interactions of each respective variable pair ( Xj,Xk). The\\nthird sum represents third-order interactions, and so on. For many problems\\nencountered in practice, low-order interaction eﬀects tend to dominate.\\nWhen this is the case, models that produce strong higher-order interaction\\neﬀects, such as large decision trees, suﬀer in accuracy.\\nThe interaction level of tree-based approximations is limited by the tree\\nsizeJ. Namely, no interaction eﬀects of level greater that J−1 are pos-\\nsible. Since boosted models are additive in the trees (10.28), this limit\\nextends to them as well. Setting J= 2 (single split “decision stump”)\\nproduces boosted models with only main eﬀects; no interactions are per-\\nmitted. With J= 3, two-variable interaction eﬀects are also allowed, and'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 381}, page_content='10.11 Right-Sized Trees for Boosting 363\\nNumber of TermsTest Error\\n0 100 200 300 4000.0 0.1 0.2 0.3 0.4Stumps\\n10 Node\\n100 Node\\nAdaboost\\nFIGURE 10.9. Boosting with diﬀerent sized trees, applied to the example (10. 2)\\nused in Figure 10.2. Since the generative model is additive, stu mps perform the\\nbest. The boosting algorithm used the binomial deviance loss in Algorithm 10.3;\\nshown for comparison is the AdaBoost Algorithm 10.1.\\nso on. This suggests that the value chosen for Jshould reﬂect the level\\nof dominant interactions of η(x). This is of course generally unknown, but\\nin most situations it will tend to be low. Figure 10.9 illustrates the eﬀ ect\\nof interaction order (choice of J) on the simulation example (10.2). The\\ngenerative function is additive (sum of quadratic monomials), so boosting\\nmodels with J >2 incurs unnecessary variance and hence the higher test\\nerror. Figure 10.10 compares the coordinate functions found by boosted\\nstumps with the true functions.\\nAlthough in many applications J= 2 will be insuﬃcient, it is unlikely\\nthatJ >10 will be required. Experience so far indicates that 4 ≤J≤8\\nworks well in the context of boosting, with results being fairly insensiti ve\\nto particular choices in this range. One can ﬁne-tune the value for Jby\\ntrying several diﬀerent values and choosing the one that produces the low-\\nest risk on a validation sample. However, this seldom provides signiﬁcant\\nimprovement over using J≃6.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 382}, page_content='364 10. Boosting and Additive Trees\\nCoordinate Functions for Additive Logistic Trees\\nf1(x1) f2(x2) f3(x3) f4(x4) f5(x5)\\nf6(x6) f7(x7) f8(x8) f9(x9) f10(x10)\\nFIGURE 10.10. Coordinate functions estimated by boosting stumps for the sim-\\nulated example used in Figure 10.9. The true quadratic functio ns are shown for\\ncomparison.\\n10.12 Regularization\\nBesides the size of the constituent trees, J, the other meta-parameter of\\ngradient boosting is the number of boosting iterations M. Each iteration\\nusually reduces the training risk L(fM), so that for Mlarge enough this risk\\ncan be made arbitrarily small. However, ﬁtting the training data too well\\ncan lead to overﬁtting, which degrades the risk on future predictions. Thus,\\nthere is an optimal number M∗minimizing future risk that is application\\ndependent. A convenient way to estimate M∗is to monitor prediction risk\\nas a function of Mon a validation sample. The value of Mthat minimizes\\nthis risk is taken to be an estimate of M∗. This is analogous to the early\\nstopping strategy often used with neural networks (Section 11.4).\\n10.12.1 Shrinkage\\nControlling the value of Mis not the only possible regularization strategy.\\nAs with ridge regression and neural networks, shrinkage techniques can be\\nemployed as well (see Sections 3.4.1 and 11.5). The simplest implementation\\nof shrinkage in the context of boosting is to scale the contribution of each\\ntree by a factor 0 < ν < 1 when it is added to the current approximation.\\nThat is, line 2(d) of Algorithm 10.3 is replaced by\\nfm(x) =fm−1(x) +ν≤J∑\\nj=1γjmI(x∈Rjm). (10.41)\\nThe parameter νcan be regarded as controlling the learning rate of the\\nboosting procedure. Smaller values of ν(more shrinkage) result in larger\\ntraining risk for the same number of iterations M. Thus, both νandM\\ncontrol prediction risk on the training data. However, these parameters do'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 383}, page_content='10.12 Regularization 365\\nnot operate independently. Smaller values of νlead to larger values of M\\nfor the same training risk, so that there is a tradeoﬀ between them.\\nEmpirically it has been found (Friedman, 2001) that smaller values of ν\\nfavor better test error, and require correspondingly larger values of M. In\\nfact, the best strategy appears to be to set νto be very small ( ν <0.1)\\nand then choose Mby early stopping. This yields dramatic improvements\\n(over no shrinkage ν= 1) for regression and for probability estimation. The\\ncorresponding improvements in misclassiﬁcation risk via (10.20) are les s,\\nbut still substantial. The price paid for these improvements is computa-\\ntional: smaller values of νgive rise to larger values of M, and computation\\nis proportional to the latter. However, as seen below, many iterations ar e\\ngenerally computationally feasible even on very large data sets. This is\\npartly due to the fact that small trees are induced at each step with no\\npruning.\\nFigure 10.11 shows test error curves for the simulated example (10.2) of\\nFigure 10.2. A gradient boosted model (MART) was trained using binomial\\ndeviance, using either stumps or six terminal-node trees, and with or with-\\nout shrinkage. The beneﬁts of shrinkage are evident, especially when the\\nbinomial deviance is tracked. With shrinkage, each test error curve reaches\\na lower value, and stays there for many iterations.\\nSection 16.2.1 draws a connection between forward stagewise shrinkage\\nin boosting and the use of an L1penalty for regularizing model parame-\\nters (the “lasso”). We argue that L1penalties may be superior to the L2\\npenalties used by methods such as the support vector machine.\\n10.12.2 Subsampling\\nWe saw in Section 8.7 that bootstrap averaging (bagging) improves the\\nperformance of a noisy classiﬁer through averaging. Chapter 15 discusses\\nin some detail the variance-reduction mechanism of this sampling followed\\nby averaging. We can exploit the same device in gradient boosting, both\\nto improve performance and computational eﬃciency.\\nWithstochastic gradient boosting (Friedman, 1999), at each iteration we\\nsample a fraction ηof the training observations (without replacement),\\nand grow the next tree using that subsample. The rest of the algorithm is\\nidentical. A typical value for ηcan be1\\n2, although for large N,ηcan be\\nsubstantially smaller than1\\n2.\\nNot only does the sampling reduce the computing time by the same\\nfraction η, but in many cases it actually produces a more accurate model.\\nFigure 10.12 illustrates the eﬀect of subsampling using the simulated\\nexample (10.2), both as a classiﬁcation and as a regression example. We\\nsee in both cases that sampling along with shrinkage slightly outperform ed\\nthe rest. It appears here that subsampling without shrinkage does poorly.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 384}, page_content='366 10. Boosting and Additive Trees\\nBoosting IterationsTest Set Deviance\\n0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage\\nShrinkage=0.2Stumps\\nDeviance\\nBoosting IterationsTest Set Misclassification Error\\n0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage\\nShrinkage=0.2Stumps\\nMisclassification Error\\nBoosting IterationsTest Set Deviance\\n0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage\\nShrinkage=0.66-Node Trees\\nDeviance\\nBoosting IterationsTest Set Misclassification Error\\n0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage\\nShrinkage=0.66-Node Trees\\nMisclassification Error\\nFIGURE 10.11. Test error curves for simulated example (10.2) of Figure 10.9 ,\\nusing gradient boosting (MART). The models were trained using bino mial de-\\nviance, either stumps or six terminal-node trees, and with or wit hout shrinkage.\\nThe left panels report test deviance, while the right panels sho w misclassiﬁcation\\nerror. The beneﬁcial eﬀect of shrinkage can be seen in all cases, especially for\\ndeviance in the left panels.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 385}, page_content='10.13 Interpretation 367\\n0 200 400 600 800 10000.4 0.6 0.8 1.0 1.2 1.4\\nBoosting IterationsTest Set DevianceDeviance4−Node Trees\\n0 200 400 600 800 10000.30 0.35 0.40 0.45 0.50\\nBoosting IterationsTest Set Absolute ErrorNo shrinkage\\nShrink=0.1\\nSample=0.5\\nShrink=0.1 Sample=0.5Absolute Error\\nFIGURE 10.12. Test-error curves for the simulated example (10.2), showing\\nthe eﬀect of stochasticity. For the curves labeled “Sample = 0.5”, a diﬀerent 50%\\nsubsample of the training data was used each time a tree was grow n. In the left\\npanel the models were ﬁt by gbmusing a binomial deviance loss function; in the\\nright-hand panel using square-error loss.\\nThe downside is that we now have four parameters to set: J,M,νand\\nη. Typically some early explorations determine suitable values for J,νand\\nη, leaving Mas the primary parameter.\\n10.13 Interpretation\\nSingle decision trees are highly interpretable. The entire model can be com-\\npletely represented by a simple two-dimensional graphic (binary tree) that\\nis easily visualized. Linear combinations of trees (10.28) lose this import ant\\nfeature, and must therefore be interpreted in a diﬀerent way.\\n10.13.1 Relative Importance of Predictor Variables\\nIn data mining applications the input predictor variables are seldom equally\\nrelevant. Often only a few of them have substantial inﬂuence on the re-\\nsponse; the vast majority are irrelevant and could just as well have not\\nbeen included. It is often useful to learn the relative importance or contri-\\nbution of each input variable in predicting the response.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 386}, page_content='368 10. Boosting and Additive Trees\\nFor a single decision tree T, Breiman et al. (1984) proposed\\nI2\\nℓ(T) =J−1∑\\nt=1ˆı2\\ntI(v(t) =ℓ) (10.42)\\nas a measure of relevance for each predictor variable Xℓ. The sum is over\\ntheJ−1 internal nodes of the tree. At each such node t, one of the input\\nvariables Xv(t)is used to partition the region associated with that node into\\ntwo subregions; within each a separate constant is ﬁt to the response values.\\nThe particular variable chosen is the one that gives maximal estimated\\nimprovement ˆ ı2\\ntin squared error risk over that for a constant ﬁt over the\\nentire region. The squared relative importance of variable Xℓis the sum of\\nsuch squared improvements over all internal nodes for which it was chosen\\nas the splitting variable.\\nThis importance measure is easily generalized to additive tree expansions\\n(10.28); it is simply averaged over the trees\\nI2\\nℓ=1\\nMM∑\\nm=1I2\\nℓ(Tm). (10.43)\\nDue to the stabilizing eﬀect of averaging, this measure turns out to be more\\nreliable than is its counterpart (10.42) for a single tree. Also, because of\\nshrinkage (Section 10.12.1) the masking of important variables by other s\\nwith which they are highly correlated is much less of a problem. Note\\nthat (10.42) and (10.43) refer to squared relevance; the actual relevances\\nare their respective square roots. Since these measures are relative, it is\\ncustomary to assign the largest a value of 100 and then scale the others\\naccordingly. Figure 10.6 shows the relevant importance of the 57 inputs in\\npredicting spamversusemail.\\nForK-class classiﬁcation, Kseparate models fk(x),k= 1,2,... ,K are\\ninduced, each consisting of a sum of trees\\nfk(x) =M∑\\nm=1Tkm(x). (10.44)\\nIn this case (10.43) generalizes to\\nI2\\nℓk=1\\nMM∑\\nm=1I2\\nℓ(Tkm). (10.45)\\nHereIℓkis the relevance of Xℓin separating the class kobservations from\\nthe other classes. The overall relevance of Xℓis obtained by averaging over\\nall of the classes\\nI2\\nℓ=1\\nKK∑\\nk=1I2\\nℓk. (10.46)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 387}, page_content='10.13 Interpretation 369\\nFigures 10.23 and 10.24 illustrate the use of these averaged and separate\\nrelative importances.\\n10.13.2 Partial Dependence Plots\\nAfter the most relevant variables have been identiﬁed, the next step is to\\nattempt to understand the nature of the dependence of the approximation\\nf(X) on their joint values. Graphical renderings of the f(X) as a function\\nof its arguments provides a comprehensive summary of its dependence on\\nthe joint values of the input variables.\\nUnfortunately, such visualization is limited to low-dimensional views.\\nWe can easily display functions of one or two arguments, either continuous\\nor discrete (or mixed), in a variety of diﬀerent ways; this book is ﬁlled\\nwith such displays. Functions of slightly higher dimensions can be plotted\\nby conditioning on particular sets of values of all but one or two of the\\narguments, producing a trellis of plots (Becker et al., 1996).1\\nFor more than two or three variables, viewing functions of the corre-\\nsponding higher-dimensional arguments is more diﬃcult. A useful alterna-\\ntive can sometimes be to view a collection of plots, each one of which shows\\nthe partial dependence of the approximation f(X) on a selected small sub-\\nset of the input variables. Although such a collection can seldom provide a\\ncomprehensive depiction of the approximation, it can often produce helpful\\nclues, especially when f(x) is dominated by low-order interactions (10.40).\\nConsider the subvector XSofℓ < pof the input predictor variables XT=\\n(X1,X2,... ,X p), indexed by S ⊂ { 1,2,... ,p }. LetCbe the complement\\nset, with S ∪ C ={1,2,... ,p }. A general function f(X) will in principle\\ndepend on all of the input variables: f(X) =f(XS,XC). One way to deﬁne\\nthe average or partial dependence of f(X) onXSis\\nfS(XS) = E XCf(XS,XC). (10.47)\\nThis is a marginal average of f, and can serve as a useful description of the\\neﬀect of the chosen subset on f(X) when, for example, the variables in XS\\ndo not have strong interactions with those in XC.\\nPartial dependence functions can be used to interpret the results of any\\n“black box” learning method. They can be estimated by\\n¯fS(XS) =1\\nNN∑\\ni=1f(XS,xiC), (10.48)\\nwhere {x1C,x2C,... ,x NC}are the values of XCoccurring in the training\\ndata. This requires a pass over the data for each set of joint values of XSfor\\nwhich ¯fS(XS) is to be evaluated. This can be computationally intensive,\\n1lattice in R.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 388}, page_content='370 10. Boosting and Additive Trees\\neven for moderately sized data sets. Fortunately with decision trees, ¯fS(XS)\\n(10.48) can be rapidly computed from the tree itself without reference to\\nthe data (Exercise 10.11).\\nIt is important to note that partial dependence functions deﬁned in\\n(10.47) represent the eﬀect of XSonf(X) after accounting for the (av-\\nerage) eﬀects of the other variables XConf(X). They are notthe eﬀect\\nofXSonf(X)ignoring the eﬀects of XC. The latter is given by the con-\\nditional expectation\\n˜fS(XS) = E( f(XS,XC)|XS), (10.49)\\nand is the best least squares approximation to f(X) by a function of XS\\nalone. The quantities ˜fS(XS) and ¯fS(XS) will be the same only in the\\nunlikely event that XSandXCare independent. For example, if the eﬀect\\nof the chosen variable subset happens to be purely additive,\\nf(X) =h1(XS) +h2(XC). (10.50)\\nThen (10.47) produces the h1(XS) up to an additive constant. If the eﬀect\\nis purely multiplicative,\\nf(X) =h1(XS)≤h2(XC), (10.51)\\nthen (10.47) produces h1(XS) up to a multiplicative constant factor. On\\nthe other hand, (10.49) will not produce h1(XS) in either case. In fact,\\n(10.49) can produce strong eﬀects on variable subsets for which f(X) has\\nno dependence at all.\\nViewing plots of the partial dependence of the boosted-tree approxima-\\ntion (10.28) on selected variables subsets can help to provide a qualitative\\ndescription of its properties. Illustrations are shown in Sections 10.8 and\\n10.14. Owing to the limitations of computer graphics, and human percep-\\ntion, the size of the subsets XSmust be small ( l≈1,2,3). There are of\\ncourse a large number of such subsets, but only those chosen from among\\nthe usually much smaller set of highly relevant predictors are likely to be\\ninformative. Also, those subsets whose eﬀect on f(X) is approximately\\nadditive (10.50) or multiplicative (10.51) will be most revealing.\\nForK-class classiﬁcation, there are Kseparate models (10.44), one for\\neach class. Each one is related to the respective probabilities (10.21) thro ugh\\nfk(X) = log pk(X)−1\\nKK∑\\nl=1logpl(X). (10.52)\\nThus each fk(X) is a monotone increasing function of its respective prob-\\nability on a logarithmic scale. Partial dependence plots of each respective\\nfk(X) (10.44) on its most relevant predictors (10.45) can help reveal how\\nthe log-odds of realizing that class depend on the respective input variables.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 389}, page_content='10.14 Illustrations 371\\n10.14 Illustrations\\nIn this section we illustrate gradient boosting on a number of larger data sets,\\nusing diﬀerent loss functions as appropriate.\\n10.14.1 California Housing\\nThis data set (Pace and Barry, 1997) is available from the Carnegie-Mellon\\nStatLib repository2. It consists of aggregated data from each of 20,460\\nneighborhoods (1990 census block groups) in California. The response vari-\\nableYis the median house value in each neighborhood measured in units of\\n$100,000. The predictor variables are demographics such as median income\\nMedInc , housing density as reﬂected by the number of houses House, and the\\naverage occupancy in each house AveOccup . Also included as predictors are\\nthe location of each neighborhood ( longitude andlatitude ), and several\\nquantities reﬂecting the properties of the houses in the neighborhood: av-\\nerage number of rooms AveRooms and bedrooms AveBedrms . There are thus\\na total of eight predictors, all numeric.\\nWe ﬁt a gradient boosting model using the MART procedure, with J= 6\\nterminal nodes, a learning rate (10.41) of ν= 0.1, and the Huber loss\\ncriterion for predicting the numeric response. We randomly divided the\\ndataset into a training set (80%) and a test set (20%).\\nFigure 10.13 shows the average absolute error\\nAAE = E|y−ˆfM(x)| (10.53)\\nas a function for number of iterations Mon both the training data and test\\ndata. The test error is seen to decrease monotonically with increasing M,\\nmore rapidly during the early stages and then leveling oﬀ to being nearly\\nconstant as iterations increase. Thus, the choice of a particular value of M\\nis not critical, as long as it is not too small. This tends to be the case in\\nmany applications. The shrinkage strategy (10.41) tends to eliminate the\\nproblem of overﬁtting, especially for larger data sets.\\nThe value of AAE after 800 iterations is 0.31. This can be compared to\\nthat of the optimal constant predictor median {yi}which is 0.89. In terms of\\nmore familiar quantities, the squared multiple correlation coeﬃcient of t his\\nmodel is R2= 0.84. Pace and Barry (1997) use a sophisticated spatial auto-\\nregression procedure, where prediction for each neighborhood is based on\\nmedian house values in nearby neighborhoods, using the other predictors as\\ncovariates. Experimenting with transformations they achieved R2= 0.85,\\npredicting log Y. Using log Yas the response the corresponding value for\\ngradient boosting was R2= 0.86.\\n2http://lib.stat.cmu.edu.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 390}, page_content='372 10. Boosting and Additive Trees\\n0 200 400 600 8000.0 0.2 0.4 0.6 0.8\\nIterations MAbsolute ErrorTraining and Test Absolute Error\\nTrain Error\\nTest Error\\nFIGURE 10.13. Average-absolute error as a function of number of iterations\\nfor the California housing data.\\nFigure 10.14 displays the relative variable importances for each of the\\neight predictor variables. Not surprisingly, median income in the neigh-\\nborhood is the most relevant predictor. Longitude, latitude, and average\\noccupancy all have roughly half the relevance of income, whereas the others\\nare somewhat less inﬂuential.\\nFigure 10.15 shows single-variable partial dependence plots on the most\\nrelevant nonlocation predictors. Note that the plots are not strictly smoot h.\\nThis is a consequence of using tree-based models. Decision trees produce\\ndiscontinuous piecewise constant models (10.25). This carries over to sums\\nof trees (10.28), with of course many more pieces. Unlike most of the meth-\\nods discussed in this book, there is no smoothness constraint imposed on\\nthe result. Arbitrarily sharp discontinuities can be modeled. The fact that\\nthese curves generally exhibit a smooth trend is because that is what is\\nestimated to best predict the response for this problem. This is often the\\ncase.\\nThe hash marks at the base of each plot delineate the deciles of the\\ndata distribution of the corresponding variables. Note that here the data\\ndensity is lower near the edges, especially for larger values. This causes the\\ncurves to be somewhat less well determined in those regions. The vertical\\nscales of the plots are the same, and give a visual comparison of the relativ e\\nimportance of the diﬀerent variables.\\nThe partial dependence of median house value on median income is\\nmonotonic increasing, being nearly linear over the main body of data. House\\nvalue is generally monotonic decreasing with increasing average occupancy,\\nexcept perhaps for average occupancy rates less than one. Median house'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 391}, page_content='10.14 Illustrations 373\\nMedIncLongitudeAveOccupLatitudeHouseAgeAveRoomsAveBedrmsPopulation\\n0 20 40 60 80 100\\nRelative importance\\nFIGURE 10.14. Relative importance of the predictors for the California hous ing\\ndata.\\nvalue has a nonmonotonic partial dependence on average number of rooms.\\nIt has a minimum at approximately three rooms and is increasing both for\\nsmaller and larger values.\\nMedian house value is seen to have a very weak partial dependence on\\nhouse age that is inconsistent with its importance ranking (Figure 10.14) .\\nThis suggests that this weak main eﬀect may be masking stronger interac-\\ntion eﬀects with other variables. Figure 10.16 shows the two-variable part ial\\ndependence of housing value on joint values of median age and average oc-\\ncupancy. An interaction between these two variables is apparent. For values\\nof average occupancy greater than two, house value is nearly independent\\nof median age, whereas for values less than two there is a strong dependence\\non age.\\nFigure 10.17 shows the two-variable partial dependence of the ﬁtted\\nmodel on joint values of longitude and latitude, displayed as a shaded\\ncontour plot. There is clearly a very strong dependence of median house\\nvalue on the neighborhood location in California. Note that Figure 10. 17 is\\nnota plot of house value versus location ignoring the eﬀects of the other\\npredictors (10.49). Like all partial dependence plots, it represents the eﬀect\\nof location after accounting for the eﬀects of the other neighborhood and\\nhouse attributes (10.47). It can be viewed as representing an extra premium\\none pays for location. This premium is seen to be relatively large near the\\nPaciﬁc coast especially in the Bay Area and Los Angeles–San Diego re-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 392}, page_content='374 10. Boosting and Additive Trees\\nMedIncPartial Dependence\\n2 4 6 8 10-0.5 0.0 0.5 1.0 1.5 2.0\\nAveOccupPartial Dependence\\n2 3 4 5-1.0 -0.5 0.0 0.5 1.0 1.5\\nHouseAgePartial Dependence\\n10 20 30 40 50-1.0 -0.5 0.0 0.5 1.0\\nAveRoomsPartial Dependence\\n4 6 8 10-1.0 -0.5 0.0 0.5 1.0 1.5\\nFIGURE 10.15. Partial dependence of housing value on the nonlocation vari-\\nables for the California housing data. The red ticks at the base of the plot are\\ndeciles of the input variables.\\n2\\n3\\n4\\n510203040500.00.51.0\\nAveOccupHouseAge\\nFIGURE 10.16. Partial dependence of house value on median age and aver-\\nage occupancy. There appears to be a strong interaction eﬀect be tween these two\\nvariables.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 393}, page_content='10.14 Illustrations 375\\n−124 −122 −120 −118 −116 −11434 36 38 40 42\\nLongitudeLatitude\\n−1.0−0.5 0.0 0.5 1.0\\nFIGURE 10.17. Partial dependence of median house value on location in Cal-\\nifornia. One unit is $100,000, at1990prices, and the values plotted are relative\\nto the overall median of $180,000.\\ngions. In the northern, central valley, and southeastern desert regions of\\nCalifornia, location costs considerably less.\\n10.14.2 New Zealand Fish\\nPlant and animal ecologists use regression models to predict species pres-\\nence, abundance and richness as a function of environmental variables.\\nAlthough for many years simple linear and parametric models were popu-\\nlar, recent literature shows increasing interest in more sophisticated mod-\\nels such as generalized additive models (Section 9.1, GAM), multivariate\\nadaptive regression splines (Section 9.4, MARS) and boosted regression\\ntrees (Leathwick et al., 2005; Leathwick et al., 2006). Here we model the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 394}, page_content='376 10. Boosting and Additive Trees\\npresence and abundance of the Black Oreo Dory , a marine ﬁsh found in the\\noceanic waters around New Zealand.3\\nFigure 10.18 shows the locations of 17,000 trawls (deep-water net ﬁshing,\\nwith a maximum depth of 2km), and the red points indicate those 2353\\ntrawls for which the Black Oreo was present, one of over a hundred species\\nregularly recorded. The catch size in kg for each species was recorded for\\neach trawl. Along with the species catch, a number of environmental mea-\\nsurements are available for each trawl. These include the average depth of\\nthe trawl ( AvgDepth ), and the temperature and salinity of the water. Since\\nthe latter two are strongly correlated with depth, Leathwick et al. (2006)\\nderived instead TempResid andSalResid , the residuals obtained when these\\ntwo measures are adjusted for depth (via separate non-parametric regres-\\nsions).SSTGrad is a measure of the gradient of the sea surface temperature,\\nandChlais a broad indicator of ecosytem productivity via satellite-image\\nmeasurements. SusPartMatter provides a measure of suspended particulate\\nmatter, particularly in coastal waters, and is also satellite derived.\\nThe goal of this analysis is to estimate the probability of ﬁnding Black\\nOreo in a trawl, as well as the expected catch size, standardized to take\\ninto account the eﬀects of variation in trawl speed and distance, as well\\nas the mesh size of the trawl net. The authors used logistic regression\\nfor estimating the probability. For the catch size, it might seem natural\\nto assume a Poisson distribution and model the log of the mean count,\\nbut this is often not appropriate because of the excessive number of zeros.\\nAlthough specialized approaches have been developed, such as the zero-\\ninﬂated Poisson (Lambert, 1992), they chose a simpler approach. If Yis\\nthe (non-negative) catch size,\\nE(Y|X) = E( Y|Y >0,X)≤Pr(Y >0|X). (10.54)\\nThe second term is estimated by the logistic regression, and the ﬁrst term\\ncan be estimated using only the 2353 trawls with a positive catch.\\nFor the logistic regression the authors used a gradient boosted model\\n(GBM)4with binomial deviance loss function, depth-10 trees, and a shrink-\\nage factor ν= 0.025. For the positive-catch regression, they modeled\\nlog(Y) using a GBM with squared-error loss (also depth-10 trees, but\\nν= 0.01), and un-logged the predictions. In both cases they used 10-fold\\ncross-validation for selecting the number of terms, as well as the shrinkage\\nfactor.\\n3The models, data, and maps shown here were kindly provided by Dr John Leathwick\\nof the National Institute of Water and Atmospheric Research in New Zealand, and Dr\\nJane Elith, School of Botany, University of Melbourne. The co llection of the research\\ntrawl data took place from 1979–2005, and was funded by the Ne w Zealand Ministry of\\nFisheries.\\n4Version 1.5-7 of package gbmin R, ver. 2.2.0.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 395}, page_content='10.14 Illustrations 377\\nFIGURE 10.18. Map of New Zealand and its surrounding exclusive economic\\nzone, showing the locations of 17,000 trawls (small blue dots) t aken between 1979\\nand 2005. The red points indicate trawls for which the species Black Oreo Dory\\nwere present.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 396}, page_content='378 10. Boosting and Additive Trees\\n0 500 1000 15000.24 0.26 0.28 0.30 0.32 0.34\\nNumber of TreesMean DevianceGBM Test\\nGBM CV\\nGAM Test\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\\nSpecificitySensitivity\\nAUC\\nGAM 0.97\\nGBM 0.98\\nFIGURE 10.19. The left panel shows the mean deviance as a function of the\\nnumber of trees for the GBM logistic regression model ﬁt to the p resence/absence\\ndata. Shown are 10-fold cross-validation on the training data ( and1×s.e. bars),\\nand test deviance on the test data. Also shown for comparison is the test deviance\\nusing a GAM model with 8d ffor each term. The right panel shows ROC curves\\non the test data for the chosen GBM model (vertical line in left plot) and the\\nGAM model.\\nFigure 10.19 (left panel) shows the mean binomial deviance for the se-\\nquence of GBM models, both for 10-fold CV and test data. There is a mod-\\nest improvement over the performance of a GAM model, ﬁt using smoothing\\nsplines with 8 degrees-of-freedom (df) per term. The right panel shows the\\nROC curves (see Section 9.2.5) for both models, which measures predictive\\nperformance. From this point of view, the performance looks very simi-\\nlar, with GBM perhaps having a slight edge as summarized by the AUC\\n(area under the curve). At the point of equal sensitivity/speciﬁcity, GBM\\nachieves 91%, and GAM 90%.\\nFigure 10.20 summarizes the contributions of the variables in the logistic\\nGBM ﬁt. We see that there is a well-deﬁned depth range over which Black\\nOreo are caught, with much more frequent capture in colder waters. We do\\nnot give details of the quantitative catch model; the important variabl es\\nwere much the same.\\nAll the predictors used in these models are available on a ﬁne geographi-\\ncal grid; in fact they were derived from environmental atlases, satellite im -\\nages and the like—see Leathwick et al. (2006) for details. This also means\\nthat predictions can be made on this grid, and imported into GIS mapping\\nsystems. Figure 10.21 shows prediction maps for both presence and catch\\nsize, with both standardized to a common set of trawl conditions; since the\\npredictors vary in a continuous fashion with geographical location, so do\\nthe predictions.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 397}, page_content='10.14 Illustrations 379\\nOrbVelSpeedDistanceDisOrgMatterCodendSizePentadeTidalCurrSlopeChlaCase2SSTGradSalResidSusPartMatterAvgDepthTempResid\\nRelative influence0 10 25 −4 0 2 4 6−7 −5 −3 −1\\nTempResidf(TempResid)\\n0 500 1000 2000−6 −4 −2\\nAvgDepthf(AvgDepth)\\n0 5 10 15−7 −5 −3\\nSusPartMatterf(SusPartMatter)\\n−0.8 −0.4 0.0 0.4−7 −5 −3 −1\\nSalResidf(SalResid)\\n0.00 0.05 0.10 0.15−7 −5 −3 −1\\nSSTGradf(SSTGrad)\\nFIGURE 10.20. The top-left panel shows the relative inﬂuence computed from\\nthe GBM logistic regression model. The remaining panels show th e partial de-\\npendence plots for the leading ﬁve variables, all plotted on the s ame scale for\\ncomparison.\\nBecause of their ability to model interactions and automatically select\\nvariables, as well as robustness to outliers and missing data, GBM models\\nare rapidly gaining popularity in this data-rich and enthusiastic community .\\n10.14.3 Demographics Data\\nIn this section we illustrate gradient boosting on a multiclass classiﬁca -\\ntion problem, using MART. The data come from 9243 questionnaires ﬁlled\\nout by shopping mall customers in the San Francisco Bay Area (Impact\\nResources, Inc., Columbus, OH). Among the questions are 14 concerning\\ndemographics. For this illustration the goal is to predict occupation us-\\ning the other 13 variables as predictors, and hence identify demographic\\nvariables that discriminate between diﬀerent occupational categories. We\\nrandomly divided the data into a training set (80%) and test set (20%),\\nand used J= 6 node trees with a learning rate ν= 0.1.\\nFigure 10.22 shows the K= 9 occupation class values along with their\\ncorresponding error rates. The overall error rate is 42.5%, which can be\\ncompared to the null rate of 69% obtained by predicting the most numerous'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 398}, page_content='380 10. Boosting and Additive Trees\\nFIGURE 10.21. Geological prediction maps of the presence probability (lef t\\nmap) and catch size (right map) obtained from the gradient boost ed models.\\nclassProf/Man (Professional/Managerial). The four best predicted classes\\nare seen to be Retired ,Student ,Prof/Man , andHomemaker .\\nFigure 10.23 shows the relative predictor variable importances as aver-\\naged over all classes (10.46). Figure 10.24 displays the individual relati ve\\nimportance distributions (10.45) for each of the four best predicted classes.\\nOne sees that the most relevant predictors are generally diﬀerent for each\\nrespective class. An exception is agewhich is among the three most relevant\\nfor predicting Retired ,Student , andProf/Man .\\nFigure 10.25 shows the partial dependence of the log-odds (10.52) on age\\nfor these three classes. The abscissa values are ordered codes for respective\\nequally spaced age intervals. One sees that after accounting for the contri-\\nbutions of the other variables, the odds of being retired are higher for older\\npeople, whereas the opposite is the case for being a student. The odds of\\nbeing professional/managerial are highest for middle-aged people. These\\nresults are of course not surprising. They illustrate that inspecting partial\\ndependences separately for each class can lead to sensible results.\\nBibliographic Notes\\nSchapire (1990) developed the ﬁrst simple boosting procedure in the PAC\\nlearning framework (Valiant, 1984; Kearns and Vazirani, 1994). Schapire'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 399}, page_content='10.14 Illustrations 381\\nSalesUnemployedMilitaryClericalLaborHomemakerProf/ManRetiredStudent\\n0.0 0.2 0.4 0.6 0.8 1.0\\nError RateOverall Error Rate = 0.425\\nFIGURE 10.22. Error rate for each occupation in the demographics data.\\nageincomeeduhsld-statmar-dlincsexethnicmar-stattyp-homelangnum-hsldchildrenyrs-BA\\n0 20 40 60 80 100\\nRelative Importance\\nFIGURE 10.23. Relative importance of the predictors as averaged over all\\nclasses for the demographics data.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 400}, page_content='382 10. Boosting and Additive Trees\\nagemar-dlincsexethnicincomehsld-statmar-statlangtyp-homechildrenedunum-hsldyrs-BA\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Retired\\nhsld-statageincomemar-stateduethnicnum-hsldtyp-homesexmar-dlinclangyrs-BAchildren\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Student\\neduincomeagemar-dlincethnichsld-stattyp-homesexnum-hsldlangmar-statyrs-BAchildren\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Prof/Man\\nsexmar-dlincchildrenethnicnum-hsldedumar-statlangtyp-homeincomeagehsld-statyrs-BA\\n0 20 40 60 80 100\\nRelative ImportanceClass =  Homemaker\\nFIGURE 10.24. Predictor variable importances separately for each of the fo ur\\nclasses with lowest error rate for the demographics data.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 401}, page_content='10.14 Illustrations 383\\nagePartial Dependence\\n1 2 3 4 5 6 70 1 2 3 4Retired\\nagePartial Dependence\\n1 2 3 4 5 6 7-2 -1 0 1 2Student\\nagePartial Dependence\\n1 2 3 4 5 6 7-2 -1 0 1 2Prof/Man\\nFIGURE 10.25. Partial dependence of the odds of three diﬀerent occupations\\non age, for the demographics data.\\nshowed that a weak learner could always improve its performance by train-\\ning two additional classiﬁers on ﬁltered versions of the input data stream.\\nA weak learner is an algorithm for producing a two-class classiﬁer with\\nperformance guaranteed (with high probability) to be signiﬁcantly better\\nthan a coin-ﬂip. After learning an initial classiﬁer G1on the ﬁrst Ntraining\\npoints,\\n•G2is learned on a new sample of Npoints, half of which are misclas-\\nsiﬁed by G1;\\n•G3is learned on Npoints for which G1andG2disagree;\\n•the boosted classiﬁer is GB=majority vote (G1,G2,G3).\\nSchapire’s “Strength of Weak Learnability” theorem proves that GBhas\\nimproved performance over G1.\\nFreund (1995) proposed a “boost by majority” variation which combined\\nmany weak learners simultaneously and improved the performance of the\\nsimple boosting algorithm of Schapire. The theory supporting both of these'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 402}, page_content='384 10. Boosting and Additive Trees\\nalgorithms requires the weak learner to produce a classiﬁer with a ﬁxed\\nerror rate. This led to the more adaptive and realistic AdaBoost (Freund\\nand Schapire, 1996a) and its oﬀspring, where this assumption was dropped.\\nFreund and Schapire (1996a) and Schapire and Singer (1999) provide\\nsome theory to support their algorithms, in the form of upper bounds on\\ngeneralization error. This theory has evolved in the computational learning\\ncommunity, initially based on the concepts of PAC learning. Other theo-\\nries attempting to explain boosting come from game theory (Freund and\\nSchapire, 1996b; Breiman, 1999; Breiman, 1998), and VC theory (Schapire\\net al., 1998). The bounds and the theory associated with the AdaBoost\\nalgorithms are interesting, but tend to be too loose to be of practical im-\\nportance. In practice, boosting achieves results far more impressive than\\nthe bounds would imply. Schapire (2002) and Meir and R¨ atsch (2003) give\\nuseful overviews more recent than the ﬁrst edition of this book.\\nFriedman et al. (2000) and Friedman (2001) form the basis for our expo-\\nsition in this chapter. Friedman et al. (2000) analyze AdaBoost statist ically,\\nderive the exponential criterion, and show that it estimates the log-odds\\nof the class probability. They propose additive tree models, the right-sized\\ntrees and ANOVA representation of Section 10.11, and the multiclass logit\\nformulation. Friedman (2001) developed gradient boosting and shrinkage\\nfor classiﬁcation and regression, while Friedman (1999) explored stochast ic\\nvariants of boosting. Mason et al. (2000) also embraced a gradient appro ach\\nto boosting. As the published discussions of Friedman et al. (2000) shows,\\nthere is some controversy about how and why boosting works.\\nSince the publication of the ﬁrst edition of this book, these debates have\\ncontinued, and spread into the statistical community with a series of papers\\non consistency of boosting (Jiang, 2004; Lugosi and Vayatis, 2004; Zhang\\nand Yu, 2005; Bartlett and Traskin, 2007). Mease and Wyner (2008),\\nthrough a series of simulation examples, challenge some of our interpre-\\ntations of boosting; our response (Friedman et al., 2008a) puts most of\\nthese objections to rest. A recent survey by B¨ uhlmann and Hothorn (2007)\\nsupports our approach to boosting.\\nExercises\\nEx. 10.1 Derive expression (10.12) for the update parameter in AdaBoost.\\nEx. 10.2 Prove result (10.16), that is, the minimizer of the population\\nversion of the AdaBoost criterion, is one-half of the log odds.\\nEx. 10.3 Show that the marginal average (10.47) recovers additive and\\nmultiplicative functions (10.50) and (10.51), while the conditional expec-\\ntation (10.49) does not.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 403}, page_content='Exercises 385\\nEx. 10.4\\n(a) Write a program implementing AdaBoost with trees.\\n(b) Redo the computations for the example of Figure 10.2. Plot the train-\\ning error as well as test error, and discuss its behavior.\\n(c) Investigate the number of iterations needed to make the test error\\nﬁnally start to rise.\\n(d) Change the setup of this example as follows: deﬁne two classes, with\\nthe features in Class 1 being X1,X2,... ,X 10, standard indepen-\\ndent Gaussian variates. In Class 2, the features X1,X2,... ,X 10are\\nalso standard independent Gaussian, but conditioned on the event∑\\njX2\\nj>12. Now the classes have signiﬁcant overlap in feature space.\\nRepeat the AdaBoost experiments as in Figure 10.2 and discuss the\\nresults.\\nEx. 10.5 Multiclass exponential loss (Zhu et al., 2005). For a K-class clas-\\nsiﬁcation problem, consider the coding Y= (Y1,... ,Y K)Twith\\nYk={1, ifG=Gk\\n−1\\nK−1,otherwise .(10.55)\\nLetf= (f1,... ,f K)Twith∑K\\nk=1fk= 0, and deﬁne\\nL(Y,f) = exp(\\n−1\\nKYTf)\\n. (10.56)\\n(a) Using Lagrange multipliers, derive the population minimizer f∗of\\nE(Y,f), subject to the zero-sum constraint, and relate these to the\\nclass probabilities.\\n(b) Show that a multiclass boosting using this loss function leads to a\\nreweighting algorithm similar to Adaboost, as in Section 10.4.\\nEx. 10.6 McNemar test (Agresti, 1996). We report the test error rates on\\nthe spam data to be 5.5% for a generalized additive model (GAM), and\\n4.5% for gradient boosting (GBM), with a test sample of size 1536.\\n(a) Show that the standard error of these estimates is about 0.6%.\\nSince the same test data are used for both methods, the error rates are\\ncorrelated, and we cannot perform a two-sample t-test. We can compare\\nthe methods directly on each test observation, leading to the summary\\nGBM\\nGAM Correct Error\\nCorrect 1434 18\\nError 33 51'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 404}, page_content='386 10. Boosting and Additive Trees\\nThe McNemar test focuses on the discordant errors, 33 vs. 18.\\n(b) Conduct a test to show that GAM makes signiﬁcantly more errors\\nthan gradient boosting, with a two-sided p-value of 0 .036.\\nEx. 10.7 Derive expression (10.32).\\nEx. 10.8 Consider a K-class problem where the targets yikare coded as\\n1 if observation iis in class kand zero otherwise. Suppose we have a\\ncurrent model fk(x), k= 1,... ,K , with∑K\\nk=1fk(x) = 0 (see (10.21) in\\nSection 10.6). We wish to update the model for observations in a region R\\nin predictor space, by adding constants fk(x) +γk, with γK= 0.\\n(a) Write down the multinomial log-likelihood for this problem, and its\\nﬁrst and second derivatives.\\n(b) Using only the diagonal of the Hessian matrix in (1), and starting\\nfromγk= 0∀k, show that a one-step approximate Newton update\\nforγkis\\nγ1\\nk=∑\\nxi∈R(yik−pik)∑\\nxi∈Rpik(1−pik), k= 1,... ,K −1, (10.57)\\nwhere pik= exp( fk(xi))/(∑K\\nℓ=1fℓ(xi)).\\n(c) We prefer our update to sum to zero, as the current model does. Using\\nsymmetry arguments, show that\\nˆγk=K−1\\nK(γ1\\nk−1\\nKK∑\\nℓ=1γ1\\nℓ), k= 1,... ,K (10.58)\\nis an appropriate update, where γ1\\nkis deﬁned as in (10.57) for all\\nk= 1,... ,K .\\nEx. 10.9 Consider a K-class problem where the targets yikare coded as\\n1 if observation iis in class kand zero otherwise. Using the multinomial\\ndeviance loss function (10.22) and the symmetric logistic transform, use\\nthe arguments leading to the gradient boosting Algorithm 10.3 to derive\\nAlgorithm 10.4. Hint: See exercise 10.8 for step 2(b)iii.\\nEx. 10.10 Show that for K= 2 class classiﬁcation, only one tree needs to\\nbe grown at each gradient-boosting iteration.\\nEx. 10.11 Show how to compute the partial dependence function fS(XS)\\nin (10.47) eﬃciently.\\nEx. 10.12 Referring to (10.49), let S={1}andC={2}, with f(X1,X2) =\\nX1. Assume X1andX2are bivariate Gaussian, each with mean zero, vari-\\nance one, and E( X1,X2) =ρ. Show that E(f(X1,X2|X2) =ρX2, even\\nthough fis not a function of X2.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 405}, page_content='Exercises 387\\nAlgorithm 10.4 Gradient Boosting for K-class Classiﬁcation.\\n1. Initialize fk0(x) = 0, k= 1,2,... ,K .\\n2. For m=1 to M:\\n(a) Set\\npk(x) =efk(x)\\n∑K\\nℓ=1efℓ(x), k= 1,2,... ,K.\\n(b) For k= 1 to K:\\ni. Compute rikm=yik−pk(xi), i= 1,2,... ,N .\\nii. Fit a regression tree to the targets rikm, i= 1,2,... ,N ,\\ngiving terminal regions Rjkm, j= 1,2,... ,J m.\\niii. Compute\\nγjkm=K−1\\nK∑\\nxi∈Rjkmrikm∑\\nxi∈Rjkm|rikm|(1− |rikm|), j= 1,2,... ,J m.\\niv. Update fkm(x) =fk,m−1(x) +∑Jm\\nj=1γjkmI(x∈Rjkm).\\n3. Output ˆfk(x) =fkM(x), k= 1,2,... ,K .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 406}, page_content='388 10. Boosting and Additive Trees'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 407}, page_content='This is page 389\\nPrinter: Opaque this\\n11\\nNeural Networks\\n11.1 Introduction\\nIn this chapter we describe a class of learning methods that was developed\\nseparately in diﬀerent ﬁelds—statistics and artiﬁcial intelligence—based\\non essentially identical models. The central idea is to extract linear com-\\nbinations of the inputs as derived features, and then model the target as\\na nonlinear function of these features. The result is a powerful learning\\nmethod, with widespread applications in many ﬁelds. We ﬁrst discuss the\\nprojection pursuit model, which evolved in the domain of semiparamet-\\nric statistics and smoothing. The rest of the chapter is devoted to neural\\nnetwork models.\\n11.2 Projection Pursuit Regression\\nAs in our generic supervised learning problem, assume we have an input\\nvector Xwithpcomponents, and a target Y. Letωm, m= 1,2,... ,M, be\\nunitp-vectors of unknown parameters. The projection pursuit regression\\n(PPR) model has the form\\nf(X) =M∑\\nm=1gm(ωT\\nmX). (11.1)\\nThis is an additive model, but in the derived features Vm=ωT\\nmXrather\\nthan the inputs themselves. The functions gmare unspeciﬁed and are esti-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 408}, page_content='390 Neural Networks\\ng(V)\\nX1X2g(V)\\nX1X2\\nFIGURE 11.1. Perspective plots of two ridge functions.\\n(Left:) g(V) = 1/[1 + exp( −5(V−0.5))], where V= (X1+X2)/√\\n2.\\n(Right:) g(V) = (V+ 0.1) sin(1 /(V/3 + 0.1)), where V=X1.\\nmated along with the directions ωmusing some ﬂexible smoothing method\\n(see below).\\nThe function gm(ωT\\nmX) is called a ridge function in IRp. It varies only\\nin the direction deﬁned by the vector ωm. The scalar variable Vm=ωT\\nmX\\nis the projection of Xonto the unit vector ωm, and we seek ωmso that\\nthe model ﬁts well, hence the name “projection pursuit.” Figure 11.1 shows\\nsome examples of ridge functions. In the example on the left ω= (1/√\\n2)(1,1)T,\\nso that the function only varies in the direction X1+X2. In the example\\non the right, ω= (1,0).\\nThe PPR model (11.1) is very general, since the operation of forming\\nnonlinear functions of linear combinations generates a surprisingly large\\nclass of models. For example, the product X1≤X2can be written as [( X1+\\nX2)2−(X1−X2)2]/4, and higher-order products can be represented simi-\\nlarly.\\nIn fact, if Mis taken arbitrarily large, for appropriate choice of gmthe\\nPPR model can approximate any continuous function in IRparbitrarily\\nwell. Such a class of models is called a universal approximator . However\\nthis generality comes at a price. Interpretation of the ﬁtted model is usually\\ndiﬃcult, because each input enters into the model in a complex and multi-\\nfaceted way. As a result, the PPR model is most useful for prediction, and\\nnot very useful for producing an understandable model for the data. The\\nM= 1 model, known as the single index model in econometrics, is an\\nexception. It is slightly more general than the linear regression model, and\\noﬀers a similar interpretation.\\nHow do we ﬁt a PPR model, given training data ( xi,yi),i= 1,2,... ,N ?\\nWe seek the approximate minimizers of the error function\\nN∑\\ni=1[\\nyi−M∑\\nm=1gm(ωT\\nmxi)]2\\n(11.2)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 409}, page_content='11.2 Projection Pursuit Regression 391\\nover functions gmand direction vectors ωm,m= 1,2,... ,M . As in other\\nsmoothing problems, we need either explicitly or implicitly to impose com-\\nplexity constraints on the gm, to avoid overﬁt solutions.\\nConsider just one term ( M= 1, and drop the subscript). Given the\\ndirection vector ω, we form the derived variables vi=ωTxi. Then we have\\na one-dimensional smoothing problem, and we can apply any scatterplot\\nsmoother, such as a smoothing spline, to obtain an estimate of g.\\nOn the other hand, given g, we want to minimize (11.2) over ω. A Gauss–\\nNewton search is convenient for this task. This is a quasi-Newton method,\\nin which the part of the Hessian involving the second derivative of gis\\ndiscarded. It can be simply derived as follows. Let ωoldbe the current\\nestimate for ω. We write\\ng(ωTxi)≈g(ωT\\noldxi) +g′(ωT\\noldxi)(ω−ωold)Txi (11.3)\\nto give\\nN∑\\ni=1[\\nyi−g(ωTxi)]2≈N∑\\ni=1g′(ωT\\noldxi)2[(\\nωT\\noldxi+yi−g(ωT\\noldxi)\\ng′(ωT\\noldxi))\\n−ωTxi]2\\n.\\n(11.4)\\nTo minimize the right-hand side, we carry out a least squares regression\\nwith target ωT\\noldxi+(yi−g(ωT\\noldxi))/g′(ωT\\noldxi) on the input xi, with weights\\ng′(ωT\\noldxi)2and no intercept (bias) term. This produces the updated coef-\\nﬁcient vector ωnew.\\nThese two steps, estimation of gandω, are iterated until convergence.\\nWith more than one term in the PPR model, the model is built in a forward\\nstage-wise manner, adding a pair ( ωm,gm) at each stage.\\nThere are a number of implementation details.\\n•Although any smoothing method can in principle be used, it is conve-\\nnient if the method provides derivatives. Local regression and smooth-\\ning splines are convenient.\\n•After each step the gm’s from previous steps can be readjusted using\\nthe backﬁtting procedure described in Chapter 9. While this may\\nlead ultimately to fewer terms, it is not clear whether it improves\\nprediction performance.\\n•Usually the ωmare not readjusted (partly to avoid excessive compu-\\ntation), although in principle they could be as well.\\n•The number of terms Mis usually estimated as part of the forward\\nstage-wise strategy. The model building stops when the next term\\ndoes not appreciably improve the ﬁt of the model. Cross-validation\\ncan also be used to determine M.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 410}, page_content='392 Neural Networks\\nThere are many other applications, such as density estimation (Friedman\\net al., 1984; Friedman, 1987), where the projection pursuit idea can be used.\\nIn particular, see the discussion of ICA in Section 14.7 and its relationship\\nwith exploratory projection pursuit. However the projection pursuit re-\\ngression model has not been widely used in the ﬁeld of statistics, perhaps\\nbecause at the time of its introduction (1981), its computational demands\\nexceeded the capabilities of most readily available computers. But it does\\nrepresent an important intellectual advance, one that has blossomed in its\\nreincarnation in the ﬁeld of neural networks, the topic of the rest of this\\nchapter.\\n11.3 Neural Networks\\nThe term neural network has evolved to encompass a large class of models\\nand learning methods. Here we describe the most widely used “vanilla” neu-\\nral net, sometimes called the single hidden layer back-propagation network,\\nor single layer perceptron. There has been a great deal of hypesurrounding\\nneural networks, making them seem magical and mysterious. As we make\\nclear in this section, they are just nonlinear statistical models, much like\\nthe projection pursuit regression model discussed above.\\nA neural network is a two-stage regression or classiﬁcation model, typ-\\nically represented by a network diagram as in Figure 11.2. This network\\napplies both to regression or classiﬁcation. For regression, typically K= 1\\nand there is only one output unit Y1at the top. However, these networks\\ncan handle multiple quantitative responses in a seamless fashion, so we will\\ndeal with the general case.\\nForK-class classiﬁcation, there are Kunits at the top, with the kth\\nunit modeling the probability of class k. There are Ktarget measurements\\nYk, k= 1,... ,K , each being coded as a 0 −1 variable for the kth class.\\nDerived features Zmare created from linear combinations of the inputs,\\nand then the target Ykis modeled as a function of linear combinations of\\ntheZm,\\nZm=σ(α0m+αT\\nmX), m= 1,... ,M,\\nTk=β0k+βT\\nkZ, k= 1,... ,K,\\nfk(X) =gk(T), k= 1,... ,K,(11.5)\\nwhere Z= (Z1,Z2,... ,Z M), and T= (T1,T2,... ,T K).\\nThe activation function σ(v) is usually chosen to be the sigmoid σ(v) =\\n1/(1 +e−v); see Figure 11.3 for a plot of 1 /(1 +e−v). Sometimes Gaussian\\nradial basis functions (Chapter 6) are used for the σ(v), producing what is\\nknown as a radial basis function network .\\nNeural network diagrams like Figure 11.2 are sometimes drawn with an\\nadditional biasunit feeding into every unit in the hidden and output layers.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 411}, page_content='11.3 Neural Networks 393\\n Y  Y Y 2 1 K\\n Z  Z  Z1 Z2 3 m\\n X  X Z  Z1 Z2 3\\n1  Xp  X p-1  X2  X3M\\n X p-1 3 X 2 X 1p Z Y  Y Y\\n XK 1 2\\n                                                                                                                                                /0/0/0\\n/1/1/1\\n/0/0/0\\n/1/1/1\\n/0/0/0\\n/1/1/1\\n/0/0/0\\n/1/1/1\\n/0/0/0\\n/1/1/1\\n/0/0/0\\n/1/1/1\\nFIGURE 11.2. Schematic of a single hidden layer, feed-forward neural network .\\nThinking of the constant “1” as an additional input feature, this bias unit\\ncaptures the intercepts α0mandβ0kin model (11.5).\\nThe output function gk(T) allows a ﬁnal transformation of the vector of\\noutputs T. For regression we typically choose the identity function gk(T) =\\nTk. Early work in K-class classiﬁcation also used the identity function, but\\nthis was later abandoned in favor of the softmax function\\ngk(T) =eTk\\n∑K\\nℓ=1eTℓ. (11.6)\\nThis is of course exactly the transformation used in the multilogit model\\n(Section 4.4), and produces positive estimates that sum to one. In Sec-\\ntion 4.2 we discuss other problems with linear activation functions, in par-\\nticular potentially severe masking eﬀects.\\nThe units in the middle of the network, computing the derived features\\nZm, are called hidden units because the values Zmare not directly ob-\\nserved. In general there can be more than one hidden layer, as illustrated\\nin the example at the end of this chapter. We can think of the Zmas a\\nbasis expansion of the original inputs X; the neural network is then a stan-\\ndard linear model, or linear multilogit model, using these transformations\\nas inputs. There is, however, an important enhancement over the basis-\\nexpansion techniques discussed in Chapter 5; here the parameters of the\\nbasis functions are learned from the data.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 412}, page_content='394 Neural Networks\\n-10 -5 0 5 100.0 0.5 1.01/(1 +e−v)\\nv\\nFIGURE 11.3. Plot of the sigmoid function σ(v) = 1/(1+exp( −v))(red curve),\\ncommonly used in the hidden layer of a neural network. Included ar eσ(sv)for\\ns=1\\n2(blue curve) and s= 10(purple curve). The scale parameter scontrols\\nthe activation rate, and we can see that large samounts to a hard activation at\\nv= 0. Note that σ(s(v−v0))shifts the activation threshold from 0tov0.\\nNotice that if σis the identity function, then the entire model collapses\\nto a linear model in the inputs. Hence a neural network can be thought of\\nas a nonlinear generalization of the linear model, both for regression and\\nclassiﬁcation. By introducing the nonlinear transformation σ, it greatly\\nenlarges the class of linear models. In Figure 11.3 we see that the rate of\\nactivation of the sigmoid depends on the norm of αm, and if ∥αm∥is very\\nsmall, the unit will indeed be operating in the linear part of its activation\\nfunction.\\nNotice also that the neural network model with one hidden layer has\\nexactly the same form as the projection pursuit model described above.\\nThe diﬀerence is that the PPR model uses nonparametric functions gm(v),\\nwhile the neural network uses a far simpler function based on σ(v), with\\nthree free parameters in its argument. In detail, viewing the neural network\\nmodel as a PPR model, we identify\\ngm(ωT\\nmX) = βmσ(α0m+αT\\nmX)\\n=βmσ(α0m+∥αm∥(ωT\\nmX)), (11.7)\\nwhere ωm=αm/∥αm∥is the mth unit-vector. Since σβ,α0,s(v) =βσ(α0+\\nsv) has lower complexity than a more general nonparametric g(v), it is not\\nsurprising that a neural network might use 20 or 100 such functions, while\\nthe PPR model typically uses fewer terms ( M= 5 or 10, for example).\\nFinally, we note that the name “neural networks” derives from the fact\\nthat they were ﬁrst developed as models for the human brain. Each unit\\nrepresents a neuron, and the connections (links in Figure 11.2) represent\\nsynapses. In early models, the neurons ﬁred when the total signal passed to\\nthat unit exceeded a certain threshold. In the model above, this corresponds'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 413}, page_content='11.4 Fitting Neural Networks 395\\nto use of a step function for σ(Z) and gm(T). Later the neural network was\\nrecognized as a useful tool for nonlinear statistical modeling, and for this\\npurpose the step function is not smooth enough for optimization. Hence the\\nstep function was replaced by a smoother threshold function, the sigmoid\\nin Figure 11.3.\\n11.4 Fitting Neural Networks\\nThe neural network model has unknown parameters, often called weights ,\\nand we seek values for them that make the model ﬁt the training data well.\\nWe denote the complete set of weights by θ, which consists of\\n{α0m,αm;m= 1,2,... ,M }M(p+ 1) weights ,\\n{β0k,βk;k= 1,2,... ,K }K(M+ 1) weights .(11.8)\\nFor regression, we use sum-of-squared errors as our measure of ﬁt (error\\nfunction)\\nR(θ) =K∑\\nk=1N∑\\ni=1(yik−fk(xi))2. (11.9)\\nFor classiﬁcation we use either squared error or cross-entropy (deviance):\\nR(θ) =−N∑\\ni=1K∑\\nk=1yiklogfk(xi), (11.10)\\nand the corresponding classiﬁer is G(x) = argmaxkfk(x). With the softmax\\nactivation function and the cross-entropy error function, the neural network\\nmodel is exactly a linear logistic regression model in the hidden units, and\\nall the parameters are estimated by maximum likelihood.\\nTypically we don’t want the global minimizer of R(θ), as this is likely\\nto be an overﬁt solution. Instead some regularization is needed: this is\\nachieved directly through a penalty term, or indirectly by early stopping.\\nDetails are given in the next section.\\nThe generic approach to minimizing R(θ) is by gradient descent, called\\nback-propagation in this setting. Because of the compositional form of the\\nmodel, the gradient can be easily derived using the chain rule for diﬀeren-\\ntiation. This can be computed by a forward and backward sweep over the\\nnetwork, keeping track only of quantities local to each unit.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 414}, page_content='396 Neural Networks\\nHere is back-propagation in detail for squared error loss. Let zmi=\\nσ(α0m+αT\\nmxi), from (11.5) and let zi= (z1i,z2i,... ,z Mi). Then we have\\nR(θ)≡N∑\\ni=1Ri\\n=N∑\\ni=1K∑\\nk=1(yik−fk(xi))2, (11.11)\\nwith derivatives\\n∂Ri\\n∂βkm=−2(yik−fk(xi))g′\\nk(βT\\nkzi)zmi,\\n∂Ri\\n∂αmℓ=−K∑\\nk=12(yik−fk(xi))g′\\nk(βT\\nkzi)βkmσ′(αT\\nmxi)xiℓ.(11.12)\\nGiven these derivatives, a gradient descent update at the ( r+ 1)st iter-\\nation has the form\\nβ(r+1)\\nkm=β(r)\\nkm−γrN∑\\ni=1∂Ri\\n∂β(r)\\nkm,\\nα(r+1)\\nmℓ=α(r)\\nmℓ−γrN∑\\ni=1∂Ri\\n∂α(r)\\nmℓ,(11.13)\\nwhere γris the learning rate , discussed below.\\nNow write (11.12) as\\n∂Ri\\n∂βkm=δkizmi,\\n∂Ri\\n∂αmℓ=smixiℓ.(11.14)\\nThe quantities δkiandsmiare “errors” from the current model at the\\noutput and hidden layer units, respectively. From their deﬁnitions, these\\nerrors satisfy\\nsmi=σ′(αT\\nmxi)K∑\\nk=1βkmδki, (11.15)\\nknown as the back-propagation equations . Using this, the updates in (11.13)\\ncan be implemented with a two-pass algorithm. In the forward pass , the\\ncurrent weights are ﬁxed and the predicted values ˆfk(xi) are computed\\nfrom formula (11.5). In the backward pass , the errors δkiare computed,\\nand then back-propagated via (11.15) to give the errors smi. Both sets of\\nerrors are then used to compute the gradients for the updates in (11.13),\\nvia (11.14).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 415}, page_content='11.5 Some Issues in Training Neural Networks 397\\nThis two-pass procedure is what is known as back-propagation. It has\\nalso been called the delta rule (Widrow and Hoﬀ, 1960). The computational\\ncomponents for cross-entropy have the same form as those for the sum of\\nsquares error function, and are derived in Exercise 11.3.\\nThe advantages of back-propagation are its simple, local nature. In the\\nback propagation algorithm, each hidden unit passes and receives infor-\\nmation only to and from units that share a connection. Hence it can be\\nimplemented eﬃciently on a parallel architecture computer.\\nThe updates in (11.13) are a kind of batch learning , with the parame-\\nter updates being a sum over all of the training cases. Learning can also\\nbe carried out online—processing each observation one at a time, updat-\\ning the gradient after each training case, and cycling through the training\\ncases many times. In this case, the sums in equations (11.13) are replaced\\nby a single summand. A training epoch refers to one sweep through the\\nentire training set. Online training allows the network to handle very large\\ntraining sets, and also to update the weights as new observations come in.\\nThe learning rate γrfor batch learning is usually taken to be a con-\\nstant, and can also be optimized by a line search that minimizes the error\\nfunction at each update. With online learning γrshould decrease to zero\\nas the iteration r→ ∞. This learning is a form of stochastic approxima-\\ntion(Robbins and Munro, 1951); results in this ﬁeld ensure convergence if\\nγr→0,∑\\nrγr=∞, and∑\\nrγ2\\nr<∞(satisﬁed, for example, by γr= 1/r).\\nBack-propagation can be very slow, and for that reason is usually not\\nthe method of choice. Second-order techniques such as Newton’s method\\nare not attractive here, because the second derivative matrix of R(the\\nHessian) can be very large. Better approaches to ﬁtting include conjugate\\ngradients and variable metric methods. These avoid explicit computation\\nof the second derivative matrix while still providing faster convergence.\\n11.5 Some Issues in Training Neural Networks\\nThere is quite an art in training neural networks. The model is generally\\noverparametrized, and the optimization problem is nonconvex and unstable\\nunless certain guidelines are followed. In this section we summarize some\\nof the important issues.\\n11.5.1 Starting Values\\nNote that if the weights are near zero, then the operative part of the sigmoid\\n(Figure 11.3) is roughly linear, and hence the neural network collapses into\\nan approximately linear model (Exercise 11.2). Usually starting values fo r\\nweights are chosen to be random values near zero. Hence the model starts\\nout nearly linear, and becomes nonlinear as the weights increase. Individual'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 416}, page_content='398 Neural Networks\\nunits localize to directions and introduce nonlinearities where needed. Use\\nof exact zero weights leads to zero derivatives and perfect symmetry, and\\nthe algorithm never moves. Starting instead with large weights often leads\\nto poor solutions.\\n11.5.2 Overﬁtting\\nOften neural networks have too many weights and will overﬁt the data at\\nthe global minimum of R. In early developments of neural networks, either\\nby design or by accident, an early stopping rule was used to avoid over-\\nﬁtting. Here we train the model only for a while, and stop well before we\\napproach the global minimum. Since the weights start at a highly regular-\\nized (linear) solution, this has the eﬀect of shrinking the ﬁnal model toward\\na linear model. A validation dataset is useful for determining when to stop,\\nsince we expect the validation error to start increasing.\\nA more explicit method for regularization is weight decay , which is anal-\\nogous to ridge regression used for linear models (Section 3.4.1). We add a\\npenalty to the error function R(θ) +λJ(θ), where\\nJ(θ) =∑\\nkmβ2\\nkm+∑\\nmℓα2\\nmℓ (11.16)\\nandλ≥0 is a tuning parameter. Larger values of λwill tend to shrink\\nthe weights toward zero: typically cross-validation is used to estimate λ.\\nThe eﬀect of the penalty is to simply add terms 2 βkmand 2 αmℓto the\\nrespective gradient expressions (11.13). Other forms for the penalty have\\nbeen proposed, for example,\\nJ(θ) =∑\\nkmβ2\\nkm\\n1 +β2\\nkm+∑\\nmℓα2\\nmℓ\\n1 +α2\\nmℓ, (11.17)\\nknown as the weight elimination penalty. This has the eﬀect of shrinking\\nsmaller weights more than (11.16) does.\\nFigure 11.4 shows the result of training a neural network with ten hidden\\nunits, without weight decay (upper panel) and with weight decay (lower\\npanel), to the mixture example of Chapter 2. Weight decay has clearly\\nimproved the prediction. Figure 11.5 shows heat maps of the estimated\\nweights from the training (grayscale versions of these are called Hinton\\ndiagrams. ) We see that weight decay has dampened the weights in both\\nlayers: the resulting weights are spread fairly evenly over the ten hidden\\nunits.\\n11.5.3 Scaling of the Inputs\\nSince the scaling of the inputs determines the eﬀective scaling of the weights\\nin the bottom layer, it can have a large eﬀect on the quality of the ﬁnal'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='11.5 Some Issues in Training Neural Networks 399\\nNeural Network - 10 Units, No Weight Decay'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='. . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . .. . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.100\\nTest Error:       0.259\\nBayes Error:    0.210\\nNeural Network - 10 Units, Weight Decay=0.02'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='. .. .. .. .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='. . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 417}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.160\\nTest Error:       0.223\\nBayes Error:    0.210\\nFIGURE 11.4. A neural network on the mixture example of Chapter 2. The\\nupper panel uses no weight decay, and overﬁts the training data. The lower panel\\nuses weight decay, and achieves close to the Bayes error rate ( broken purple\\nboundary). Both use the softmax activation function and cross- entropy error.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 418}, page_content='400 Neural Networks\\n1 1\\n11\\nx1 x1x2 x2y1 y1y2 y2\\nz1z1\\nz1z1\\nz2z2\\nz2z2\\nz3z3\\nz3z3\\nz1z1\\nz1z1\\nz5z5\\nz5z5\\nz6z6\\nz6z6\\nz7z7\\nz7z7\\nz8z8\\nz8z8\\nz9z9\\nz9z9\\nz10z10\\nz10z10No weight decay Weight decay\\nFIGURE 11.5. Heat maps of the estimated weights from the training of neural\\nnetworks from Figure 11.4. The display ranges from bright gree n (negative) to\\nbright red (positive).\\nsolution. At the outset it is best to standardize all inputs to have mean zero\\nand standard deviation one. This ensures all inputs are treated equally in\\nthe regularization process, and allows one to choose a meaningful range for\\nthe random starting weights. With standardized inputs, it is typical to take\\nrandom uniform weights over the range [ −0.7,+0.7].\\n11.5.4 Number of Hidden Units and Layers\\nGenerally speaking it is better to have too many hidden units than too few.\\nWith too few hidden units, the model might not have enough ﬂexibility to\\ncapture the nonlinearities in the data; with too many hidden units, the\\nextra weights can be shrunk toward zero if appropriate regularization is\\nused. Typically the number of hidden units is somewhere in the range of\\n5 to 100, with the number increasing with the number of inputs and num-\\nber of training cases. It is most common to put down a reasonably large\\nnumber of units and train them with regularization. Some researchers use\\ncross-validation to estimate the optimal number, but this seems unneces-\\nsary if cross-validation is used to estimate the regularization parameter .\\nChoice of the number of hidden layers is guided by background knowledge\\nand experimentation. Each layer extracts features of the input for regres-\\nsion or classiﬁcation. Use of multiple hidden layers allows construction of\\nhierarchical features at diﬀerent levels of resolution. An example of the\\neﬀective use of multiple layers is given in Section 11.6.\\n11.5.5 Multiple Minima\\nThe error function R(θ) is nonconvex, possessing many local minima. As a\\nresult, the ﬁnal solution obtained is quite dependent on the choice of start-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 419}, page_content='11.6 Example: Simulated Data 401\\ning weights. One must at least try a number of random starting conﬁgura-\\ntions, and choose the solution giving lowest (penalized) error. Probably a\\nbetter approach is to use the average predictions over the collection of net-\\nworks as the ﬁnal prediction (Ripley, 1996). This is preferable to averaging\\nthe weights, since the nonlinearity of the model implies that this averaged\\nsolution could be quite poor. Another approach is via bagging , which aver-\\nages the predictions of networks training from randomly perturbed versions\\nof the training data. This is described in Section 8.7.\\n11.6 Example: Simulated Data\\nWe generated data from two additive error models Y=f(X) +ε:\\nSum of sigmoids: Y=σ(aT\\n1X) +σ(aT\\n2X) +ε1;\\nRadial: Y=10∏\\nm=1φ(Xm) +ε2.\\nHereXT= (X1,X2,... ,X p), each Xjbeing a standard Gaussian variate,\\nwithp= 2 in the ﬁrst model, and p= 10 in the second.\\nFor the sigmoid model, a1= (3,3), a2= (3,−3); for the radial model,\\nφ(t) = (1 /2π)1/2exp(−t2/2). Both ε1andε2are Gaussian errors, with\\nvariance chosen so that the signal-to-noise ratio\\nVar(E( Y|X))\\nVar(Y−E(Y|X))=Var(f(X))\\nVar(ε)(11.18)\\nis 4 in both models. We took a training sample of size 100 and a test sample\\nof size 10 ,000. We ﬁt neural networks with weight decay and various num-\\nbers of hidden units, and recorded the average test error E Test(Y−ˆf(X))2\\nfor each of 10 random starting weights. Only one training set was gen-\\nerated, but the results are typical for an “average” training set. The test\\nerrors are shown in Figure 11.6. Note that the zero hidden unit model refers\\nto linear least squares regression. The neural network is perfectly suited to\\nthe sum of sigmoids model, and the two-unit model does perform the best,\\nachieving an error close to the Bayes rate. (Recall that the Bayes rate for\\nregression with squared error is the error variance; in the ﬁgures, we report\\ntest error relative to the Bayes error). Notice, however, that with more hid-\\nden units, overﬁtting quickly creeps in, and with some starting weights the\\nmodel does worse than the linear model (zero hidden unit) model. Even\\nwith two hidden units, two of the ten starting weight conﬁgurations pro-\\nduced results no better than the linear model, conﬁrming the importance\\nof multiple starting values.\\nA radial function is in a sense the most diﬃcult for the neural net, as it is\\nspherically symmetric and with no preferred directions. We see in the right'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 420}, page_content='402 Neural Networks\\n1.0 1.5 2.0 2.5 3.0\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorSum of Sigmoids\\n0 5 10 15 20 25 30\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorRadial\\nFIGURE 11.6. Boxplots of test error, for simulated data example, relative t o\\nthe Bayes error (broken horizontal line). True function is a sum of two sigmoids\\non the left, and a radial function is on the right. The test error is displayed for\\n10diﬀerent starting weights, for a single hidden layer neural netwo rk with the\\nnumber of units as indicated.\\npanel of Figure 11.6 that it does poorly in this case, with the test error\\nstaying well above the Bayes error (note the diﬀerent vertical scale from\\nthe left panel). In fact, since a constant ﬁt (such as the sample average)\\nachieves a relative error of 5 (when the SNR is 4), we see that the neural\\nnetworks perform increasingly worse than the mean.\\nIn this example we used a ﬁxed weight decay parameter of 0 .0005, rep-\\nresenting a mild amount of regularization. The results in the left panel of\\nFigure 11.6 suggest that more regularization is needed with greater num-\\nbers of hidden units.\\nIn Figure 11.7 we repeated the experiment for the sum of sigmoids model,\\nwith no weight decay in the left panel, and stronger weight decay ( λ= 0.1)\\nin the right panel. With no weight decay, overﬁtting becomes even more\\nsevere for larger numbers of hidden units. The weight decay value λ= 0.1\\nproduces good results for all numbers of hidden units, and there does not\\nappear to be overﬁtting as the number of units increase. Finally, Figure 11.8\\nshows the test error for a ten hidden unit network, varying the weight decay\\nparameter over a wide range. The value 0 .1 is approximately optimal.\\nIn summary, there are two free parameters to select: the weight decay λ\\nand number of hidden units M. As a learning strategy, one could ﬁx either\\nparameter at the value corresponding to the least constrained model, to\\nensure that the model is rich enough, and use cross-validation to choose\\nthe other parameter. Here the least constrained values are zero weight decay\\nand ten hidden units. Comparing the left panel of Figure 11.7 to Figure\\n11.8, we see that the test error is less sensitive to the value of the weight'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 421}, page_content='11.6 Example: Simulated Data 4031.0 1.5 2.0 2.5 3.0\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorNo Weight Decay\\n1.0 1.5 2.0 2.5 3.0\\n0 1 2 3 4 5 6 7 8 9 10\\nNumber of Hidden UnitsTest ErrorWeight Decay=0.1\\nFIGURE 11.7. Boxplots of test error, for simulated data example, relative t o the\\nBayes error. True function is a sum of two sigmoids. The test er ror is displayed\\nfor ten diﬀerent starting weights, for a single hidden layer neur al network with\\nthe number units as indicated. The two panels represent no weight de cay (left)\\nand strong weight decay λ= 0.1(right).1.0 1.2 1.4 1.6 1.8 2.0 2.2\\n0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14\\nWeight Decay ParameterTest ErrorSum of Sigmoids, 10 Hidden Unit Model\\nFIGURE 11.8. Boxplots of test error, for simulated data example. True functi on\\nis a sum of two sigmoids. The test error is displayed for ten di ﬀerent starting\\nweights, for a single hidden layer neural network with ten hidde n units and weight\\ndecay parameter value as indicated.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 422}, page_content='404 Neural Networks\\nFIGURE 11.9. Examples of training cases from ZIP code data. Each image is\\na16×16 8-bit grayscale representation of a handwritten digit.\\ndecay parameter, and hence cross-validation of this parameter would be\\npreferred.\\n11.7 Example: ZIP Code Data\\nThis example is a character recognition task: classiﬁcation of handwritten\\nnumerals. This problem captured the attention of the machine learning and\\nneural network community for many years, and has remained a benchmark\\nproblem in the ﬁeld. Figure 11.9 shows some examples of normalized hand-\\nwritten digits, automatically scanned from envelopes by the U.S. Postal\\nService. The original scanned digits are binary and of diﬀerent sizes and\\norientations; the images shown here have been deslanted and size normal-\\nized, resulting in 16 ×16 grayscale images (Le Cun et al., 1990). These 256\\npixel values are used as inputs to the neural network classiﬁer.\\nAblack box neural network is not ideally suited to this pattern recogni-\\ntion task, partly because the pixel representation of the images lack certain\\ninvariances (such as small rotations of the image). Consequently early at -\\ntempts with neural networks yielded misclassiﬁcation rates around 4 .5%\\non various examples of the problem. In this section we show some of the\\npioneering eﬀorts to handcraft the neural network to overcome some these\\ndeﬁciencies (Le Cun, 1989), which ultimately led to the state of the art in\\nneural network performance(Le Cun et al., 1998)1.\\nAlthough current digit datasets have tens of thousands of training and\\ntest examples, the sample size here is deliberately modest in order to em-\\n1The ﬁgures and tables in this example were recreated from Le C un (1989).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 423}, page_content='11.7 Example: ZIP Code Data 405\\n16x168x8x2\\n16x1610\\n4x44x4\\n8x8x210\\nShared WeightsNet-5Net-4Net-1\\n4x4x4Local Connectivity10\\n1010\\nNet-3Net-28x812\\n16x1616x1616x16\\nFIGURE 11.10. Architecture of the ﬁve networks used in the ZIP code example.\\nphasize the eﬀects. The examples were obtained by scanning some actual\\nhand-drawn digits, and then generating additional images by random hor-\\nizontal shifts. Details may be found in Le Cun (1989). There are 320 digi ts\\nin the training set, and 160 in the test set.\\nFive diﬀerent networks were ﬁt to the data:\\nNet-1: No hidden layer, equivalent to multinomial logistic regression.\\nNet-2: One hidden layer, 12 hidden units fully connected.\\nNet-3: Two hidden layers locally connected.\\nNet-4: Two hidden layers, locally connected with weight sharing.\\nNet-5: Two hidden layers, locally connected, two levels of weight sharing.\\nThese are depicted in Figure 11.10. Net-1 for example has 256 inputs, one\\neach for the 16 ×16 input pixels, and ten output units for each of the digits\\n0–9. The predicted value ˆfk(x) represents the estimated probability that\\nan image xhas digit class k, fork= 0,1,2,... ,9.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 424}, page_content='406 Neural Networks\\nTraining Epochs% Correct on Test Data\\n0 5 10 15 20 25 3060708090100\\nNet-1Net-2Net-3Net-4Net-5\\nFIGURE 11.11. Test performance curves, as a function of the number of train-\\ning epochs, for the ﬁve networks of Table 11.1 applied to the ZIP c ode data.\\n(Le Cun, 1989)\\nThe networks all have sigmoidal output units, and were all ﬁt with the\\nsum-of-squares error function. The ﬁrst network has no hidden layer, and\\nhence is nearly equivalent to a linear multinomial regression model (Exer-\\ncise 11.4). Net-2 is a single hidden layer network with 12 hidden units, of\\nthe kind described above.\\nThe training set error for all of the networks was 0%, since in all cases\\nthere are more parameters than training observations. The evolution of the\\ntest error during the training epochs is shown in Figure 11.11. The linear\\nnetwork (Net-1) starts to overﬁt fairly quickly, while test performance o f\\nthe others level oﬀ at successively superior values.\\nThe other three networks have additional features which demonstrate\\nthe power and ﬂexibility of the neural network paradigm. They introduce\\nconstraints on the network, natural for the problem at hand, which allow\\nfor more complex connectivity but fewer parameters.\\nNet-3 uses local connectivity: this means that each hidden unit is con-\\nnected to only a small patch of units in the layer below. In the ﬁrst hidden\\nlayer (an 8 ×8 array), each unit takes inputs from a 3 ×3 patch of the input\\nlayer; for units in the ﬁrst hidden layer that are one unit apart, their recep-\\ntive ﬁelds overlap by one row or column, and hence are two pixels apart.\\nIn the second hidden layer, inputs are from a 5 ×5 patch, and again units\\nthat are one unit apart have receptive ﬁelds that are two units apart. The\\nweights for all other connections are set to zero. Local connectivity makes\\neach unit responsible for extracting local features from the layer below, and'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 425}, page_content='11.7 Example: ZIP Code Data 407\\nTABLE 11.1. Test set performance of ﬁve diﬀerent neural networks on a hand-\\nwritten digit classiﬁcation example (Le Cun, 1989).\\nNetwork Architecture Links Weights % Correct\\nNet-1: Single layer network 2570 2570 80.0%\\nNet-2: Two layer network 3214 3214 87.0%\\nNet-3: Locally connected 1226 1226 88.5%\\nNet-4: Constrained network 1 2266 1132 94.0%\\nNet-5: Constrained network 2 5194 1060 98.4%\\nreduces considerably the total number of weights. With many more hidden\\nunits than Net-2, Net-3 has fewer links and hence weights (1226 vs. 3214),\\nand achieves similar performance.\\nNet-4 and Net-5 have local connectivity with shared weights. All units\\nin a local feature map perform the sameoperation on diﬀerent parts of the\\nimage, achieved by sharing the same weights. The ﬁrst hidden layer of Net-\\n4 has two 8 ×8 arrays, and each unit takes input from a 3 ×3 patch just like\\nin Net-3. However, each of the units in a single 8 ×8 feature map share the\\nsame set of nine weights (but have their own bias parameter). This forces\\nthe extracted features in diﬀerent parts of the image to be computed by\\nthe same linear functional, and consequently these networks are sometimes\\nknown as convolutional networks . The second hidden layer of Net-4 has\\nno weight sharing, and is the same as in Net-3. The gradient of the error\\nfunction Rwith respect to a shared weight is the sum of the gradients of\\nRwith respect to each connection controlled by the weights in question.\\nTable 11.1 gives the number of links, the number of weights and the\\noptimal test performance for each of the networks. We see that Net-4 has\\nmore links but fewer weights than Net-3, and superior test performance.\\nNet-5 has four 4 ×4 feature maps in the second hidden layer, each unit\\nconnected to a 5 ×5 local patch in the layer below. Weights are shared\\nin each of these feature maps. We see that Net-5 does the best, having\\nerrors of only 1.6%, compared to 13% for the “vanilla” network Net-2.\\nThe clever design of network Net-5, motivated by the fact that features of\\nhandwriting style should appear in more than one part of a digit, was the\\nresult of many person years of experimentation. This and similar networks\\ngave better performance on ZIP code problems than any other learning\\nmethod at that time (early 1990s). This example also shows that neural\\nnetworks are not a fully automatic tool, as they are sometimes advertised.\\nAs with all statistical models, subject matter knowledge can and should be\\nused to improve their performance.\\nThis network was later outperformed by the tangent distance approach\\n(Simard et al., 1993) described in Section 13.3.3, which explicitly incorpo-\\nrates natural aﬃne invariances. At this point the digit recognition datasets\\nbecome test beds for every new learning procedure, and researchers worked'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 426}, page_content='408 Neural Networks\\nhard to drive down the error rates. As of this writing, the best error rates o n\\na large database (60 ,000 training, 10 ,000 test observations), derived from\\nstandard NIST2databases, were reported to be the following: (Le Cun et\\nal., 1998):\\n•1.1% for tangent distance with a 1-nearest neighbor classiﬁer (Sec-\\ntion 13.3.3);\\n•0.8% for a degree-9 polynomial SVM (Section 12.3);\\n•0.8% for LeNet-5 , a more complex version of the convolutional net-\\nwork described here;\\n•0.7% for boosted LeNet-4 . Boosting is described in Chapter 8. LeNet-\\n4is a predecessor of LeNet-5.\\nLe Cun et al. (1998) report a much larger table of performance results, and\\nit is evident that many groups have been working very hard to bring these\\ntest error rates down. They report a standard error of 0 .1% on the error\\nestimates, which is based on a binomial average with N= 10,000 and\\np≈0.01. This implies that error rates within 0 .1—0.2% of one another\\nare statistically equivalent. Realistically the standard error is even hi gher,\\nsince the test data has been implicitly used in the tuning of the various\\nprocedures.\\n11.8 Discussion\\nBoth projection pursuit regression and neural networks take nonlinear func-\\ntions of linear combinations (“derived features”) of the inputs. This is a\\npowerful and very general approach for regression and classiﬁcation, and\\nhas been shown to compete well with the best learning methods on many\\nproblems.\\nThese tools are especially eﬀective in problems with a high signal-to-noise\\nratio and settings where prediction without interpretation is the goal. They\\nare less eﬀective for problems where the goal is to describe the physical pro-\\ncess that generated the data and the roles of individual inputs. Each input\\nenters into the model in many places, in a nonlinear fashion. Some authors\\n(Hinton, 1989) plot a diagram of the estimated weights into each hidden\\nunit, to try to understand the feature that each unit is extracting. This\\nis limited however by the lack of identiﬁability of the parameter vectors\\nαm, m= 1,... ,M . Often there are solutions with αmspanning the same\\nlinear space as the ones found during training, giving predicted values that\\n2The National Institute of Standards and Technology maintai n large databases, in-\\ncluding handwritten character databases; http://www.nist.gov/srd/ .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 427}, page_content='11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 409\\nare roughly the same. Some authors suggest carrying out a principal com-\\nponent analysis of these weights, to try to ﬁnd an interpretable solution. In\\ngeneral, the diﬃculty of interpreting these models has limited their use in\\nﬁelds like medicine, where interpretation of the model is very important.\\nThere has been a great deal of research on the training of neural net-\\nworks. Unlike methods like CART and MARS, neural networks are smooth\\nfunctions of real-valued parameters. This facilitates the development of\\nBayesian inference for these models. The next sections discusses a success-\\nful Bayesian implementation of neural networks.\\n11.9 Bayesian Neural Nets and the NIPS 2003\\nChallenge\\nA classiﬁcation competition was held in 2003, in which ﬁve labeled train-\\ning datasets were provided to participants. It was organized for a Neural\\nInformation Processing Systems (NIPS) workshop. Each of the data sets\\nconstituted a two-class classiﬁcation problems, with diﬀerent sizes and from\\na variety of domains (see Table 11.2). Feature measurements for a valida-\\ntion dataset were also available.\\nParticipants developed and applied statistical learning procedures to\\nmake predictions on the datasets, and could submit predictions to a web-\\nsite on the validation set for a period of 12 weeks. With this feedback,\\nparticipants were then asked to submit predictions for a separate test set\\nand they received their results. Finally, the class labels for the validation\\nset were released and participants had one week to train their algorithms\\non the combined training and validation sets, and submit their ﬁnal pre-\\ndictions to the competition website. A total of 75 groups participated, with\\n20 and 16 eventually making submissions on the validation and test sets,\\nrespectively.\\nThere was an emphasis on feature extraction in the competition. Arti-\\nﬁcial “probes” were added to the data: these are noise features with dis-\\ntributions resembling the real features but independent of the class labels.\\nThe percentage of probes that were added to each dataset, relative to the\\ntotal set of features, is shown on Table 11.2. Thus each learning algorithm\\nhad to ﬁgure out a way of identifying the probes and downweighting or\\neliminating them.\\nA number of metrics were used to evaluate the entries, including the\\npercentage correct on the test set, the area under the ROC curve, and a\\ncombined score that compared each pair of classiﬁers head-to-head. The\\nresults of the competition are very interesting and are detailed in Guyon et\\nal. (2006). The most notable result: the entries of Neal and Zhang (2006)\\nwere the clear overall winners. In the ﬁnal competition they ﬁnished ﬁrst'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 428}, page_content='410 Neural Networks\\nTABLE 11.2. NIPS 2003 challenge data sets. The column labeled pis the number\\nof features. For the Dorothea dataset the features are binary. Ntr,NvalandNte\\nare the number of training, validation and test cases, respectiv ely\\nDataset Domain Feature p Percent Ntr Nval Nte\\nType Probes\\nArcene Mass spectrometry Dense 10,000 30 100 100 700\\nDexter Text classiﬁcation Sparse 20,000 50 300 300 2000\\nDorothea Drug discovery Sparse 100,000 50 800 350 800\\nGisette Digit recognition Dense 5000 30 6000 1000 6500\\nMadelon Artiﬁcial Dense 500 96 2000 600 1800\\nin three of the ﬁve datasets, and were 5th and 7th on the remaining two\\ndatasets.\\nIn their winning entries, Neal and Zhang (2006) used a series of pre-\\nprocessing feature-selection steps, followed by Bayesian neural networks,\\nDirichlet diﬀusion trees, and combinations of these methods. Here we focus\\nonly on the Bayesian neural network approach, and try to discern which\\naspects of their approach were important for its success. We rerun their\\nprograms and compare the results to boosted neural networks and boosted\\ntrees, and other related methods.\\n11.9.1 Bayes, Boosting and Bagging\\nLet us ﬁrst review brieﬂy the Bayesian approach to inference and its appli-\\ncation to neural networks. Given training data Xtr,ytr, we assume a sam-\\npling model with parameters θ; Neal and Zhang (2006) use a two-hidden-\\nlayer neural network, with output nodes the class probabilities Pr( Y|X,θ)\\nfor the binary outcomes. Given a prior distribution Pr( θ), the posterior\\ndistribution for the parameters is\\nPr(θ|Xtr,ytr) =Pr(θ)Pr(ytr|Xtr,θ)∫\\nPr(θ)Pr(ytr|Xtr,θ)dθ(11.19)\\nFor a test case with features Xnew, the predictive distribution for the\\nlabelYnewis\\nPr(Ynew|Xnew,Xtr,ytr) =∫\\nPr(Ynew|Xnew,θ)Pr(θ|Xtr,ytr)dθ(11.20)\\n(c.f. equation 8.24). Since the integral in (11.20) is intractable, sophis ticated\\nMarkov Chain Monte Carlo (MCMC) methods are used to sample from the\\nposterior distribution Pr( Ynew|Xnew,Xtr,ytr). A few hundred values θare\\ngenerated and then a simple average of these values estimates the integral.\\nNeal and Zhang (2006) use diﬀuse Gaussian priors for all of the parame-\\nters. The particular MCMC approach that was used is called hybrid Monte\\nCarlo, and may be important for the success of the method. It includes\\nan auxiliary momentum vector and implements Hamiltonian dynamics in\\nwhich the potential function is the target density. This is done to avoid'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 429}, page_content='11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 411\\nrandom walk behavior; the successive candidates move across the sample\\nspace in larger steps. They tend to be less correlated and hence converge\\nto the target distribution more rapidly.\\nNeal and Zhang (2006) also tried diﬀerent forms of pre-processing of the\\nfeatures:\\n1. univariate screening using t-tests, and\\n2. automatic relevance determination.\\nIn the latter method (ARD), the weights (coeﬃcients) for the jth feature\\nto each of the ﬁrst hidden layer units all share a common prior variance\\nσ2\\nj, and prior mean zero. The posterior distributions for each variance σ2\\nj\\nare computed, and the features whose posterior variance concentrates on\\nsmall values are discarded.\\nThere are thus three main features of this approach that could be im-\\nportant for its success:\\n(a) the feature selection and pre-processing,\\n(b) the neural network model, and\\n(c) the Bayesian inference for the model using MCMC.\\nAccording to Neal and Zhang (2006), feature screening in (a) is carried\\nout purely for computational eﬃciency; the MCMC procedure is slow with\\na large number of features. There is no need to use feature selection to avoid\\noverﬁtting. The posterior average (11.20) takes care of this automatica lly.\\nWe would like to understand the reasons for the success of the Bayesian\\nmethod. In our view, power of modern Bayesian methods does not lie in\\ntheir use as a formal inference procedure; most people would not believe\\nthat the priors in a high-dimensional, complex neural network model are\\nactually correct. Rather the Bayesian/MCMC approach gives an eﬃcient\\nway of sampling the relevant parts of model space, and then averaging the\\npredictions for the high-probability models.\\nBagging and boosting are non-Bayesian procedures that have some simi-\\nlarity to MCMC in a Bayesian model. The Bayesian approach ﬁxes the data\\nand perturbs the parameters, according to current estimate of the poste-\\nrior distribution. Bagging perturbs the data in an i.i.d fashion and then\\nre-estimates the model to give a new set of model parameters. At the end,\\na simple average of the model predictions from diﬀerent bagged samples is\\ncomputed. Boosting is similar to bagging, but ﬁts a model that is additive\\nin the models of each individual base learner, which are learned using non\\ni.i.d. samples. We can write all of these models in the form\\nˆf(xnew) =L∑\\nℓ=1wℓE(Ynew|xnew,ˆθℓ) (11.21)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 430}, page_content='412 Neural Networks\\nIn all cases the ˆθℓare a large collection of model parameters. For the\\nBayesian model the wℓ= 1/L, and the average estimates the posterior\\nmean (11.21) by sampling θℓfrom the posterior distribution. For bagging,\\nwℓ= 1/Las well, and the ˆθℓare the parameters reﬁt to bootstrap re-\\nsamples of the training data. For boosting, the weights are all equal to\\n1, but the ˆθℓare typically chosen in a nonrandom sequential fashion to\\nconstantly improve the ﬁt.\\n11.9.2 Performance Comparisons\\nBased on the similarities above, we decided to compare Bayesian neural\\nnetworks to boosted trees, boosted neural networks, random forests and\\nbagged neural networks on the ﬁve datasets in Table 11.2. Bagging and\\nboosting of neural networks are not methods that we have previously used\\nin our work. We decided to try them here, because of the success of Bayesian\\nneural networks in this competition, and the good performance of bagging\\nand boosting with trees. We also felt that by bagging and boosting neural\\nnets, we could assess both the choice of model as well as the model search\\nstrategy.\\nHere are the details of the learning methods that were compared:\\nBayesian neural nets. The results here are taken from Neal and Zhang\\n(2006), using their Bayesian approach to ﬁtting neural networks. The\\nmodels had two hidden layers of 20 and 8 units. We re-ran some\\nnetworks for timing purposes only.\\nBoosted trees. We used the gbmpackage (version 1.5-7) in the R language.\\nTree depth and shrinkage factors varied from dataset to dataset. We\\nconsistently bagged 80% of the data at each boosting iteration (the\\ndefault is 50%). Shrinkage was between 0.001 and 0.1. Tree depth was\\nbetween 2 and 9.\\nBoosted neural networks. Since boosting is typically most eﬀective with\\n“weak” learners, we boosted a single hidden layer neural network with\\ntwo or four units, ﬁt with the nnetpackage (version 7.2-36) in R.\\nRandom forests. We used the R package randomForest (version 4.5-16)\\nwith default settings for the parameters.\\nBagged neural networks. We used the same architecture as in the Bayesian\\nneural network above (two hidden layers of 20 and 8 units), ﬁt using\\nboth Neal’s C language package “Flexible Bayesian Modeling” (2004-\\n11-10 release), and Matlab neural-net toolbox (version 5.1).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 431}, page_content='11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 413Test Error (%)\\nArcene Dexter Dorothea Gisette Madelon5 15 25Univariate Screened Features\\nBayesian neural nets\\nboosted trees \\nboosted neural nets\\nrandom forests\\nbagged neural networks \\nTest Error (%)\\nArcene Dexter Dorothea Gisette Madelon5 15 25ARD Reduced Features\\nFIGURE 11.12. Performance of diﬀerent learning methods on ﬁve problems,\\nusing both univariate screening of features (top panel) and a reduc ed feature set\\nfrom automatic relevance determination. The error bars at the t op of each plot\\nhave width equal to one standard error of the diﬀerence between t wo error rates.\\nOn most of the problems several competitors are within this e rror bound.\\nThis analysis was carried out by Nicholas Johnson, and full details may\\nbe found in Johnson (2008)3. The results are shown in Figure 11.12 and\\nTable 11.3.\\nThe ﬁgure and table show Bayesian, boosted and bagged neural networks,\\nboosted trees, and random forests, using both the screened and reduced\\nfeatures sets. The error bars at the top of each plot indicate one standard\\nerror of the diﬀerence between two error rates. Bayesian neural networks\\nagain emerge as the winner, although for some datasets the diﬀerences\\nbetween the test error rates is not statistically signiﬁcant. Random forests\\nperforms the best among the competitors using the selected feature set,\\nwhile the boosted neural networks perform best with the reduced feature\\nset, and nearly match the Bayesian neural net.\\nThe superiority of boosted neural networks over boosted trees suggest\\nthat the neural network model is better suited to these particular prob-\\nlems. Speciﬁcally, individual features might not be good predictors here\\n3We also thank Isabelle Guyon for help in preparing the result s of this section.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 432}, page_content='414 Neural Networks\\nTABLE 11.3. Performance of diﬀerent methods. Values are average rank of tes t\\nerror across the ﬁve problems (low is good), and mean computati on time and\\nstandard error of the mean, in minutes.\\nScreened Features ARD Reduced Features\\nMethod Average Average Average Average\\nRank Time Rank Time\\nBayesian neural networks 1.5 384(138) 1.6 600(186)\\nBoosted trees 3.4 3.03(2.5) 4.0 34.1(32.4)\\nBoosted neural networks 3.8 9.4(8.6) 2.2 35.6(33.5)\\nRandom forests 2.7 1.9(1.7) 3.2 11.2(9.3)\\nBagged neural networks 3.6 3.5(1.1) 4.0 6.4(4.4)\\nand linear combinations of features work better. However the impressive\\nperformance of random forests is at odds with this explanation, and came\\nas a surprise to us.\\nSince the reduced feature sets come from the Bayesian neural network\\napproach, only the methods that use the screened features are legitimate,\\nself-contained procedures. However, this does suggest that better methods\\nfor internal feature selection might help the overall performance of boosted\\nneural networks.\\nThe table also shows the approximate training time required for each\\nmethod. Here the non-Bayesian methods show a clear advantage.\\nOverall, the superior performance of Bayesian neural networks here may\\nbe due to the fact that\\n(a) the neural network model is well suited to these ﬁve problems, and\\n(b) the MCMC approach provides an eﬃcient way of exploring the im-\\nportant part of the parameter space, and then averaging the resulting\\nmodels according to their quality.\\nThe Bayesian approach works well for smoothly parametrized models like\\nneural nets; it is not yet clear that it works as well for non-smooth models\\nlike trees.\\n11.10 Computational Considerations\\nWithNobservations, ppredictors, Mhidden units and Ltraining epochs, a\\nneural network ﬁt typically requires O(NpML ) operations. There are many\\npackages available for ﬁtting neural networks, probably many more than\\nexist for mainstream statistical methods. Because the available softwar e\\nvaries widely in quality, and the learning problem for neural networks is\\nsensitive to issues such as input scaling, such software should be carefully\\nchosen and tested.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 433}, page_content='Exercises 415\\nBibliographic Notes\\nProjection pursuit was proposed by Friedman and Tukey (1974), and spe-\\ncialized to regression by Friedman and Stuetzle (1981). Huber (1985) gives\\na scholarly overview, and Roosen and Hastie (1994) present a formulatio n\\nusing smoothing splines. The motivation for neural networks dates back\\nto McCulloch and Pitts (1943), Widrow and Hoﬀ (1960) (reprinted in An-\\nderson and Rosenfeld (1988)) and Rosenblatt (1962). Hebb (1949) heavily\\ninﬂuenced the development of learning algorithms. The resurgence of neural\\nnetworks in the mid 1980s was due to Werbos (1974), Parker (1985) and\\nRumelhart et al. (1986), who proposed the back-propagation algorithm.\\nToday there are many books written on the topic, for a broad range of\\naudiences. For readers of this book, Hertz et al. (1991), Bishop (1995) and\\nRipley (1996) may be the most informative. Bayesian learning for neural\\nnetworks is described in Neal (1996). The ZIP code example was taken from\\nLe Cun (1989); see also Le Cun et al. (1990) and Le Cun et al. (1998).\\nWe do not discuss theoretical topics such as approximation properties of\\nneural networks, such as the work of Barron (1993), Girosi et al. (1995 )\\nand Jones (1992). Some of these results are summarized by Ripley (1996).\\nExercises\\nEx. 11.1 Establish the exact correspondence between the projection pur-\\nsuit regression model (11.1) and the neural network (11.5). In particular,\\nshow that the single-layer regression network is equivalent to a PPR model\\nwithgm(ωT\\nmx) =βmσ(α0m+sm(ωT\\nmx)), where ωmis the mth unit vector.\\nEstablish a similar equivalence for a classiﬁcation network.\\nEx. 11.2 Consider a neural network for a quantitative outcome as in (11.5),\\nusing squared-error loss and identity output function gk(t) =t. Suppose\\nthat the weights αmfrom the input to hidden layer are nearly zero. Show\\nthat the resulting model is nearly linear in the inputs.\\nEx. 11.3 Derive the forward and backward propagation equations for the\\ncross-entropy loss function.\\nEx. 11.4 Consider a neural network for a Kclass outcome that uses cross-\\nentropy loss. If the network has no hidden layer, show that the model is\\nequivalent to the multinomial logistic model described in Chapter 4.\\nEx. 11.5\\n(a) Write a program to ﬁt a single hidden layer neural network (ten hidden\\nunits) via back-propagation and weight decay.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 434}, page_content='416 Neural Networks\\n(b) Apply it to 100 observations from the model\\nY=σ(aT\\n1X) + (aT\\n2X)2+ 0.30≤Z,\\nwhere σis the sigmoid function, Zis standard normal, XT= (X1,X2),\\neachXjbeing independent standard normal, and a1= (3,3),a2=\\n(3,−3). Generate a test sample of size 1000, and plot the training and\\ntest error curves as a function of the number of training epochs, for\\ndiﬀerent values of the weight decay parameter. Discuss the overﬁtting\\nbehavior in each case.\\n(c) Vary the number of hidden units in the network, from 1 up to 10, and\\ndetermine the minimum number needed to perform well for this task.\\nEx. 11.6 Write a program to carry out projection pursuit regression, using\\ncubic smoothing splines with ﬁxed degrees of freedom. Fit it to the data\\nfrom the previous exercise, for various values of the smoothing parameter\\nand number of model terms. Find the minimum number of model terms\\nnecessary for the model to perform well and compare this to the number\\nof hidden units from the previous exercise.\\nEx. 11.7 Fit a neural network to the spamdata of Section 9.1.2, and compare\\nthe results to those for the additive model given in that chapter. Compare\\nboth the classiﬁcation performance and interpretability of the ﬁnal model.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 435}, page_content='This is page 417\\nPrinter: Opaque this\\n12\\nSupport Vector Machines and\\nFlexible Discriminants\\n12.1 Introduction\\nIn this chapter we describe generalizations of linear decision boundaries\\nfor classiﬁcation. Optimal separating hyperplanes are introduced in Chap-\\nter 4 for the case when two classes are linearly separable. Here we cover\\nextensions to the nonseparable case, where the classes overlap. These tech-\\nniques are then generalized to what is known as the support vector machine ,\\nwhich produces nonlinear boundaries by constructing a linear boundary in\\na large, transformed version of the feature space. The second set of methods\\ngeneralize Fisher’s linear discriminant analysis (LDA). The generalizations\\ninclude ﬂexible discriminant analysis which facilitates construction of non-\\nlinear boundaries in a manner very similar to the support vector machines,\\npenalized discriminant analysis for problems such as signal and image clas-\\nsiﬁcation where the large number of features are highly correlated, and\\nmixture discriminant analysis for irregularly shaped classes.\\n12.2 The Support Vector Classiﬁer\\nIn Chapter 4 we discussed a technique for constructing an optimal separat-\\ning hyperplane between two perfectly separated classes. We review this and\\ngeneralize to the nonseparable case, where the classes may not be separable\\nby a linear boundary.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 436}, page_content='418 12. Flexible Discriminants\\n••\\n•\\n••\\n•••\\n•\\n••\\n•••\\n••\\n••\\n•\\n•\\nmarginM=1\\n∥β∥\\nM=1\\n∥β∥xTβ+β0= 0\\n••\\n•\\n••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n••\\n•\\n• •\\nmarginξ∗\\n1ξ∗\\n1ξ∗\\n1\\nξ∗\\n2ξ∗\\n2ξ∗\\n2ξ∗\\n3ξ∗\\n3ξ∗\\n4ξ∗\\n4ξ∗\\n4ξ∗\\n5\\nM=1\\n∥β∥\\nM=1\\n∥β∥xTβ+β0= 0\\nFIGURE 12.1. Support vector classiﬁers. The left panel shows the separable\\ncase. The decision boundary is the solid line, while broken line s bound the shaded\\nmaximal margin of width 2M= 2/∥β∥. The right panel shows the nonseparable\\n(overlap) case. The points labeled ξ∗\\njare on the wrong side of their margin by\\nan amount ξ∗\\nj=Mξj; points on the correct side have ξ∗\\nj= 0. The margin is\\nmaximized subject to a total budgetPξi≤constant. HencePξ∗\\njis the total\\ndistance of points on the wrong side of their margin.\\nOur training data consists of Npairs ( x1,y1),(x2,y2),... ,(xN,yN), with\\nxi∈IRpandyi∈ {− 1,1}. Deﬁne a hyperplane by\\n{x:f(x) =xTβ+β0= 0}, (12.1)\\nwhere βis a unit vector: ∥β∥= 1. A classiﬁcation rule induced by f(x) is\\nG(x) = sign[ xTβ+β0]. (12.2)\\nThe geometry of hyperplanes is reviewed in Section 4.5, where we show that\\nf(x) in (12.1) gives the signed distance from a point xto the hyperplane\\nf(x) =xTβ+β0= 0. Since the classes are separable, we can ﬁnd a function\\nf(x) =xTβ+β0withyif(xi)>0∀i. Hence we are able to ﬁnd the\\nhyperplane that creates the biggest margin between the training points for\\nclass 1 and −1 (see Figure 12.1). The optimization problem\\nmax\\nβ,β0,∥β∥=1M\\nsubject to yi(xT\\niβ+β0)≥M, i= 1,... ,N,(12.3)\\ncaptures this concept. The band in the ﬁgure is Munits away from the\\nhyperplane on either side, and hence 2 Munits wide. It is called the margin .\\nWe showed that this problem can be more conveniently rephrased as\\nmin\\nβ,β0∥β∥\\nsubject to yi(xT\\niβ+β0)≥1, i= 1,... ,N,(12.4)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 437}, page_content='12.2 The Support Vector Classiﬁer 419\\nwhere we have dropped the norm constraint on β. Note that M= 1/∥β∥.\\nExpression (12.4) is the usual way of writing the support vector criterion\\nfor separated data. This is a convex optimization problem (quadratic cri-\\nterion, linear inequality constraints), and the solution is characterized in\\nSection 4.5.2.\\nSuppose now that the classes overlap in feature space. One way to deal\\nwith the overlap is to still maximize M, but allow for some points to be on\\nthe wrong side of the margin. Deﬁne the slack variables ξ= (ξ1,ξ2,... ,ξ N).\\nThere are two natural ways to modify the constraint in (12.3):\\nyi(xT\\niβ+β0)≥M−ξi, (12.5)\\nor\\nyi(xT\\niβ+β0)≥M(1−ξi), (12.6)\\n∀i, ξi≥0,∑N\\ni=1ξi≤constant. The two choices lead to diﬀerent solutions.\\nThe ﬁrst choice seems more natural, since it measures overlap in actual\\ndistance from the margin; the second choice measures the overlap in relative\\ndistance, which changes with the width of the margin M. However, the ﬁrst\\nchoice results in a nonconvex optimization problem, while the second is\\nconvex; thus (12.6) leads to the “standard” support vector classiﬁer, which\\nwe use from here on.\\nHere is the idea of the formulation. The value ξiin the constraint yi(xT\\niβ+\\nβ0)≥M(1−ξi) is the proportional amount by which the prediction\\nf(xi) =xT\\niβ+β0is on the wrong side of its margin. Hence by bounding the\\nsum∑ξi, we bound the total proportional amount by which predictions\\nfall on the wrong side of their margin. Misclassiﬁcations occur when ξi>1,\\nso bounding∑ξiat a value Ksay, bounds the total number of training\\nmisclassiﬁcations at K.\\nAs in (4.48) in Section 4.5.2, we can drop the norm constraint on β,\\ndeﬁne M= 1/∥β∥, and write (12.4) in the equivalent form\\nmin∥β∥subject to{\\nyi(xT\\niβ+β0)≥1−ξi∀i,\\nξi≥0,∑ξi≤constant .(12.7)\\nThis is the usual way the support vector classiﬁer is deﬁned for the non-\\nseparable case. However we ﬁnd confusing the presence of the ﬁxed scale\\n“1” in the constraint yi(xT\\niβ+β0)≥1−ξi, and prefer to start with (12.6).\\nThe right panel of Figure 12.1 illustrates this overlapping case.\\nBy the nature of the criterion (12.7), we see that points well inside their\\nclass boundary do not play a big role in shaping the boundary. This seems\\nlike an attractive property, and one that diﬀerentiates it from linear dis-\\ncriminant analysis (Section 4.3). In LDA, the decision boundary is deter-\\nmined by the covariance of the class distributions and the positions of the\\nclass centroids. We will see in Section 12.3.3 that logistic regression i s more\\nsimilar to the support vector classiﬁer in this regard.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 438}, page_content='420 12. Flexible Discriminants\\n12.2.1 Computing the Support Vector Classiﬁer\\nThe problem (12.7) is quadratic with linear inequality constraints, hence it\\nis a convex optimization problem. We describe a quadratic programming\\nsolution using Lagrange multipliers. Computationally it is convenient to\\nre-express (12.7) in the equivalent form\\nmin\\nβ,β01\\n2∥β∥2+CN∑\\ni=1ξi\\nsubject to ξi≥0, yi(xT\\niβ+β0)≥1−ξi∀i,(12.8)\\nwhere the “cost” parameter Creplaces the constant in (12.7); the separable\\ncase corresponds to C=∞.\\nThe Lagrange (primal) function is\\nLP=1\\n2∥β∥2+CN∑\\ni=1ξi−N∑\\ni=1αi[yi(xT\\niβ+β0)−(1−ξi)]−N∑\\ni=1θiξi,(12.9)\\nwhich we minimize w.r.t β,β0andξi. Setting the respective derivatives to\\nzero, we get\\nβ=N∑\\ni=1αiyixi, (12.10)\\n0 =N∑\\ni=1αiyi, (12.11)\\nαi=C−θi,∀i, (12.12)\\nas well as the positivity constraints αi, θi, ξi≥0∀i. By substituting\\n(12.10)–(12.12) into (12.9), we obtain the Lagrangian (Wolfe) dua l objec-\\ntive function\\nLD=N∑\\ni=1αi−1\\n2N∑\\ni=1N∑\\ni′=1αiαi′yiyi′xT\\nixi′, (12.13)\\nwhich gives a lower bound on the objective function (12.8) for any feasible\\npoint. We maximize LDsubject to 0 ≤αi≤Cand∑N\\ni=1αiyi= 0. In\\naddition to (12.10)–(12.12), the Karush–Kuhn–Tucker conditions include\\nthe constraints\\nαi[yi(xT\\niβ+β0)−(1−ξi)] = 0 , (12.14)\\nθiξi= 0, (12.15)\\nyi(xT\\niβ+β0)−(1−ξi)≥0, (12.16)\\nfori= 1,... ,N . Together these equations (12.10)–(12.16) uniquely char-\\nacterize the solution to the primal and dual problem.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 439}, page_content='12.2 The Support Vector Classiﬁer 421\\nFrom (12.10) we see that the solution for βhas the form\\nˆβ=N∑\\ni=1ˆαiyixi, (12.17)\\nwith nonzero coeﬃcients ˆ αionly for those observations ifor which the\\nconstraints in (12.16) are exactly met (due to (12.14)). These observati ons\\nare called the support vectors , since ˆβis represented in terms of them\\nalone. Among these support points, some will lie on the edge of the margin\\n(ˆξi= 0), and hence from (12.15) and (12.12) will be characterized by\\n0<ˆαi< C; the remainder ( ˆξi>0) have ˆ αi=C. From (12.14) we can\\nsee that any of these margin points (0 <ˆαi,ˆξi= 0) can be used to solve\\nforβ0, and we typically use an average of all the solutions for numerical\\nstability.\\nMaximizing the dual (12.13) is a simpler convex quadratic programming\\nproblem than the primal (12.9), and can be solved with standard techniques\\n(Murray et al., 1981, for example).\\nGiven the solutions ˆβ0andˆβ, the decision function can be written as\\nˆG(x) = sign[ ˆf(x)]\\n= sign[ xTˆβ+ˆβ0]. (12.18)\\nThe tuning parameter of this procedure is the cost parameter C.\\n12.2.2 Mixture Example (Continued)\\nFigure 12.2 shows the support vector boundary for the mixture example\\nof Figure 2.5 on page 21, with two overlapping classes, for two diﬀerent\\nvalues of the cost parameter C. The classiﬁers are rather similar in their\\nperformance. Points on the wrong side of the boundary are support vectors.\\nIn addition, points on the correct side of the boundary but close to it (in\\nthe margin), are also support vectors. The margin is larger for C= 0.01\\nthan it is for C= 10,000. Hence larger values of Cfocus attention more\\non (correctly classiﬁed) points near the decision boundary, while smaller\\nvalues involve data further away. Either way, misclassiﬁed points are gi ven\\nweight, no matter how far away. In this example the procedure is not very\\nsensitive to choices of C, because of the rigidity of a linear boundary.\\nThe optimal value for Ccan be estimated by cross-validation, as dis-\\ncussed in Chapter 7. Interestingly, the leave-one-out cross-validation error\\ncan be bounded above by the proportion of support points in the data. The\\nreason is that leaving out an observation that is not a support vector will\\nnot change the solution. Hence these observations, being classiﬁed correctly\\nby the original boundary, will be classiﬁed correctly in the cross-validatio n\\nprocess. However this bound tends to be too high, and not generally useful\\nfor choosing C(62% and 85%, respectively, in our examples).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='422 12. Flexible Discriminants'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='.. . . .. . . . . . .. . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='. . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . .. . . . ..'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n••\\n•\\nTraining Error: 0.270\\nTest Error:       0.288\\nBayes Error:    0.210\\nC= 10000'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='.. .. . . .. . . . .. . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . .. . . . .. . . .. ..'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 440}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no•\\nTraining Error: 0.26\\nTest Error:       0.30\\nBayes Error:    0.21\\nC= 0.01\\nFIGURE 12.2. The linear support vector boundary for the mixture data exam-\\nple with two overlapping classes, for two diﬀerent values of C. The broken lines\\nindicate the margins, where f(x) =±1. The support points ( αi>0) are all the\\npoints on the wrong side of their margin. The black solid dots are those support\\npoints falling exactly on the margin ( ξi= 0, αi>0). In the upper panel 62%of\\nthe observations are support points, while in the lower panel 85%are. The broken\\npurple curve in the background is the Bayes decision boundary.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 441}, page_content='12.3 Support Vector Machines and Kernels 423\\n12.3 Support Vector Machines and Kernels\\nThe support vector classiﬁer described so far ﬁnds linear boundaries in the\\ninput feature space. As with other linear methods, we can make the pro-\\ncedure more ﬂexible by enlarging the feature space using basis expansions\\nsuch as polynomials or splines (Chapter 5). Generally linear boundaries\\nin the enlarged space achieve better training-class separation, and trans-\\nlate to nonlinear boundaries in the original space. Once the basis functions\\nhm(x), m= 1,... ,M are selected, the procedure is the same as before. We\\nﬁt the SV classiﬁer using input features h(xi) = (h1(xi),h2(xi),... ,h M(xi)),\\ni= 1,... ,N , and produce the (nonlinear) function ˆf(x) =h(x)Tˆβ+ˆβ0.\\nThe classiﬁer is ˆG(x) = sign( ˆf(x)) as before.\\nThesupport vector machine classiﬁer is an extension of this idea, where\\nthe dimension of the enlarged space is allowed to get very large, inﬁnite\\nin some cases. It might seem that the computations would become pro-\\nhibitive. It would also seem that with suﬃcient basis functions, the data\\nwould be separable, and overﬁtting would occur. We ﬁrst show how the\\nSVM technology deals with these issues. We then see that in fact the SVM\\nclassiﬁer is solving a function-ﬁtting problem using a particular criterion\\nand form of regularization, and is part of a much bigger class of problems\\nthat includes the smoothing splines of Chapter 5. The reader may wish\\nto consult Section 5.8, which provides background material and overlaps\\nsomewhat with the next two sections.\\n12.3.1 Computing the SVM for Classiﬁcation\\nWe can represent the optimization problem (12.9) and its solution in a\\nspecial way that only involves the input features via inner products. We do\\nthis directly for the transformed feature vectors h(xi). We then see that for\\nparticular choices of h, these inner products can be computed very cheaply.\\nThe Lagrange dual function (12.13) has the form\\nLD=N∑\\ni=1αi−1\\n2N∑\\ni=1N∑\\ni′=1αiαi′yiyi′⟨h(xi),h(xi′)⟩. (12.19)\\nFrom (12.10) we see that the solution function f(x) can be written\\nf(x) = h(x)Tβ+β0\\n=N∑\\ni=1αiyi⟨h(x),h(xi)⟩+β0. (12.20)\\nAs before, given αi,β0can be determined by solving yif(xi) = 1 in (12.20)\\nfor any (or all) xifor which 0 < αi< C.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 442}, page_content='424 12. Flexible Discriminants\\nSo both (12.19) and (12.20) involve h(x) only through inner products. In\\nfact, we need not specify the transformation h(x) at all, but require only\\nknowledge of the kernel function\\nK(x,x′) =⟨h(x),h(x′)⟩ (12.21)\\nthat computes inner products in the transformed space. Kshould be a\\nsymmetric positive (semi-) deﬁnite function; see Section 5.8.1.\\nThree popular choices for Kin the SVM literature are\\ndth-Degree polynomial: K(x,x′) = (1 + ⟨x,x′⟩)d,\\nRadial basis: K(x,x′) = exp( −γ∥x−x′∥2),\\nNeural network: K(x,x′) = tanh( κ1⟨x,x′⟩+κ2).(12.22)\\nConsider for example a feature space with two inputs X1andX2, and a\\npolynomial kernel of degree 2. Then\\nK(X,X′) = (1 + ⟨X,X′⟩)2\\n= (1 + X1X′\\n1+X2X′\\n2)2\\n= 1 + 2 X1X′\\n1+ 2X2X′\\n2+ (X1X′\\n1)2+ (X2X′\\n2)2+ 2X1X′\\n1X2X′\\n2.\\n(12.23)\\nThen M= 6, and if we choose h1(X) = 1, h2(X) =√\\n2X1,h3(X) =√\\n2X2,h4(X) =X2\\n1,h5(X) =X2\\n2, andh6(X) =√\\n2X1X2, then K(X,X′) =\\n⟨h(X),h(X′)⟩. From (12.20) we see that the solution can be written\\nˆf(x) =N∑\\ni=1ˆαiyiK(x,xi) +ˆβ0. (12.24)\\nThe role of the parameter Cis clearer in an enlarged feature space,\\nsince perfect separation is often achievable there. A large value of Cwill\\ndiscourage any positive ξi, and lead to an overﬁt wiggly boundary in the\\noriginal feature space; a small value of Cwill encourage a small value of\\n∥β∥, which in turn causes f(x) and hence the boundary to be smoother.\\nFigure 12.3 show two nonlinear support vector machines applied to the\\nmixture example of Chapter 2. The regularization parameter was chosen\\nin both cases to achieve good test error. The radial basis kernel produces\\na boundary quite similar to the Bayes optimal boundary for this example;\\ncompare Figure 2.5.\\nIn the early literature on support vectors, there were claims that the\\nkernel property of the support vector machine is unique to it and allows\\none to ﬁnesse the curse of dimensionality. Neither of these claims is true,\\nand we go into both of these issues in the next three subsections.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='12.3 Support Vector Machines and Kernels 425\\nSVM - Degree-4 Polynomial in Feature Space'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . .. . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•••••••\\n•\\n• ••\\n•\\n•••\\nTraining Error: 0.180\\nTest Error:       0.245\\nBayes Error:    0.210\\nSVM - Radial Kernel in Feature Space'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . .. . . .. . . .. . . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 443}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•\\n•••\\n•\\n••••\\n••••\\n••••\\n••\\n••\\n••••\\n••\\n•\\n••\\n•\\nTraining Error: 0.160\\nTest Error:       0.218\\nBayes Error:    0.210\\nFIGURE 12.3. Two nonlinear SVMs for the mixture data. The upper plot uses\\na4th degree polynomial kernel, the lower a radial basis kernel (wi thγ= 1). In\\neach case Cwas tuned to approximately achieve the best test error perform ance,\\nandC= 1worked well in both cases. The radial basis kernel performs th e best\\n(close to Bayes optimal), as might be expected given the data a rise from mixtures\\nof Gaussians. The broken purple curve in the background is the B ayes decision\\nboundary.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 444}, page_content='426 12. Flexible Discriminants\\n−3 −2 −1 0 1 2 30.0 0.5 1.0 1.5 2.0 2.5 3.0Hinge Loss\\nBinomial Deviance\\nSquared Error\\nClass HuberLoss\\nyf\\nFIGURE 12.4. The support vector loss function (hinge loss), compared to the\\nnegative log-likelihood loss (binomial deviance) for logisti c regression, squared-er-\\nror loss, and a “Huberized” version of the squared hinge loss. A ll are shown as a\\nfunction of yfrather than f, because of the symmetry between the y= +1 and\\ny=−1case. The deviance and Huber have the same asymptotes as the SVM\\nloss, but are rounded in the interior. All are scaled to have the limiting left-tail\\nslope of −1.\\n12.3.2 The SVM as a Penalization Method\\nWith f(x) =h(x)Tβ+β0, consider the optimization problem\\nmin\\nβ0, βN∑\\ni=1[1−yif(xi)]++λ\\n2∥β∥2(12.25)\\nwhere the subscript “+” indicates positive part. This has the form loss+\\npenalty , which is a familiar paradigm in function estimation. It is easy to\\nshow (Exercise 12.1) that the solution to (12.25), with λ= 1/C, is the\\nsame as that for (12.8).\\nExamination of the “hinge” loss function L(y,f) = [1 −yf]+shows that\\nit is reasonable for two-class classiﬁcation, when compared to other more\\ntraditional loss functions. Figure 12.4 compares it to the log-likelihood l oss\\nfor logistic regression, as well as squared-error loss and a variant thereo f.\\nThe (negative) log-likelihood or binomial deviance has similar tails as the\\nSVM loss, giving zero penalty to points well inside their margin, and a'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 445}, page_content='12.3 Support Vector Machines and Kernels 427\\nTABLE 12.1. The population minimizers for the diﬀerent loss functions in Fig -\\nure 12.4. Logistic regression uses the binomial log-likelih ood or deviance. Linear\\ndiscriminant analysis (Exercise 4.2) uses squared-error loss. The SVM hinge loss\\nestimates the mode of the posterior class probabilities, wh ereas the others estimate\\na linear transformation of these probabilities.\\nLoss Function L[y, f(x)] Minimizing Function\\nBinomial\\nDeviance log[1 + e−yf(x)]f(x) = logPr(Y= +1|x)\\nPr(Y= -1|x)\\nSVM Hinge\\nLoss[1−yf(x)]+ f(x) = sign[Pr( Y= +1|x)−1\\n2]\\nSquared\\nError[y−f(x)]2= [1−yf(x)]2f(x) = 2Pr( Y= +1|x)−1\\n“Huberised”\\nSquare\\nHinge Loss−4yf(x), yf (x)<-1\\n[1−yf(x)]2\\n+otherwisef(x) = 2Pr( Y= +1|x)−1\\nlinear penalty to points on the wrong side and far away. Squared-error, on\\nthe other hand gives a quadratic penalty, and points well inside their own\\nmargin have a strong inﬂuence on the model as well. The squared hinge\\nlossL(y,f) = [1 −yf]2\\n+is like the quadratic, except it is zero for points\\ninside their margin. It still rises quadratically in the left tail, and wil l be\\nless robust than hinge or deviance to misclassiﬁed observations. Recently\\nRosset and Zhu (2007) proposed a “Huberized” version of the squared hinge\\nloss, which converts smoothly to a linear loss at yf=−1.\\nWe can characterize these loss functions in terms of what they are es-\\ntimating at the population level. We consider minimizing E L(Y,f(X)).\\nTable 12.1 summarizes the results. Whereas the hinge loss estimates the\\nclassiﬁer G(x) itself, all the others estimate a transformation of the class\\nposterior probabilities. The “Huberized” square hinge loss shares attractive\\nproperties of logistic regression (smooth loss function, estimates proba bili-\\nties), as well as the SVM hinge loss (support points).\\nFormulation (12.25) casts the SVM as a regularized function estimation\\nproblem, where the coeﬃcients of the linear expansion f(x) =β0+h(x)Tβ\\nare shrunk toward zero (excluding the constant). If h(x) represents a hierar-\\nchical basis having some ordered structure (such as ordered in roughness),'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 446}, page_content='428 12. Flexible Discriminants\\nthen the uniform shrinkage makes more sense if the rougher elements hjin\\nthe vector hhave smaller norm.\\nAll the loss-function in Table 12.1 except squared-error are so called\\n“margin maximizing loss-functions” (Rosset et al., 2004b). This means that\\nif the data are separable, then the limit of ˆβλin (12.25) as λ→0 deﬁnes\\nthe optimal separating hyperplane1.\\n12.3.3 Function Estimation and Reproducing Kernels\\nHere we describe SVMs in terms of function estimation in reproducing\\nkernel Hilbert spaces, where the kernel property abounds. This material is\\ndiscussed in some detail in Section 5.8. This provides another view of the\\nsupport vector classiﬁer, and helps to clarify how it works.\\nSuppose the basis harises from the (possibly ﬁnite) eigen-expansion of\\na positive deﬁnite kernel K,\\nK(x,x′) =∞∑\\nm=1φm(x)φm(x′)δm (12.26)\\nandhm(x) =√δmφm(x). Then with θm=√δmβm, we can write (12.25)\\nas\\nmin\\nβ0, θN∑\\ni=1[\\n1−yi(β0+∞∑\\nm=1θmφm(xi))]\\n++λ\\n2∞∑\\nm=1θ2\\nm\\nδm. (12.27)\\nNow (12.27) is identical in form to (5.49) on page 169 in Section 5.8, a nd\\nthe theory of reproducing kernel Hilbert spaces described there guarantees\\na ﬁnite-dimensional solution of the form\\nf(x) =β0+N∑\\ni=1αiK(x,xi). (12.28)\\nIn particular we see there an equivalent version of the optimization crite-\\nrion (12.19) [Equation (5.67) in Section 5.8.2; see also Wahba et al. (2000)],\\nmin\\nβ0,αN∑\\ni=1(1−yif(xi))++λ\\n2αTKα, (12.29)\\nwhereKis the N×Nmatrix of kernel evaluations for all pairs of training\\nfeatures (Exercise 12.2).\\nThese models are quite general, and include, for example, the entire fam-\\nily of smoothing splines, additive and interaction spline models discussed\\n1For logistic regression with separable data, ˆβλdiverges, but ˆβλ/||ˆβλconverges to\\nthe optimal separating direction.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 447}, page_content='12.3 Support Vector Machines and Kernels 429\\nin Chapters 5 and 9, and in more detail in Wahba (1990) and Hastie and\\nTibshirani (1990). They can be expressed more generally as\\nmin\\nf∈HN∑\\ni=1[1−yif(xi)]++λJ(f), (12.30)\\nwhere His the structured space of functions, and J(f) an appropriate reg-\\nularizer on that space. For example, suppose His the space of additive\\nfunctions f(x) =∑p\\nj=1fj(xj), and J(f) =∑\\nj∫\\n{f′′\\nj(xj)}2dxj. Then the\\nsolution to (12.30) is an additive cubic spline, and has a kernel representa-\\ntion (12.28) with K(x,x′) =∑p\\nj=1Kj(xj,x′\\nj). Each of the Kjis the kernel\\nappropriate for the univariate smoothing spline in xj(Wahba, 1990).\\nConversely this discussion also shows that, for example, anyof the kernels\\ndescribed in (12.22) above can be used with anyconvex loss function, and\\nwill also lead to a ﬁnite-dimensional representation of the form (12.28).\\nFigure 12.5 uses the same kernel functions as in Figure 12.3, except using\\nthe binomial log-likelihood as a loss function2. The ﬁtted function is hence\\nan estimate of the log-odds,\\nˆf(x) = logˆPr(Y= +1|x)\\nˆPr(Y=−1|x)\\n=ˆβ0+N∑\\ni=1ˆαiK(x,xi), (12.31)\\nor conversely we get an estimate of the class probabilities\\nˆPr(Y= +1|x) =1\\n1 +e−ˆβ0−PN\\ni=1ˆαiK(x,xi). (12.32)\\nThe ﬁtted models are quite similar in shape and performance. Examples\\nand more details are given in Section 5.8.\\nIt does happen that for SVMs, a sizable fraction of the Nvalues of αi\\ncan be zero (the nonsupport points). In the two examples in Figure 12.3,\\nthese fractions are 42% and 45%, respectively. This is a consequence of the\\npiecewise linear nature of the ﬁrst part of the criterion (12.25). The lower\\nthe class overlap (on the training data), the greater this fraction will be.\\nReducing λwill generally reduce the overlap (allowing a more ﬂexible f).\\nA small number of support points means that ˆf(x) can be evaluated more\\nquickly, which is important at lookup time. Of course, reducing the overlap\\ntoo much can lead to poor generalization.\\n2Ji Zhu assisted in the preparation of these examples.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='430 12. Flexible Discriminants\\nLR - Degree-4 Polynomial in Feature Space'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . .. . . . . . .. . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.190\\nTest Error:       0.263\\nBayes Error:    0.210\\nLR - Radial Kernel in Feature Space'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='.. . . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ..'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 448}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.150\\nTest Error:       0.221\\nBayes Error:    0.210\\nFIGURE 12.5. The logistic regression versions of the SVM models in Fig-\\nure 12.3, using the identical kernels and hence penalties, but the l og-likelihood\\nloss instead of the SVM loss function. The two broken contours corr espond to\\nposterior probabilities of 0.75and0.25for the +1 class (or vice versa). The bro-\\nken purple curve in the background is the Bayes decision bounda ry.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 449}, page_content='12.3 Support Vector Machines and Kernels 431\\nTABLE 12.2. Skin of the orange: Shown are mean (standard error of the mean)\\nof the test error over 50simulations. BRUTO ﬁts an additive spline model adap-\\ntively, while MARS ﬁts a low-order interaction model adaptivel y.\\nTest Error (SE)\\nMethod No Noise Features Six Noise Features\\n1 SV Classiﬁer 0.450 (0.003) 0.472 (0.003)\\n2 SVM/poly 2 0.078 (0.003) 0.152 (0.004)\\n3 SVM/poly 5 0.180 (0.004) 0.370 (0.004)\\n4 SVM/poly 10 0.230 (0.003) 0.434 (0.002)\\n5 BRUTO 0.084 (0.003) 0.090 (0.003)\\n6 MARS 0.156 (0.004) 0.173 (0.005)\\nBayes 0.029 0.029\\n12.3.4 SVMs and the Curse of Dimensionality\\nIn this section, we address the question of whether SVMs have some edge\\non the curse of dimensionality. Notice that in expression (12.23) we are not\\nallowed a fully general inner product in the space of powers and products.\\nFor example, all terms of the form 2 XjX′\\njare given equal weight, and the\\nkernel cannot adapt itself to concentrate on subspaces. If the number of\\nfeatures pwere large, but the class separation occurred only in the linear\\nsubspace spanned by say X1andX2, this kernel would not easily ﬁnd the\\nstructure and would suﬀer from having many dimensions to search over.\\nOne would have to build knowledge about the subspace into the kernel;\\nthat is, tell it to ignore all but the ﬁrst two inputs. If such knowledge were\\navailable a priori, much of statistical learning would be made much easier .\\nA major goal of adaptive methods is to discover such structure.\\nWe support these statements with an illustrative example. We generated\\n100 observations in each of two classes. The ﬁrst class has four standard\\nnormal independent features X1,X2,X3,X4. The second class also has four\\nstandard normal independent features, but conditioned on 9 ≤∑X2\\nj≤16.\\nThis is a relatively easy problem. As a second harder problem, we aug-\\nmented the features with an additional six standard Gaussian noise fea-\\ntures. Hence the second class almost completely surrounds the ﬁrst, like the\\nskin surrounding the orange, in a four-dimensional subspace. The Bayes er-\\nror rate for this problem is 0 .029 (irrespective of dimension). We generated\\n1000 test observations to compare diﬀerent procedures. The average test\\nerrors over 50 simulations, with and without noise features, are shown in\\nTable 12.2.\\nLine 1 uses the support vector classiﬁer in the original feature space.\\nLines 2–4 refer to the support vector machine with a 2-, 5- and 10-dimension-\\nal polynomial kernel. For all support vector procedures, we chose the cost\\nparameter Cto minimize the test error, to be as fair as possible to the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 450}, page_content='432 12. Flexible Discriminants\\n1e−01 1e+01 1e+030.20 0.25 0.30 0.35\\n1e−01 1e+01 1e+03 1e−01 1e+01 1e+03 1e−01 1e+01 1e+03Test Error\\nCTest Error Curves − SVM with Radial Kernel\\nγ= 5 γ= 1 γ= 0.5 γ= 0.1\\nFIGURE 12.6. Test-error curves as a function of the cost parameter Cfor the\\nradial-kernel SVM classiﬁer on the mixture data. At the top of eac h plot is the\\nscale parameter γfor the radial kernel: Kγ(x, y) = exp −γ||x−y||2. The optimal\\nvalue for Cdepends quite strongly on the scale of the kernel. The Bayes erro r\\nrate is indicated by the broken horizontal lines.\\nmethod. Line 5 ﬁts an additive spline model to the ( −1,+1) response by\\nleast squares, using the BRUTO algorithm for additive models, described\\nin Hastie and Tibshirani (1990). Line 6 uses MARS (multivariate adaptiv e\\nregression splines) allowing interaction of all orders, as described in Chap-\\nter 9; as such it is comparable with the SVM/poly 10. Both BRUTO and\\nMARS have the ability to ignore redundant variables. Test error was not\\nused to choose the smoothing parameters in either of lines 5 or 6.\\nIn the original feature space, a hyperplane cannot separate the classes,\\nand the support vector classiﬁer (line 1) does poorly. The polynomial sup-\\nport vector machine makes a substantial improvement in test error rate,\\nbut is adversely aﬀected by the six noise features. It is also very sensitive to\\nthe choice of kernel: the second degree polynomial kernel (line 2) does best,\\nsince the true decision boundary is a second-degree polynomial. However,\\nhigher-degree polynomial kernels (lines 3 and 4) do much worse. BRUTO\\nperforms well, since the boundary is additive. BRUTO and MARS adapt\\nwell: their performance does not deteriorate much in the presence of noise.\\n12.3.5 A Path Algorithm for the SVM Classiﬁer\\nThe regularization parameter for the SVM classiﬁer is the cost parameter\\nC, or its inverse λin (12.25). Common usage is to set Chigh, leading often\\nto somewhat overﬁt classiﬁers.\\nFigure 12.6 shows the test error on the mixture data as a function of\\nC, using diﬀerent radial-kernel parameters γ. When γ= 5 (narrow peaked\\nkernels), the heaviest regularization (small C) is called for. With γ= 1'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 451}, page_content='12.3 Support Vector Machines and Kernels 433\\n−0.5 0.0 0.5 1.0 1.5 2.0−1.0 −0.5 0.0 0.5 1.0 1.5789\\n1011\\n12\\n123\\n45\\n61/||β|| f(x) = 0f(x) = +1\\nf(x) =−1\\n0.0 0.2 0.4 0.6 0.8 1.00 2 4 6 8 101\\n2\\n34\\n56\\n789\\n1011\\n12\\nαi(λ)λ\\nFIGURE 12.7. A simple example illustrates the SVM path algorithm. (left\\npanel:) This plot illustrates the state of the model at λ= 0.05. The ‘ ‘ + 1”\\npoints are orange, the “ −1” blue. λ= 1/2, and the width of the soft margin\\nis2/||β||= 2×0.587. Two blue points {3,5}are misclassiﬁed, while the two or-\\nange points {10,12}are correctly classiﬁed, but on the wrong side of their margin\\nf(x) = +1 ; each of these has yif(xi)<1. The three square shaped points {2,6,7}\\nare exactly on their margins. (right panel:) This plot shows the piecewise linear\\nproﬁles αi(λ). The horizontal broken line at λ= 1/2indicates the state of the αi\\nfor the model in the left plot.\\n(the value used in Figure 12.3), an intermediate value of Cis required.\\nClearly in situations such as these, we need to determine a good choice\\nforC, perhaps by cross-validation. Here we describe a path algorithm (in\\nthe spirit of Section 3.8) for eﬃciently ﬁtting the entire sequence of SVM\\nmodels obtained by varying C.\\nIt is convenient to use the loss+penalty formulation (12.25), along with\\nFigure 12.4. This leads to a solution for βat a given value of λ:\\nβλ=1\\nλN∑\\ni=1αiyixi. (12.33)\\nTheαiare again Lagrange multipliers, but in this case they all lie in [0 ,1].\\nFigure 12.7 illustrates the setup. It can be shown that the KKT optimal-\\nity conditions imply that the labeled points ( xi,yi) fall into three distinct\\ngroups:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 452}, page_content='434 12. Flexible Discriminants\\n•Observations correctly classiﬁed and outside their margins. They have\\nyif(xi)>1, and Lagrange multipliers αi= 0. Examples are the\\norange points 8, 9 and 11, and the blue points 1 and 4.\\n•Observations sitting on their margins with yif(xi) = 1, with Lagrange\\nmultipliers αi∈[0,1]. Examples are the orange 7 and the blue 2 and\\n8.\\n•Observations inside their margins have yif(xi)<1, with αi= 1.\\nExamples are the blue 3 and 5, and the orange 10 and 12.\\nThe idea for the path algorithm is as follows. Initially λis large, the\\nmargin 1 /||βλ||is wide, and all points are inside their margin and have\\nαi= 1. As λdecreases, 1 /||βλ||decreases, and the margin gets narrower.\\nSome points will move from inside their margins to outside their margins,\\nand their αiwill change from 1 to 0. By continuity of the αi(λ), these points\\nwilllinger on the margin during this transition. From (12.33) we see that\\nthe points with αi= 1 make ﬁxed contributions to β(λ), and those with\\nαi= 0 make no contribution. So all that changes as λdecreases are the\\nαi∈[0,1] of those (small number) of points on the margin. Since all these\\npoints have yif(xi) = 1, this results in a small set of linear equations that\\nprescribe how αi(λ) and hence βλchanges during these transitions. This\\nresults in piecewise linear paths for each of the αi(λ). The breaks occur\\nwhen points cross the margin. Figure 12.7 (right panel) shows the αi(λ)\\nproﬁles for the small example in the left panel.\\nAlthough we have described this for linear SVMs, exactly the same idea\\nworks for nonlinear models, in which (12.33) is replaced by\\nfλ(x) =1\\nλN∑\\ni=1αiyiK(x,xi). (12.34)\\nDetails can be found in Hastie et al. (2004). An Rpackagesvmpath is\\navailable on CRAN for ﬁtting these models.\\n12.3.6 Support Vector Machines for Regression\\nIn this section we show how SVMs can be adapted for regression with a\\nquantitative response, in ways that inherit some of the properties of the\\nSVM classiﬁer. We ﬁrst discuss the linear regression model\\nf(x) =xTβ+β0, (12.35)\\nand then handle nonlinear generalizations. To estimate β, we consider min-\\nimization of\\nH(β,β0) =N∑\\ni=1V(yi−f(xi)) +λ\\n2∥β∥2, (12.36)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 453}, page_content='12.3 Support Vector Machines and Kernels 435\\n-4 -2 0 2 4-1 0 1 2 3 4\\n-4 -2 0 2 40 2 4 6 8 10 12\\nǫ −ǫ c −cVH(r)Vǫ(r)\\nr r\\nFIGURE 12.8. The left panel shows the ǫ-insensitive error function used by the\\nsupport vector regression machine. The right panel shows the e rror function used\\nin Huber’s robust regression (blue curve). Beyond |c|, the function changes from\\nquadratic to linear.\\nwhere\\nVǫ(r) ={\\n0 if |r|< ǫ,\\n|r| −ǫ,otherwise.(12.37)\\nThis is an “ ǫ-insensitive” error measure, ignoring errors of size less than\\nǫ(left panel of Figure 12.8). There is a rough analogy with the support\\nvector classiﬁcation setup, where points on the correct side of the deci-\\nsion boundary and far away from it, are ignored in the optimization. In\\nregression, these “low error” points are the ones with small residuals.\\nIt is interesting to contrast this with error measures used in robust re-\\ngression in statistics. The most popular, due to Huber (1964), has the for m\\nVH(r) ={\\nr2/2 if |r| ≤c,\\nc|r| −c2/2,|r|> c,(12.38)\\nshown in the right panel of Figure 12.8. This function reduces from quadratic\\nto linear the contributions of observations with absolute residual greater\\nthan a prechosen constant c. This makes the ﬁtting less sensitive to out-\\nliers. The support vector error measure (12.37) also has linear tails (beyo nd\\nǫ), but in addition it ﬂattens the contributions of those cases with small\\nresiduals.\\nIfˆβ,ˆβ0are the minimizers of H, the solution function can be shown to\\nhave the form\\nˆβ=N∑\\ni=1(ˆα∗\\ni−ˆαi)xi, (12.39)\\nˆf(x) =N∑\\ni=1(ˆα∗\\ni−ˆαi)⟨x,xi⟩+β0, (12.40)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 454}, page_content='436 12. Flexible Discriminants\\nwhere ˆ αi,ˆα∗\\niare positive and solve the quadratic programming problem\\nmin\\nαi,α∗\\niǫN∑\\ni=1(α∗\\ni+αi)−N∑\\ni=1yi(α∗\\ni−αi) +1\\n2N∑\\ni,i′=1(α∗\\ni−αi)(α∗\\ni′−αi′)⟨xi,xi′⟩\\nsubject to the constraints\\n0≤αi, α∗\\ni≤1/λ,\\nN∑\\ni=1(α∗\\ni−αi) = 0, (12.41)\\nαiα∗\\ni= 0.\\nDue to the nature of these constraints, typically only a subset of the solution\\nvalues (ˆ α∗\\ni−ˆαi) are nonzero, and the associated data values are called the\\nsupport vectors. As was the case in the classiﬁcation setting, the solution\\ndepends on the input values only through the inner products ⟨xi,xi′⟩. Thus\\nwe can generalize the methods to richer spaces by deﬁning an appropriate\\ninner product, for example, one of those deﬁned in (12.22).\\nNote that there are parameters, ǫandλ, associated with the criterion\\n(12.36). These seem to play diﬀerent roles. ǫis a parameter of the loss\\nfunction Vǫ, just like cis for VH. Note that both VǫandVHdepend on the\\nscale of yand hence r. If we scale our response (and hence use VH(r/σ) and\\nVǫ(r/σ) instead), then we might consider using preset values for candǫ(the\\nvalue c= 1.345 achieves 95% eﬃciency for the Gaussian). The quantity λ\\nis a more traditional regularization parameter, and can be estimated for\\nexample by cross-validation.\\n12.3.7 Regression and Kernels\\nAs discussed in Section 12.3.3, this kernel property is not unique to sup-\\nport vector machines. Suppose we consider approximation of the regression\\nfunction in terms of a set of basis functions {hm(x)},m= 1,2,... ,M :\\nf(x) =M∑\\nm=1βmhm(x) +β0. (12.42)\\nTo estimate βandβ0we minimize\\nH(β,β0) =N∑\\ni=1V(yi−f(xi)) +λ\\n2∑\\nβ2\\nm (12.43)\\nfor some general error measure V(r). For any choice of V(r), the solution\\nˆf(x) =∑ˆβmhm(x) +ˆβ0has the form\\nˆf(x) =N∑\\ni=1ˆaiK(x,xi) (12.44)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 455}, page_content='12.3 Support Vector Machines and Kernels 437\\nwithK(x,y) =∑M\\nm=1hm(x)hm(y). Notice that this has the same form\\nas both the radial basis function expansion and a regularization estimate,\\ndiscussed in Chapters 5 and 6.\\nFor concreteness, let’s work out the case V(r) =r2. LetHbe the N×M\\nbasis matrix with imth element hm(xi), and suppose that M > N is large.\\nFor simplicity we assume that β0= 0, or that the constant is absorbed in\\nh; see Exercise 12.3 for an alternative.\\nWe estimate βby minimizing the penalized least squares criterion\\nH(β) = (y−Hβ)T(y−Hβ) +λ∥β∥2. (12.45)\\nThe solution is\\nˆy=Hˆβ (12.46)\\nwithˆβdetermined by\\n−HT(y−Hˆβ) +λˆβ= 0. (12.47)\\nFrom this it appears that we need to evaluate the M×Mmatrix of inner\\nproducts in the transformed space. However, we can premultiply by Hto\\ngive\\nHˆβ= (HHT+λI)−1HHTy. (12.48)\\nTheN×Nmatrix HHTconsists of inner products between pairs of obser-\\nvations i,i′; that is, the evaluation of an inner product kernel {HHT}i,i′=\\nK(xi,xi′). It is easy to show (12.44) directly in this case, that the predicted\\nvalues at an arbitrary xsatisfy\\nˆf(x) = h(x)Tˆβ\\n=N∑\\ni=1ˆαiK(x,xi), (12.49)\\nwhere ˆ α= (HHT+λI)−1y. As in the support vector machine, we need not\\nspecify or evaluate the large set of functions h1(x),h2(x),... ,h M(x). Only\\nthe inner product kernel K(xi,xi′) need be evaluated, at the Ntraining\\npoints for each i,i′and at points xfor predictions there. Careful choice\\nofhm(such as the eigenfunctions of particular, easy-to-evaluate kernels\\nK) means, for example, that HHTcan be computed at a cost of N2/2\\nevaluations of K, rather than the direct cost N2M.\\nNote, however, that this property depends on the choice of squared norm\\n∥β∥2in the penalty. It does not hold, for example, for the L1norm |β|,\\nwhich may lead to a superior model.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 456}, page_content='438 12. Flexible Discriminants\\n12.3.8 Discussion\\nThe support vector machine can be extended to multiclass problems, es-\\nsentially by solving many two-class problems. A classiﬁer is built for each\\npair of classes, and the ﬁnal classiﬁer is the one that dominates the most\\n(Kressel, 1999; Friedman, 1996; Hastie and Tibshirani, 1998). Alternati vely,\\none could use the multinomial loss function along with a suitable kernel,\\nas in Section 12.3.3. SVMs have applications in many other supervised\\nand unsupervised learning problems. At the time of this writing, empirical\\nevidence suggests that it performs well in many real learning problems.\\nFinally, we mention the connection of the support vector machine and\\nstructural risk minimization (7.9). Suppose the training points (or their\\nbasis expansion) are contained in a sphere of radius R, and let G(x) =\\nsign[f(x)] = sign[ βTx+β0] as in (12.2). Then one can show that the class\\nof functions {G(x),∥β∥ ≤A}has VC-dimension hsatisfying\\nh≤R2A2. (12.50)\\nIff(x) separates the training data, optimally for ∥β∥ ≤A, then with\\nprobability at least 1 −ηover training sets (Vapnik, 1996, page 139):\\nError Test≤4h[log (2 N/h) + 1]−log (η/4)\\nN. (12.51)\\nThe support vector classiﬁer was one of the ﬁrst practical learning pro-\\ncedures for which useful bounds on the VC dimension could be obtained,\\nand hence the SRM program could be carried out. However in the deriva-\\ntion, balls are put around the data points—a process that depends on the\\nobserved values of the features. Hence in a strict sense, the VC complexity\\nof the class is not ﬁxed a priori, before seeing the features.\\nThe regularization parameter Ccontrols an upper bound on the VC\\ndimension of the classiﬁer. Following the SRM paradigm, we could choose C\\nby minimizing the upper bound on the test error, given in (12.51). However,\\nit is not clear that this has any advantage over the use of cross-validation\\nfor choice of C.\\n12.4 Generalizing Linear Discriminant Analysis\\nIn Section 4.3 we discussed linear discriminant analysis (LDA), a funda-\\nmental tool for classiﬁcation. For the remainder of this chapter we discuss\\na class of techniques that produce better classiﬁers than LDA by directly\\ngeneralizing LDA.\\nSome of the virtues of LDA are as follows:\\n•It is a simple prototype classiﬁer. A new observation is classiﬁed to the\\nclass with closest centroid. A slight twist is that distance is measured\\nin the Mahalanobis metric, using a pooled covariance estimate.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 457}, page_content='12.4 Generalizing Linear Discriminant Analysis 439\\n•LDA is the estimated Bayes classiﬁer if the observations are multi-\\nvariate Gaussian in each class, with a common covariance matrix.\\nSince this assumption is unlikely to be true, this might not seem to\\nbe much of a virtue.\\n•The decision boundaries created by LDA are linear, leading to deci-\\nsion rules that are simple to describe and implement.\\n•LDA provides natural low-dimensional views of the data. For exam-\\nple, Figure 12.12 is an informative two-dimensional view of data in\\n256 dimensions with ten classes.\\n•Often LDA produces the best classiﬁcation results, because of its\\nsimplicity and low variance. LDA was among the top three classiﬁers\\nfor 11 of the 22 datasets studied in the STATLOG project (Michie et\\nal., 1994)3.\\nUnfortunately the simplicity of LDA causes it to fail in a number of situa-\\ntions as well:\\n•Often linear decision boundaries do not adequately separate the classes.\\nWhen Nis large, it is possible to estimate more complex decision\\nboundaries. Quadratic discriminant analysis (QDA) is often useful\\nhere, and allows for quadratic decision boundaries. More generally\\nwe would like to be able to model irregular decision boundaries.\\n•The aforementioned shortcoming of LDA can often be paraphrased\\nby saying that a single prototype per class is insuﬃcient. LDA uses\\na single prototype (class centroid) plus a common covariance matrix\\nto describe the spread of the data in each class. In many situations,\\nseveral prototypes are more appropriate.\\n•At the other end of the spectrum, we may have way too many (corre-\\nlated) predictors, for example, in the case of digitized analogue signals\\nand images. In this case LDA uses too many parameters, which are\\nestimated with high variance, and its performance suﬀers. In cases\\nsuch as this we need to restrict or regularize LDA even further.\\nIn the remainder of this chapter we describe a class of techniques that\\nattend to all these issues by generalizing the LDA model. This is achieved\\nlargely by three diﬀerent ideas.\\nThe ﬁrst idea is to recast the LDA problem as a linear regression problem.\\nMany techniques exist for generalizing linear regression to more ﬂexible,\\nnonparametric forms of regression. This in turn leads to more ﬂexible forms\\nof discriminant analysis, which we call FDA. In most cases of interest, t he\\n3This study predated the emergence of SVMs.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 458}, page_content='440 12. Flexible Discriminants\\nregression procedures can be seen to identify an enlarged set of predictors\\nvia basis expansions. FDA amounts to LDA in this enlarged space, the\\nsame paradigm used in SVMs.\\nIn the case of too many predictors, such as the pixels of a digitized image,\\nwe do not want to expand the set: it is already too large. The second idea is\\nto ﬁt an LDA model, but penalize its coeﬃcients to be smooth or otherwise\\ncoherent in the spatial domain, that is, as an image. We call this procedure\\npenalized discriminant analysis or PDA. With FDA itself, the expanded\\nbasis set is often so large that regularization is also required (again a s in\\nSVMs). Both of these can be achieved via a suitably regularized regression\\nin the context of the FDA model.\\nThe third idea is to model each class by a mixture of two or more Gaus-\\nsians with diﬀerent centroids, but with every component Gaussian, both\\nwithin and between classes, sharing the same covariance matrix. This allows\\nfor more complex decision boundaries, and allows for subspace reduction\\nas in LDA. We call this extension mixture discriminant analysis or MDA.\\nAll three of these generalizations use a common framework by exploiting\\ntheir connection with LDA.\\n12.5 Flexible Discriminant Analysis\\nIn this section we describe a method for performing LDA using linear re-\\ngression on derived responses. This in turn leads to nonparametric and ﬂex-\\nible alternatives to LDA. As in Chapter 4, we assume we have observations\\nwith a quantitative response Gfalling into one of Kclasses G={1,... ,K },\\neach having measured features X. Suppose θ:G ↦→IR1is a function that\\nassigns scores to the classes, such that the transformed class labels are op-\\ntimally predicted by linear regression on X: If our training sample has the\\nform ( gi,xi), i= 1,2,... ,N , then we solve\\nmin\\nβ,θN∑\\ni=1(\\nθ(gi)−xT\\niβ)2, (12.52)\\nwith restrictions on θto avoid a trivial solution (mean zero and unit vari-\\nance over the training data). This produces a one-dimensional separation\\nbetween the classes.\\nMore generally, we can ﬁnd up to L≤K−1 sets of independent scorings\\nfor the class labels, θ1,θ2,... ,θ L, andLcorresponding linear maps ηℓ(X) =\\nXTβℓ, ℓ= 1,... ,L , chosen to be optimal for multiple regression in IRp. The\\nscores θℓ(g) and the maps βℓare chosen to minimize the average squared\\nresidual,\\nASR=1\\nNL∑\\nℓ=1[N∑\\ni=1(\\nθℓ(gi)−xT\\niβℓ)2]\\n. (12.53)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 459}, page_content='12.5 Flexible Discriminant Analysis 441\\nThe set of scores are assumed to be mutually orthogonal and normalized\\nwith respect to an appropriate inner product to prevent trivial zero\\nsolutions.\\nWhy are we going down this road? It can be shown that the sequence\\nof discriminant (canonical) vectors νℓderived in Section 4.3.3 are identical\\nto the sequence βℓup to a constant (Mardia et al., 1979; Hastie et al.,\\n1995). Moreover, the Mahalanobis distance of a test point xto the kth\\nclass centroid ˆ θkis given by\\nδJ(x,ˆθk) =K−1∑\\nℓ=1wℓ(ˆηℓ(x)−¯ηk\\nℓ)2+D(x), (12.54)\\nwhere ¯ ηk\\nℓis the mean of the ˆ ηℓ(xi) in the kth class, and D(x) does not\\ndepend on k. Here wℓare coordinate weights that are deﬁned in terms of\\nthe mean squared residual r2\\nℓof the ℓth optimally scored ﬁt\\nwℓ=1\\nr2\\nℓ(1−r2\\nℓ). (12.55)\\nIn Section 4.3.2 we saw that these canonical distances are all that is needed\\nfor classiﬁcation in the Gaussian setup, with equal covariances in each class.\\nTo summarize:\\nLDA can be performed by a sequence of linear regressions, fol-\\nlowed by classiﬁcation to the closest class centroid in the space\\nof ﬁts. The analogy applies both to the reduced rank version,\\nor the full rank case when L=K−1.\\nThe real power of this result is in the generalizations that it invites. We\\ncan replace the linear regression ﬁts ηℓ(x) =xTβℓby far more ﬂexible,\\nnonparametric ﬁts, and by analogy achieve a more ﬂexible classiﬁer than\\nLDA. We have in mind generalized additive ﬁts, spline functions, MARS\\nmodels and the like. In this more general form the regression problems are\\ndeﬁned via the criterion\\nASR({θℓ,ηℓ}L\\nℓ=1) =1\\nNL∑\\nℓ=1[N∑\\ni=1(θℓ(gi)−ηℓ(xi))2+λJ(ηℓ)]\\n,(12.56)\\nwhere Jis a regularizer appropriate for some forms of nonparametric regres-\\nsion, such as smoothing splines, additive splines and lower-order ANOVA\\nspline models. Also included are the classes of functions and associated\\npenalties generated by kernels, as in Section 12.3.3.\\nBefore we describe the computations involved in this generalization, let\\nus consider a very simple example. Suppose we use degree-2 polynomial\\nregression for each ηℓ. The decision boundaries implied by the (12.54) will\\nbe quadratic surfaces, since each of the ﬁtted functions is quadratic, and as'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 460}, page_content='442 12. Flexible Discriminants\\n-2 0 2-2 0 2o\\nooo\\noo\\nooo\\noo\\no o\\no\\noooo\\noo\\noo oo\\no\\no\\nooo\\noooo\\noo\\nooo\\noo\\noo\\noo\\noo\\nooo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\nooo\\noo o\\noo\\nooo\\noo\\no\\nFIGURE 12.9. The data consist of 50points generated from each of N(0, I)and\\nN(0,9\\n4I). The solid black ellipse is the decision boundary found by FDA u sing\\ndegree-two polynomial regression. The dashed purple circle i s the Bayes decision\\nboundary.\\nin LDA their squares cancel out when comparing distances. We could have\\nachieved identical quadratic boundaries in a more conventional way, by\\naugmenting our original predictors with their squares and cross-products.\\nIn the enlarged space one performs an LDA, and the linear boundaries in\\nthe enlarged space map down to quadratic boundaries in the original space.\\nA classic example is a pair of multivariate Gaussians centered at the origi n,\\none having covariance matrix I, and the other cIforc >1; Figure 12.9\\nillustrates. The Bayes decision boundary is the sphere ∥x∥=pclogc\\n2(c−1), which\\nis a linear boundary in the enlarged space.\\nMany nonparametric regression procedures operate by generating a basis\\nexpansion of derived variables, and then performing a linear regression in\\nthe enlarged space. The MARS procedure (Chapter 9) is exactly of this\\nform. Smoothing splines and additive spline models generate an extremely\\nlarge basis set ( N×pbasis functions for additive splines), but then perform\\na penalized regression ﬁt in the enlarged space. SVMs do as well; see also\\nthe kernel-based regression example in Section 12.3.7. FDA in this case can\\nbe shown to perform a penalized linear discriminant analysis in the enlarged\\nspace. We elaborate in Section 12.6. Linear boundaries in the enlarged space\\nmap down to nonlinear boundaries in the reduced space. This is exactly the\\nsame paradigm that is used with support vector machines (Section 12.3).\\nWe illustrate FDA on the speech recognition example used in Chapter\\n4.), with K= 11 classes and p= 10 predictors. The classes correspond to'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 461}, page_content='12.5 Flexible Discriminant Analysis 443\\noooo oo\\noooooooo\\noooo\\noo\\no\\noo\\no\\noooooooooooo\\nooo\\no\\noooooooo\\nooooooo\\no\\noo\\no\\nooooo\\no\\nooooooo\\noooooo\\no\\no\\nooooooooooooo\\nooooooooooo\\noooo\\noooo\\no\\nooooo\\noooooo o\\no\\noooooooooooo\\noo\\no\\noo\\nooooooooooooo\\noooooooooooooooooooooooooooooo\\no\\no\\nooooooo\\no\\no\\nooooooo\\noooooooooooo\\no\\nooooooooooooooooo\\noo\\no\\no\\noooooooo\\nooooooooooooooooo\\noooo\\no\\no\\nooooo\\no\\nooooo\\no\\noo\\no\\no\\no\\no\\nooooooooooooo\\noo\\noooooo\\nooooooooo\\nooo\\nooooooooo\\no\\noo\\no\\nooo\\nooooooooooooooo\\nooo\\noooooooo\\noooo\\nooooooooooooo\\no\\noooo\\nooooo\\noooooo\\noo\\no\\no\\no\\no\\noooooo\\no\\noooooo\\noooo\\no\\nooooooooooooo\\no ooooooooooooooooo\\no\\no\\nooooo\\nooooooooooo\\no\\no\\nooo\\nooooooo\\noooooo\\noooooo\\noooooooooo\\no\\nooooooo\\noooooooooooo\\noo\\no\\nooo\\nCoordinate 1 for Training DataCoordinate 2 for Training DataLinear Discriminant Analysis\\noo oooooo\\noooo\\noooooooooooo\\noooooo\\no\\nooo\\no\\noooooo\\noooo\\nooo\\nooooooo\\nooooo\\noooooo\\nooooooooooo\\nooooooo\\nooooooo\\no\\noooo\\noooooo\\nooo ooooooooo\\noooooooo\\noooo\\noooooo\\no\\noooo\\nooooooo\\noooooo\\noooooooooooooooooo\\noooooooooooooooooooooooooooo\\nooooooooooooo\\nooo\\no\\no oooooooooooooooooooo\\noooo\\no\\nooooooo\\noo\\noooo\\nooo\\noo\\nooooo\\nooo\\no\\noooo ooooooooo\\no\\no\\noooo\\noooooooooo\\nooo\\noo\\noo\\nooo\\nooo\\no\\no\\no\\noooooo\\no\\noooo\\no\\noooooo\\nooo\\nooooooo\\nooo\\nooooooooo\\noooo\\no\\nooo\\nooooo\\no\\nooooooo\\no\\noooooo\\no\\noooo\\noooo\\no\\nooo\\noooo\\no\\noooooo\\no\\noooooooooooo\\noo\\no\\nooo\\no\\no\\no\\noooooo\\no\\noooooooooooooooooo\\noooooooo\\noooo\\noooooo\\nooooooooo\\no\\noo\\nooooooo\\noooooo\\no\\noooo\\nooooooooooo\\nooooo oo\\noooo\\noooo\\noooo\\nCoordinate 1 for Training DataCoordinate 2 for Training DataFlexible Discriminant Analysis -- Bruto\\nFIGURE 12.10. The left plot shows the ﬁrst two LDA canonical variates for\\nthe vowel training data. The right plot shows the corresponding p rojection when\\nFDA/BRUTO is used to ﬁt the model; plotted are the ﬁtted regress ion functions\\nˆη1(xi)andˆη2(xi). Notice the improved separation. The colors represent the ele ven\\ndiﬀerent vowel sounds.\\n11 vowel sounds, each contained in 11 diﬀerent words. Here are the words,\\npreceded by the symbols that represent them:\\nVowel Word Vowel Word Vowel Word Vowel Word\\ni: heed O hod I hid C: hoard\\nE head U hood A had u: who’d\\na: hard 3: heard Y hud\\nEach of eight speakers spoke each word six times in the training set, and\\nlikewise seven speakers in the test set. The ten predictors are derived from\\nthe digitized speech in a rather complicated way, but standard in the speech\\nrecognition world. There are thus 528 training observations, and 462 test\\nobservations. Figure 12.10 shows two-dimensional projections produced by\\nLDA and FDA. The FDA model used adaptive additive-spline regression\\nfunctions to model the ηℓ(x), and the points plotted in the right plot have\\ncoordinates ˆ η1(xi) and ˆ η2(xi). The routine used in S-PLUS is called bruto ,\\nhence the heading on the plot and in Table 12.3. We see that ﬂexible model-\\ning has helped to separate the classes in this case. Table 12.3 shows training\\nand test error rates for a number of classiﬁcation techniques. FDA/MARS\\nrefers to Friedman’s multivariate adaptive regression splines; degree = 2\\nmeans pairwise products are permitted. Notice that for FDA/MARS, the\\nbest classiﬁcation results are obtained in a reduced-rank subspace.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 462}, page_content='444 12. Flexible Discriminants\\nTABLE 12.3. Vowel recognition data performance results. The results for ne ural\\nnetworks are the best among a much larger set, taken from a neural network\\narchive. The notation FDA/BRUTO refers to the regression met hod used with\\nFDA.\\nTechnique Error Rates\\nTraining Test\\n(1) LDA 0.32 0.56\\nSoftmax 0.48 0.67\\n(2) QDA 0.01 0.53\\n(3) CART 0.05 0.56\\n(4) CART (linear combination splits) 0.05 0.54\\n(5) Single-layer perceptron 0.67\\n(6) Multi-layer perceptron (88 hidden units) 0.49\\n(7) Gaussian node network (528 hidden units) 0.45\\n(8) Nearest neighbor 0.44\\n(9) FDA/BRUTO 0.06 0.44\\nSoftmax 0.11 0.50\\n(10) FDA/MARS (degree = 1) 0.09 0.45\\nBest reduced dimension (=2) 0.18 0.42\\nSoftmax 0.14 0.48\\n(11) FDA/MARS (degree = 2) 0.02 0.42\\nBest reduced dimension (=6) 0.13 0.39\\nSoftmax 0.10 0.50\\n12.5.1 Computing the FDA Estimates\\nThe computations for the FDA coordinates can be simpliﬁed in many im-\\nportant cases, in particular when the nonparametric regression procedure\\ncan be represented as a linear operator. We will denote this operator by\\nSλ; that is, ˆy=Sλy, where yis the vector of responses and ˆythe vector\\nof ﬁts. Additive splines have this property, if the smoothing parameters are\\nﬁxed, as does MARS once the basis functions are selected. The subscript λ\\ndenotes the entire set of smoothing parameters. In this case optimal scoring\\nis equivalent to a canonical correlation problem, and the solution can be\\ncomputed by a single eigen-decomposition. This is pursued in Exercise 12.6,\\nand the resulting algorithm is presented here.\\nWe create an N×Kindicator response matrix Yfrom the responses gi,\\nsuch that yik= 1 if gi=k, otherwise yik= 0. For a ﬁve-class problem Y\\nmight look like the following:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 463}, page_content='12.5 Flexible Discriminant Analysis 445\\n0\\nBBBBBBBBB@C1C2C3C4C5\\ng1= 2 0 1 0 0 0\\ng2= 1 1 0 0 0 0\\ng3= 1 1 0 0 0 0\\ng4= 5 0 0 0 0 1\\ng5= 4 0 0 0 1 0\\n......\\ngN= 3 0 0 1 0 01\\nCCCCCCCCCA\\nHere are the computational steps:\\n1.Multivariate nonparametric regression. Fit a multiresponse, adaptive\\nnonparametric regression of YonX, giving ﬁtted values ˆY. LetSλ\\nbe the linear operator that ﬁts the ﬁnal chosen model, and η∗(x) be\\nthe vector of ﬁtted regression functions.\\n2.Optimal scores. Compute the eigen-decomposition of YTˆY=YTSλY,\\nwhere the eigenvectors Θare normalized: ΘTDπΘ=I. Here Dπ=\\nYTY/Nis a diagonal matrix of the estimated class prior\\nprobabilities.\\n3.Update the model from step 1 using the optimal scores: η(x) =ΘTη∗(x).\\nThe ﬁrst of the Kfunctions in η(x) is the constant function— a trivial\\nsolution; the remaining K−1 functions are the discriminant functions. The\\nconstant function, along with the normalization, causes all the remaining\\nfunctions to be centered.\\nAgainSλcan correspond to any regression method. When Sλ=HX, the\\nlinear regression projection operator, then FDA is linear discriminant anal-\\nysis. The software that we reference in the Computational Considerations\\nsection on page 455 makes good use of this modularity; the fdafunction\\nhas amethod= argument that allows one to supply anyregression function,\\nas long as it follows some natural conventions. The regression functions\\nwe provide allow for polynomial regression, adaptive additive models and\\nMARS. They all eﬃciently handle multiple responses, so step (1) is a single\\ncall to a regression routine. The eigen-decomposition in step (2) simulta-\\nneously computes all the optimal scoring functions.\\nIn Section 4.2 we discussed the pitfalls of using linear regression on an\\nindicator response matrix as a method for classiﬁcation. In particular, se-\\nvere masking can occur with three or more classes. FDA uses the ﬁts from\\nsuch a regression in step (1), but then transforms them further to produce\\nuseful discriminant functions that are devoid of these pitfalls. Exercise 12.9\\ntakes another view of this phenomenon.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 464}, page_content='446 12. Flexible Discriminants\\n12.6 Penalized Discriminant Analysis\\nAlthough FDA is motivated by generalizing optimal scoring, it can also be\\nviewed directly as a form of regularized discriminant analysis. Suppose the\\nregression procedure used in FDA amounts to a linear regression onto a\\nbasis expansion h(X), with a quadratic penalty on the coeﬃcients:\\nASR({θℓ,βℓ}L\\nℓ=1) =1\\nNL∑\\nℓ=1[N∑\\ni=1(θℓ(gi)−hT(xi)βℓ)2+λβT\\nℓΩβℓ]\\n.(12.57)\\nThe choice of Ωdepends on the problem. If ηℓ(x) =h(x)βℓis an expansion\\non spline basis functions, Ωmight constrain ηℓto be smooth over IRp. In\\nthe case of additive splines, there are Nspline basis functions for each\\ncoordinate, resulting in a total of Npbasis functions in h(x);Ωin this case\\nisNp×Npand block diagonal.\\nThe steps in FDA can then be viewed as a generalized form of LDA,\\nwhich we call penalized discriminant analysis , or PDA:\\n•Enlarge the set of predictors Xvia a basis expansion h(X).\\n•Use (penalized) LDA in the enlarged space, where the penalized\\nMahalanobis distance is given by\\nD(x,θ) = (h(x)−h(θ))T(ΣW+λΩ)−1(h(x)−h(θ)),(12.58)\\nwhere ΣWis the within-class covariance matrix of the derived vari-\\nables h(xi).\\n•Decompose the classiﬁcation subspace using a penalized metric:\\nmaxuTΣBetusubject to uT(ΣW+λΩ)u= 1.\\nLoosely speaking, the penalized Mahalanobis distance tends to give less\\nweight to “rough” coordinates, and more weight to “smooth” ones; since\\nthe penalty is not diagonal, the same applies to linear combinations that\\nare rough or smooth.\\nFor some classes of problems, the ﬁrst step, involving the basis expansion,\\nis not needed; we already have far too many (correlated) predictors. A\\nleading example is when the objects to be classiﬁed are digitized analog\\nsignals:\\n•the log-periodogram of a fragment of spoken speech, sampled at a set\\nof 256 frequencies; see Figure 5.5 on page 149.\\n•the grayscale pixel values in a digitized image of a handwritten digit.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 465}, page_content='12.6 Penalized Discriminant Analysis 447\\nLDA: Coefficient 1 PDA: Coefficient 1 LDA: Coefficient 2 PDA: Coefficient 2 LDA: Coefficient 3 PDA: Coefficient 3\\nLDA: Coefficient 4 PDA: Coefficient 4 LDA: Coefficient 5 PDA: Coefficient 5 LDA: Coefficient 6 PDA: Coefficient 6\\nLDA: Coefficient 7 PDA: Coefficient 7 LDA: Coefficient 8 PDA: Coefficient 8 LDA: Coefficient 9 PDA: Coefficient 9\\nFIGURE 12.11. The images appear in pairs, and represent the nine discrim-\\ninant coeﬃcient functions for the digit recognition problem. The l eft member of\\neach pair is the LDA coeﬃcient, while the right member is the PD A coeﬃcient,\\nregularized to enforce spatial smoothness.\\nIt is also intuitively clear in these cases why regularization is needed.\\nTake the digitized image as an example. Neighboring pixel values will tend\\nto be correlated, being often almost the same. This implies that the pair\\nof corresponding LDA coeﬃcients for these pixels can be wildly diﬀerent\\nand opposite in sign, and thus cancel when applied to similar pixel values.\\nPositively correlated predictors lead to noisy, negatively correlated coeﬃ-\\ncient estimates, and this noise results in unwanted sampling variance. A\\nreasonable strategy is to regularize the coeﬃcients to be smooth over the\\nspatial domain, as with images. This is what PDA does. The computations\\nproceed just as for FDA, except that an appropriate penalized regression\\nmethod is used. Here hT(X)βℓ=Xβℓ, and Ω is chosen so that βT\\nℓΩβℓ\\npenalizes roughness in βℓwhen viewed as an image. Figure 1.2 on page 4\\nshows some examples of handwritten digits. Figure 12.11 shows the dis-\\ncriminant variates using LDA and PDA. Those produced by LDA appear\\nassalt-and-pepper images, while those produced by PDA are smooth im-\\nages. The ﬁrst smooth image can be seen as the coeﬃcients of a linear\\ncontrast functional for separating images with a dark central vertical stri p\\n(ones, possibly sevens) from images that are hollow in the middle (zeros,\\nsome fours). Figure 12.12 supports this interpretation, and with more di f-\\nﬁculty allows an interpretation of the second coordinate. This and other'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 466}, page_content='448 12. Flexible Discriminants\\n-5 0 5-6 -4 -2 0 2 4 60\\n0\\n00\\n0000\\n0\\n0000\\n00\\n00\\n00000\\n00\\n00000\\n0\\n0000\\n000\\n00\\n0\\n00\\n00000\\n0\\n0000\\n0\\n0\\n0000\\n0\\n00000\\n0\\n000\\n00\\n00\\n000\\n000\\n0000\\n00\\n00\\n000\\n00\\n000\\n00\\n000\\n0\\n0000\\n0000\\n000\\n0\\n00\\n00\\n00\\n00\\n00\\n00\\n000\\n00000 000\\n0000\\n00\\n0\\n0000\\n0000\\n000\\n000\\n00 0\\n0\\n0000\\n0\\n00\\n0 000\\n0\\n000\\n00\\n0\\n0\\n000\\n00 00\\n000\\n00\\n0\\n000\\n00\\n0\\n00\\n00 0 0\\n000\\n0\\n00\\n0\\n000000\\n0\\n0 0\\n000\\n00\\n0 00\\n000 0\\n00\\n000\\n00\\n000\\n000\\n0\\n000\\n000\\n000\\n000\\n000\\n00\\n000\\n00\\n00\\n00\\n000\\n0\\n000\\n0000\\n0\\n00 00\\n0000\\n00\\n0\\n000\\n00\\n00\\n0\\n00\\n0\\n0000\\n0\\n000000\\n00\\n000\\n0\\n000\\n000\\n00\\n000\\n000\\n00\\n000\\n00000\\n00\\n0\\n0000\\n00\\n0\\n0\\n011\\n11\\n11\\n11\\n1\\n1111\\n111\\n111\\n11\\n1\\n11\\n11\\n111\\n11\\n111\\n11\\n111\\n11\\n1\\n111\\n1\\n11\\n111\\n1\\n111\\n111\\n111\\n11\\n11\\n1111\\n111\\n11\\n1\\n1\\n111\\n1\\n11\\n1111\\n1\\n111\\n111\\n111\\n11\\n1\\n11111\\n11\\n1\\n11\\n111\\n11111\\n111\\n1\\n11111\\n1\\n111\\n1111\\n11\\n11\\n11\\n11111\\n1111\\n11\\n1\\n11\\n11\\n1\\n1\\n1 1\\n11\\n1\\n111\\n111\\n11\\n1\\n1\\n111\\n1\\n11\\n111\\n111\\n11111\\n111\\n1\\n11111\\n111111\\n111\\n1\\n11\\n11\\n1111\\n111\\n11\\n11\\n11\\n111\\n11\\n1111111\\n1\\n11\\n111\\n111\\n1\\n11111111\\n1\\n111\\n111\\n1\\n22\\n22\\n2222\\n222\\n22\\n2\\n222\\n22\\n2\\n22\\n2\\n22\\n2222\\n222\\n2\\n22\\n2\\n2\\n2\\n2222\\n2222\\n22222\\n22\\n2\\n22\\n22\\n2\\n22\\n222\\n222\\n2\\n2\\n22\\n22\\n2\\n222\\n2\\n2\\n22\\n22\\n2\\n2\\n22\\n22\\n222222\\n222\\n2\\n2\\n2222222\\n22222\\n222\\n222\\n2\\n22222\\n2\\n2\\n2222\\n222\\n22\\n222\\n222\\n2\\n22\\n2\\n2222\\n2\\n222\\n2222\\n22\\n222\\n22\\n222\\n2\\n2\\n222 22\\n2\\n22\\n22\\n22\\n2222\\n22\\n222\\n22\\n22\\n2222\\n333\\n3\\n33\\n3\\n3 3\\n333\\n333\\n333\\n33\\n33\\n3\\n33\\n3\\n3\\n333\\n33333\\n33\\n33\\n3\\n33333\\n33\\n3\\n3\\n333\\n3333\\n33\\n3333\\n33\\n33\\n3\\n33\\n33\\n33\\n3\\n333\\n3\\n3\\n333\\n33\\n333\\n333\\n333\\n3\\n3\\n333\\n33333\\n3\\n3333\\n33\\n3\\n3\\n33\\n3\\n33\\n3333\\n33\\n3333\\n33\\n33\\n333\\n333\\n333\\n33\\n3\\n333333\\n3\\n3\\n33 3\\n333\\n333\\n33\\n33\\n33\\n4\\n4444\\n4\\n4 44444\\n44\\n4\\n44\\n444\\n4\\n444\\n44\\n44\\n4\\n44\\n444\\n44\\n4\\n4\\n44\\n44\\n44\\n44\\n444\\n44444\\n44\\n444\\n4\\n44\\n44\\n4\\n44\\n4\\n44444\\n4\\n444\\n44\\n44\\n4 44\\n4\\n4\\n444\\n4\\n444\\n44\\n4444\\n4\\n4444\\n4444\\n4\\n444\\n44\\n44\\n444\\n44\\n444\\n44\\n44\\n4\\n4\\n44444\\n44\\n4\\n4\\n444\\n44 44\\n44\\n444\\n4\\n44\\n4\\n44\\n4\\n444\\n444444\\n444\\n4444\\n44\\n44\\n4\\n44\\n4\\n444\\n444\\n44\\n444\\n44\\n444\\n4455\\n55\\n5\\n55\\n555\\n5555\\n5\\n5 5\\n555\\n5\\n555\\n55\\n55\\n55\\n5\\n5555\\n555\\n55\\n5555\\n5555\\n5\\n55\\n55\\n55\\n5\\n555\\n55\\n55\\n55555\\n555\\n5 55\\n5\\n5555\\n555\\n5\\n555\\n5\\n555\\n555\\n55\\n555\\n55\\n5\\n555\\n55\\n55\\n555\\n555\\n55\\n55\\n555\\n55\\n5 555\\n55\\n5\\n5\\n5\\n55\\n5\\n555555\\n5\\n55\\n55\\n55\\n55\\n55\\n55\\n555\\n5\\n55\\n6\\n6666\\n66\\n66\\n6\\n66\\n66\\n666\\n6\\n66\\n66\\n6\\n666\\n666\\n6\\n6\\n6\\n666\\n6\\n66\\n66666\\n66\\n6\\n66\\n6\\n666\\n66\\n666\\n66\\n66\\n6\\n66\\n6\\n66\\n666\\n6\\n6\\n66\\n666\\n6\\n6666\\n6\\n666\\n66\\n66\\n666\\n66 6\\n6 66\\n66\\n6\\n666\\n666\\n6\\n6\\n66\\n666\\n666\\n6666\\n6\\n6 66\\n6\\n66\\n6\\n66\\n66\\n6666\\n666\\n66\\n6666\\n666\\n6666\\n66\\n666\\n666\\n66\\n666\\n6\\n6\\n6\\n7\\n7777\\n77\\n777\\n77\\n777\\n777\\n77777\\n777\\n7\\n777\\n77\\n77\\n7\\n777\\n777777\\n7\\n777\\n77\\n7 77\\n7\\n77\\n77\\n77777\\n7\\n77\\n77 7\\n777\\n77\\n7\\n77\\n777\\n77777\\n77\\n77\\n7\\n77777\\n777\\n77\\n7\\n77\\n77\\n777\\n7\\n7\\n7\\n77\\n77\\n77\\n77\\n77\\n7777\\n7\\n77\\n77\\n777\\n77\\n7777\\n77777\\n7\\n7\\n788\\n8\\n8\\n88\\n8888\\n88\\n8\\n8\\n888\\n88\\n8\\n888\\n88\\n88\\n8\\n888\\n8\\n888\\n88\\n88\\n888\\n88\\n888\\n88 888\\n8888\\n88\\n8888\\n88 88\\n88\\n88\\n8888\\n88 8\\n8\\n8 8\\n88\\n888\\n888\\n88\\n8\\n888\\n88\\n888\\n8\\n8\\n88\\n8888\\n888\\n888\\n88\\n8\\n888\\n88\\n88\\n8\\n8\\n88888\\n88\\n8\\n8888888 8\\n8\\n8\\n8\\n88\\n888 888\\n888\\n88\\n8\\n888\\n8\\n8 888\\n9999\\n999\\n999\\n99999\\n999\\n99\\n999\\n999\\n9\\n9\\n99\\n999\\n9\\n9\\n999\\n999\\n999\\n9999\\n9\\n99\\n999\\n99\\n999\\n99\\n9\\n9\\n9999\\n99\\n999\\n99\\n9\\n99\\n9\\n9\\n99\\n99\\n9\\n9\\n9\\n9\\n999\\n999\\n9\\n9\\n99\\n9\\n99\\n9\\n99\\n999\\n99\\n9999\\n99999\\n9\\n999\\n99\\n999\\n99\\n9\\n999\\n9\\n99\\n999\\n9\\n99\\n99\\n99\\n9\\n99\\n99\\n99\\n9\\n99\\n9\\n999\\n99\\n999\\n9\\n9999\\n9\\n99\\n999\\n9901\\n2\\n3\\n456\\n78\\n9\\nPDA: Discriminant Coordinate 1PDA: Discriminant Coordinate 2\\nFIGURE 12.12. The ﬁrst two penalized canonical variates, evaluated for the\\ntest data. The circles indicate the class centroids. The ﬁrst co ordinate contrasts\\nmainly 0’s and 1’s, while the second contrasts 6’s and 7/9’s.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 467}, page_content='12.7 Mixture Discriminant Analysis 449\\nexamples are discussed in more detail in Hastie et al. (1995), who also show\\nthat the regularization improves the classiﬁcation performance of LDA on\\nindependent test data by a factor of around 25% in the cases they tried.\\n12.7 Mixture Discriminant Analysis\\nLinear discriminant analysis can be viewed as a prototype classiﬁer. Each\\nclass is represented by its centroid, and we classify to the closest using an\\nappropriate metric. In many situations a single prototype is not suﬃcient\\nto represent inhomogeneous classes, and mixture models are more appro-\\npriate. In this section we review Gaussian mixture models and show how\\nthey can be generalized via the FDA and PDA methods discussed earlier.\\nA Gaussian mixture model for the kth class has density\\nP(X|G=k) =Rk∑\\nr=1πkrφ(X;θkr,Σ), (12.59)\\nwhere the mixing proportions πkrsum to one. This has Rkprototypes for\\nthekth class, and in our speciﬁcation, the same covariance matrix Σis\\nused as the metric throughout. Given such a model for each class, the class\\nposterior probabilities are given by\\nP(G=k|X=x) =∑Rk\\nr=1πkrφ(X;θkr,Σ)Πk∑K\\nℓ=1∑Rℓ\\nr=1πℓrφ(X;θℓr,Σ)Πℓ, (12.60)\\nwhere Π krepresent the class prior probabilities.\\nWe saw these calculations for the special case of two components in\\nChapter 8. As in LDA, we estimate the parameters by maximum likelihood,\\nusing the joint log-likelihood based on P(G,X):\\nK∑\\nk=1∑\\ngi=klog[Rk∑\\nr=1πkrφ(xi;θkr,Σ)Πk]\\n. (12.61)\\nThe sum within the log makes this a rather messy optimization problem\\nif tackled directly. The classical and natural method for computing the\\nmaximum-likelihood estimates (MLEs) for mixture distributions is the EM\\nalgorithm (Dempster et al., 1977), which is known to possess good conver -\\ngence properties. EM alternates between the two steps:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 468}, page_content='450 12. Flexible Discriminants\\nE-step: Given the current parameters, compute the responsibility of sub-\\nclassckrwithin class kfor each of the class- kobservations ( gi=k):\\nW(ckr|xi,gi) =πkrφ(xi;θkr,Σ)∑Rk\\nℓ=1πkℓφ(xi;θkℓ,Σ). (12.62)\\nM-step: Compute the weighted MLEs for the parameters of each of the\\ncomponent Gaussians within each of the classes, using the weights\\nfrom the E-step.\\nIn the E-step, the algorithm apportions the unit weight of an observation\\nin class kto the various subclasses assigned to that class. If it is close to the\\ncentroid of a particular subclass, and far from the others, it will receive a\\nmass close to one for that subclass. On the other hand, observations halfway\\nbetween two subclasses will get approximately equal weight for both.\\nIn the M-step, an observation in class kis used Rktimes, to estimate the\\nparameters in each of the Rkcomponent densities, with a diﬀerent weight\\nfor each. The EM algorithm is studied in detail in Chapter 8. The algorithm\\nrequires initialization, which can have an impact, since mixture likelihoods\\nare generally multimodal. Our software (referenced in the Computational\\nConsiderations on page 455) allows several strategies; here we describe the\\ndefault. The user supplies the number Rkof subclasses per class. Within\\nclassk, ak-means clustering model, with multiple random starts, is ﬁtted\\nto the data. This partitions the observations into Rkdisjoint groups, from\\nwhich an initial weight matrix, consisting of zeros and ones, is created.\\nOur assumption of an equal component covariance matrix Σthroughout\\nbuys an additional simplicity; we can incorporate rank restrictions in the\\nmixture formulation just like in LDA. To understand this, we review a litt le-\\nknown fact about LDA. The rank- LLDA ﬁt (Section 4.3.3) is equivalent to\\nthe maximum-likelihood ﬁt of a Gaussian model,where the diﬀerent mean\\nvectors in each class are conﬁned to a rank- Lsubspace of IRp(Exercise 4.8).\\nWe can inherit this property for the mixture model, and maximize the log-\\nlikelihood (12.61) subject to rank constraints on allthe∑\\nkRkcentroids:\\nrank{θkℓ}=L.\\nAgain the EM algorithm is available, and the M-step turns out to be\\na weighted version of LDA, with R=∑K\\nk=1Rk“classes.” Furthermore,\\nwe can use optimal scoring as before to solve the weighted LDA problem,\\nwhich allows us to use a weighted version of FDA or PDA at this stage.\\nOne would expect, in addition to an increase in the number of “classes,” a\\nsimilar increase in the number of “observations” in the kth class by a factor\\nofRk. It turns out that this is not the case if linear operators are used for\\nthe optimal scoring regression. The enlarged indicator Ymatrix collapses\\nin this case to a blurred response matrix Z, which is intuitively pleasing.\\nFor example, suppose there are K= 3 classes, and Rk= 3 subclasses per\\nclass. Then Zmight be'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 469}, page_content='12.7 Mixture Discriminant Analysis 451\\n\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8ec\\uf8edc11c12c13c21c22c23c31c32c33\\ng1= 2 0 0 0 0 .3 0.5 0.2 0 0 0\\ng2= 1 0 .9 0.1 0.0 0 0 0 0 0 0\\ng3= 1 0 .1 0.8 0.1 0 0 0 0 0 0\\ng4= 3 0 0 0 0 0 0 0 .5 0.4 0.1\\ng5= 2 0 0 0 0 .7 0.1 0.2 0 0 0\\n......\\ngN= 3 0 0 0 0 0 0 0 .1 0.1 0.8\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f7\\uf8f8,(12.63)\\nwhere the entries in a class- krow correspond to W(ckr|x,gi).\\nThe remaining steps are the same:\\nˆZ=SZ\\nZTˆZ=ΘDΘT\\nUpdate πs and Πs\\uf8fc\\n\\uf8fd\\n\\uf8feM-step of MDA .\\nThese simple modiﬁcations add considerable ﬂexibility to the mixture\\nmodel:\\n•The dimension reduction step in LDA, FDA or PDA is limited by\\nthe number of classes; in particular, for K= 2 classes no reduction is\\npossible. MDA substitutes subclasses for classes, and then allows us\\nto look at low-dimensional views of the subspace spanned by these\\nsubclass centroids. This subspace will often be an important one for\\ndiscrimination.\\n•By using FDA or PDA in the M-step, we can adapt even more to par-\\nticular situations. For example, we can ﬁt MDA models to digitized\\nanalog signals and images, with smoothness constraints built in.\\nFigure 12.13 compares FDA and MDA on the mixture example.\\n12.7.1 Example: Waveform Data\\nWe now illustrate some of these ideas on a popular simulated example,\\ntaken from Breiman et al. (1984, pages 49–55), and used in Hastie and\\nTibshirani (1996b) and elsewhere. It is a three-class problem with 21 vari-\\nables, and is considered to be a diﬃcult pattern recognition problem. The\\npredictors are deﬁned by\\nXj=Uh1(j) + (1 −U)h2(j) +ǫjClass 1 ,\\nXj=Uh1(j) + (1 −U)h3(j) +ǫjClass 2 , (12.64)\\nXj=Uh2(j) + (1 −U)h3(j) +ǫjClass 3 ,\\nwhere j= 1,2,... ,21,Uis uniform on (0 ,1),ǫjare standard normal vari-\\nates, and the hℓare the shifted triangular waveforms: h1(j) = max(6 −'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='452 12. Flexible Discriminants\\nFDA / MARS - Degree 2'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.185\\nTest Error:       0.235\\nBayes Error:    0.210\\nMDA - 5 Subclasses per Class'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='.... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='. . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 470}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•••\\n••\\n•••\\n••••\\n••\\n•••\\n••\\n•\\nTraining Error: 0.17\\nTest Error:       0.22\\nBayes Error:    0.21\\nFIGURE 12.13. FDA and MDA on the mixture data. The upper plot uses\\nFDA with MARS as the regression procedure. The lower plot uses MDA with\\nﬁve mixture centers per class (indicated). The MDA solution is cl ose to Bayes\\noptimal, as might be expected given the data arise from mixture s of Gaussians.\\nThe broken purple curve in the background is the Bayes decisio n boundary.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 471}, page_content='12.7 Mixture Discriminant Analysis 453\\n1 1 1 1 1 1111111111\\n1\\n1\\n1\\n1\\n1\\n1 2 2 2 2 22222222222\\n2\\n2\\n2\\n2\\n2\\n2 3 3 3 3 3333333\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3\\n3 4 4 4 4 4444444\\n4\\n4\\n4\\n4\\n4\\n44444 5 5 5 5 55555555555\\n5\\n5\\n5\\n5\\n5\\n5Class 1\\n11111111111\\n1\\n1\\n1\\n1\\n1\\n1 1 1 1 1 22222222222\\n2\\n2\\n2\\n2\\n2\\n2 2 2 2 2 33333333333\\n3\\n3\\n3\\n3\\n3\\n3 3 3 3 3 4444444\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4\\n4 4 4 4 4 5555555\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5\\n5 5 5 5 5Class 2\\n1111111\\n1\\n1\\n1\\n1\\n1\\n111111111 2222222\\n2\\n2\\n2\\n2\\n2\\n222222222 3333333\\n3\\n3333333\\n3\\n3\\n3\\n3\\n3\\n3 4444444\\n4\\n4\\n4\\n4\\n4\\n444444444 5555555\\n5\\n5\\n5\\n5\\n5\\n555555555Class 3\\nFIGURE 12.14. Some examples of the waveforms generated from model (12.64)\\nbefore the Gaussian noise is added.\\n|j−11|,0),h2(j) =h1(j−4) and h3(j) =h1(j+ 4). Figure 12.14 shows\\nsome example waveforms from each class.\\nTable 12.4 shows the results of MDA applied to the waveform data, as\\nwell as several other methods from this and other chapters. Each train-\\ning sample has 300 observations, and equal priors were used, so there are\\nroughly 100 observations in each class. We used test samples of size 500.\\nThe two MDA models are described in the caption.\\nFigure 12.15 shows the leading canonical variates for the penalized MDA\\nmodel, evaluated at the test data. As we might have guessed, the classes\\nappear to lie on the edges of a triangle. This is because the hj(i) are repre-\\nsented by three points in 21-space, thereby forming vertices of a triangle,\\nand each class is represented as a convex combination of a pair of vertices,\\nand hence lie on an edge. Also it is clear visually that all the information\\nlies in the ﬁrst two dimensions; the percentage of variance explained by the\\nﬁrst two coordinates is 99 .8%, and we would lose nothing by truncating the\\nsolution there. The Bayes risk for this problem has been estimated to be\\nabout 0 .14 (Breiman et al., 1984). MDA comes close to the optimal rate,\\nwhich is not surprising since the structure of the MDA model is similar to\\nthe generating model.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 472}, page_content='454 12. Flexible Discriminants\\nTABLE 12.4. Results for waveform data. The values are averages over ten s im-\\nulations, with the standard error of the average in parentheses . The ﬁve entries\\nabove the line are taken from Hastie et al. (1994). The ﬁrst mode l below the line\\nis MDA with three subclasses per class. The next line is the same, except that the\\ndiscriminant coeﬃcients are penalized via a roughness penalty to e ﬀectively 4df.\\nThe third is the corresponding penalized LDA or PDA model.\\nTechnique Error Rates\\nTraining Test\\nLDA 0.121(0.006) 0.191(0.006)\\nQDA 0.039(0.004) 0.205(0.006)\\nCART 0.072(0.003) 0.289(0.004)\\nFDA/MARS (degree = 1) 0.100(0.006) 0.191(0.006)\\nFDA/MARS (degree = 2) 0.068(0.004) 0.215(0.002)\\nMDA (3 subclasses) 0.087(0.005) 0.169(0.006)\\nMDA (3 subclasses, penalized 4 df) 0.137(0.006) 0.157(0.005)\\nPDA (penalized 4 df) 0.150(0.005) 0.171(0.005)\\nBayes 0.140\\nDiscriminant Var 1Discriminant Var 2\\n-6 -4 -2 0 2 4 6-6 -4 -2 0 2 411\\n111\\n11\\n111\\n11\\n11\\n111\\n111\\n1\\n1\\n11\\n1\\n111\\n1111\\n111\\n11\\n11\\n11\\n11\\n11\\n111\\n11\\n111\\n11\\n1\\n11\\n1\\n1\\n1\\n11\\n1\\n111\\n111111\\n111\\n11\\n11\\n11\\n11\\n11\\n11\\n11\\n11\\n1\\n1111\\n1\\n11\\n111\\n111\\n11\\n11\\n111\\n1\\n11\\n1\\n11111\\n11\\n1\\n11111\\n1\\n11\\n11\\n111\\n11\\n1111\\n111\\n111\\n1\\n1\\n1\\n11\\n11\\n1\\n11\\n111\\n11\\n1111\\n11\\n11111\\n11\\n22\\n22\\n2\\n22222\\n2 22\\n2\\n222\\n22\\n2\\n222\\n22\\n2\\n222\\n22\\n222\\n22\\n22222\\n2\\n2222\\n22\\n22\\n22\\n222\\n2\\n222\\n22\\n222\\n2\\n2\\n2\\n22\\n222\\n22\\n22\\n2\\n222\\n2\\n22\\n2\\n2\\n222\\n222\\n2\\n2\\n22\\n2\\n222\\n22\\n22\\n222\\n22\\n22\\n22\\n2\\n222222\\n22\\n2\\n22\\n2\\n2\\n22\\n222\\n2\\n22\\n22\\n22\\n2\\n2\\n2\\n22\\n2\\n22\\n22\\n2222\\n2\\n22\\n33\\n3 333\\n3333 33\\n33\\n3\\n3 3333\\n33\\n333\\n333\\n333\\n3\\n33\\n33\\n333\\n33\\n33\\n3333333\\n3\\n3\\n333\\n3333\\n333\\n33\\n333333\\n333\\n33\\n3\\n33\\n333\\n3\\n333 3\\n33\\n3\\n33\\n3\\n3\\n33\\n3\\n3\\n333\\n3\\n333 3\\n3 3\\n333\\n33\\n33\\n33\\n3\\n333\\n3\\n3\\n333\\n33\\n333\\n333\\n3\\n33\\n333\\n333\\n33\\n333\\n3\\n33\\n333\\n33 333\\n33\\n3\\n3 33\\n3\\n3\\n33\\n3\\n33333\\n333 Subclasses, Penalized 4 df\\nDiscriminant Var 3Discriminant Var 4\\n-2 -1 0 1 2-1.0 0.0 0.5 1.01\\n1\\n11\\n111\\n11\\n11\\n1\\n11\\n11\\n11\\n1\\n111\\n11\\n11\\n11\\n11\\n11\\n11 111\\n111\\n11\\n1\\n11\\n11\\n11\\n11\\n1\\n111\\n11\\n11111\\n11111\\n1\\n1111\\n1\\n11\\n1 1\\n11\\n1 111\\n11\\n1\\n11\\n1111\\n11\\n11\\n11\\n1\\n11\\n1\\n11\\n11\\n11\\n11\\n111\\n111\\n1\\n11\\n11\\n1\\n11\\n11\\n11\\n1\\n11111\\n1\\n111\\n11\\n111\\n1\\n1\\n1\\n111\\n11\\n11\\n11\\n111\\n1\\n1\\n111\\n1111\\n11\\n111\\n11\\n1111\\n22\\n22\\n22\\n222\\n22\\n222\\n22 2\\n22\\n22\\n22\\n22\\n22\\n22\\n22\\n222\\n22\\n2\\n22222\\n2\\n222\\n22\\n2\\n22222\\n22\\n22\\n22\\n22\\n2\\n2 22\\n222\\n2\\n222\\n22\\n2\\n22222\\n222\\n22\\n222\\n222\\n2222\\n222\\n22\\n2222\\n2\\n22\\n2\\n2\\n22\\n2\\n222\\n2\\n222\\n2\\n22\\n22\\n22\\n2\\n22\\n22 2\\n22\\n22\\n2\\n2\\n2\\n22\\n2222\\n2\\n22\\n222\\n222\\n3\\n333\\n3\\n33\\n3 33\\n3\\n33\\n33\\n33\\n333\\n3\\n33\\n33\\n3333\\n3\\n333\\n3\\n33\\n333\\n33\\n3\\n3\\n33\\n33\\n33\\n3\\n333\\n3\\n33333\\n3\\n3\\n33\\n33\\n33\\n3\\n3\\n3333\\n33\\n3333\\n3 3\\n33\\n3333\\n3\\n3333\\n33\\n33\\n3333\\n33\\n33\\n333\\n3\\n33 3\\n33\\n3333\\n333\\n33\\n3\\n33\\n3\\n3\\n333\\n33\\n33\\n333\\n3333333\\n33\\n3333\\n3\\n33\\n333\\n3\\n33\\n333\\n3\\n33\\n3\\n33\\n3\\n333\\n33333 Subclasses, Penalized 4 df\\nFIGURE 12.15. Some two-dimensional views of the MDA model ﬁtted to a\\nsample of the waveform model. The points are independent test dat a, projected\\non to the leading two canonical coordinates (left panel), and the th ird and fourth\\n(right panel). The subclass centers are indicated.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 473}, page_content='Exercises 455\\nComputational Considerations\\nWith Ntraining cases, ppredictors, and msupport vectors, the support\\nvector machine requires m3+mN+mpN operations, assuming m≈N.\\nThey do not scale well with N, although computational shortcuts are avail-\\nable (Platt, 1999). Since these are evolving rapidly, the reader is urged to\\nsearch the web for the latest technology.\\nLDA requires Np2+p3operations, as does PDA. The complexity of\\nFDA depends on the regression method used. Many techniques are linear\\ninN, such as additive models and MARS. General splines and kernel-based\\nregression methods will typically require N3operations.\\nSoftware is available for ﬁtting FDA, PDA and MDA models in the R\\npackagemda, which is also available in S-PLUS.\\nBibliographic Notes\\nThe theory behind support vector machines is due to Vapnik and is de-\\nscribed in Vapnik (1996). There is a burgeoning literature on SVMs; an\\nonline bibliography, created and maintained by Alex Smola and Bernhard\\nSch¨ olkopf, can be found at:\\nhttp://www.kernel-machines.org .\\nOur treatment is based on Wahba et al. (2000) and Evgeniou et al. (2000),\\nand the tutorial by Burges (Burges, 1998).\\nLinear discriminant analysis is due to Fisher (1936) and Rao (1973). The\\nconnection with optimal scoring dates back at least to Breiman and Ihaka\\n(1984), and in a simple form to Fisher (1936). There are strong connections\\nwith correspondence analysis (Greenacre, 1984). The description of ﬂexible,\\npenalized and mixture discriminant analysis is taken from Hastie et al.\\n(1994), Hastie et al. (1995) and Hastie and Tibshirani (1996b), and al l\\nthree are summarized in Hastie et al. (1998); see also Ripley (1996).\\nExercises\\nEx. 12.1 Show that the criteria (12.25) and (12.8) are equivalent.\\nEx. 12.2 Show that the solution to (12.29) is the same as the solution to\\n(12.25) for a particular kernel.\\nEx. 12.3 Consider a modiﬁcation to (12.43) where you do not penalize the\\nconstant. Formulate the problem, and characterize its solution.\\nEx. 12.4 Suppose you perform a reduced-subspace linear discriminant anal-\\nysis for a K-group problem. You compute the canonical variables of di-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 474}, page_content='456 12. Flexible Discriminants\\nmension L≤K−1 given by z=UTx, where Uis the p×Lmatrix of\\ndiscriminant coeﬃcients, and p > K is the dimension of x.\\n(a) If L=K−1 show that\\n∥z−¯zk∥2− ∥z−¯zk′∥2=∥x−¯xk∥2\\nW− ∥x−¯xk′∥2\\nW,\\nwhere ∥≤∥Wdenotes Mahalanobis distance with respect to the covari-\\nanceW.\\n(b) If L < K −1, show that the same expression on the left measures\\nthe diﬀerence in Mahalanobis squared distances for the distributions\\nprojected onto the subspace spanned by U.\\nEx. 12.5 The data in phoneme.subset , available from this book’s website\\nhttp://www-stat.stanford.edu/ElemStatLearn\\nconsists of digitized log-periodograms for phonemes uttered by 60 speakers,\\neach speaker having produced phonemes from each of ﬁve classes. It is\\nappropriate to plot each vector of 256 “features” against the frequencies\\n0–255.\\n(a) Produce a separate plot of all the phoneme curves against frequency\\nfor each class.\\n(b) You plan to use a nearest prototype classiﬁcation scheme to classify\\nthe curves into phoneme classes. In particular, you will use a K-means\\nclustering algorithm in each class ( kmeans() inR), and then classify\\nobservations to the class of the closest cluster center. The curves are\\nhigh-dimensional and you have a rather small sample-size-to-variables\\nratio. You decide to restrict all the prototypes to be smooth functions\\nof frequency. In particular, you decide to represent each prototype m\\nasm=Bθwhere Bis a 256 ×Jmatrix of natural spline basis\\nfunctions with Jknots uniformly chosen in (0 ,255) and boundary\\nknots at 0 and 255. Describe how to proceed analytically, and in\\nparticular, how to avoid costly high-dimensional ﬁtting procedures.\\n(Hint: It may help to restrict Bto be orthogonal.)\\n(c) Implement your procedure on the phoneme data, and try it out. Divide\\nthe data into a training set and a test set (50-50), making sure that\\nspeakers are not split across sets (why?). Use K= 1,3,5,7 centers\\nper class, and for each use J= 5,10,15 knots (taking care to start\\ntheK-means procedure at the same starting values for each value of\\nJ), and compare the results.\\nEx. 12.6 Suppose that the regression procedure used in FDA (Section 12.5.1)\\nis a linear expansion of basis functions hm(x), m= 1,... ,M . LetDπ=\\nYTY/Nbe the diagonal matrix of class proportions.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 475}, page_content='Exercises 457\\n(a) Show that the optimal scoring problem (12.52) can be written in vector\\nnotation as\\nmin\\nθ,β∥Yθ−Hβ∥2, (12.65)\\nwhere θis a vector of Kreal numbers, and His the N×Mmatrix\\nof evaluations hj(xi).\\n(b) Suppose that the normalization on θisθTDπ1 = 0 and θTDπθ= 1.\\nInterpret these normalizations in terms of the original scored θ(gi).\\n(c) Show that, with this normalization, (12.65) can be partially optimized\\nw.r.t. β, and leads to\\nmax\\nθθTSθ, (12.66)\\nsubject to the normalization constraints, where Sis the projection\\noperator corresponding to the basis matrix H.\\n(d) Suppose that the hjinclude the constant function. Show that the\\nlargest eigenvalue of Sis 1.\\n(e) Let Θbe aK×Kmatrix of scores (in columns), and suppose the\\nnormalization is ΘTDπΘ=I. Show that the solution to (12.53) is\\ngiven by the complete set of eigenvectors of S; the ﬁrst eigenvector is\\ntrivial, and takes care of the centering of the scores. The remainder\\ncharacterize the optimal scoring solution.\\nEx. 12.7 Derive the solution to the penalized optimal scoring problem\\n(12.57).\\nEx. 12.8 Show that coeﬃcients βℓfound by optimal scoring are proportional\\nto the discriminant directions νℓfound by linear discriminant analysis.\\nEx. 12.9 LetˆY=XˆBbe the ﬁtted N×Kindicator response matrix after\\nlinear regression on the N×pmatrix X, where p > K . Consider the reduced\\nfeatures x∗\\ni=ˆBTxi. Show that LDA using x∗\\niis equivalent to LDA in the\\noriginal space.\\nEx. 12.10 Kernels and linear discriminant analysis . Suppose you wish to\\ncarry out a linear discriminant analysis (two classes) using a vector of\\ntransformations of the input variables h(x). Since h(x) is high-dimensional,\\nyou will use a regularized within-class covariance matrix Wh+γI. Show\\nthat the model can be estimated using only the inner products K(xi,xi′) =\\n⟨h(xi),h(xi′)⟩. Hence the kernel property of support vector machines is also\\nshared by regularized linear discriminant analysis.\\nEx. 12.11 The MDA procedure models each class as a mixture of Gaussians.\\nHence each mixture center belongs to one and only one class. A more\\ngeneral model allows each mixture center to be shared by all classes. We\\ntake the joint density of labels and features to be'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 476}, page_content='458 12. Flexible Discriminants\\nP(G,X) =R∑\\nr=1πrPr(G,X), (12.67)\\na mixture of joint densities. Furthermore we assume\\nPr(G,X) =Pr(G)φ(X;θr,Σ). (12.68)\\nThis model consists of regions centered at θr, and for each there is a class\\nproﬁle Pr(G). The posterior class distribution is given by\\nP(G=k|X=x) =∑R\\nr=1πrPr(G=k)φ(x;θr,Σ)\\n∑R\\nr=1πrφ(x;θr,Σ), (12.69)\\nwhere the denominator is the marginal distribution P(X).\\n(a) Show that this model (called MDA2) can be viewed as a generalization\\nof MDA since\\nP(X|G=k) =∑R\\nr=1πrPr(G=k)φ(x;θr,Σ)\\n∑R\\nr=1πrPr(G=k), (12.70)\\nwhere πrk=πrPr(G=k)/∑R\\nr=1πrPr(G=k) corresponds to the\\nmixing proportions for the kth class.\\n(b) Derive the EM algorithm for MDA2.\\n(c) Show that if the initial weight matrix is constructed as in MDA, in-\\nvolving separate k-means clustering in each class, then the algorithm\\nfor MDA2 is identical to the original MDA procedure.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 477}, page_content='This is page 459\\nPrinter: Opaque this\\n13\\nPrototype Methods and\\nNearest-Neighbors\\n13.1 Introduction\\nIn this chapter we discuss some simple and essentially model-free methods\\nfor classiﬁcation and pattern recognition. Because they are highly unstruc-\\ntured, they typically are not useful for understanding the nature of the\\nrelationship between the features and class outcome. However, as black box\\nprediction engines, they can be very eﬀective, and are often among the best\\nperformers in real data problems. The nearest-neighbor technique can also\\nbe used in regression; this was touched on in Chapter 2 and works reason-\\nably well for low-dimensional problems. However, with high-dimensional\\nfeatures, the bias–variance tradeoﬀ does not work as favorably for nearest-\\nneighbor regression as it does for classiﬁcation.\\n13.2 Prototype Methods\\nThroughout this chapter, our training data consists of the Npairs ( x1,g1),\\n... ,(xn,gN) where giis a class label taking values in {1,2,... ,K }. Pro-\\ntotype methods represent the training data by a set of points in feature\\nspace. These prototypes are typically not examples from the training sam-\\nple, except in the case of 1-nearest-neighbor classiﬁcation discussed later.\\nEach prototype has an associated class label, and classiﬁcation of a query\\npoint xis made to the class of the closest prototype. “Closest” is usually\\ndeﬁned by Euclidean distance in the feature space, after each feature has'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 478}, page_content='460 13. Prototypes and Nearest-Neighbors\\nbeen standardized to have overall mean 0 and variance 1 in the training\\nsample. Euclidean distance is appropriate for quantitative features. We\\ndiscuss distance measures between qualitative and other kinds of feature\\nvalues in Chapter 14.\\nThese methods can be very eﬀective if the prototypes are well positioned\\nto capture the distribution of each class. Irregular class boundaries can be\\nrepresented, with enough prototypes in the right places in feature space.\\nThe main challenge is to ﬁgure out how many prototypes to use and where\\nto put them. Methods diﬀer according to the number and way in which\\nprototypes are selected.\\n13.2.1 K-means Clustering\\nK-means clustering is a method for ﬁnding clusters and cluster centers in a\\nset of unlabeled data. One chooses the desired number of cluster centers, say\\nR, and the K-means procedure iteratively moves the centers to minimize\\nthe total within cluster variance.1Given an initial set of centers, the K-\\nmeans algorithm alternates the two steps:\\n•for each center we identify the subset of training points (its cluster)\\nthat is closer to it than any other center;\\n•the means of each feature for the data points in each cluster are\\ncomputed, and this mean vector becomes the new center for that\\ncluster.\\nThese two steps are iterated until convergence. Typically the initial centers\\nareRrandomly chosen observations from the training data. Details of the\\nK-means procedure, as well as generalizations allowing for diﬀerent variable\\ntypes and more general distance measures, are given in Chapter 14.\\nTo use K-means clustering for classiﬁcation of labeled data, the steps\\nare:\\n•apply K-means clustering to the training data in each class sepa-\\nrately, using Rprototypes per class;\\n•assign a class label to each of the K×Rprototypes;\\n•classify a new feature xto the class of the closest prototype.\\nFigure 13.1 (upper panel) shows a simulated example with three classes\\nand two features. We used R= 5 prototypes per class, and show the clas-\\nsiﬁcation regions and the decision boundary. Notice that a number of the\\n1The “ K” inK-means refers to the number of cluster centers. Since we have already\\nreserved Kto denote the number of classes, we denote the number of clust ers by R.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 479}, page_content='13.2 Prototype Methods 461\\nK-means - 5 Prototypes  per Class\\n................................................................................................................................................................................................................................................................................................................................................................................................................................. ........................ ................................ ..................................... ............................................................................................................................................................................. .................................. ....................................... ......................................... ............................................ .............................................. .................................................. ......................... .............................. ........................ ................................... ......................... ..................................... .......................... ...................................... ............................. .................................. .................................. ....................... ....................................... ............. ............................................... ...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 479}, page_content='................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ........... ........................................ ............................................................. ........ ........................................... ........................................................ ..... ................................................................................................. .. .............................................................................................. . .......................................................................................... .... .................................................................................... ....... .............................................................................. .......... ............................................................................ ......... ............................................................................. ......... ............................................................................. ......... ............................................................................. ......... ............................................................................... ....... .................................................................................... ..... ........................................................................................ ... ............................................................................................ . ............................................................................................... . ................................................................................................ ... ................................................................................................ ..... ................................................................................................ ....... ............................................ ................................ .................... .... ....................................................................... ..................... .... ........................................................... ................. ............. .......................................... ............. ...................... ........................ .............. ....'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 479}, page_content='................................................................................ ......................... ......................................... .................... .................................................... ................ ........................................................... ............... ............................................................. ............... ............................................................... ............... ............................................................... ............... ........................................................... ................ ....................................................... ................ .................................................... ................ ................................................... ............... ................................................. .............. .............................................. .............. ......................................... .................................................... ............................................... ............................................... ........... .......................................... .......... ................................................ .......... ...................................................... ......... .................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ .............................................................................................. ............................................................................................... ................................................................................................ ................................................................................................. ................................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n•\\n•••\\n••\\n•••\\n•\\n••\\n••\\n•• •\\n••\\n•••\\n•••\\n••\\n•••\\nLVQ - 5 Prototypes per Class'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 479}, page_content='ooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n•\\n•••\\n••\\n•••\\n•\\n••\\n••\\n•• •\\n••\\n•••\\n•••\\n••\\n•••\\nLVQ - 5 Prototypes per Class\\n......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... ................................... ........................................ .............................................. .................................................. .................... .................................. ..................... ...................................... ..................... ......................................... ....................... ..................................... .............................. ............................ ..................................... .............. ............................................ .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 479}, page_content='................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... .... ............................................... ............................................................ .... ............................................... ........................................................... .... ............................................... .......................................................... ... ................................................ ........................................................ ... ................................................ ...................................................... ... ................................................ ..................................................... .. .................................................................................................... .. .................................................................................................. . ................................................................................................. . ................................................................................................ . ............................................................................................... ............................................................................................. ........................................................................................... . ......................................................................................... . ............................................................ .................... ......... ........................................... ........................ ..............'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 479}, page_content='.......................... ................................. ............................... ....................... ................................................. ................ .......................................................... ............... ............................................................ ............... ............................................................. ............... .............................................................. ............... .............................................................. ............... ................................................................ .............. .................................................................. .............. ................................................................... .............. ................................................................ .............. ........................................................... ............... ..................................................... ................ ................................................ ............... ............................................. .............. ......................................... ................................................... ................................................ .................................................. ........... .............................................. ........... ...................................................... .......... ....................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ ..........................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n••••\\n••\\n•••\\n•\\n• •\\n••\\n•• •\\n••\\n•••\\n•••\\n••\\n•••\\nFIGURE 13.1. Simulated example with three classes and ﬁve prototypes per\\nclass. The data in each class are generated from a mixture of Gau ssians. In the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 479}, page_content='o\\n••••\\n••\\n•••\\n•\\n• •\\n••\\n•• •\\n••\\n•••\\n•••\\n••\\n•••\\nFIGURE 13.1. Simulated example with three classes and ﬁve prototypes per\\nclass. The data in each class are generated from a mixture of Gau ssians. In the\\nupper panel, the prototypes were found by applying the K-means clustering algo-\\nrithm separately in each class. In the lower panel, the LVQ alg orithm (starting\\nfrom the K-means solution) moves the prototypes away from the decision b ound-\\nary. The broken purple curve in the background is the Bayes dec ision boundary.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 480}, page_content='462 13. Prototypes and Nearest-Neighbors\\nAlgorithm 13.1 Learning Vector Quantization—LVQ.\\n1. Choose Rinitial prototypes for each class: m1(k),m2(k),... ,m R(k),\\nk= 1,2,... ,K , for example, by sampling Rtraining points at random\\nfrom each class.\\n2. Sample a training point xirandomly (with replacement), and let ( j,k)\\nindex the closest prototype mj(k) toxi.\\n(a) If gi=k(i.e., they are in the same class), move the prototype\\ntowards the training point:\\nmj(k)←mj(k) +ǫ(xi−mj(k)),\\nwhere ǫis the learning rate .\\n(b) If gi̸=k(i.e., they are in diﬀerent classes), move the prototype\\naway from the training point:\\nmj(k)←mj(k)−ǫ(xi−mj(k)).\\n3. Repeat step 2, decreasing the learning rate ǫwith each iteration to-\\nwards zero.\\nprototypes are near the class boundaries, leading to potential misclassiﬁca-\\ntion errors for points near these boundaries. This results from an obvious\\nshortcoming with this method: for each class, the other classes do not have\\na say in the positioning of the prototypes for that class. A better approach,\\ndiscussed next, uses all of the data to position all prototypes.\\n13.2.2 Learning Vector Quantization\\nIn this technique due to Kohonen (1989), prototypes are placed strategically\\nwith respect to the decision boundaries in an ad-hoc way. LVQ is an online\\nalgorithm—observations are processed one at a time.\\nThe idea is that the training points attract prototypes of the correct class,\\nand repel other prototypes. When the iterations settle down, prototypes\\nshould be close to the training points in their class. The learning rate ǫis\\ndecreased to zero with each iteration, following the guidelines for stochastic\\napproximation learning rates (Section 11.4.)\\nFigure 13.1 (lower panel) shows the result of LVQ, using the K-means\\nsolution as starting values. The prototypes have tended to move away from\\nthe decision boundaries, and away from prototypes of competing classes.\\nThe procedure just described is actually called LVQ1. Modiﬁcations\\n(LVQ2, LVQ3, etc.) have been proposed, that can sometimes improve per-\\nformance. A drawback of learning vector quantization methods is the fact'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 481}, page_content='13.3k-Nearest-Neighbor Classiﬁers 463\\nthat they are deﬁned by algorithms, rather than optimization of some ﬁxed\\ncriteria; this makes it diﬃcult to understand their properties.\\n13.2.3 Gaussian Mixtures\\nThe Gaussian mixture model can also be thought of as a prototype method,\\nsimilar in spirit to K-means and LVQ. We discuss Gaussian mixtures in\\nsome detail in Sections 6.8, 8.5 and 12.7. Each cluster is described in terms\\nof a Gaussian density, which has a centroid (as in K-means), and a covari-\\nance matrix. The comparison becomes crisper if we restrict the component\\nGaussians to have a scalar covariance matrix (Exercise 13.1). The two st eps\\nof the alternating EM algorithm are very similar to the two steps in K-\\nmeans:\\n•In the E-step, each observation is assigned a responsibility or weight\\nfor each cluster, based on the likelihood of each of the correspond-\\ning Gaussians. Observations close to the center of a cluster will most\\nlikely get weight 1 for that cluster, and weight 0 for every other clus-\\nter. Observations half-way between two clusters divide their weight\\naccordingly.\\n•In the M-step, each observation contributes to the weighted means\\n(and covariances) for every cluster.\\nAs a consequence, the Gaussian mixture model is often referred to as a soft\\nclustering method, while K-means is hard.\\nSimilarly, when Gaussian mixture models are used to represent the fea-\\nture density in each class, it produces smooth posterior probabilities ˆ p(x) =\\n{ˆp1(x),... ,ˆpK(x)}for classifying x(see (12.60) on page 449.) Often this\\nis interpreted as a soft classiﬁcation, while in fact the classiﬁcation rule i s\\nˆG(x) = arg max kˆpk(x). Figure 13.2 compares the results of K-means and\\nGaussian mixtures on the simulated mixture problem of Chapter 2. We\\nsee that although the decision boundaries are roughly similar, those for the\\nmixture model are smoother (although the prototypes are in approximately\\nthe same positions.) We also see that while both procedures devote a blue\\nprototype (incorrectly) to a region in the northwest, the Gaussian mixtur e\\nclassiﬁer can ultimately ignore this region, while K-means cannot. LVQ\\ngave very similar results to K-means on this example, and is not shown.\\n13.3 k-Nearest-Neighbor Classiﬁers\\nThese classiﬁers are memory-based , and require no model to be ﬁt. Given\\na query point x0, we ﬁnd the ktraining points x(r),r= 1,... ,k closest in\\ndistance to x0, and then classify using majority vote among the kneighbors.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='464 13. Prototypes and Nearest-Neighbors\\nK-means - 5 Prototypes per Class'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='... .. . . .. . . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='. . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no••\\n••\\n•••\\n••\\n•\\n•••\\n••\\n•••\\n••\\nTraining Error: 0.170\\nTest Error:       0.243\\nBayes Error:    0.210\\nGaussian Mixtures - 5 Subclasses per Class'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='.... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='. . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 482}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\n•••\\n••\\n•••\\n••••\\n••\\n•••\\n••\\n•\\nTraining Error: 0.17\\nTest Error:       0.22\\nBayes Error:    0.21\\nFIGURE 13.2. The upper panel shows the K-means classiﬁer applied to the\\nmixture data example. The decision boundary is piecewise linear . The lower panel\\nshows a Gaussian mixture model with a common covariance for all component\\nGaussians. The EM algorithm for the mixture model was started a t theK-means\\nsolution. The broken purple curve in the background is the Baye s decision\\nboundary.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 483}, page_content='13.3k-Nearest-Neighbor Classiﬁers 465\\nTies are broken at random. For simplicity we will assume that the features\\nare real-valued, and we use Euclidean distance in feature space:\\nd(i)=||x(i)−x0||. (13.1)\\nTypically we ﬁrst standardize each of the features to have mean zero and\\nvariance 1, since it is possible that they are measured in diﬀerent units. In\\nChapter 14 we discuss distance measures appropriate for qualitative and\\nordinal features, and how to combine them for mixed data. Adaptively\\nchosen distance metrics are discussed later in this chapter.\\nDespite its simplicity, k-nearest-neighbors has been successful in a large\\nnumber of classiﬁcation problems, including handwritten digits, satellite\\nimage scenes and EKG patterns. It is often successful where each class\\nhas many possible prototypes, and the decision boundary is very irregular.\\nFigure 13.3 (upper panel) shows the decision boundary of a 15-nearest-\\nneighbor classiﬁer applied to the three-class simulated data. The decision\\nboundary is fairly smooth compared to the lower panel, where a 1-nearest-\\nneighbor classiﬁer was used. There is a close relationship between nearest-\\nneighbor and prototype methods: in 1-nearest-neighbor classiﬁcation, each\\ntraining point is a prototype.\\nFigure 13.4 shows the training, test and tenfold cross-validation errors\\nas a function of the neighborhood size, for the two-class mixture problem.\\nSince the tenfold CV errors are averages of ten numbers, we can estimate\\na standard error.\\nBecause it uses only the training point closest to the query point, the bias\\nof the 1-nearest-neighbor estimate is often low, but the variance is high.\\nA famous result of Cover and Hart (1967) shows that asymptotically the\\nerror rate of the 1-nearest-neighbor classiﬁer is never more than twice the\\nBayes rate. The rough idea of the proof is as follows (using squared-error\\nloss). We assume that the query point coincides with one of the training\\npoints, so that the bias is zero. This is true asymptotically if the dimensio n\\nof the feature space is ﬁxed and the training data ﬁlls up the space in a\\ndense fashion. Then the error of the Bayes rule is just the variance of a\\nBernoulli random variate (the target at the query point), while the error of\\n1-nearest-neighbor rule is twicethe variance of a Bernoulli random variate,\\none contribution each for the training and query targets.\\nWe now give more detail for misclassiﬁcation loss. At xletk∗be the\\ndominant class, and pk(x) the true conditional probability for class k. Then\\nBayes error = 1 −pk∗(x), (13.2)\\n1-nearest-neighbor error =K∑\\nk=1pk(x)(1−pk(x)), (13.3)\\n≥1−pk∗(x). (13.4)\\nThe asymptotic 1-nearest-neighbor error rate is that of a random rule; we\\npick both the classiﬁcation and the test point at random with probabili-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 484}, page_content='466 13. Prototypes and Nearest-Neighbors\\n15-Nearest Neighbors\\n. ........ ......... ..... ...... ......... ......................................................................................... ............................................................................................................................................................................................................................................................................ .............................................................................................................. .. .. . ...................................... .. . .... .... ......................................... ........... ....................................................... ......... ............................................................ ...... .......................................................................... ............................................................................... ...................... .................................................. ................................................................... ........................... ...................................... ........ .. .................................................................................... . .....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 484}, page_content='................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................. . . ................................................... ............................................................. . .................................................. ............................................................ ................................................... ............................................................... .. ................................................. .................................................... ...... ... ................................................... ............................................... ..... ... .. ................................................. .............................................. .... .... ................................................... ............................................... ...... ................................................... ..................................................... .... ............................................... ........................................................ .... ............................................... ........................ ............................... ... .............................................................. .... . . ........................ ....... ...................................................... ........................ ................. ................................... ................. .............................. ............... ....... ... ........................................ ................................................................ .............................. . ...................... ......'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 484}, page_content='.... .. . ..................... .............................. .................................................................................................. .......................................... .. ......... ............................................... ........... ..................................................... .............. ............................................................. .............. .................................................................. ........... .................................................................. . ............. .............................................................. ................ .......................................................... .. . . ............ .................... .................................... . . ............... .. ......... .. ................................... .................................................... ............................................... ................................................................................................................................ .................................................................................................... . ....................................................... . ........ ............................................................. ......................................................................... ................ ... ..................................................... ........................................................................... ............................................................................ .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................. ...........................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n1-Nearest Neighbor'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 484}, page_content='oooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\n1-Nearest Neighbor\\n................. ................. ............ .............. .................. ....................... ... ................ ..... ............... ..... ............. .... ................ ......... ..... ..... ...... ................... ..... ...... ...... .... ....... ... .......... ... .... .. .... . ........ ............ ............ ...... ............ .......... ........ .............. ........ ........ ............. .......... ...... ............... ....... ............. ............. ......... .......................... ............. ......................... . ..... .. .. ............................. ....... .. . ...... .......................... ....... ............ .............. ................ ............... .............................. ............. ........... ............ ....................... ............. ........... ....................... .......................................... ...... ..................... ..................... ..... ......................... .......... ............... ........ ....... ... .... ....................... .... ... ................ ...................... . ........................ . ......... ..... .... .............. ............... .............. ...... ... .................................. ......... .......... ... .............................. ........... ...... .......... ............................. ......... .......... ...................................... ... ................................................... ...................................... ..................................... .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 484}, page_content='................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................ ................................... .............................................................. ....... ............................................ .............................................. ...... .... ......................................................................................... ........ ... ................................................ ............................................ ........ ............... .............................................................................. ........ .......... ............................................................................... ....... ........ ......................................................... .............. ......... ... .. .......................................................... ............ ....... ... ........ ...................................................... .......... ...... .... .......... .................................................. ...... .. ....... ................ .................................................... . .. .............. .............. ..................................................... ... ............ .... ... ........... ............................................. .. ............. ... .................. ....................................... .............. ..... ... ..... ..... ....... .......................................... ............. ...... . ....... ... ......... ....................................... ............... .. .......... ...... ..... ... .............................................. .................. .......... ........ . ............................................ ..... .......... ......... ............. .................................. ... ..... ......... ..................... ........................... . ... ... ........ ........................ ....................... ..... ... ....... ............ ............. ................ .. .... ..... ............. ... .... .................... .... ..... . ........... ..... ...... ...........'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 484}, page_content='................................................. ............ ................. ................................................................. .................. ..................... ...................... ............................ ......................................... ................................................ .. ... .......... .......................... ...... . ...... ...................................... ...... . ..... ............ .................................... .. .............. .................................... .. . ................ ................................................ ... ....... .................................................. ...... .......... ........................................... ..... ........ ......... ........................................... ... . .. .. ... ................................... .. .. .. .. .................................. ........... .... . ...... . .... ............................................... ..... ..... ................................................... ...... . .................................................. .... .. . ......................................................... ..... ..... .................................................... ...... ............................................................ .................. ........................................................ ................... ......................................................... ................... .......................................................... ............................................................................... .............................................................................. .............................................................................. ................................................................................ .................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ........................................................................................... .........................................\\noo\\noo\\no\\nooo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooo\\no\\nooo\\noo\\nooo\\no\\noo\\noo\\noo\\nooo\\noo\\no\\noo\\nooo\\nooo\\noo\\noo\\no\\noo\\no\\no\\noo\\noo\\nooo\\nooo\\noo\\noo\\no\\nooo\\nooo\\no\\noo\\noo\\no\\noo\\noooo\\nooooo\\noo\\noooo\\nooo\\no\\nooo\\nooo\\noo oo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\noo\\noooo\\no\\no\\noo\\noo oo\\no\\nooo\\nooo\\noo\\noooooo\\no\\nooo\\noooo\\noo\\noooo\\nooo\\noo\\no\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\no\\noo\\noooo\\no\\noo\\nooo\\noooo\\no o o\\nooo\\noo\\no\\nooo\\nooooo\\noo\\noo\\no\\no\\noooo\\no\\noo\\noo\\noooo\\nooooo\\no\\nFIGURE 13.3. k-nearest-neighbor classiﬁers applied to the simulation data o f\\nFigure 13.1. The broken purple curve in the background is the B ayes decision\\nboundary.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 485}, page_content='13.3k-Nearest-Neighbor Classiﬁers 467\\nNumber of NeighborsMisclassification Errors\\n0 5 10 15 20 25 300.0 0.05 0.10 0.15 0.20 0.25 0.30•\\n••••••• • ••• •••\\n•• ••••\\n••••• •• • ••\\n•••••• •••• ••••\\nTest Error\\n10-fold CV\\nTraining Error\\nBayes Error\\n7-Nearest Neighbors'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 485}, page_content='.. .. . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 485}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 485}, page_content='. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 485}, page_content='. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 485}, page_content='o\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\noo\\no\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\no\\noooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo o\\nooo\\noo\\noo\\noooo\\no\\noo\\noo\\no\\noooo\\nooo\\no\\no\\nooo\\no\\nooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.145\\nTest Error:       0.225\\nBayes Error:    0.210\\nFIGURE 13.4. k-nearest-neighbors on the two-class mixture data. The upper\\npanel shows the misclassiﬁcation errors as a function of neighbo rhood size. Stan-\\ndard error bars are included for 10-fold cross validation. The lower panel shows\\nthe decision boundary for 7-nearest-neighbors, which appears to be optimal for\\nminimizing test error. The broken purple curve in the backgrou nd is the Bayes\\ndecision boundary.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 486}, page_content='468 13. Prototypes and Nearest-Neighbors\\ntiespk(x), k= 1,... ,K . For K= 2 the 1-nearest-neighbor error rate is\\n2pk∗(x)(1−pk∗(x))≤2(1−pk∗(x)) (twice the Bayes error rate). More\\ngenerally, one can show (Exercise 13.3)\\nK∑\\nk=1pk(x)(1−pk(x))≤2(1−pk∗(x))−K\\nK−1(1−pk∗(x))2.(13.5)\\nMany additional results of this kind have been derived; Ripley (1996) sum-\\nmarizes a number of them.\\nThis result can provide a rough idea about the best performance that\\nis possible in a given problem. For example, if the 1-nearest-neighbor rule\\nhas a 10% error rate, then asymptotically the Bayes error rate is at least\\n5%. The kicker here is the asymptotic part, which assumes the bias of the\\nnearest-neighbor rule is zero. In real problems the bias can be substantial.\\nThe adaptive nearest-neighbor rules, described later in this chapter, are an\\nattempt to alleviate this bias. For simple nearest-neighbors, the bias and\\nvariance characteristics can dictate the optimal number of near neighbors\\nfor a given problem. This is illustrated in the next example.\\n13.3.1 Example: A Comparative Study\\nWe tested the nearest-neighbors, K-means and LVQ classiﬁers on two sim-\\nulated problems. There are 10 independent features Xj, each uniformly\\ndistributed on [0 ,1]. The two-class 0-1 target variable is deﬁned as follows:\\nY=I(\\nX1>1\\n2)\\n; problem 1: “easy”,\\nY=I\\uf8eb\\n\\uf8edsign\\uf8f1\\n\\uf8f2\\n\\uf8f33∏\\nj=1(\\nXj−1\\n2)\\uf8fc\\n\\uf8fd\\n\\uf8fe>0\\uf8f6\\n\\uf8f8; problem 2: “diﬃcult.”(13.6)\\nHence in the ﬁrst problem the two classes are separated by the hyperplane\\nX1= 1/2; in the second problem, the two classes form a checkerboard\\npattern in the hypercube deﬁned by the ﬁrst three features. The Bayes\\nerror rate is zero in both problems. There were 100 training and 1000 test\\nobservations.\\nFigure 13.5 shows the mean and standard error of the misclassiﬁcation\\nerror for nearest-neighbors, K-means and LVQ over ten realizations, as\\nthe tuning parameters are varied. We see that K-means and LVQ give\\nnearly identical results. For the best choices of their tuning parameters,\\nK-means and LVQ outperform nearest-neighbors for the ﬁrst problem, and\\nthey perform similarly for the second problem. Notice that the best value\\nof each tuning parameter is clearly situation dependent. For example 25-\\nnearest-neighbors outperforms 1-nearest-neighbor by a factor of 70% in the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 487}, page_content='13.3k-Nearest-Neighbor Classiﬁers 469\\nNumber of NeighborsMisclassification Error\\n0 20 40 600.1 0.2 0.3 0.4 0.5Nearest Neighbors / Easy\\nNumber of Prototypes per ClassMisclassification Error\\n0 5 10 15 20 25 300.1 0.2 0.3 0.4 0.5K-means & LVQ / Easy\\nNumber of NeighborsMisclassification Error\\n0 20 40 600.40 0.45 0.50 0.55 0.60Nearest Neighbors / Difficult\\nNumber of Prototypes per ClassMisclassification Error\\n0 5 10 15 20 25 300.40 0.45 0.50 0.55 0.60K-means & LVQ / Difficult\\nFIGURE 13.5. Mean ±one standard error of misclassiﬁcation error for near-\\nest-neighbors, K-means (blue) and LVQ (red) over ten realizations for two sim-\\nulated problems: “easy” and “diﬃcult,” described in the text.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 488}, page_content='470 13. Prototypes and Nearest-Neighbors\\nSpectral Band 1 Spectral Band 2 Spectral Band 3\\nSpectral Band 4 Land Usage Predicted Land Usage\\nFIGURE 13.6. The ﬁrst four panels are LANDSAT images for an agricultural\\narea in four spectral bands, depicted by heatmap shading. The r emaining two\\npanels give the actual land usage (color coded) and the predicte d land usage using\\na ﬁve-nearest-neighbor rule described in the text.\\nﬁrst problem, while 1-nearest-neighbor is best in the second problem by a\\nfactor of 18%. These results underline the importance of using an objective,\\ndata-based method like cross-validation to estimate the best value of a\\ntuning parameter (see Figure 13.4 and Chapter 7).\\n13.3.2 Example: k-Nearest-Neighbors and Image Scene\\nClassiﬁcation\\nThe STATLOG project (Michie et al., 1994) used part of a LANDSAT\\nimage as a benchmark for classiﬁcation (82 ×100 pixels). Figure 13.6 shows\\nfour heat-map images, two in the visible spectrum and two in the infrared,\\nfor an area of agricultural land in Australia. Each pixel has a class label\\nfrom the 7-element set G={red soil, cotton, vegetation stubble, mixture,\\ngray soil, damp gray soil, very damp gray soil }, determined manually by\\nresearch assistants surveying the area. The lower middle panel shows the\\nactual land usage, shaded by diﬀerent colors to indicate the classes. The\\nobjective is to classify the land usage at a pixel, based on the information\\nin the four spectral bands.\\nFive-nearest-neighbors produced the predicted map shown in the bot-\\ntom right panel, and was computed as follows. For each pixel we extracted\\nan 8-neighbor feature map—the pixel itself and its 8 immediate neighbors'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 489}, page_content='13.3k-Nearest-Neighbor Classiﬁers 471\\nN\\nN\\nN\\n N\\nX\\nN\\nN\\nN\\nN\\nFIGURE 13.7. A pixel and its 8-neighbor feature map.\\n(see Figure 13.7). This is done separately in the four spectral bands, giving\\n(1+8) ×4 = 36 input features per pixel. Then ﬁve-nearest-neighbors classi-\\nﬁcation was carried out in this 36-dimensional feature space. The resulting\\ntest error rate was about 9 .5% (see Figure 13.8). Of all the methods used\\nin the STATLOG project, including LVQ, CART, neural networks, linear\\ndiscriminant analysis and many others, k-nearest-neighbors performed best\\non this task. Hence it is likely that the decision boundaries in IR36are quite\\nirregular.\\n13.3.3 Invariant Metrics and Tangent Distance\\nIn some problems, the training features are invariant under certain natural\\ntransformations. The nearest-neighbor classiﬁer can exploit such invari-\\nances by incorporating them into the metric used to measure the distances\\nbetween objects. Here we give an example where this idea was used with\\ngreat success, and the resulting classiﬁer outperformed all others at the\\ntime of its development (Simard et al., 1993).\\nThe problem is handwritten digit recognition, as discussed is Chapter 1\\nand Section 11.7. The inputs are grayscale images with 16 ×16 = 256\\npixels; some examples are shown in Figure 13.9. At the top of Figure 13.1 0,\\na “3” is shown, in its actual orientation (middle) and rotated 7 .5◦and 15◦\\nin either direction. Such rotations can often occur in real handwriting, and\\nit is obvious to our eye that this “3” is still a “3” after small rotati ons.\\nHence we want our nearest-neighbor classiﬁer to consider these two “3”s\\nto be close together (similar). However the 256 grayscale pixel values for a\\nrotated “3” will look quite diﬀerent from those in the original image, a nd\\nhence the two objects can be far apart in Euclidean distance in IR256.\\nWe wish to remove the eﬀect of rotation in measuring distances between\\ntwo digits of the same class. Consider the set of pixel values consisting of\\nthe original “3” and its rotated versions. This is a one-dimensional curve in\\nIR256, depicted by the green curve passing through the “3” in Figure 13.10.\\nFigure 13.11 shows a stylized version of IR256, with two images indicated by\\nxiandxi′. These might be two diﬀerent “3”s, for example. Through each\\nimage we have drawn the curve of rotated versions of that image, called'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 490}, page_content='472 13. Prototypes and Nearest-Neighbors\\nSTATLOG results\\nMethodTest Misclassification Error\\n2 4 6 8 10 12 140.0 0.05 0.10 0.15LVQRBFALLOC80CART NeuralNewID C4.5QDASMARTLogisticLDA\\nDANNK-NN\\nFIGURE 13.8. Test-error performance for a number of classiﬁers, as reported\\nby the STATLOG project. The entry DANN is a variant of k-nearest neighbors,\\nusing an adaptive metric (Section 13.4.2).\\nFIGURE 13.9. Examples of grayscale images of handwritten digits.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 491}, page_content='13.3k-Nearest-Neighbor Classiﬁers 473\\nTangent+ α.Transformations of 30 7.5 −15 −7.5\\n3\\nα=0 α=0.1 α=− 0.2 α=− 0.1 α=0.2\\nLinear equation for \\nimages above15\\nPixel space\\nFIGURE 13.10. The top row shows a “ 3” in its original orientation (middle)\\nand rotated versions of it. The green curve in the middle of the ﬁg ure depicts\\nthis set of rotated “ 3” in256-dimensional space. The red line is the tangent line\\nto the curve at the original image, with some “ 3”s on this tangent line, and its\\nequation shown at the bottom of the ﬁgure.\\ninvariance manifolds in this context. Now, rather than using the usual\\nEuclidean distance between the two images, we use the shortest distance\\nbetween the two curves. In other words, the distance between the two\\nimages is taken to be the shortest Euclidean distance between any rotated\\nversion of ﬁrst image, and any rotated version of the second image. This\\ndistance is called an invariant metric .\\nIn principle one could carry out 1-nearest-neighbor classiﬁcation using\\nthis invariant metric. However there are two problems with it. First, it is\\nvery diﬃcult to calculate for real images. Second, it allows large trans-\\nformations that can lead to poor performance. For example a “6” would\\nbe considered close to a “9” after a rotation of 180◦. We need to restrict\\nattention to small rotations.\\nThe use of tangent distance solves both of these problems. As shown in\\nFigure 13.10, we can approximate the invariance manifold of the image\\n“3” by its tangent at the original image. This tangent can be computed\\nby estimating the direction vector from small rotations of the image, or b y\\nmore sophisticated spatial smoothing methods (Exercise 13.4.) For large\\nrotations, the tangent image no longer looks like a “3,” so the problem\\nwith large transformations is alleviated.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 492}, page_content='474 13. Prototypes and Nearest-Neighbors\\nTransformationsTransformations\\nxixi′ofxi\\nofxi′Tangent distance\\nEuclidean distance\\nbetween xiandxi′Distance between\\ntransformed\\nxiandxi′\\nFIGURE 13.11. Tangent distance computation for two images xiandxi′.\\nRather than using the Euclidean distance between xiandxi′, or the shortest\\ndistance between the two curves, we use the shortest distance b etween the two\\ntangent lines.\\nThe idea then is to compute the invariant tangent line for each training\\nimage. For a query image to be classiﬁed, we compute its invariant tangent\\nline, and ﬁnd the closest line to it among the lines in the training set. The\\nclass (digit) corresponding to this closest line is our predicted class for the\\nquery image. In Figure 13.11 the two tangent lines intersect, but this is only\\nbecause we have been forced to draw a two-dimensional representation of\\nthe actual 256-dimensional situation. In IR256the probability of two such\\nlines intersecting is eﬀectively zero.\\nNow a simpler way to achieve this invariance would be to add into the\\ntraining set a number of rotated versions of each training image, and then\\njust use a standard nearest-neighbor classiﬁer. This idea is called “hints” in\\nAbu-Mostafa (1995), and works well when the space of invariances is small.\\nSo far we have presented a simpliﬁed version of the problem. In addition to\\nrotation, there are six other types of transformations under which we would\\nlike our classiﬁer to be invariant. There are translation (two directio ns),\\nscaling (two directions), sheer, and character thickness. Hence the curves\\nand tangent lines in Figures 13.10 and 13.11 are actually 7-dimensional\\nmanifolds and hyperplanes. It is infeasible to add transformed versions\\nof each training image to capture all of these possibilities. The tangent\\nmanifolds provide an elegant way of capturing the invariances.\\nTable 13.1 shows the test misclassiﬁcation error for a problem with 7291\\ntraining images and 2007 test digits (the U.S. Postal Services database), for\\na carefully constructed neural network, and simple 1-nearest-neighbor and'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 493}, page_content='13.4 Adaptive Nearest-Neighbor Methods 475\\nTABLE 13.1. Test error rates for the handwritten ZIP code problem.\\nMethod Error rate\\nNeural-net 0.049\\n1-nearest-neighbor/Euclidean distance 0.055\\n1-nearest-neighbor/tangent distance 0.026\\ntangent distance 1-nearest-neighbor rules. The tangent distance nearest-\\nneighbor classiﬁer works remarkably well, with test error rates near those\\nfor the human eye (this is a notoriously diﬃcult test set). In practice,\\nit turned out that nearest-neighbors are too slow for online classiﬁcation\\nin this application (see Section 13.5), and neural network classiﬁers were\\nsubsequently developed to mimic it.\\n13.4 Adaptive Nearest-Neighbor Methods\\nWhen nearest-neighbor classiﬁcation is carried out in a high-dimensional\\nfeature space, the nearest neighbors of a point can be very far away, causing\\nbias and degrading the performance of the rule.\\nTo quantify this, consider Ndata points uniformly distributed in the unit\\ncube [−1\\n2,1\\n2]p. LetRbe the radius of a 1-nearest-neighborhood centered at\\nthe origin. Then\\nmedian( R) =v−1/p\\np(\\n1−1\\n21/N)1/p\\n, (13.7)\\nwhere vprpis the volume of the sphere of radius rinpdimensions. Fig-\\nure 13.12 shows the median radius for various training sample sizes and\\ndimensions. We see that median radius quickly approaches 0 .5, the dis-\\ntance to the edge of the cube.\\nWhat can be done about this problem? Consider the two-class situation\\nin Figure 13.13. There are two features, and a nearest-neighborhood at\\na query point is depicted by the circular region. Implicit in near-neighbor\\nclassiﬁcation is the assumption that the class probabilities are roughly con-\\nstant in the neighborhood, and hence simple averages give good estimates.\\nHowever, in this example the class probabilities vary only in the horizontal\\ndirection. If we knew this, we would stretch the neighborhood in the verti-\\ncal direction, as shown by the tall rectangular region. This will reduce the\\nbias of our estimate and leave the variance the same.\\nIn general, this calls for adapting the metric used in nearest-neighbor\\nclassiﬁcation, so that the resulting neighborhoods stretch out in directions\\nfor which the class probabilities don’t change much. In high-dimensional\\nfeature space, the class probabilities might change only a low-dimensional\\nsubspace and hence there can be considerable advantage to adapting the\\nmetric.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 494}, page_content='476 13. Prototypes and Nearest-Neighbors\\nDimensionMedian Radius\\n0 5 10 150.0 0.1 0.2 0.3 0.4 0.5 0.6N=100N=1,000\\nN=10,000\\nFIGURE 13.12. Median radius of a 1-nearest-neighborhood, for uniform data\\nwithNobservations in pdimensions.\\no\\noo\\noo\\noo\\nooo\\no\\noo\\no\\nooo\\noo\\no\\noo\\noo\\no\\n•5-Nearest Neighborhoods\\nFIGURE 13.13. The points are uniform in the cube, with the vertical line sepa-\\nrating class red and green. The vertical strip denotes the 5-nearest-neighbor region\\nusing only the horizontal coordinate to ﬁnd the nearest-neighbors fo r the target\\npoint (solid dot). The sphere shows the 5-nearest-neighbor region using both co-\\nordinates, and we see in this case it has extended into the class-re d region (and\\nis dominated by the wrong class in this instance).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 495}, page_content='13.4 Adaptive Nearest-Neighbor Methods 477\\nFriedman (1994a) proposed a method in which rectangular neighbor-\\nhoods are found adaptively by successively carving away edges of a box\\ncontaining the training data. Here we describe the discriminant adaptive\\nnearest-neighbor (DANN) rule of Hastie and Tibshirani (1996a). Earlier,\\nrelated proposals appear in Short and Fukunaga (1981) and Myles and\\nHand (1990).\\nAt each query point a neighborhood of say 50 points is formed, and the\\nclass distribution among the points is used to decide how to deform the\\nneighborhood—that is, to adapt the metric. The adapted metric is then\\nused in a nearest-neighbor rule at the query point. Thus at each query\\npoint a potentially diﬀerent metric is used.\\nIn Figure 13.13 it is clear that the neighborhood should be stretched in\\nthe direction orthogonal to line joining the class centroids. This direction\\nalso coincides with the linear discriminant boundary, and is the direction\\nin which the class probabilities change the least. In general this direction\\nof maximum change will not be orthogonal to the line joining the class cen-\\ntroids (see Figure 4.9 on page 116.) Assuming a local discriminant model,\\nthe information contained in the local within- and between-class covari-\\nance matrices is all that is needed to determine the optimal shape of the\\nneighborhood.\\nThediscriminant adaptive nearest-neighbor (DANN) metric at a query\\npoint x0is deﬁned by\\nD(x,x0) = (x−x0)TΣ(x−x0), (13.8)\\nwhere\\nΣ=W−1/2[W−1/2BW−1/2+ǫI]W−1/2\\n=W−1/2[B∗+ǫI]W−1/2. (13.9)\\nHereWis the pooled within-class covariance matrix∑K\\nk=1πkWkandB\\nis the between class covariance matrix∑K\\nk=1πk(¯xk−¯x)(¯xk−¯x)T, with\\nWandBcomputed using only the 50 nearest neighbors around x0. After\\ncomputation of the metric, it is used in a nearest-neighbor rule at x0.\\nThis complicated formula is actually quite simple in its operation. It ﬁrst\\nspheres the data with respect to W, and then stretches the neighborhood\\nin the zero-eigenvalue directions of B∗(the between-matrix for the sphered\\ndata ). This makes sense, since locally the observed class means do not dif-\\nfer in these directions. The ǫparameter rounds the neighborhood, from an\\ninﬁnite strip to an ellipsoid, to avoid using points far away from the quer y\\npoint. The value of ǫ= 1 seems to work well in general. Figure 13.14 shows\\nthe resulting neighborhoods for a problem where the classes form two con-\\ncentric circles. Notice how the neighborhoods stretch out orthogonally to\\nthe decision boundaries when both classes are present in the neighborhood.\\nIn the pure regions with only one class, the neighborhoods remain circular;'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 496}, page_content='478 13. Prototypes and Nearest-Neighbors\\no\\noo ooo\\noo\\noo\\noo\\noo\\no\\noooo\\no\\no\\nooo\\no\\noo\\nooo\\nooooo\\noo\\no\\nooo\\noo ooo\\noo\\no\\nooo\\nooo\\noo\\no oo\\nooo\\no\\nooo\\nooo\\noooooo\\no\\no\\noo\\noo\\nooooo\\noo\\no\\noooo\\noo\\no\\no\\no\\noo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\noo\\no\\nooo\\noo\\nooo\\nooo\\no\\noo\\noooo\\noo o\\noo\\noooo\\noo\\noooo\\noo\\no\\no\\noo\\no\\nooo\\nooo\\noo\\no\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\no\\noo\\no\\nooo\\no\\noooo\\noo\\nooo\\noooo\\nooo\\no\\nooo\\nooo\\noo\\noo\\noo\\noo\\noooo\\noooo\\no\\noo\\nooooo\\noo\\no\\noo\\nooo\\noo\\no o\\nooo\\nooo\\no\\noooo\\nooo\\noo\\no\\nooo\\noo\\noo\\noo\\noo\\noo\\no\\noo\\no o\\noooo\\noo\\noo\\noo\\noooo\\no\\nooo\\nooo\\noo\\noo oo\\no\\noo\\noooo\\noo\\nooo\\no\\noooo\\no\\noo\\nooo o\\no ooo\\nooooo\\noo\\noo\\no\\no\\nooo\\noo\\nooo\\no\\noo\\nooo\\noo\\nooo\\noo\\noooo ooo\\noo\\noo\\no\\no\\noo\\nooo\\nFIGURE 13.14. Neighborhoods found by the DANN procedure, at various query\\npoints (centers of the crosses). There are two classes in the da ta, with one class\\nsurrounding the other. 50nearest-neighbors were used to estimate the local met-\\nrics. Shown are the resulting metrics used to form 15-nearest-neighborhoods.\\nin these cases the between matrix B= 0, and the Σin (13.8) is the identity\\nmatrix.\\n13.4.1 Example\\nHere we generate two-class data in ten dimensions, analogous to the two-\\ndimensional example of Figure 13.14. All ten predictors in class 1 are in-\\ndependent standard normal, conditioned on the radius being greater than\\n22.4 and less than 40, while the predictors in class 2 are independent stan-\\ndard normal without the restriction. There are 250 observations in each\\nclass. Hence the ﬁrst class almost completely surrounds the second class in\\nthe full ten-dimensional space.\\nIn this example there are no pure noise variables, the kind that a nearest-\\nneighbor subset selection rule might be able to weed out. At any given\\npoint in the feature space, the class discrimination occurs along only one\\ndirection. However, this direction changes as we move across the feature\\nspace and all variables are important somewhere in the space.\\nFigure 13.15 shows boxplots of the test error rates over ten realiza-\\ntions, for standard 5-nearest-neighbors, LVQ, and discriminant adaptive\\n5-nearest-neighbors. We used 50 prototypes per class for LVQ, to make\\nit comparable to 5 nearest-neighbors (since 250 /5 = 50). The adaptive\\nmetric signiﬁcantly reduces the error rate, compared to LVQ or standard\\nnearest-neighbors.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 497}, page_content='13.4 Adaptive Nearest-Neighbor Methods 4790.0 0.1 0.2 0.3 0.4\\n5NN LVQ DANNTest Error\\nFIGURE 13.15. Ten-dimensional simulated example: boxplots of the test error\\nrates over ten realizations, for standard 5-nearest-neighbors, LVQ with 50centers,\\nand discriminant-adaptive 5-nearest-neighbors\\n13.4.2 Global Dimension Reduction for Nearest-Neighbors\\nThe discriminant-adaptive nearest-neighbor method carries out local di-\\nmension reduction—that is, dimension reduction separately at each query\\npoint. In many problems we can also beneﬁt from global dimension re-\\nduction, that is, apply a nearest-neighbor rule in some optimally chosen\\nsubspace of the original feature space. For example, suppose that the two\\nclasses form two nested spheres in four dimensions of feature space, and\\nthere are an additional six noise features whose distribution is independent\\nof class. Then we would like to discover the important four-dimensional\\nsubspace, and carry out nearest-neighbor classiﬁcation in that reduced sub-\\nspace. Hastie and Tibshirani (1996a) discuss a variation of the discriminan t-\\nadaptive nearest-neighbor method for this purpose. At each training point\\nxi, the between-centroids sum of squares matrix Biis computed, and then\\nthese matrices are averaged over all training points:\\n¯B=1\\nNN∑\\ni=1Bi. (13.10)\\nLete1,e2,... ,e pbe the eigenvectors of the matrix ¯B, ordered from largest\\nto smallest eigenvalue θk. Then these eigenvectors span the optimal sub-\\nspaces for global subspace reduction. The derivation is based on the fact\\nthat the best rank- Lapproximation to ¯B,¯B[L]=∑L\\nℓ=1θℓeℓeT\\nℓ, solves the\\nleast squares problem\\nmin\\nrank(M)=LN∑\\ni=1trace[(Bi−M)2]. (13.11)\\nSince each Bicontains information on (a) the local discriminant subspace,\\nand (b) the strength of discrimination in that subspace, (13.11) can be seen'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 498}, page_content='480 13. Prototypes and Nearest-Neighbors\\nas a way of ﬁnding the best approximating subspace of dimension Lto a\\nseries of Nsubspaces by weighted least squares (Exercise 13.5.)\\nIn the four-dimensional sphere example mentioned above and examined\\nin Hastie and Tibshirani (1996a), four of the eigenvalues θℓturn out to be\\nlarge (having eigenvectors nearly spanning the interesting subspace), and\\nthe remaining six are near zero. Operationally, we project the data into\\nthe leading four-dimensional subspace, and then carry out nearest neighbor\\nclassiﬁcation. In the satellite image classiﬁcation example in Section 13. 3.2,\\nthe technique labeled DANNin Figure 13.8 used 5-nearest-neighbors in a\\nglobally reduced subspace. There are also connections of this technique\\nwith the sliced inverse regression proposal of Duan and Li (1991). These\\nauthors use similar ideas in the regression setting, but do global rather\\nthan local computations. They assume and exploit spherical symmetry of\\nthe feature distribution to estimate interesting subspaces.\\n13.5 Computational Considerations\\nOne drawback of nearest-neighbor rules in general is the computational\\nload, both in ﬁnding the neighbors and storing the entire training set. With\\nNobservations and ppredictors, nearest-neighbor classiﬁcation requires Np\\noperations to ﬁnd the neighbors per query point. There are fast algorithms\\nfor ﬁnding nearest-neighbors (Friedman et al., 1975; Friedman et al., 1977)\\nwhich can reduce this load somewhat. Hastie and Simard (1998) reduce\\nthe computations for tangent distance by developing analogs of K-means\\nclustering in the context of this invariant metric.\\nReducing the storage requirements is more diﬃcult, and various editing\\nandcondensing procedures have been proposed. The idea is to isolate a\\nsubset of the training set that suﬃces for nearest-neighbor predictions, and\\nthrow away the remaining training data. Intuitively, it seems important t o\\nkeep the training points that are near the decision boundaries and on the\\ncorrect side of those boundaries, while some points far from the boundaries\\ncould be discarded.\\nThemulti-edit algorithm of Devijver and Kittler (1982) divides the data\\ncyclically into training and test sets, computing a nearest neighbor rule on\\nthe training set and deleting test points that are misclassiﬁed. The idea is\\nto keep homogeneous clusters of training observations.\\nThecondensing procedure of Hart (1968) goes further, trying to keep\\nonly important exterior points of these clusters. Starting with a single ran-\\ndomly chosen observation as the training set, each additional data item is\\nprocessed one at a time, adding it to the training set only if it is misclas-\\nsiﬁed by a nearest-neighbor rule computed on the current training set.\\nThese procedures are surveyed in Dasarathy (1991) and Ripley (1996).\\nThey can also be applied to other learning procedures besides nearest-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 499}, page_content='Exercises 481\\nneighbors. While such methods are sometimes useful, we have not had\\nmuch practical experience with them, nor have we found any systematic\\ncomparison of their performance in the literature.\\nBibliographic Notes\\nThe nearest-neighbor method goes back at least to Fix and Hodges (1951).\\nThe extensive literature on the topic is reviewed by Dasarathy (1991);\\nChapter 6 of Ripley (1996) contains a good summary. K-means cluster-\\ning is due to Lloyd (1957) and MacQueen (1967). Kohonen (1989) intro-\\nduced learning vector quantization. The tangent distance method is due to\\nSimard et al. (1993). Hastie and Tibshirani (1996a) proposed the discrim -\\ninant adaptive nearest-neighbor technique.\\nExercises\\nEx. 13.1 Consider a Gaussian mixture model where the covariance matrices\\nare assumed to be scalar: Σr=σI∀r= 1,... ,R , and σis a ﬁxed param-\\neter. Discuss the analogy between the K-means clustering algorithm and\\nthe EM algorithm for ﬁtting this mixture model in detail. Show that in the\\nlimitσ→0 the two methods coincide.\\nEx. 13.2 Derive formula (13.7) for the median radius of the 1-nearest-\\nneighborhood.\\nEx. 13.3 LetE∗be the error rate of the Bayes rule in a K-class problem,\\nwhere the true class probabilities are given by pk(x), k= 1,... ,K . As-\\nsuming the test point and training point have identical features x, prove\\n(13.5)\\nK∑\\nk=1pk(x)(1−pk(x))≤2(1−pk∗(x))−K\\nK−1(1−pk∗(x))2.\\nwhere k∗= arg max kpk(x). Hence argue that the error rate of the 1-\\nnearest-neighbor rule converges in L1, as the size of the training set in-\\ncreases, to a value E1, bounded above by\\nE∗(\\n2−E∗K\\nK−1)\\n. (13.12)\\n[This statement of the theorem of Cover and Hart (1967) is taken from\\nChapter 6 of Ripley (1996), where a short proof is also given].'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 500}, page_content='482 13. Prototypes and Nearest-Neighbors\\nEx. 13.4 Consider an image to be a function F(x) : IR2↦→IR1over the two-\\ndimensional spatial domain (paper coordinates). Then F(c+x0+A(x−x0))\\nrepresents an aﬃne transformation of the image F, where Ais a 2 ×2\\nmatrix.\\n1. Decompose A(via Q-R) in such a way that parameters identifying\\nthe four aﬃne transformations (two scale, shear and rotation) are\\nclearly identiﬁed.\\n2. Using the chain rule, show that the derivative of F(c+x0+A(x−x0))\\nw.r.t. each of these parameters can be represented in terms of the two\\nspatial derivatives of F.\\n3. Using a two-dimensional kernel smoother (Chapter 6), describe how\\nto implement this procedure when the images are quantized to 16 ×16\\npixels.\\nEx. 13.5 LetBi,i= 1,2,... ,N be square p×ppositive semi-deﬁnite ma-\\ntrices and let ¯B= (1/N)∑Bi. Write the eigen-decomposition of ¯Bas∑p\\nℓ=1θℓeℓeT\\nℓwithθℓ≥θℓ−1≥ ≤≤≤ ≥ θ1. Show that the best rank- Lapprox-\\nimation for the Bi,\\nmin\\nrank(M)=LN∑\\ni=1trace[(Bi−M)2],\\nis given by ¯B[L]=∑L\\nℓ=1θℓeℓeT\\nℓ. (Hint: Write∑N\\ni=1trace[(Bi−M)2] as\\nN∑\\ni=1trace[(Bi−¯B)2] +N∑\\ni=1trace[(M−¯B)2]).\\nEx. 13.6 Here we consider the problem of shape averaging . In particular,\\nLi, i= 1,... ,M are each N×2 matrices of points in IR2, each sampled\\nfrom corresponding positions of handwritten (cursive) letters. We seek an\\naﬃne invariant average V, also N×2,VTV=I, of the Mletters Liwith\\nthe following property: Vminimizes\\nM∑\\nj=1min\\nAj∥Lj−VAj∥2.\\nCharacterize the solution.\\nThis solution can suﬀer if some of the letters are bigand dominate the\\naverage. An alternative approach is to minimize instead:\\nM∑\\nj=1min\\nAj\\ued79\\ued79LjA∗\\nj−V\\ued79\\ued792.\\nDerive the solution to this problem. How do the criteria diﬀer? Use the\\nSVD of the Ljto simplify the comparison of the two approaches.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 501}, page_content='Exercises 483\\nEx. 13.7 Consider the application of nearest-neighbors to the “easy” and\\n“hard” problems in the left panel of Figure 13.5.\\n1. Replicate the results in the left panel of Figure 13.5.\\n2. Estimate the misclassiﬁcation errors using ﬁvefold cross-validation,\\nand compare the error rate curves to those in 1.\\n3. Consider an “AIC-like” penalization of the training set misclassiﬁca-\\ntion error. Speciﬁcally, add 2 t/Nto the training set misclassiﬁcation\\nerror, where tis the approximate number of parameters N/r,rbe-\\ning the number of nearest-neighbors. Compare plots of the resulting\\npenalized misclassiﬁcation error to those in 1 and 2. Which method\\ngives a better estimate of the optimal number of nearest-neighbors:\\ncross-validation or AIC?\\nEx. 13.8 Generate data in two classes, with two features. These features\\nare all independent Gaussian variates with standard deviation 1. Their\\nmean vectors are ( −1,−1) in class 1 and (1 ,1) in class 2. To each feature\\nvector apply a random rotation of angle θ,θchosen uniformly from 0 to\\n2π. Generate 50 observations from each class to form the training set, and\\n500 in each class as the test set. Apply four diﬀerent classiﬁers:\\n1. Nearest-neighbors.\\n2. Nearest-neighbors with hints: ten randomly rotated versions of each\\ndata point are added to the training set before applying nearest-\\nneighbors.\\n3. Invariant metric nearest-neighbors, using Euclidean distance invari-\\nant to rotations about the origin.\\n4. Tangent distance nearest-neighbors.\\nIn each case choose the number of neighbors by tenfold cross-validation.\\nCompare the results.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 502}, page_content='484 13. Prototypes and Nearest-Neighbors'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 503}, page_content='This is page 485\\nPrinter: Opaque this\\n14\\nUnsupervised Learning\\n14.1 Introduction\\nThe previous chapters have been concerned with predicting the values\\nof one or more outputs or response variables Y= (Y1,... ,Y m) for a\\ngiven set of input or predictor variables XT= (X1,... ,X p). Denote by\\nxT\\ni= (xi1,... ,x ip) the inputs for the ith training case, and let yibe a\\nresponse measurement. The predictions are based on the training sample\\n(x1,y1),... ,(xN,yN) of previously solved cases, where the joint values of\\nall of the variables are known. This is called supervised learning or “learn-\\ning with a teacher.” Under this metaphor the “student” presents an an-\\nswer ˆyifor each xiin the training sample, and the supervisor or “teacher”\\nprovides either the correct answer and/or an error associated with the stu-\\ndent’s answer. This is usually characterized by some loss function L(y,ˆy),\\nfor example, L(y,ˆy) = (y−ˆy)2.\\nIf one supposes that ( X,Y) are random variables represented by some\\njoint probability density Pr( X,Y), then supervised learning can be formally\\ncharacterized as a density estimation problem where one is concerned with\\ndetermining properties of the conditional density Pr( Y|X). Usually the\\nproperties of interest are the “location” parameters θthat minimize the\\nexpected error at each x,\\nθ(x) = argmin\\nθEY|XL(Y,θ). (14.1)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 504}, page_content='486 14. Unsupervised Learning\\nConditioning one has\\nPr(X,Y) = Pr( Y|X)≤Pr(X),\\nwhere Pr( X) is the joint marginal density of the Xvalues alone. In su-\\npervised learning Pr( X) is typically of no direct concern. One is interested\\nmainly in the properties of the conditional density Pr( Y|X). Since Yis of-\\nten of low dimension (usually one), and only its location θ(x) is of interest,\\nthe problem is greatly simpliﬁed. As discussed in the previous chapters,\\nthere are many approaches for successfully addressing supervised learning\\nin a variety of contexts.\\nIn this chapter we address unsupervised learning or “learning without a\\nteacher.” In this case one has a set of Nobservations ( x1,x2,... ,x N) of a\\nrandom p-vector Xhaving joint density Pr( X). The goal is to directly infer\\nthe properties of this probability density without the help of a supervisor or\\nteacher providing correct answers or degree-of-error for each observation.\\nThe dimension of Xis sometimes much higher than in supervised learn-\\ning, and the properties of interest are often more complicated than simple\\nlocation estimates. These factors are somewhat mitigated by the fact that\\nXrepresents all of the variables under consideration; one is not required\\nto infer how the properties of Pr( X) change, conditioned on the changing\\nvalues of another set of variables.\\nIn low-dimensional problems (say p≤3), there are a variety of eﬀective\\nnonparametric methods for directly estimating the density Pr( X) itself at\\nallX-values, and representing it graphically (Silverman, 1986, e.g.). Owing\\nto the curse of dimensionality, these methods fail in high dimensions. One\\nmust settle for estimating rather crude global models, such as Gaussian\\nmixtures or various simple descriptive statistics that characterize Pr( X).\\nGenerally, these descriptive statistics attempt to characterize X-values,\\nor collections of such values, where Pr( X) is relatively large. Principal\\ncomponents, multidimensional scaling, self-organizing maps, and principal\\ncurves, for example, attempt to identify low-dimensional manifolds within\\ntheX-space that represent high data density. This provides information\\nabout the associations among the variables and whether or not they can be\\nconsidered as functions of a smaller set of “latent” variables. Cluster anal-\\nysis attempts to ﬁnd multiple convex regions of the X-space that contain\\nmodes of Pr( X). This can tell whether or not Pr( X) can be represented by\\na mixture of simpler densities representing distinct types or classes of ob-\\nservations. Mixture modeling has a similar goal. Association rules att empt\\nto construct simple descriptions (conjunctive rules) that describe regions\\nof high density in the special case of very high dimensional binary-valued\\ndata.\\nWith supervised learning there is a clear measure of success, or lack\\nthereof, that can be used to judge adequacy in particular situations and\\nto compare the eﬀectiveness of diﬀerent methods over various situations.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 505}, page_content='14.2 Association Rules 487\\nLack of success is directly measured by expected loss over the joint dis-\\ntribution Pr( X,Y). This can be estimated in a variety of ways including\\ncross-validation. In the context of unsupervised learning, there is no such\\ndirect measure of success. It is diﬃcult to ascertain the validity of inferences\\ndrawn from the output of most unsupervised learning algorithms. One must\\nresort to heuristic arguments not only for motivating the algorithms, as is\\noften the case in supervised learning as well, but also for judgments as to\\nthe quality of the results. This uncomfortable situation has led to heavy\\nproliferation of proposed methods, since eﬀectiveness is a matter of opinion\\nand cannot be veriﬁed directly.\\nIn this chapter we present those unsupervised learning techniques that\\nare among the most commonly used in practice, and additionally, a few\\nothers that are favored by the authors.\\n14.2 Association Rules\\nAssociation rule analysis has emerged as a popular tool for mining com-\\nmercial data bases. The goal is to ﬁnd joint values of the variables X=\\n(X1,X2,... ,X p) that appear most frequently in the data base. It is most\\noften applied to binary-valued data Xj∈ {0,1}, where it is referred to as\\n“market basket” analysis. In this context the observations are sales trans -\\nactions, such as those occurring at the checkout counter of a store. The\\nvariables represent all of the items sold in the store. For observation i, each\\nvariable Xjis assigned one of two values; xij= 1 if the jth item is pur-\\nchased as part of the transaction, whereas xij= 0 if it was not purchased.\\nThose variables that frequently have joint values of one represent items that\\nare frequently purchased together. This information can be quite useful for\\nstocking shelves, cross-marketing in sales promotions, catalog design, and\\nconsumer segmentation based on buying patterns.\\nMore generally, the basic goal of association rule analysis is to ﬁnd a\\ncollection of prototype X-values v1,... ,v Lfor the feature vector X, such\\nthat the probability density Pr( vl) evaluated at each of those values is rela-\\ntively large. In this general framework, the problem can be viewed as “mode\\nﬁnding” or “bump hunting.” As formulated, this problem is impossibly dif-\\nﬁcult. A natural estimator for each Pr( vl) is the fraction of observations\\nfor which X=vl. For problems that involve more than a small number\\nof variables, each of which can assume more than a small number of val-\\nues, the number of observations for which X=vlwill nearly always be too\\nsmall for reliable estimation. In order to have a tractable problem, both t he\\ngoals of the analysis and the generality of the data to which it is applied\\nmust be greatly simpliﬁed.\\nThe ﬁrst simpliﬁcation modiﬁes the goal. Instead of seeking values x\\nwhere Pr( x) is large, one seeks regions of the X-space with high probability'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 506}, page_content='488 14. Unsupervised Learning\\ncontent relative to their size or support. Let Sjrepresent the set of all\\npossible values of the jth variable (its support ), and let sj⊆ Sjbe a subset\\nof these values. The modiﬁed goal can be stated as attempting to ﬁnd\\nsubsets of variable values s1,... ,s psuch that the probability of each of the\\nvariables simultaneously assuming a value within its respective subset,\\nPr\\uf8ee\\n\\uf8f0p⋂\\nj=1(Xj∈sj)\\uf8f9\\n\\uf8fb, (14.2)\\nis relatively large. The intersection of subsets ∩p\\nj=1(Xj∈sj) is called a\\nconjunctive rule . For quantitative variables the subsets sjare contiguous\\nintervals; for categorical variables the subsets are delineated explicitly. No te\\nthat if the subset sjis in fact the entire set of values sj=Sj, as is often\\nthe case, the variable Xjis said notto appear in the rule (14.2).\\n14.2.1 Market Basket Analysis\\nGeneral approaches to solving (14.2) are discussed in Section 14.2.5. These\\ncan be quite useful in many applications. However, they are not feasible\\nfor the very large ( p≈104,N≈108) commercial data bases to which\\nmarket basket analysis is often applied. Several further simpliﬁcations of\\n(14.2) are required. First, only two types of subsets are considered; either\\nsjconsists of a single value of Xj,sj=v0j, or it consists of the entire set\\nof values that Xjcan assume, sj=Sj. This simpliﬁes the problem (14.2)\\nto ﬁnding subsets of the integers J ⊂ { 1,... ,p }, and corresponding values\\nv0j, j∈ J, such that\\nPr\\uf8ee\\n\\uf8f0⋂\\nj∈J(Xj=v0j)\\uf8f9\\n\\uf8fb (14.3)\\nis large. Figure 14.1 illustrates this assumption.\\nOne can apply the technique of dummy variables to turn (14.3) into\\na problem involving only binary-valued variables. Here we assume that\\nthe support Sjis ﬁnite for each variable Xj. Speciﬁcally, a new set of\\nvariables Z1,... ,Z Kis created, one such variable for each of the values\\nvljattainable by each of the original variables X1,... ,X p. The number of\\ndummy variables Kis\\nK=p∑\\nj=1|Sj|,\\nwhere |Sj|is the number of distinct values attainable by Xj. Each dummy\\nvariable is assigned the value Zk= 1 if the variable with which it is as-\\nsociated takes on the corresponding value to which Zkis assigned, and\\nZk= 0 otherwise. This transforms (14.3) to ﬁnding a subset of the integers\\nK ⊂ { 1,... ,K }such that'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 507}, page_content='14.2 Association Rules 489\\nX1 X1 X1\\nX2X2X2\\nFIGURE 14.1. Simpliﬁcations for association rules. Here there are two inputs\\nX1andX2, taking four and six distinct values, respectively. The red squ ares\\nindicate areas of high density. To simplify the computations, w e assume that the\\nderived subset corresponds to either a single value of an input o r all values. With\\nthis assumption we could ﬁnd either the middle or right pattern, but not the left\\none.\\nPr[⋂\\nk∈K(Zk= 1)]\\n= Pr[∏\\nk∈KZk= 1]\\n(14.4)\\nis large. This is the standard formulation of the market basket problem.\\nThe set Kis called an “item set.” The number of variables Zkin the item\\nset is called its “size” (note that the size is no bigger than p). The estimated\\nvalue of (14.4) is taken to be the fraction of observations in the data bas e\\nfor which the conjunction in (14.4) is true:\\nˆPr[∏\\nk∈K(Zk= 1)]\\n=1\\nNN∑\\ni=1∏\\nk∈Kzik. (14.5)\\nHerezikis the value of Zkfor this ith case. This is called the “support” or\\n“prevalence” T(K) of the item set K. An observation ifor which∏\\nk∈Kzik=\\n1 is said to “contain” the item set K.\\nIn association rule mining a lower support bound tis speciﬁed, and one\\nseeksallitem sets Klthat can be formed from the variables Z1,... ,Z K\\nwith support in the data base greater than this lower bound t\\n{Kl|T(Kl)> t}. (14.6)\\n14.2.2 The Apriori Algorithm\\nThe solution to this problem (14.6) can be obtained with feasible compu-\\ntation for very large data bases provided the threshold tis adjusted so that\\n(14.6) consists of only a small fraction of all 2Kpossible item sets. The\\n“Apriori” algorithm (Agrawal et al., 1995) exploits several aspects o f the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 508}, page_content='490 14. Unsupervised Learning\\ncurse of dimensionality to solve (14.6) with a small number of passes over\\nthe data. Speciﬁcally, for a given support threshold t:\\n•The cardinality |{K|T(K)> t}|is relatively small.\\n•Any item set Lconsisting of a subset of the items in Kmust have\\nsupport greater than or equal to that of K,L ⊆ K ⇒ T(L)≥T(K).\\nThe ﬁrst pass over the data computes the support of all single-item sets.\\nThose whose support is less than the threshold are discarded. The second\\npass computes the support of all item sets of size two that can be formed\\nfrom pairs of the single items surviving the ﬁrst pass. In other words, to\\ngenerate all frequent itemsets with |K|=m, we need to consider only\\ncandidates such that allof their mancestral item sets of size m−1 are\\nfrequent. Those size-two item sets with support less than the threshold are\\ndiscarded. Each successive pass over the data considers only those item\\nsets that can be formed by combining those that survived the previous\\npass with those retained from the ﬁrst pass. Passes over the data continue\\nuntil all candidate rules from the previous pass have support less than the\\nspeciﬁed threshold. The Apriori algorithm requires only one pass over the\\ndata for each value of |K|, which is crucial since we assume the data cannot\\nbe ﬁtted into a computer’s main memory. If the data are suﬃciently sparse\\n(or if the threshold tis high enough), then the process will terminate in\\nreasonable time even for huge data sets.\\nThere are many additional tricks that can be used as part of this strat-\\negy to increase speed and convergence (Agrawal et al., 1995). The Apriori\\nalgorithm represents one of the major advances in data mining technology.\\nEach high support item set K(14.6) returned by the Apriori algorithm is\\ncast into a set of “association rules.” The items Zk,k∈ K, are partitioned\\ninto two disjoint subsets, A∪B=K, and written\\nA⇒B. (14.7)\\nThe ﬁrst item subset Ais called the “antecedent” and the second Bthe\\n“consequent.” Association rules are deﬁned to have several properties based\\non the prevalence of the antecedent and consequent item sets in the data\\nbase. The “support” of the rule T(A⇒B) is the fraction of observations\\nin the union of the antecedent and consequent, which is just the support\\nof the item set Kfrom which they were derived. It can be viewed as an\\nestimate (14.5) of the probability of simultaneously observing both item\\nsets Pr( AandB) in a randomly selected market basket. The “conﬁdence”\\nor “predictability” C(A⇒B) of the rule is its support divided by the\\nsupport of the antecedent\\nC(A⇒B) =T(A⇒B)\\nT(A), (14.8)\\nwhich can be viewed as an estimate of Pr( B|A). The notation Pr( A), the\\nprobability of an item set Aoccurring in a basket, is an abbreviation for'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 509}, page_content='14.2 Association Rules 491\\nPr(∏\\nk∈AZk= 1). The “expected conﬁdence” is deﬁned as the support of\\nthe consequent T(B), which is an estimate of the unconditional probability\\nPr(B). Finally, the “lift” of the rule is deﬁned as the conﬁdence divided by\\nthe expected conﬁdence\\nL(A⇒B) =C(A⇒B)\\nT(B).\\nThis is an estimate of the association measure Pr( AandB)/Pr(A)Pr(B).\\nAs an example, suppose the item set K={peanut butter, jelly, bread }\\nand consider the rule {peanut butter, jelly } ⇒ {bread}. A support value\\nof 0.03 for this rule means that peanut butter ,jelly, andbread appeared\\ntogether in 3% of the market baskets. A conﬁdence of 0.82 for this rule im-\\nplies that when peanut butter andjelly were purchased, 82% of the time\\nbread was also purchased. If bread appeared in 43% of all market baskets\\nthen the rule {peanut butter, jelly } ⇒ {bread}would have a lift of 1 .95.\\nThe goal of this analysis is to produce association rules (14.7) with bot h\\nhigh values of support and conﬁdence (14.8). The Apriori algorithm returns\\nall item sets with high support as deﬁned by the support threshold t(14.6).\\nA conﬁdence threshold cis set, and all rules that can be formed from those\\nitem sets (14.6) with conﬁdence greater than this value\\n{A⇒B|C(A⇒B)> c} (14.9)\\nare reported. For each item set Kof size |K|there are 2|K|−1−1 rules of\\nthe form A⇒(K −A),A⊂ K. Agrawal et al. (1995) present a variant of\\nthe Apriori algorithm that can rapidly determine which rules survive the\\nconﬁdence threshold (14.9) from all possible rules that can be formed from\\nthe solution item sets (14.6).\\nThe output of the entire analysis is a collection of association rules (14.7 )\\nthat satisfy the constraints\\nT(A⇒B)> t and C(A⇒B)> c.\\nThese are generally stored in a data base that can be queried by the user.\\nTypical requests might be to display the rules in sorted order of conﬁdence,\\nlift or support. More speciﬁcally, one might request such a list conditioned\\non particular items in the antecedent or especially the consequent. For\\nexample, a request might be the following:\\nDisplay all transactions in which ice skates are the consequ ent\\nthat have conﬁdence over 80%and support of more than 2%.\\nThis could provide information on those items (antecedent) that predicate\\nsales of ice skates. Focusing on a particular consequent casts the problem\\ninto the framework of supervised learning.\\nAssociation rules have become a popular tool for analyzing very large\\ncommercial data bases in settings where market basket is relevant. That is'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 510}, page_content='492 14. Unsupervised Learning\\nwhen the data can be cast in the form of a multidimensional contingency\\ntable. The output is in the form of conjunctive rules (14.4) that are easily\\nunderstood and interpreted. The Apriori algorithm allows this analysis to\\nbe applied to huge data bases, much larger that are amenable to other types\\nof analyses. Association rules are among data mining’s biggest successes.\\nBesides the restrictive form of the data to which they can be applied, as-\\nsociation rules have other limitations. Critical to computational feasibi lity\\nis the support threshold (14.6). The number of solution item sets, their size,\\nand the number of passes required over the data can grow exponentially\\nwith decreasing size of this lower bound. Thus, rules with high conﬁdence\\nor lift, but low support, will not be discovered. For example, a high conﬁ-\\ndence rule such as vodka ⇒caviar will not be uncovered owing to the low\\nsales volume of the consequent caviar .\\n14.2.3 Example: Market Basket Analysis\\nWe illustrate the use of Apriori on a moderately sized demographics data\\nbase. This data set consists of N= 9409 questionnaires ﬁlled out by shop-\\nping mall customers in the San Francisco Bay Area (Impact Resources, Inc.,\\nColumbus OH, 1987). Here we use answers to the ﬁrst 14 questions, relat-\\ning to demographics, for illustration. These questions are listed in Table\\n14.1. The data are seen to consist of a mixture of ordinal and (unordered)\\ncategorical variables, many of the latter having more than a few values.\\nThere are many missing values.\\nWe used a freeware implementation of the Apriori algorithm due to Chris-\\ntian Borgelt1. After removing observations with missing values, each ordinal\\npredictor was cut at its median and coded by two dummy variables; each\\ncategorical predictor with kcategories was coded by kdummy variables.\\nThis resulted in a 6876 ×50 matrix of 6876 observations on 50 dummy\\nvariables.\\nThe algorithm found a total of 6288 association rules, involving ≤5\\npredictors, with support of at least 10%. Understanding this large set of\\nrules is itself a challenging data analysis task. We will not attempt this here,\\nbut only illustrate in Figure 14.2 the relative frequency of each dummy\\nvariable in the data (top) and the association rules (bottom). Prevalent\\ncategories tend to appear more often in the rules, for example, the ﬁrst\\ncategory in language (English). However, others such as occupation are\\nunder-represented, with the exception of the ﬁrst and ﬁfth level.\\nHere are three examples of association rules found by the Apriori algo-\\nrithm:\\nAssociation rule 1: Support 25%, conﬁdence 99.7% and lift 1.03.\\n1Seehttp://fuzzy.cs.uni-magdeburg.de/ ∼borgelt.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 511}, page_content='14.2 Association Rules 493\\n0 10 20 30 40 500.0 0.02 0.04 0.06\\nAttributeRelative Frequency in Data\\nincomesexmarstatageeduc occup yrs−baydualincperhousperyounghousetypehome ethnic language\\n0 10 20 30 40 500.0 0.04 0.08 0.12\\nAttributeRelative Frequency in Association Rules\\nincomesexmarstatageeduc occup yrs−baydualincperhousperyounghousetypehome ethnic languageFIGURE 14.2. Market basket analysis: relative frequency of each dummy vari -\\nable (coding an input category) in the data (top), and the associ ation rules found\\nby the Apriori algorithm (bottom).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 512}, page_content='494 14. Unsupervised Learning\\nTABLE 14.1. Inputs for the demographic data.\\nFeature Demographic # Values Type\\n1 Sex 2 Categorical\\n2 Marital status 5 Categorical\\n3 Age 7 Ordinal\\n4 Education 6 Ordinal\\n5 Occupation 9 Categorical\\n6 Income 9 Ordinal\\n7 Years in Bay Area 5 Ordinal\\n8 Dual incomes 3 Categorical\\n9 Number in household 9 Ordinal\\n10 Number of children 9 Ordinal\\n11 Householder status 3 Categorical\\n12 Type of home 5 Categorical\\n13 Ethnic classiﬁcation 8 Categorical\\n14 Language in home 3 Categorical\\n[number in household = 1\\nnumber of children = 0]\\n⇓\\nlanguage in home = English\\nAssociation rule 2: Support 13.4%, conﬁdence 80.8%, and lift 2.13.\\n\\uf8ee\\n\\uf8f0language in home = English\\nhouseholder status = own\\noccupation = {professional/managerial }\\uf8f9\\n\\uf8fb\\n⇓\\nincome ≥$40,000\\nAssociation rule 3: Support 26.5%, conﬁdence 82.8% and lift 2.15.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0language in home = English\\nincome <$40,000\\nmarital status = not married\\nnumber of children = 0\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\n⇓\\neducation /∈ {college graduate, graduate study }'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 513}, page_content='14.2 Association Rules 495\\nWe chose the ﬁrst and third rules based on their high support. The second\\nrule is an association rule with a high-income consequent, and could be\\nused to try to target high-income individuals.\\nAs stated above, we created dummy variables for each category of the\\ninput predictors, for example, Z1=I(income <$40,000) and Z2=\\nI(income ≥$40,000) for below and above the median income. If we were\\ninterested only in ﬁnding associations with the high-income category, we\\nwould include Z2but not Z1. This is often the case in actual market basket\\nproblems, where we are interested in ﬁnding associations with the presence\\nof a relatively rare item, but not associations with its absence.\\n14.2.4 Unsupervised as Supervised Learning\\nHere we discuss a technique for transforming the density estimation prob-\\nlem into one of supervised function approximation. This forms the basis\\nfor the generalized association rules described in the next section.\\nLetg(x) be the unknown data probability density to be estimated, and\\ng0(x) be a speciﬁed probability density function used for reference. For ex-\\nample, g0(x) might be the uniform density over the range of the variables.\\nOther possibilities are discussed below. The data set x1,x2,... ,x Nis pre-\\nsumed to be an i.i.d.random sample drawn from g(x). A sample of size N0\\ncan be drawn from g0(x) using Monte Carlo methods. Pooling these two\\ndata sets, and assigning mass w=N0/(N+N0) to those drawn from g(x),\\nandw0=N/(N+N0) to those drawn from g0(x), results in a random\\nsample drawn from the mixture density ( g(x) +g0(x))/2. If one assigns\\nthe value Y= 1 to each sample point drawn from g(x) and Y= 0 those\\ndrawn from g0(x), then\\nθ(x) =E(Y|x) =g(x)\\ng(x) +g0(x)\\n=g(x)/g0(x)\\n1 +g(x)/g0(x)(14.10)\\ncan be estimated by supervised learning using the combined sample\\n(y1,x1),(y2,x2),... ,(yN+N0,xN+N0) (14.11)\\nas training data. The resulting estimate ˆ θ(x) can be inverted to provide an\\nestimate for g(x)\\nˆg(x) =g0(x)ˆθ(x)\\n1−ˆθ(x). (14.12)\\nGeneralized versions of logistic regression (Section 4.4) are especially wel l\\nsuited for this application since the log-odds,\\nf(x) = logg(x)\\ng0(x), (14.13)\\nare estimated directly. In this case one has'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 514}, page_content='496 14. Unsupervised Learning\\n-1 0 1 2-2 0 2 4 6••\\n••••\\n••\\n•\\n•••\\n••\\n••\\n••\\n••\\n•\\n••••\\n••••\\n••\\n•••\\n••\\n••\\n••\\n•\\n••••\\n•••\\n•\\n••\\n••••••\\n••••\\n• •\\n••\\n••••\\n••••\\n•\\n•••\\n•••••\\n•\\n• • •\\n••\\n••\\n••\\n•\\n••\\n•••\\n••\\n••\\n••\\n•••\\n•••\\n••\\n••\\n••\\n••\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n•\\n••••\\n••••\\n•••••\\n•••\\n••••••••\\n•\\n••\\n•••\\n••\\n••\\n•••\\n•••\\n• •\\n••\\n••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n••\\n•••••\\n•\\n-1 0 1 2-2 0 2 4 6••\\n••••\\n••\\n•\\n•••\\n••\\n••\\n••\\n••\\n•\\n••••\\n••••\\n••\\n•••\\n••\\n••\\n••\\n•\\n••••\\n•••\\n•\\n••\\n••••••\\n••••\\n• •\\n••\\n••••\\n••••\\n•\\n•••\\n•••••\\n•\\n• • •\\n••\\n••\\n••\\n•\\n••\\n•••\\n••\\n••\\n••\\n•••\\n•••\\n••\\n••\\n••\\n••\\n•••\\n••\\n••\\n••\\n•••\\n••••\\n•\\n••••\\n••••\\n•••••\\n•••\\n••••••••\\n•\\n••\\n•••\\n••\\n••\\n•••\\n•••\\n• •\\n••\\n••\\n•\\n•••\\n•\\n••\\n•••\\n•\\n••\\n•••••\\n••\\n• ••\\n•\\n•••\\n••\\n•\\n••••\\n••\\n••••\\n•\\n••\\n••••\\n•\\n••\\n••\\n••\\n••\\n•••••• •\\n••••\\n•\\n•••\\n•\\n••\\n•••\\n••\\n•\\n••\\n•••••\\n•\\n•••••\\n•\\n•• ••\\n••\\n••\\n••\\n•••\\n••\\n•\\n•\\n••\\n•\\n••\\n•••\\n•\\n••••\\n•\\n••\\n••\\n••\\n•••\\n•\\n•• •••\\n•••••\\n••\\n•••\\n•••\\n•\\n••\\n••\\n•••\\n••\\n•\\n••\\n••\\n•\\n••\\n••\\n••\\n••••\\n•••\\n•••\\n•\\n••\\n••\\n•\\n••\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n• •\\n••\\n••••\\n••\\n• •\\nX1 X1\\nX2X2\\nFIGURE 14.3. Density estimation via classiﬁcation. (Left panel:) Training set\\nof200data points. (Right panel:) Training set plus 200reference data points,\\ngenerated uniformly over the rectangle containing the training data . The training\\nsample was labeled as class 1, and the reference sample class 0, and a semipara-\\nmetric logistic regression model was ﬁt to the data. Some contou rs for ˆg(x)are\\nshown.\\nˆg(x) =g0(x)eˆf(x). (14.14)\\nAn example is shown in Figure 14.3. We generated a training set of size\\n200 shown in the left panel. The right panel shows the reference data (blue)\\ngenerated uniformly over the rectangle containing the training data. The\\ntraining sample was labeled as class 1, and the reference sample class 0,\\nand a logistic regression model, using a tensor product of natural splines\\n(Section 5.2.1), was ﬁt to the data. Some probability contours of ˆ θ(x) are\\nshown in the right panel; these are also the contours of the density estimate\\nˆg(x), since ˆ g(x) = ˆθ(x)/(1−ˆθ(x)), is a monotone function. The contours\\nroughly capture the data density.\\nIn principle any reference density can be used for g0(x) in (14.14). In\\npractice the accuracy of the estimate ˆ g(x) can depend greatly on partic-\\nular choices. Good choices will depend on the data density g(x) and the\\nprocedure used to estimate (14.10) or (14.13). If accuracy is the goal, g0(x)\\nshould be chosen so that the resulting functions θ(x) orf(x) are approx-\\nimated easily by the method being used. However, accuracy is not always\\nthe primary goal. Both θ(x) and f(x) are monotonic functions of the den-\\nsity ratio g(x)/g0(x). They can thus be viewed as “contrast” statistics that\\nprovide information concerning departures of the data density g(x) from\\nthe chosen reference density g0(x). Therefore, in data analytic settings, a\\nchoice for g0(x) is dictated by types of departures that are deemed most\\ninteresting in the context of the speciﬁc problem at hand. For example, if\\ndepartures from uniformity are of interest, g0(x) might be the a uniform\\ndensity over the range of the variables. If departures from joint normality'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 515}, page_content='14.2 Association Rules 497\\nare of interest, a good choice for g0(x) would be a Gaussian distribution\\nwith the same mean vector and covariance matrix as the data. Departures\\nfrom independence could be investigated by using\\ng0(x) =p∏\\nj=1gj(xj), (14.15)\\nwhere gj(xj) is the marginal data density of Xj, thejth coordinate of X.\\nA sample from this independent density (14.15) is easily generated from the\\ndata itself by applying a diﬀerent random permutation to the data values\\nof each of the variables.\\nAs discussed above, unsupervised learning is concerned with revealing\\nproperties of the data density g(x). Each technique focuses on a particu-\\nlar property or set of properties. Although this approach of transforming\\nthe problem to one of supervised learning (14.10)–(14.14) seems to have\\nbeen part of the statistics folklore for some time, it does not appear to\\nhave had much impact despite its potential to bring well-developed su-\\npervised learning methodology to bear on unsupervised learning problems.\\nOne reason may be that the problem must be enlarged with a simulated\\ndata set generated by Monte Carlo techniques. Since the size of this data\\nset should be at least as large as the data sample N0≥N, the compu-\\ntation and memory requirements of the estimation procedure are at least\\ndoubled. Also, substantial computation may be required to generate the\\nMonte Carlo sample itself. Although perhaps a deterrent in the past, these\\nincreased computational requirements are becoming much less of a burden\\nas increased resources become routinely available. We illustrate the use of\\nsupervising learning methods for unsupervised learning in the next section.\\n14.2.5 Generalized Association Rules\\nThe more general problem (14.2) of ﬁnding high-density regions in the data\\nspace can be addressed using the supervised learning approach described\\nabove. Although not applicable to the huge data bases for which market\\nbasket analysis is feasible, useful information can be obtained from mod-\\nerately sized data sets. The problem (14.2) can be formulated as ﬁnding\\nsubsets of the integers J ⊂ { 1,2,... ,p }and corresponding value subsets\\nsj, j∈ Jfor the corresponding variables Xj, such that\\nˆPr\\uf8eb\\n\\uf8ed⋂\\nj∈J(Xj∈sj)\\uf8f6\\n\\uf8f8=1\\nNN∑\\ni=1I\\uf8eb\\n\\uf8ed⋂\\nj∈J(xij∈sj)\\uf8f6\\n\\uf8f8 (14.16)\\nis large. Following the nomenclature of association rule analysis, {(Xj∈\\nsj)}j∈Jwill be called a “generalized” item set. The subsets sjcorrespond-\\ning to quantitative variables are taken to be contiguous intervals wit hin'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 516}, page_content='498 14. Unsupervised Learning\\ntheir range of values, and subsets for categorical variables can involve more\\nthan a single value. The ambitious nature of this formulation precludes a\\nthorough search for all generalized item sets with support (14.16) greater\\nthan a speciﬁed minimum threshold, as was possible in the more restric-\\ntive setting of market basket analysis. Heuristic search methods must be\\nemployed, and the most one can hope for is to ﬁnd a useful collection of\\nsuch generalized item sets.\\nBoth market basket analysis (14.5) and the generalized formulation (14.1 6)\\nimplicitly reference the uniform probability distribution. One seeks item\\nsets that are more frequent than would be expected if all joint data values\\n(x1,x2,... ,x N) were uniformly distributed. This favors the discovery of\\nitem sets whose marginal constituents ( Xj∈sj) areindividually frequent,\\nthat is, the quantity\\n1\\nNN∑\\ni=1I(xij∈sj) (14.17)\\nis large. Conjunctions of frequent subsets (14.17) will tend to appear more\\noften among item sets of high support (14.16) than conjunctions of margin-\\nally less frequent subsets. This is why the rule vodka ⇒caviar is not likely\\nto be discovered in spite of a high association (lift); neither item has high\\nmarginal support, so that their joint support is especially small. Reference\\nto the uniform distribution can cause highly frequent item sets with low\\nassociations among their constituents to dominate the collection of highest\\nsupport item sets.\\nHighly frequent subsets sjare formed as disjunctions of the most fre-\\nquent Xj-values. Using the product of the variable marginal data densities\\n(14.15) as a reference distribution removes the preference for highly fre-\\nquent values of the individual variables in the discovered item sets. This is\\nbecause the density ratio g(x)/g0(x) is uniform if there are no associations\\namong the variables (complete independence), regardless of the frequency\\ndistribution of the individual variable values. Rules like vodka ⇒caviar\\nwould have a chance to emerge. It is not clear however, how to incorporate\\nreference distributions other than the uniform into the Apriori algorithm.\\nAs explained in Section 14.2.4, it is straightforward to generate a sampl e\\nfrom the product density (14.15), given the original data set.\\nAfter choosing a reference distribution, and drawing a sample from it\\nas in (14.11), one has a supervised learning problem with a binary-valued\\noutput variable Y∈ {0,1}. The goal is to use this training data to ﬁnd\\nregions\\nR=⋂\\nj∈J(Xj∈sj) (14.18)\\nfor which the target function θ(x) =E(Y|x) is relatively large. In addition,\\none might wish to require that the datasupport of these regions'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 517}, page_content='14.2 Association Rules 499\\nT(R) =∫\\nx∈Rg(x)dx (14.19)\\nnot be too small.\\n14.2.6 Choice of Supervised Learning Method\\nThe regions (14.18) are deﬁned by conjunctive rules. Hence supervised\\nmethods that learn such rules would be most appropriate in this context.\\nThe terminal nodes of a CART decision tree are deﬁned by rules precisely\\nof the form (14.18). Applying CART to the pooled data (14.11) will pro-\\nduce a decision tree that attempts to model the target (14.10) over the\\nentire data space by a disjoint set of regions (terminal nodes). Each region\\nis deﬁned by a rule of the form (14.18). Those terminal nodes twith high\\naverage y-values\\n¯yt= ave( yi|xi∈t)\\nare candidates for high-support generalized item sets (14.16). The actual\\n(data) support is given by\\nT(R) = ¯yt≤Nt\\nN+N0,\\nwhere Ntis the number of (pooled) observations within the region repre-\\nsented by the terminal node. By examining the resulting decision tree, one\\nmight discover interesting generalized item sets of relatively high-support.\\nThese can then be partitioned into antecedents and consequents in a search\\nfor generalized association rules of high conﬁdence and/or lift.\\nAnother natural learning method for this purpose is the patient rule\\ninduction method PRIM described in Section 9.3. PRIM also produces\\nrules precisely of the form (14.18), but it is especially designed for ﬁnding\\nhigh-support regions that maximize the average target (14.10) value within\\nthem, rather than trying to model the target function over the entire data\\nspace. It also provides more control over the support/average-target-value\\ntradeoﬀ.\\nExercise 14.3 addresses an issue that arises with either of these methods\\nwhen we generate random data from the product of the marginal distribu-\\ntions.\\n14.2.7 Example: Market Basket Analysis (Continued)\\nWe illustrate the use of PRIM on the demographics data of Table 14.1.\\nThree of the high-support generalized item sets emerging from the PRIM\\nanalysis were the following:\\nItem set 1: Support= 24%.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 518}, page_content='500 14. Unsupervised Learning\\n\\uf8ee\\n\\uf8f0marital status = married\\nhouseholder status = own\\ntype of home ̸=apartment\\uf8f9\\n\\uf8fb\\nItem set 2: Support= 24%.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8f0age≤24\\nmarital status ∈ {living together-not married, single }\\noccupation /∈ {professional, homemaker, retired }\\nhouseholder status ∈ {rent, live with family }\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fb\\nItem set 3: Support= 15%.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0householder status = rent\\ntype of home ̸=house\\nnumber in household ≤2\\nnumber of children = 0\\noccupation /∈ {homemaker, student, unemployed }\\nincome ∈[$20,000 ,$150,000]\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\nGeneralized association rules derived from these item sets with conﬁdence\\n(14.8) greater than 95% are the following:\\nAssociation rule 1: Support 25%, conﬁdence 99.7% and lift 1.35.\\n[\\nmarital status = married\\nhouseholder status = own]\\n⇓\\ntype of home ̸=apartment\\nAssociation rule 2: Support 25%, conﬁdence 98.7% and lift 1.97.\\n\\uf8ee\\n\\uf8f0age≤24\\noccupation /∈ {professional, homemaker, retired }\\nhouseholder status ∈ {rent, live with family }\\uf8f9\\n\\uf8fb\\n⇓\\nmarital status ∈ {single, living together-not married }\\nAssociation rule 3: Support 25%, conﬁdence 95.9% and lift 2.61.\\n[householder status = own\\ntype of home ̸=apartment]\\n⇓\\nmarital status = married'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 519}, page_content='14.3 Cluster Analysis 501\\nAssociation rule 4: Support 15%, conﬁdence 95.4% and lift 1.50.\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0householder status = rent\\ntype of home ̸=house\\nnumber in household ≤2\\noccupation /∈ {homemaker, student, unemployed }\\nincome ∈[$20,000 ,$150,000]\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n⇓\\nnumber of children = 0\\nThere are no great surprises among these particular rules. For the most\\npart they verify intuition. In other contexts where there is less prior in-\\nformation available, unexpected results have a greater chance to emerge.\\nThese results do illustrate the type of information generalized associatio n\\nrules can provide, and that the supervised learning approach, coupled with\\na ruled induction method such as CART or PRIM, can uncover item sets\\nexhibiting high associations among their constituents.\\nHow do these generalized association rules compare to those found earlier\\nby the Apriori algorithm? Since the Apriori procedure gives thousands of\\nrules, it is diﬃcult to compare them. However some general points can be\\nmade. The Apriori algorithm is exhaustive—it ﬁnds allrules with support\\ngreater than a speciﬁed amount. In contrast, PRIM is a greedy algorithm\\nand is not guaranteed to give an “optimal” set of rules. On the other hand,\\nthe Apriori algorithm can deal only with dummy variables and hence could\\nnot ﬁnd some of the above rules. For example, since type of home is a\\ncategorical input, with a dummy variable for each level, Apriori could not\\nﬁnd a rule involving the set\\ntype of home ̸=apartment .\\nTo ﬁnd this set, we would have to code a dummy variable for apartment\\nversus the other categories of type of home. It will not generally be feasible\\nto precode all such potentially interesting comparisons.\\n14.3 Cluster Analysis\\nCluster analysis, also called data segmentation, has a variety of goals. All\\nrelate to grouping or segmenting a collection of objects into subsets or\\n“clusters,” such that those within each cluster are more closely related to\\none another than objects assigned to diﬀerent clusters. An object can be\\ndescribed by a set of measurements, or by its relation to other objects.\\nIn addition, the goal is sometimes to arrange the clusters into a natural\\nhierarchy. This involves successively grouping the clusters themselves so'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 520}, page_content='502 14. Unsupervised Learning\\n• •••\\n••••\\n• •••\\n••\\n•••••• • ••\\n•••\\n••\\n••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\n••\\n••••\\n••\\n••••\\n•••\\n••\\n••\\n••\\n•••\\n••\\n••• ••\\n•\\n•••\\n•••••\\n••••••\\n•\\n••••\\n•••••\\n•••••\\n•••••\\n••••\\n•• •\\n•••••\\n•••\\n••\\n•••\\n••\\n•••\\n••••\\n•••\\n••• ••\\nX1X2\\nFIGURE 14.4. Simulated data in the plane, clustered into three classes (repr e-\\nsented by orange, blue and green) by the K-means clustering algorithm\\nthat at each level of the hierarchy, clusters within the same group are more\\nsimilar to each other than those in diﬀerent groups.\\nCluster analysis is also used to form descriptive statistics to ascertain\\nwhether or not the data consists of a set distinct subgroups, each group\\nrepresenting objects with substantially diﬀerent properties. This latter goa l\\nrequires an assessment of the degree of diﬀerence between the objects as-\\nsigned to the respective clusters.\\nCentral to all of the goals of cluster analysis is the notion of the degree of\\nsimilarity (or dissimilarity) between the individual objects being clustered.\\nA clustering method attempts to group the objects based on the deﬁnition\\nof similarity supplied to it. This can only come from subject matter consid-\\nerations. The situation is somewhat similar to the speciﬁcation of a loss or\\ncost function in prediction problems (supervised learning). There the cost\\nassociated with an inaccurate prediction depends on considerations outside\\nthe data.\\nFigure 14.4 shows some simulated data clustered into three groups via\\nthe popular K-means algorithm. In this case two of the clusters are not\\nwell separated, so that “segmentation” more accurately describes the part\\nof this process than “clustering.” K-means clustering starts with guesses\\nfor the three cluster centers. Then it alternates the following steps until\\nconvergence:\\n•for each data point, the closest cluster center (in Euclidean distance)\\nis identiﬁed;'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 521}, page_content='14.3 Cluster Analysis 503\\n•each cluster center is replaced by the coordinate-wise average of all\\ndata points that are closest to it.\\nWe describe K-means clustering in more detail later, including the prob-\\nlem of how to choose the number of clusters (three in this example). K-\\nmeans clustering is a top-down procedure, while other cluster approaches\\nthat we discuss are bottom-up . Fundamental to all clustering techniques is\\nthe choice of distance or dissimilarity measure between two objects. We\\nﬁrst discuss distance measures before describing a variety of algorithms for\\nclustering.\\n14.3.1 Proximity Matrices\\nSometimes the data is represented directly in terms of the proximity (alike-\\nness or aﬃnity) between pairs of objects. These can be either similarities or\\ndissimilarities (diﬀerence or lack of aﬃnity). For example, in social science\\nexperiments, participants are asked to judge by how much certain objects\\ndiﬀer from one another. Dissimilarities can then be computed by averaging\\nover the collection of such judgments. This type of data can be represented\\nby an N×Nmatrix D, where Nis the number of objects, and each element\\ndii′records the proximity between the ith and i′th objects. This matrix is\\nthen provided as input to the clustering algorithm.\\nMost algorithms presume a matrix of dissimilarities with nonnegative\\nentries and zero diagonal elements: dii= 0, i= 1,2,... ,N. If the original\\ndata were collected as similarities, a suitable monotone-decreasing function\\ncan be used to convert them to dissimilarities. Also, most algorithms as -\\nsume symmetric dissimilarity matrices, so if the original matrix Dis not\\nsymmetric it must be replaced by ( D+DT)/2. Subjectively judged dissimi-\\nlarities are seldom distances in the strict sense, since the triangle inequality\\ndii′≤dik+di′k, for all k∈ {1,... ,N }does not hold. Thus, some algorithms\\nthat assume distances cannot be used with such data.\\n14.3.2 Dissimilarities Based on Attributes\\nMost often we have measurements xijfori= 1,2,... ,N , on variables\\nj= 1,2,... ,p (also called attributes ). Since most of the popular clustering\\nalgorithms take a dissimilarity matrix as their input, we must ﬁrst const ruct\\npairwise dissimilarities between the observations. In the most common cas e,\\nwe deﬁne a dissimilarity dj(xij,xi′j) between values of the jth attribute,\\nand then deﬁne\\nD(xi,xi′) =p∑\\nj=1dj(xij,xi′j) (14.20)\\nas the dissimilarity between objects iandi′. By far the most common\\nchoice is squared distance'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 522}, page_content='504 14. Unsupervised Learning\\ndj(xij,xi′j) = (xij−xi′j)2. (14.21)\\nHowever, other choices are possible, and can lead to potentially diﬀerent\\nresults. For nonquantitative attributes (e.g., categorical data), squared dis-\\ntance may not be appropriate. In addition, it is sometimes desirable to\\nweigh attributes diﬀerently rather than giving them equal weight as in\\n(14.20).\\nWe ﬁrst discuss alternatives in terms of the attribute type:\\nQuantitative variables. Measurements of this type of variable or attribute\\nare represented by continuous real-valued numbers. It is natural to\\ndeﬁne the “error” between them as a monotone-increasing function\\nof their absolute diﬀerence\\nd(xi,xi′) =l(|xi−xi′|).\\nBesides squared-error loss ( xi−xi′)2, a common choice is the identity\\n(absolute error). The former places more emphasis on larger diﬀer-\\nences than smaller ones. Alternatively, clustering can be based on the\\ncorrelation\\nρ(xi,xi′) =∑\\nj(xij−¯xi)(xi′j−¯xi′)√∑\\nj(xij−¯xi)2∑\\nj(xi′j−¯xi′)2, (14.22)\\nwith ¯xi=∑\\njxij/p. Note that this is averaged over variables , not ob-\\nservations. If the observations are ﬁrst standardized, then∑\\nj(xij−\\nxi′j)2∝2(1−ρ(xi,xi′)). Hence clustering based on correlation (simi-\\nlarity) is equivalent to that based on squared distance (dissimilarity).\\nOrdinal variables. The values of this type of variable are often represented\\nas contiguous integers, and the realizable values are considered to be\\nan ordered set. Examples are academic grades (A, B, C, D, F), degree\\nof preference (can’t stand, dislike, OK, like, terriﬁc). Rank data are a\\nspecial kind of ordinal data. Error measures for ordinal variables are\\ngenerally deﬁned by replacing their Moriginal values with\\ni−1/2\\nM, i= 1,... ,M (14.23)\\nin the prescribed order of their original values. They are then treated\\nas quantitative variables on this scale.\\nCategorical variables. With unordered categorical (also called nominal)\\nvariables, the degree-of-diﬀerence between pairs of values must be\\ndelineated explicitly. If the variable assumes Mdistinct values, these\\ncan be arranged in a symmetric M×Mmatrix with elements Lrr′=\\nLr′r,Lrr= 0,Lrr′≥0. The most common choice is Lrr′= 1 for all\\nr̸=r′, while unequal losses can be used to emphasize some errors\\nmore than others.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 523}, page_content='14.3 Cluster Analysis 505\\n14.3.3 Object Dissimilarity\\nNext we deﬁne a procedure for combining the p-individual attribute dissim-\\nilarities dj(xij,xi′j), j= 1,2,... ,p into a single overall measure of dissim-\\nilarity D(xi,xi′) between two objects or observations ( xi,xi′) possessing\\nthe respective attribute values. This is nearly always done by means of a\\nweighted average (convex combination)\\nD(xi,xi′) =p∑\\nj=1wj≤dj(xij,xi′j);p∑\\nj=1wj= 1. (14.24)\\nHerewjis a weight assigned to the jth attribute regulating the relative\\ninﬂuence of that variable in determining the overall dissimilarity between\\nobjects. This choice should be based on subject matter considerations.\\nIt is important to realize that setting the weight wjto the same value\\nfor each variable (say, wj= 1∀j) does notnecessarily give all attributes\\nequal inﬂuence. The inﬂuence of the jth attribute Xjon object dissimilarity\\nD(xi,xi′) (14.24) depends upon its relative contribution to the average\\nobject dissimilarity measure over all pairs of observations in the data set\\n¯D=1\\nN2N∑\\ni=1N∑\\ni′=1D(xi,xi′) =p∑\\nj=1wj≤¯dj,\\nwith\\n¯dj=1\\nN2N∑\\ni=1N∑\\ni′=1dj(xij,xi′j) (14.25)\\nbeing the average dissimilarity on the jth attribute. Thus, the relative in-\\nﬂuence of the jth variable is wj≤¯dj, and setting wj∼1/¯djwould give all\\nattributes equal inﬂuence in characterizing overall dissimilarity between ob-\\njects. For example, with pquantitative variables and squared-error distance\\nused for each coordinate, then (14.24) becomes the (weighted) squared Eu-\\nclidean distance\\nDI(xi,xi′) =p∑\\nj=1wj≤(xij−xi′j)2(14.26)\\nbetween pairs of points in an IRp, with the quantitative variables as axes.\\nIn this case (14.25) becomes\\n¯dj=1\\nN2N∑\\ni=1N∑\\ni′=1(xij−xi′j)2= 2≤varj, (14.27)\\nwhere var jis the sample estimate of Var( Xj). Thus, the relative impor-\\ntance of each such variable is proportional to its variance over the data'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 524}, page_content='506 14. Unsupervised Learning\\n-6 -4 -2 0 2 4-6 -4 -2 0 2 4•••\\n••\\n•••\\n••••\\n••\\n••••••\\n•\\n••\\n••\\n••\\n•\\n••••••••\\n••••••••\\n••\\n•••• •••\\n••••\\n•••\\n•••• •••\\n••\\n••••\\n••••\\n••\\n•••••\\n•••\\n•••\\n•••••\\n•••\\n••\\n-2 -1 0 1 2-2 -1 0 1 2••\\n••••\\n•\\n••••\\n•\\n••\\n•\\n•\\n••••\\n•\\n•••\\n•••\\n••\\n•••\\n•••\\n•••••\\n••\\n•••\\n•••\\n••\\n•\\n•\\n•••\\n•\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n•\\n•••••\\n•••\\n•\\n•••\\n••\\n••••••••\\n••\\n••\\n••\\nX1 X1\\nX2X2\\nFIGURE 14.5. Simulated data: on the left, K-means clustering (with K=2) has\\nbeen applied to the raw data. The two colors indicate the clust er memberships. On\\nthe right, the features were ﬁrst standardized before cluster ing. This is equivalent\\nto using feature weights 1/[2≤var(Xj)]. The standardization has obscured the two\\nwell-separated groups. Note that each plot uses the same unit s in the horizontal\\nand vertical axes.\\nset. In general, setting wj= 1/¯djfor all attributes, irrespective of type,\\nwill cause each one of them to equally inﬂuence the overall dissimilarity\\nbetween pairs of objects ( xi,xi′). Although this may seem reasonable, and\\nis often recommended, it can be highly counterproductive. If the goal is to\\nsegment the data into groups of similar objects, all attributes may not con-\\ntribute equally to the (problem-dependent) notion of dissimilarity between\\nobjects. Some attribute value diﬀerences may reﬂect greater actual object\\ndissimilarity in the context of the problem domain.\\nIf the goal is to discover natural groupings in the data, some attributes\\nmay exhibit more of a grouping tendency than others. Variables that are\\nmore relevant in separating the groups should be assigned a higher inﬂu-\\nence in deﬁning object dissimilarity. Giving all attributes equal inﬂuence\\nin this case will tend to obscure the groups to the point where a clustering\\nalgorithm cannot uncover them. Figure 14.5 shows an example.\\nAlthough simple generic prescriptions for choosing the individual at-\\ntribute dissimilarities dj(xij,xi′j) and their weights wjcan be comforting,\\nthere is no substitute for careful thought in the context of each individ-\\nual problem. Specifying an appropriate dissimilarity measure is far more\\nimportant in obtaining success with clustering than choice of clustering\\nalgorithm. This aspect of the problem is emphasized less in the cluster-\\ning literature than the algorithms themselves, since it depends on domain\\nknowledge speciﬁcs and is less amenable to general research.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 525}, page_content='14.3 Cluster Analysis 507\\nFinally, often observations have missing values in one or more of the\\nattributes. The most common method of incorporating missing values in\\ndissimilarity calculations (14.24) is to omit each observation pair xij,xi′j\\nhaving at least one value missing, when computing the dissimilarity be-\\ntween observations xiandx′\\ni. This method can fail in the circumstance\\nwhen both observations have no measured values in common. In this case\\nboth observations could be deleted from the analysis. Alternatively, the\\nmissing values could be imputed using the mean or median of each attribute\\nover the nonmissing data. For categorical variables, one could consider the\\nvalue “missing” as just another categorical value, if it were reasonable to\\nconsider two objects as being similar if they both have missing values on\\nthe same variables.\\n14.3.4 Clustering Algorithms\\nThe goal of cluster analysis is to partition the observations into groups\\n(“clusters”) so that the pairwise dissimilarities between those assigned t o\\nthe same cluster tend to be smaller than those in diﬀerent clusters. Clus-\\ntering algorithms fall into three distinct types: combinatorial algorit hms,\\nmixture modeling, and mode seeking.\\nCombinatorial algorithms work directly on the observed data with no\\ndirect reference to an underlying probability model. Mixture modeling sup-\\nposes that the data is an i.i.dsample from some population described by a\\nprobability density function. This density function is characterized by a pa-\\nrameterized model taken to be a mixture of component density functions;\\neach component density describes one of the clusters. This model is then ﬁt\\nto the data by maximum likelihood or corresponding Bayesian approaches.\\nMode seekers (“bump hunters”) take a nonparametric perspective, attempt-\\ning to directly estimate distinct modes of the probability density function.\\nObservations “closest” to each respective mode then deﬁne the individual\\nclusters.\\nMixture modeling is described in Section 6.8. The PRIM algorithm, dis-\\ncussed in Sections 9.3 and 14.2.5, is an example of mode seeking or “bump\\nhunting.” We discuss combinatorial algorithms next.\\n14.3.5 Combinatorial Algorithms\\nThe most popular clustering algorithms directly assign each observation\\nto a group or cluster without regard to a probability model describing the\\ndata. Each observation is uniquely labeled by an integer i∈ {1,≤ ≤ ≤,N}.\\nA prespeciﬁed number of clusters K < N is postulated, and each one is\\nlabeled by an integer k∈ {1,... ,K }. Each observation is assigned to one\\nand only one cluster. These assignments can be characterized by a many-\\nto-one mapping, or encoder k=C(i), that assigns the ith observation to\\nthekth cluster. One seeks the particular encoder C∗(i) that achieves the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 526}, page_content='508 14. Unsupervised Learning\\nrequired goal (details below), based on the dissimilarities d(xi,xi′) between\\nevery pair of observations. These are speciﬁed by the user as described\\nabove. Generally, the encoder C(i) is explicitly delineated by giving its\\nvalue (cluster assignment) for each observation i. Thus, the “parameters”\\nof the procedure are the individual cluster assignments for each of the N\\nobservations. These are adjusted so as to minimize a “loss” function that\\ncharacterizes the degree to which the clustering goal is notmet.\\nOne approach is to directly specify a mathematical loss function and\\nattempt to minimize it through some combinatorial optimization algorit hm.\\nSince the goal is to assign close points to the same cluster, a natural loss\\n(or “energy”) function would be\\nW(C) =1\\n2K∑\\nk=1∑\\nC(i)=k∑\\nC(i′)=kd(xi,xi′). (14.28)\\nThis criterion characterizes the extent to which observations assigned to\\nthe same cluster tend to be close to one another. It is sometimes referred\\nto as the “within cluster” point scatter since\\nT=1\\n2N∑\\ni=1N∑\\ni′=1dii′=1\\n2K∑\\nk=1∑\\nC(i)=k\\uf8eb\\n\\uf8ed∑\\nC(i′)=kdii′+∑\\nC(i′)̸=kdii′\\uf8f6\\n\\uf8f8,\\nor\\nT=W(C) +B(C),\\nwhere dii′=d(xi,xi′). Here Tis thetotalpoint scatter, which is a constant\\ngiven the data, independent of cluster assignment. The quantity\\nB(C) =1\\n2K∑\\nk=1∑\\nC(i)=k∑\\nC(i′)̸=kdii′ (14.29)\\nis the between-cluster point scatter. This will tend to be large when obser-\\nvations assigned to diﬀerent clusters are far apart. Thus one has\\nW(C) =T−B(C)\\nand minimizing W(C) is equivalent to maximizing B(C).\\nCluster analysis by combinatorial optimization is straightforward in prin-\\nciple. One simply minimizes Wor equivalently maximizes Bover all pos-\\nsible assignments of the Ndata points to Kclusters. Unfortunately, such\\noptimization by complete enumeration is feasible only for very small data\\nsets. The number of distinct assignments is (Jain and Dubes, 1988)\\nS(N,K) =1\\nK!K∑\\nk=1(−1)K−k(K\\nk)\\nkN. (14.30)\\nFor example, S(10,4) = 34 ,105 which is quite feasible. But, S(N,K) grows\\nvery rapidly with increasing values of its arguments. Already S(19,4)≃'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 527}, page_content='14.3 Cluster Analysis 509\\n1010, and most clustering problems involve much larger data sets than\\nN= 19. For this reason, practical clustering algorithms are able to examine\\nonly a very small fraction of all possible encoders k=C(i). The goal is to\\nidentify a small subset that is likely to contain the optimal one, or at least\\na good suboptimal partition.\\nSuch feasible strategies are based on iterative greedy descent. An initial\\npartition is speciﬁed. At each iterative step, the cluster assignments are\\nchanged in such a way that the value of the criterion is improved from\\nits previous value. Clustering algorithms of this type diﬀer in their pre-\\nscriptions for modifying the cluster assignments at each iteration. When\\nthe prescription is unable to provide an improvement, the algorithm ter-\\nminates with the current assignments as its solution. Since the assignment\\nof observations to clusters at any iteration is a perturbation of that for the\\nprevious iteration, only a very small fraction of all possible assignmen ts\\n(14.30) are examined. However, these algorithms converge to localoptima\\nwhich may be highly suboptimal when compared to the global optimum.\\n14.3.6 K-means\\nTheK-means algorithm is one of the most popular iterative descent clus-\\ntering methods. It is intended for situations in which all variables are of\\nthe quantitative type, and squared Euclidean distance\\nd(xi,xi′) =p∑\\nj=1(xij−xi′j)2=||xi−xi′||2\\nis chosen as the dissimilarity measure. Note that weighted Euclidean dis-\\ntance can be used by redeﬁning the xijvalues (Exercise 14.1).\\nThe within-point scatter (14.28) can be written as\\nW(C) =1\\n2K∑\\nk=1∑\\nC(i)=k∑\\nC(i′)=k||xi−xi′||2\\n=K∑\\nk=1Nk∑\\nC(i)=k||xi−¯xk||2, (14.31)\\nwhere ¯ xk= (¯x1k,... ,¯xpk) is the mean vector associated with the kth clus-\\nter, and Nk=∑N\\ni=1I(C(i) =k). Thus, the criterion is minimized by\\nassigning the Nobservations to the Kclusters in such a way that within\\neach cluster the average dissimilarity of the observations from the cluster\\nmean, as deﬁned by the points in that cluster, is minimized.\\nAn iterative descent algorithm for solving'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 528}, page_content='510 14. Unsupervised Learning\\nAlgorithm 14.1 K-means Clustering.\\n1. For a given cluster assignment C, the total cluster variance (14.33) is\\nminimized with respect to {m1,... ,m K}yielding the means of the\\ncurrently assigned clusters (14.32).\\n2. Given a current set of means {m1,... ,m K}, (14.33) is minimized by\\nassigning each observation to the closest (current) cluster mean. That\\nis,\\nC(i) = argmin\\n1≤k≤K||xi−mk||2. (14.34)\\n3. Steps 1 and 2 are iterated until the assignments do not change.\\nC∗= min\\nCK∑\\nk=1Nk∑\\nC(i)=k||xi−¯xk||2\\ncan be obtained by noting that for any set of observations S\\n¯xS= argmin\\nm∑\\ni∈S||xi−m||2. (14.32)\\nHence we can obtain C∗by solving the enlarged optimization problem\\nmin\\nC,{mk}K\\n1K∑\\nk=1Nk∑\\nC(i)=k||xi−mk||2. (14.33)\\nThis can be minimized by an alternating optimization procedure given in\\nAlgorithm 14.1.\\nEach of steps 1 and 2 reduces the value of the criterion (14.33), so that\\nconvergence is assured. However, the result may represent a suboptimal\\nlocal minimum. The algorithm of Hartigan and Wong (1979) goes further,\\nand ensures that there is no single switch of an observation from one group\\nto another group that will decrease the objective. In addition, one should\\nstart the algorithm with many diﬀerent random choices for the starting\\nmeans, and choose the solution having smallest value of the objective func-\\ntion.\\nFigure 14.6 shows some of the K-means iterations for the simulated data\\nof Figure 14.4. The centroids are depicted by “O”s. The straight lines show\\nthe partitioning of points, each sector being the set of points closest to\\neach centroid. This partitioning is called the Voronoi tessellation . After 20\\niterations the procedure has converged.\\n14.3.7 Gaussian Mixtures as Soft K-means Clustering\\nTheK-means clustering procedure is closely related to the EM algorithm\\nfor estimating a certain Gaussian mixture model. (Sections 6.8 and 8.5.1).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 529}, page_content='14.3 Cluster Analysis 511\\n-4 -2 0 2 4 6-2 0 2 4 6Initial Centroids\\n••••\\n••••••••\\n••\\n•••••••••\\n••••\\n••\\n••\\n••\\n•\\n•••••\\n•••\\n••\\n•••\\n••\\n••••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n••••••••\\n•••••\\n••••••••\\n•••\\n••••••\\n•••\\n••\\n•••\\n••\\n•••\\n•••\\n••\\n•••\\n•••••\\n•••\\n•••\\n••••\\n••••••••\\n••\\n•••••••••\\n••••\\n••\\n••\\n••\\n•\\n•••••\\n•••\\n••\\n•••\\n••\\n••••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n••••••••\\n•••••\\n••••••••\\n•••\\n••••••\\n•••\\n••\\n•••\\n••\\n•••\\n•••\\n••\\n•••\\n•••••\\n•••\\n•••Initial Partition\\n••••\\n••••••••\\n••\\n•••••••••\\n•••\\n••\\n••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••••\\n••\\n••\\n••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n••••••\\n••\\n••••••\\n•••\\n••••\\n••\\n••\\n••••\\n••\\n•••\\n••\\n•••\\n••\\n••\\n•••\\n•••\\n•••••Iteration Number  2\\n•••\\n•••\\n••••\\n••••••••\\n••\\n•••••••••\\n•••\\n••\\n••\\n••\\n•\\n•••••\\n••••\\n••\\n•••\\n••\\n••••\\n••\\n••••\\n•••\\n••\\n••\\n••\\n•••\\n••\\n••• ••\\n•\\n••••••••\\n••••••\\n•\\n••••\\n•••••\\n•••••\\n•••••••••\\n•••\\n•••••\\n•••\\n••\\n•••\\n••\\n•••\\n••••\\n•••\\n•••••Iteration Number  20\\n•••\\n•••\\nFIGURE 14.6. Successive iterations of the K-means clustering algorithm for\\nthe simulated data of Figure 14.4.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 530}, page_content='512 14. Unsupervised Learning\\n• •Responsibilities\\n0.0 0.2 0.4 0.6 0.8 1.0\\n• •Responsibilities\\n0.0 0.2 0.4 0.6 0.8 1.0σ= 1.0 σ= 1.0\\nσ= 0.2 σ= 0.2\\nFIGURE 14.7. (Left panels:) two Gaussian densities g0(x) and g1(x)(blue and\\norange) on the real line, and a single data point (green dot) at x= 0.5. The colored\\nsquares are plotted at x=−1.0andx= 1.0, the means of each density. (Right\\npanels:) the relative densities g0(x)/(g0(x) +g1(x))andg1(x)/(g0(x) +g1(x)),\\ncalled the “responsibilities” of each cluster, for this data point. In the top panels,\\nthe Gaussian standard deviation σ= 1.0; in the bottom panels σ= 0.2. The\\nEM algorithm uses these responsibilities to make a “soft” ass ignment of each\\ndata point to each of the two clusters. When σis fairly large, the responsibilities\\ncan be near 0.5(they are 0.36and0.64 in the top right panel). As σ→0, the\\nresponsibilities →1, for the cluster center closest to the target point, and 0for\\nall other clusters. This “hard” assignment is seen in the botto m right panel.\\nThe E-step of the EM algorithm assigns “responsibilities” for each data\\npoint based in its relative density under each mixture component, while\\nthe M-step recomputes the component density parameters based on the\\ncurrent responsibilities. Suppose we specify Kmixture components, each\\nwith a Gaussian density having scalar covariance matrix σ2I. Then the\\nrelative density under each mixture component is a monotone function of\\nthe Euclidean distance between the data point and the mixture center.\\nHence in this setup EM is a “soft” version of K-means clustering, making\\nprobabilistic (rather than deterministic) assignments of points to cluster\\ncenters. As the variance σ2→0, these probabilities become 0 and 1, and\\nthe two methods coincide. Details are given in Exercise 14.2. Figure 14.7\\nillustrates this result for two clusters on the real line.\\n14.3.8 Example: Human Tumor Microarray Data\\nWe apply K-means clustering to the human tumor microarray data de-\\nscribed in Chapter 1. This is an example of high-dimensional clustering.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 531}, page_content='14.3 Cluster Analysis 513\\nNumber of Clusters KSum of Squares\\n2 4 6 8 10160000 200000 240000•\\n•\\n•\\n•••••••\\nFIGURE 14.8. Total within-cluster sum of squares for K-means clustering ap-\\nplied to the human tumor microarray data.\\nTABLE 14.2. Human tumor data: number of cancer cases of each type, in each\\nof the three clusters from K-means clustering.\\nCluster Breast CNS Colon K562 Leukemia MCF7\\n1 3 5 0 0 0 0\\n2 2 0 0 2 6 2\\n3 2 0 7 0 0 0\\nCluster Melanoma NSCLC Ovarian Prostate Renal Unknown\\n1 1 7 6 2 9 1\\n2 7 2 0 0 0 0\\n3 0 0 0 0 0 0\\nThe data are a 6830 ×64 matrix of real numbers, each representing an\\nexpression measurement for a gene (row) and sample (column). Here we\\ncluster the samples, each of which is a vector of length 6830, correspond-\\ning to expression values for the 6830 genes. Each sample has a label such\\nasbreast (for breast cancer), melanoma , and so on; we don’t use these la-\\nbels in the clustering, but will examine posthoc which labels fall into which\\nclusters.\\nWe applied K-means clustering with Krunning from 1 to 10, and com-\\nputed the total within-sum of squares for each clustering, shown in Fig-\\nure 14.8. Typically one looks for a kink in the sum of squares curve (or its\\nlogarithm) to locate the optimal number of clusters (see Section 14.3.11).\\nHere there is no clear indication: for illustration we chose K= 3 giving the\\nthree clusters shown in Table 14.2.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 532}, page_content='514 14. Unsupervised Learning\\nFIGURE 14.9. Sir Ronald A. Fisher ( 1890−1962) was one of the founders\\nof modern day statistics, to whom we owe maximum-likelihood, suﬃciency, and\\nmany other fundamental concepts. The image on the left is a 1024×1024grayscale\\nimage at 8bits per pixel. The center image is the result of 2×2block VQ, using\\n200code vectors, with a compression rate of 1.9bits/pixel. The right image uses\\nonly four code vectors, with a compression rate of 0.50bits/pixel\\nWe see that the procedure is successful at grouping together samples of\\nthe same cancer. In fact, the two breast cancers in the second cluster were\\nlater found to be misdiagnosed and were melanomas that had metastasized.\\nHowever, K-means clustering has shortcomings in this application. For one,\\nit does not give a linear ordering of objects within a cluster: we have simply\\nlisted them in alphabetic order above. Secondly, as the number of clusters\\nKis changed, the cluster memberships can change in arbitrary ways. That\\nis, with say four clusters, the clusters need not be nested within the three\\nclusters above. For these reasons, hierarchical clustering (described later),\\nis probably preferable for this application.\\n14.3.9 Vector Quantization\\nTheK-means clustering algorithm represents a key tool in the apparently\\nunrelated area of image and signal compression, particularly in vector quan-\\ntization or VQ (Gersho and Gray, 1992). The left image in Figure 14.92is a\\ndigitized photograph of a famous statistician, Sir Ronald Fisher. It consist s\\nof 1024 ×1024 pixels, where each pixel is a grayscale value ranging from 0\\nto 255, and hence requires 8 bits of storage per pixel. The entire image oc-\\ncupies 1 megabyte of storage. The center image is a VQ-compressed version\\nof the left panel, and requires 0 .239 of the storage (at some loss in quality).\\nThe right image is compressed even more, and requires only 0 .0625 of the\\nstorage (at a considerable loss in quality).\\nThe version of VQ implemented here ﬁrst breaks the image into small\\nblocks, in this case 2 ×2 blocks of pixels. Each of the 512 ×512 blocks of four\\n2This example was prepared by Maya Gupta.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 533}, page_content='14.3 Cluster Analysis 515\\nnumbers is regarded as a vector in IR4. AK-means clustering algorithm\\n(also known as Lloyd’s algorithm in this context) is run in this space.\\nThe center image uses K= 200, while the right image K= 4. Each of\\nthe 512 ×512 pixel blocks (or points) is approximated by its closest cluster\\ncentroid, known as a codeword. The clustering process is called the encoding\\nstep, and the collection of centroids is called the codebook .\\nTo represent the approximated image, we need to supply for each block\\nthe identity of the codebook entry that approximates it. This will require\\nlog2(K) bits per block. We also need to supply the codebook itself, which\\nisK×4 real numbers (typically negligible). Overall, the storage for the\\ncompressed image amounts to log2(K)/(4≤8) of the original (0 .239 for\\nK= 200, 0 .063 for K= 4). This is typically expressed as a ratein bits\\nper pixel: log2(K)/4, which are 1 .91 and 0 .50, respectively. The process\\nof constructing the approximate image from the centroids is called the\\ndecoding step.\\nWhy do we expect VQ to work at all? The reason is that for typical\\neveryday images like photographs, many of the blocks look the same. In\\nthis case there are many almost pure white blocks, and similarly pure gray\\nblocks of various shades. These require only one block each to represent\\nthem, and then multiple pointers to that block.\\nWhat we have described is known as lossycompression, since our im-\\nages are degraded versions of the original. The degradation or distortion is\\nusually measured in terms of mean squared error. In this case D= 0.89\\nforK= 200 and D= 16.95 for K= 4. More generally a rate/distortion\\ncurve would be used to assess the tradeoﬀ. One can also perform lossless\\ncompression using block clustering, and still capitalize on the repeated pat-\\nterns. If you took the original image and losslessly compressed it, the bes t\\nyou would do is 4.48 bits per pixel.\\nWe claimed above that log2(K) bits were needed to identify each of the K\\ncodewords in the codebook. This uses a ﬁxed-length code, and is ineﬃcient\\nif some codewords occur many more times than others in the image. Using\\nShannon coding theory, we know that in general a variable length code\\nwill do better, and the rate then becomes −∑K\\nℓ=1pℓlog2(pℓ)/4. The term\\nin the numerator is the entropy of the distribution pℓof the codewords\\nin the image. Using variable length coding our rates come down to 1 .42\\nand 0.39, respectively. Finally, there are many generalizations of VQ that\\nhave been developed: for example, tree-structured VQ ﬁnds the centroids\\nwith a top-down, 2-means style algorithm, as alluded to in Section 14.3.12.\\nThis allows successive reﬁnement of the compression. Further details may\\nbe found in Gersho and Gray (1992).\\n14.3.10 K-medoids\\nAs discussed above, the K-means algorithm is appropriate when the dis-\\nsimilarity measure is taken to be squared Euclidean distance D(xi,xi′)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 534}, page_content='516 14. Unsupervised Learning\\nAlgorithm 14.2 K-medoids Clustering.\\n1. For a given cluster assignment Cﬁnd the observation in the cluster\\nminimizing total distance to other points in that cluster:\\ni∗\\nk= argmin\\n{i:C(i)=k}∑\\nC(i′)=kD(xi,xi′). (14.35)\\nThen mk=xi∗\\nk, k= 1,2,... ,K are the current estimates of the\\ncluster centers.\\n2. Given a current set of cluster centers {m1,... ,m K}, minimize the to-\\ntal error by assigning each observation to the closest (current) cluster\\ncenter:\\nC(i) = argmin\\n1≤k≤KD(xi,mk). (14.36)\\n3. Iterate steps 1 and 2 until the assignments do not change.\\n(14.112). This requires all of the variables to be of the quantitative t ype. In\\naddition, using squared Euclidean distance places the highest inﬂuence on\\nthe largest distances. This causes the procedure to lack robustness against\\noutliers that produce very large distances. These restrictions can be re-\\nmoved at the expense of computation.\\nThe only part of the K-means algorithm that assumes squared Eu-\\nclidean distance is the minimization step (14.32); the cluster representatives\\n{m1,... ,m K}in (14.33) are taken to be the means of the currently assigned\\nclusters. The algorithm can be generalized for use with arbitrarily deﬁned\\ndissimilarities D(xi,xi′) by replacing this step by an explicit optimization\\nwith respect to {m1,... ,m K}in (14.33). In the most common form, cen-\\nters for each cluster are restricted to be one of the observations assigned\\nto the cluster, as summarized in Algorithm 14.2. This algorithm assumes\\nattribute data, but the approach can also be applied to data described\\nonlyby proximity matrices (Section 14.3.1). There is no need to explicitly\\ncompute cluster centers; rather we just keep track of the indices i∗\\nk.\\nSolving (14.32) for each provisional cluster krequires an amount of com-\\nputation proportional to the number of observations assigned to it, whereas\\nfor solving (14.35) the computation increases to O(N2\\nk). Given a set of clus-\\nter “centers,” {i1,... ,i K}, obtaining the new assignments\\nC(i) = argmin\\n1≤k≤Kdii∗\\nk(14.37)\\nrequires computation proportional to K≤Nas before. Thus, K-medoids is\\nfar more computationally intensive than K-means.\\nAlternating between (14.35) and (14.37) represents a particular heuristic\\nsearch strategy for trying to solve'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 535}, page_content='14.3 Cluster Analysis 517\\nTABLE 14.3. Data from a political science survey: values are average pair wise\\ndissimilarities of countries from a questionnaire given to pol itical science students.\\nBEL BRA CHI CUB EGY FRA IND ISR USA USS YUG\\nBRA 5.58\\nCHI 7.00 6.50\\nCUB 7.08 7.00 3.83\\nEGY 4.83 5.08 8.17 5.83\\nFRA 2.17 5.75 6.67 6.92 4.92\\nIND 6.42 5.00 5.58 6.00 4.67 6.42\\nISR 3.42 5.50 6.42 6.42 5.00 3.92 6.17\\nUSA 2.50 4.92 6.25 7.33 4.50 2.25 6.33 2.75\\nUSS 6.08 6.67 4.25 2.67 6.00 6.17 6.17 6.92 6.17\\nYUG 5.25 6.83 4.50 3.75 5.75 5.42 6.08 5.83 6.67 3.67\\nZAI 4.75 3.00 6.08 6.67 5.00 5.58 4.83 6.17 5.67 6.50 6.92\\nmin\\nC,{ik}K\\n1K∑\\nk=1∑\\nC(i)=kdiik. (14.38)\\nKaufman and Rousseeuw (1990) propose an alternative strategy for directly\\nsolving (14.38) that provisionally exchanges each center ikwith an obser-\\nvation that is not currently a center, selecting the exchange that produces\\nthe greatest reduction in the value of the criterion (14.38). This is repeated\\nuntil no advantageous exchanges can be found. Massart et al. (1983) derive\\na branch-and-bound combinatorial method that ﬁnds the global minimum\\nof (14.38) that is practical only for very small data sets.\\nExample: Country Dissimilarities\\nThis example, taken from Kaufman and Rousseeuw (1990), comes from a\\nstudy in which political science students were asked to provide pairwise dis-\\nsimilarity measures for 12 countries: Belgium, Brazil, Chile, Cuba, Egypt,\\nFrance, India, Israel, United States, Union of Soviet Socialist Republics,\\nYugoslavia and Zaire. The average dissimilarity scores are given in Ta -\\nble 14.3. We applied 3-medoid clustering to these dissimilarities. Note that\\nK-means clustering could not be applied because we have only distances\\nrather than raw observations. The left panel of Figure 14.10 shows the\\ndissimilarities reordered and blocked according to the 3-medoid clustering.\\nThe right panel is a two-dimensional multidimensional scaling plot, with\\nthe 3-medoid clusters assignments indicated by colors (multidimensional\\nscaling is discussed in Section 14.8.) Both plots show three well-separated\\nclusters, but the MDS display indicates that “Egypt” falls about halfway\\nbetween two clusters.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 536}, page_content='518 14. Unsupervised Learning\\nCHICUBUSSYUGBRAINDZAIBELEGYFRAISR\\nCUBUSSYUGBRAINDZAIBELEGYFRAISRUSA\\nReordered Dissimilarity Matrix First MDS CoordinateSecond MDS Coordinate\\n-2 0 2 4-2 -1 0 1 2 3CHI\\nCUB\\nUSS\\nYUGBRAIND ZAI\\nBELEGY\\nFRAISRUSA\\nFIGURE 14.10. Survey of country dissimilarities. (Left panel:) dissimilari ties\\nreordered and blocked according to 3-medoid clustering. Heat map is coded from\\nmost similar (dark red) to least similar (bright red). (Righ t panel:) two-dimen-\\nsional multidimensional scaling plot, with 3-medoid clusters indicated by diﬀerent\\ncolors.\\n14.3.11 Practical Issues\\nIn order to apply K-means or K-medoids one must select the number of\\nclusters K∗and an initialization. The latter can be deﬁned by specifying\\nan initial set of centers {m1,... ,m K}or{i1,... ,i K}or an initial encoder\\nC(i). Usually specifying the centers is more convenient. Suggestions range\\nfrom simple random selection to a deliberate strategy based on forward\\nstepwise assignment. At each step a new center ikis chosen to minimize\\nthe criterion (14.33) or (14.38), given the centers i1,... ,i k−1chosen at the\\nprevious steps. This continues for Ksteps, thereby producing Kinitial\\ncenters with which to begin the optimization algorithm.\\nA choice for the number of clusters Kdepends on the goal. For data\\nsegmentation Kis usually deﬁned as part of the problem. For example,\\na company may employ Ksales people, and the goal is to partition a\\ncustomer database into Ksegments, one for each sales person, such that the\\ncustomers assigned to each one are as similar as possible. Often, however,\\ncluster analysis is used to provide a descriptive statistic for ascertaining t he\\nextent to which the observations comprising the data base fall into natural\\ndistinct groupings. Here the number of such groups K∗is unknown and\\none requires that it, as well as the groupings themselves, be estimated from\\nthe data.\\nData-based methods for estimating K∗typically examine the within-\\ncluster dissimilarity WKas a function of the number of clusters K. Separate\\nsolutions are obtained for K∈ {1,2,... ,K max}. The corresponding values'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 537}, page_content='14.3 Cluster Analysis 519\\n{W1,W2,... ,W Kmax}generally decrease with increasing K. This will be\\nthe case even when the criterion is evaluated on an independent test set,\\nsince a large number of cluster centers will tend to ﬁll the feature space\\ndensely and thus will be close to all data points. Thus cross-validation\\ntechniques, so useful for model selection in supervised learning, cannot be\\nutilized in this context.\\nThe intuition underlying the approach is that if there are actually K∗\\ndistinct groupings of the observations (as deﬁned by the dissimilarity mea-\\nsure), then for K < K∗the clusters returned by the algorithm will each\\ncontain a subset of the true underlying groups. That is, the solution will\\nnot assign observations in the same naturally occurring group to diﬀerent\\nestimated clusters. To the extent that this is the case, the solution criterion\\nvalue will tend to decrease substantially with each successive increase in the\\nnumber of speciﬁed clusters, WK+1≪WK, as the natural groups are suc-\\ncessively assigned to separate clusters. For K > K∗, one of the estimated\\nclusters must partition at least one of the natural groups into two sub-\\ngroups. This will tend to provide a smaller decrease in the criterion as Kis\\nfurther increased. Splitting a natural group, within which the observations\\nare all quite close to each other, reduces the criterion less than partitioning\\nthe union of two well-separated groups into their proper constituents.\\nTo the extent this scenario is realized, there will be a sharp decrease in\\nsuccessive diﬀerences in criterion value, WK−WK+1, atK=K∗. That\\nis,{WK−WK+1|K < K∗} ≫ { WK−WK+1|K≥K∗}. An estimate\\nˆK∗forK∗is then obtained by identifying a “kink” in the plot of WKas a\\nfunction of K. As with other aspects of clustering procedures, this approach\\nis somewhat heuristic.\\nThe recently proposed Gap statistic (Tibshirani et al., 2001b) compares\\nthe curve log WKto the curve obtained from data uniformly distributed\\nover a rectangle containing the data. It estimates the optimal number of\\nclusters to be the place where the gap between the two curves is largest.\\nEssentially this is an automatic way of locating the aforementioned “ki nk.”\\nIt also works reasonably well when the data fall into a single cluster, and\\nin that case will tend to estimate the optimal number of clusters to be one.\\nThis is the scenario where most other competing methods fail.\\nFigure 14.11 shows the result of the Gap statistic applied to simulated\\ndata of Figure 14.4. The left panel shows log WKfork= 1,2,... ,8 clusters\\n(green curve) and the expected value of log WKover 20 simulations from\\nuniform data (blue curve). The right panel shows the gap curve, which is the\\nexpected curve minus the observed curve. Shown also are error bars of half-\\nwidth s′\\nK=sK√\\n1 + 1/20, where sKis the standard deviation of log WK\\nover the 20 simulations. The Gap curve is maximized at K= 2 clusters. If\\nG(K) is the Gap curve at Kclusters, the formal rule for estimating K∗is\\nK∗= argmin\\nK{K|G(K)≥G(K+ 1)−s′\\nK+1}. (14.39)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 538}, page_content='520 14. Unsupervised Learning\\nNumber of Clusters2 4 6 8-3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0•\\n••\\n••\\n••••\\n•\\n•\\n•\\n•\\n•••\\nNumber of ClustersGap\\n2 4 6 8-0.5 0.0 0.5 1.0••\\n•••••\\n•logWK\\nFIGURE 14.11. (Left panel): observed (green) and expected (blue) values of\\nlogWKfor the simulated data of Figure 14.4. Both curves have been t ranslated\\nto equal zero at one cluster. (Right panel): Gap curve, equal to the diﬀerence\\nbetween the observed and expected values of logWK. The Gap estimate K∗is the\\nsmallest Kproducing a gap within one standard deviation of the gap at K+ 1;\\nhereK∗= 2.\\nThis gives K∗= 2, which looks reasonable from Figure 14.4.\\n14.3.12 Hierarchical Clustering\\nThe results of applying K-means or K-medoids clustering algorithms de-\\npend on the choice for the number of clusters to be searched and a starting\\nconﬁguration assignment. In contrast, hierarchical clustering methods do\\nnot require such speciﬁcations. Instead, they require the user to specify a\\nmeasure of dissimilarity between (disjoint) groups of observations, based\\non the pairwise dissimilarities among the observations in the two groups.\\nAs the name suggests, they produce hierarchical representations in which\\nthe clusters at each level of the hierarchy are created by merging clusters\\nat the next lower level. At the lowest level, each cluster contains a single\\nobservation. At the highest level there is only one cluster containing all of\\nthe data.\\nStrategies for hierarchical clustering divide into two basic paradigms: ag-\\nglomerative (bottom-up) and divisive (top-down). Agglomerative strategies\\nstart at the bottom and at each level recursively merge a selected pair of\\nclusters into a single cluster. This produces a grouping at the next higher\\nlevel with one less cluster. The pair chosen for merging consist of the two\\ngroups with the smallest intergroup dissimilarity. Divisive methods s tart\\nat the top and at each level recursively split one of the existing clusters at'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 539}, page_content='14.3 Cluster Analysis 521\\nthat level into two new clusters. The split is chosen to produce two new\\ngroups with the largest between-group dissimilarity. With both paradigms\\nthere are N−1 levels in the hierarchy.\\nEach level of the hierarchy represents a particular grouping of the data\\ninto disjoint clusters of observations. The entire hierarchy represents an\\nordered sequence of such groupings. It is up to the user to decide which\\nlevel (if any) actually represents a “natural” clustering in the sense that\\nobservations within each of its groups are suﬃciently more similar to eac h\\nother than to observations assigned to diﬀerent groups at that level. The\\nGap statistic described earlier can be used for this purpose.\\nRecursive binary splitting/agglomeration can be represented by a rooted\\nbinary tree. The nodes of the trees represent groups. The root node repre-\\nsents the entire data set. The Nterminal nodes each represent one of the\\nindividual observations (singleton clusters). Each nonterminal node (“par-\\nent”) has two daughter nodes. For divisive clustering the two daughters\\nrepresent the two groups resulting from the split of the parent; for agglom-\\nerative clustering the daughters represent the two groups that were merged\\nto form the parent.\\nAll agglomerative and some divisive methods (when viewed bottom-up)\\npossess a monotonicity property. That is, the dissimilarity between merged\\nclusters is monotone increasing with the level of the merger. Thus the\\nbinary tree can be plotted so that the height of each node is proportional\\nto the value of the intergroup dissimilarity between its two daughters. The\\nterminal nodes representing individual observations are all plotted at zero\\nheight. This type of graphical display is called a dendrogram .\\nA dendrogram provides a highly interpretable complete description of\\nthe hierarchical clustering in a graphical format. This is one of the main\\nreasons for the popularity of hierarchical clustering methods.\\nFor the microarray data, Figure 14.12 shows the dendrogram resulting\\nfrom agglomerative clustering with average linkage; agglomerative cl uster-\\ning and this example are discussed in more detail later in this chapter.\\nCutting the dendrogram horizontally at a particular height partitions the\\ndata into disjoint clusters represented by the vertical lines that intersect\\nit. These are the clusters that would be produced by terminating the pro-\\ncedure when the optimal intergroup dissimilarity exceeds that threshold\\ncut value. Groups that merge at high values, relative to the merger values\\nof the subgroups contained within them lower in the tree, are candidates\\nfor natural clusters. Note that this may occur at several diﬀerent levels,\\nindicating a clustering hierarchy: that is, clusters nested within clusters.\\nSuch a dendrogram is often viewed as a graphical summary of the data\\nitself, rather than a description of the results of the algorithm. However,\\nsuch interpretations should be treated with caution. First, diﬀerent hierar-\\nchical methods (see below), as well as small changes in the data, can lead\\nto quite diﬀerent dendrograms. Also, such a summary will be valid only to\\nthe extent that the pairwise observation dissimilarities possess the hierar-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 540}, page_content='522 14. Unsupervised Learning\\nCNSCNSCNSRENAL\\nBREASTCNSCNS\\nBREASTNSCLC\\nNSCLCRENAL\\nRENALRENALRENAL\\nRENALRENALRENALBREAST\\nNSCLCRENAL\\nUNKNOWN\\nOVARIAN\\nMELANOMA\\nPROSTATEOVARIANOVARIAN\\nOVARIANOVARIAN\\nOVARIAN\\nPROSTATENSCLCNSCLCNSCLCLEUKEMIAK562B-reproK562A-reproLEUKEMIA\\nLEUKEMIALEUKEMIALEUKEMIALEUKEMIA\\nCOLONCOLON\\nCOLON\\nCOLONCOLONCOLON\\nCOLONMCF7A-repro\\nBREAST\\nMCF7D-reproBREASTNSCLC\\nNSCLCNSCLCMELANOMA\\nBREASTBREAST\\nMELANOMA\\nMELANOMA\\nMELANOMAMELANOMAMELANOMA\\nMELANOMA\\nFIGURE 14.12. Dendrogram from agglomerative hierarchical clustering with\\naverage linkage to the human tumor microarray data.\\nchical structure produced by the algorithm. Hierarchical methods impose\\nhierarchical structure whether or not such structure actually exists in the\\ndata.\\nThe extent to which the hierarchical structure produced by a dendro-\\ngram actually represents the data itself can be judged by the cophenetic\\ncorrelation coeﬃcient . This is the correlation between the N(N−1)/2 pair-\\nwise observation dissimilarities dii′input to the algorithm and their corre-\\nsponding cophenetic dissimilarities Cii′derived from the dendrogram. The\\ncophenetic dissimilarity Cii′between two observations ( i,i′) is the inter-\\ngroup dissimilarity at which observations iandi′are ﬁrst joined together\\nin the same cluster.\\nThe cophenetic dissimilarity is a very restrictive dissimilarity measure.\\nFirst, the Cii′over the observations must contain many ties, since only N−1\\nof the total N(N−1)/2 values can be distinct. Also these dissimilarities\\nobey the ultrametric inequality\\nCii′≤max{Cik,Ci′k} (14.40)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 541}, page_content='14.3 Cluster Analysis 523\\nfor any three observations ( i,i′,k). As a geometric example, suppose the\\ndata were represented as points in a Euclidean coordinate system. In order\\nfor the set of interpoint distances over the data to conform to (14.40), the\\ntriangles formed by all triples of points must be isosceles triangles wit h the\\nunequal length no longer than the length of the two equal sides (Jain and\\nDubes, 1988). Therefore it is unrealistic to expect general dissimilarities\\nover arbitrary data sets to closely resemble their corresponding cophenetic\\ndissimilarities as calculated from a dendrogram, especially if there are not\\nmany tied values. Thus the dendrogram should be viewed mainly as a de-\\nscription of the clustering structure of the data as imposed by the particular\\nalgorithm employed.\\nAgglomerative Clustering\\nAgglomerative clustering algorithms begin with every observation repre-\\nsenting a singleton cluster. At each of the N−1 steps the closest two (least\\ndissimilar) clusters are merged into a single cluster, producing one less clus-\\nter at the next higher level. Therefore, a measure of dissimilarity between\\ntwo clusters (groups of observations) must be deﬁned.\\nLetGandHrepresent two such groups. The dissimilarity d(G,H) be-\\ntween GandHis computed from the set of pairwise observation dissim-\\nilarities dii′where one member of the pair iis inGand the other i′is\\ninH.Single linkage (SL) agglomerative clustering takes the intergroup\\ndissimilarity to be that of the closest (least dissimilar) pair\\ndSL(G,H) = min\\ni∈G\\ni′∈Hdii′. (14.41)\\nThis is also often called the nearest-neighbor technique. Complete linkage\\n(CL) agglomerative clustering ( furthest-neighbor technique) takes the in-\\ntergroup dissimilarity to be that of the furthest (most dissimilar) pai r\\ndCL(G,H) = max\\ni∈G\\ni′∈Hdii′. (14.42)\\nGroup average (GA) clustering uses the average dissimilarity between the\\ngroups\\ndGA(G,H) =1\\nNGNH∑\\ni∈G∑\\ni′∈Hdii′ (14.43)\\nwhere NGandNHare the respective number of observations in each group.\\nAlthough there have been many other proposals for deﬁning intergroup\\ndissimilarity in the context of agglomerative clustering, the above thr ee are\\nthe ones most commonly used. Figure 14.13 shows examples of all three.\\nIf the data dissimilarities {dii′}exhibit a strong clustering tendency, with\\neach of the clusters being compact and well separated from others, then all\\nthree methods produce similar results. Clusters are compact if all of the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 542}, page_content='524 14. Unsupervised Learning\\nAverage Linkage Complete Linkage Single Linkage\\nFIGURE 14.13. Dendrograms from agglomerative hierarchical clustering of h u-\\nman tumor microarray data.\\nobservations within them are relatively close together (small dissimilar ities)\\nas compared with observations in diﬀerent clusters. To the extent this is\\nnot the case, results will diﬀer.\\nSingle linkage (14.41) only requires that a single dissimilarity dii′,i∈G\\nandi′∈H, be small for two groups GandHto be considered close\\ntogether, irrespective of the other observation dissimilarities between the\\ngroups. It will therefore have a tendency to combine, at relatively low\\nthresholds, observations linked by a series of close intermediate observa-\\ntions. This phenomenon, referred to as chaining , is often considered a de-\\nfect of the method. The clusters produced by single linkage can violate the\\n“compactness” property that all observations within each cluster tend to\\nbe similar to one another, based on the supplied observation dissimilari-\\nties{dii′}. If we deﬁne the diameter DGof a group of observations as the\\nlargest dissimilarity among its members\\nDG= max\\ni∈G\\ni′∈Gdii′, (14.44)\\nthen single linkage can produce clusters with very large diameters.\\nComplete linkage (14.42) represents the opposite extreme. Two groups\\nGandHare considered close only if all of the observations in their union\\nare relatively similar. It will tend to produce compact clusters with small\\ndiameters (14.44). However, it can produce clusters that violate the “close-\\nness” property. That is, observations assigned to a cluster can be much'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 543}, page_content='14.3 Cluster Analysis 525\\ncloser to members of other clusters than they are to some members of their\\nown cluster.\\nGroup average clustering (14.43) represents a compromise between the\\ntwo extremes of single and complete linkage. It attempts to produce rel-\\natively compact clusters that are relatively far apart. However, its results\\ndepend on the numerical scale on which the observation dissimilarities dii′\\nare measured. Applying a monotone strictly increasing transformation h(≤)\\nto the dii′,hii′=h(dii′), can change the result produced by (14.43). In\\ncontrast, (14.41) and (14.42) depend only on the ordering of the dii′and\\nare thus invariant to such monotone transformations. This invariance is\\noften used as an argument in favor of single or complete linkage over group\\naverage methods.\\nOne can argue that group average clustering has a statistical consis-\\ntency property violated by single and complete linkage. Assume we have\\nattribute-value data XT= (X1,... ,X p) and that each cluster kis a ran-\\ndom sample from some population joint density pk(x). The complete data\\nset is a random sample from a mixture of Ksuch densities. The group\\naverage dissimilarity dGA(G,H) (14.43) is an estimate of\\n∫ ∫\\nd(x,x′)pG(x)pH(x′)dx dx′, (14.45)\\nwhere d(x,x′) is the dissimilarity between points xandx′in the space\\nof attribute values. As the sample size Napproaches inﬁnity dGA(G,H)\\n(14.43) approaches (14.45), which is a characteristic of the relationshi p\\nbetween the two densities pG(x) and pH(x). For single linkage, dSL(G,H)\\n(14.41) approaches zero as N→ ∞ independent of pG(x) and pH(x). For\\ncomplete linkage, dCL(G,H) (14.42) becomes inﬁnite as N→ ∞, again\\nindependent of the two densities. Thus, it is not clear what aspects of the\\npopulation distribution are being estimated by dSL(G,H) and dCL(G,H).\\nExample: Human Cancer Microarray Data (Continued)\\nThe left panel of Figure 14.13 shows the dendrogram resulting from average\\nlinkage agglomerative clustering of the samples (columns) of the microarra y\\ndata. The middle and right panels show the result using complete and single\\nlinkage. Average and complete linkage gave similar results, while single\\nlinkage produced unbalanced groups with long thin clusters. We focus on\\nthe average linkage clustering.\\nLikeK-means clustering, hierarchical clustering is successful at clustering\\nsimple cancers together. However it has other nice features. By cutting oﬀ\\nthe dendrogram at various heights, diﬀerent numbers of clusters emerge,\\nand the sets of clusters are nested within one another. Secondly, it gives\\nsome partial ordering information about the samples. In Figure 14.14, w e\\nhave arranged the genes (rows) and samples (columns) of the expression\\nmatrix in orderings derived from hierarchical clustering.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 544}, page_content='526 14. Unsupervised Learning\\nNote that if we ﬂip the orientation of the branches of a dendrogram at any\\nmerge, the resulting dendrogram is still consistent with the series of hierar-\\nchical clustering operations. Hence to determine an ordering of the leaves,\\nwe must add a constraint. To produce the row ordering of Figure 14.14,\\nwe have used the default rule in S-PLUS: at each merge, the subtree with\\nthe tighter cluster is placed to the left (toward the bottom in the rotated\\ndendrogram in the ﬁgure.) Individual genes are the tightest clusters possi-\\nble, and merges involving two individual genes place them in order by their\\nobservation number. The same rule was used for the columns. Many other\\nrules are possible—for example, ordering by a multidimensional scaling of\\nthe genes; see Section 14.8.\\nThe two-way rearrangement of Figure14.14 produces an informative pic-\\nture of the genes and samples. This picture is more informative than the\\nrandomly ordered rows and columns of Figure 1.3 of Chapter 1. Further-\\nmore, the dendrograms themselves are useful, as biologists can, for example,\\ninterpret the gene clusters in terms of biological processes.\\nDivisive Clustering\\nDivisive clustering algorithms begin with the entire data set as a single\\ncluster, and recursively divide one of the existing clusters into two daugh-\\nter clusters at each iteration in a top-down fashion. This approach has not\\nbeen studied nearly as extensively as agglomerative methods in the cluster-\\ning literature. It has been explored somewhat in the engineering literature\\n(Gersho and Gray, 1992) in the context of compression. In the clustering\\nsetting, a potential advantage of divisive over agglomerative methods can\\noccur when interest is focused on partitioning the data into a relatively\\nsmall number of clusters.\\nThe divisive paradigm can be employed by recursively applying any of\\nthe combinatorial methods such as K-means (Section 14.3.6) or K-medoids\\n(Section 14.3.10), with K= 2, to perform the splits at each iteration. How-\\never, such an approach would depend on the starting conﬁguration speciﬁed\\nat each step. In addition, it would not necessarily produce a splitting se-\\nquence that possesses the monotonicity property required for dendrogram\\nrepresentation.\\nA divisive algorithm that avoids these problems was proposed by Mac-\\nnaughton Smith et al. (1965). It begins by placing all observations in a\\nsingle cluster G. It then chooses that observation whose average dissimi-\\nlarity from all the other observations is largest. This observation for ms the\\nﬁrst member of a second cluster H. At each successive step that observation\\ninGwhose average distance from those in H, minus that for the remaining\\nobservations in Gis largest, is transferred to H. This continues until the\\ncorresponding diﬀerence in averages becomes negative. That is, there are\\nno longer any observations in Gthat are, on average, closer to those in\\nH. The result is a split of the original cluster into two daughter clusters,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 545}, page_content='14.3 Cluster Analysis 527\\nFIGURE 14.14. DNA microarray data: average linkage hierarchical clusteri ng\\nhas been applied independently to the rows (genes) and columns (sam ples), de-\\ntermining the ordering of the rows and columns (see text). The color s range from\\nbright green (negative, under-expressed) to bright red (posit ive, over-expressed).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 546}, page_content='528 14. Unsupervised Learning\\nthe observations transferred to H, and those remaining in G. These two\\nclusters represent the second level of the hierarchy. Each successive level\\nis produced by applying this splitting procedure to one of the clusters at\\nthe previous level. Kaufman and Rousseeuw (1990) suggest choosing the\\ncluster at each level with the largest diameter (14.44) for splitting. An al -\\nternative would be to choose the one with the largest average dissimilar ity\\namong its members\\n¯dG=1\\nNG∑\\ni∈G∑\\ni′∈Gdii′.\\nThe recursive splitting continues until all clusters either become singletons\\nor all members of each one have zero dissimilarity from one another.\\n14.4 Self-Organizing Maps\\nThis method can be viewed as a constrained version of K-means clustering,\\nin which the prototypes are encouraged to lie in a one- or two-dimensional\\nmanifold in the feature space. The resulting manifold is also referred to\\nas aconstrained topological map , since the original high-dimensional obser-\\nvations can be mapped down onto the two-dimensional coordinate system.\\nThe original SOM algorithm was online—observations are processed one at\\na time—and later a batch version was proposed. The technique also bears\\na close relationship to principal curves and surfaces , which are discussed in\\nthe next section.\\nWe consider a SOM with a two-dimensional rectangular grid of Kproto-\\ntypes mj∈IRp(other choices, such as hexagonal grids, can also be used).\\nEach of the Kprototypes are parametrized with respect to an integer\\ncoordinate pair ℓj∈ Q1× Q2. Here Q1={1,2,... ,q 1}, similarly Q2, and\\nK=q1≤q2. Themjare initialized, for example, to lie in the two-dimensional\\nprincipal component plane of the data (next section). We can think of the\\nprototypes as “buttons,” “sewn” on the principal component plane in a\\nregular pattern. The SOM procedure tries to bend the plane so that the\\nbuttons approximate the data points as well as possible. Once the model is\\nﬁt, the observations can be mapped down onto the two-dimensional grid.\\nThe observations xiare processed one at a time. We ﬁnd the closest\\nprototype mjtoxiin Euclidean distance in IRp, and then for all neighbors\\nmkofmj, move mktoward xivia the update\\nmk←mk+α(xi−mk). (14.46)\\nThe “neighbors” of mjare deﬁned to be all mksuch that the distance\\nbetween ℓjandℓkis small. The simplest approach uses Euclidean distance,\\nand “small” is determined by a threshold r. This neighborhood always\\nincludes the closest prototype mjitself.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 547}, page_content='14.4 Self-Organizing Maps 529\\nNotice that distance is deﬁned in the space Q1×Q2of integer topological\\ncoordinates of the prototypes, rather than in the feature space IRp. The\\neﬀect of the update (14.46) is to move the prototypes closer to the data,\\nbut also to maintain a smooth two-dimensional spatial relationship between\\nthe prototypes.\\nThe performance of the SOM algorithm depends on the learning rate\\nαand the distance threshold r. Typically αis decreased from say 1 .0 to\\n0.0 over a few thousand iterations (one per observation). Similarly ris\\ndecreased linearly from starting value Rto 1 over a few thousand iterations.\\nWe illustrate a method for choosing Rin the example below.\\nWe have described the simplest version of the SOM. More sophisticated\\nversions modify the update step according to distance:\\nmk←mk+αh(∥ℓj−ℓk∥)(xi−mk), (14.47)\\nwhere the neighborhood function hgives more weight to prototypes mkwith\\nindices ℓkcloser to ℓjthan to those further away.\\nIf we take the distance rsmall enough so that each neighborhood contains\\nonly one point, then the spatial connection between prototypes is lost. In\\nthat case one can show that the SOM algorithm is an online version of\\nK-means clustering, and eventually stabilizes at one of the local minima\\nfound by K-means. Since the SOM is a constrained version of K-means\\nclustering, it is important to check whether the constraint is reasonable\\nin any given problem. One can do this by computing the reconstruction\\nerror∥x−mj∥2, summed over observations, for both methods. This will\\nnecessarily be smaller for K-means, but should not be much smaller if the\\nSOM is a reasonable approximation.\\nAs an illustrative example, we generated 90 data points in three dimen-\\nsions, near the surface of a half sphere of radius 1. The points were in each\\nof three clusters—red, green, and blue—located near (0 ,1,0), (0,0,1) and\\n(1,0,0). The data are shown in Figure 14.15\\nBy design, the red cluster was much tighter than the green or blue ones.\\n(Full details of the data generation are given in Exercise 14.5.) A 5 ×5 grid\\nof prototypes was used, with initial grid size R= 2; this meant that about\\na third of the prototypes were initially in each neighborhood. We did a\\ntotal of 40 passes through the dataset of 90 observations, and let randα\\ndecrease linearly over the 3600 iterations.\\nIn Figure 14.16 the prototypes are indicated by circles, and the points\\nthat project to each prototype are plotted randomly within the correspond-\\ning circle. The left panel shows the initial conﬁguration, while the right\\npanel shows the ﬁnal one. The algorithm has succeeded in separating the\\nclusters; however, the separation of the red cluster indicates that the man-\\nifold has folded back on itself (see Figure 14.17). Since the distances in the\\ntwo-dimensional display are not used, there is little indication in the SOM\\nprojection that the red cluster is tighter than the others.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 548}, page_content='530 14. Unsupervised Learning\\n−1−0.500.511.5\\n−1−0.500.511.5−1−0.500.511.5\\nFIGURE 14.15. Simulated data in three classes, near the surface of a half–\\nsphere.\\n•••\\n••\\n•••\\n••\\n•••\\n•••••\\n••••••••\\n•••\\n•••\\n••\\n•\\n••\\n••••\\n••••\\n•• ••• ••\\n••\\n••• •\\n••••\\n•••••\\n••\\n••••\\n••• •\\n••\\n•\\n•••\\n••\\n•••\\n••\\n1 2 3 4 512345\\n••••\\n••\\n•\\n••\\n••••••\\n•\\n••\\n••••\\n•\\n•••••\\n••\\n• • • •••\\n••\\n•\\n••••\\n•••••\\n•\\n•••\\n•• •• ••\\n• •\\n•••\\n••••\\n•••\\n•••\\n••\\n••\\n•• •\\n•••\\n• • ••\\n•••\\n1 2 3 4 512345\\nFIGURE 14.16. Self-organizing map applied to half-sphere data example. Left\\npanel is the initial conﬁguration, right panel the ﬁnal one. The 5×5grid of\\nprototypes are indicated by circles, and the points that projec t to each prototype\\nare plotted randomly within the corresponding circle.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 549}, page_content='14.4 Self-Organizing Maps 531\\nFIGURE 14.17. Wiremesh representation of the ﬁtted SOM model in I R3. The\\nlines represent the horizontal and vertical edges of the topolog ical lattice. The\\ndouble lines indicate that the surface was folded diagonally ba ck on itself in order\\nto model the red points. The cluster members have been jittere d to indicate their\\ncolor, and the purple points are the node centers.\\nFigure 14.18 shows the reconstruction error, equal to the total sum of\\nsquares of each data point around its prototype. For comparison we carried\\nout aK-means clustering with 25 centroids, and indicate its reconstruction\\nerror by the horizontal line on the graph. We see that the SOM signiﬁcantly\\ndecreases the error, nearly to the level of the K-means solution. This pro-\\nvides evidence that the two-dimensional constraint used by the SOM is\\nreasonable for this particular dataset.\\nIn the batch version of the SOM, we update each mjvia\\nmj=∑wkxk∑wk. (14.48)\\nThe sum is over points xkthat mapped (i.e., were closest to) neighbors mk\\nofmj. The weight function may be rectangular, that is, equal to 1 for the\\nneighbors of mk, or may decrease smoothly with distance ∥ℓk−ℓj∥as before.\\nIf the neighborhood size is chosen small enough so that it consists only\\nofmk, with rectangular weights, this reduces to the K-means clustering\\nprocedure described earlier. It can also be thought of as a discrete version\\nof principal curves and surfaces, described in Section 14.5.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 550}, page_content='532 14. Unsupervised Learning\\nIterationReconstruction Error\\n0 500 1000 1500 2000 25000 10 20 30 40 50••\\n•\\n•••\\n••\\n••\\n••••\\n•••••••••••••••••••••••••••••••••••••\\nFIGURE 14.18. Half-sphere data: reconstruction error for the SOM as a func-\\ntion of iteration. Error for k-means clustering is indicated by the horizontal line.\\nExample: Document Organization and Retrieval\\nDocument retrieval has gained importance with the rapid development of\\nthe Internet and the Web, and SOMs have proved to be useful for organiz-\\ning and indexing large corpora. This example is taken from the WEBSOM\\nhomepage http://websom.hut.fi/ (Kohonen et al., 2000). Figure 14.19 rep-\\nresents a SOM ﬁt to 12,088 newsgroup comp.ai.neural-nets articles. The\\nlabels are generated automatically by the WEBSOM software and provide\\na guide as to the typical content of a node.\\nIn applications such as this, the documents have to be reprocessed in\\norder to create a feature vector. A term-document matrix is created, where\\neach row represents a single document. The entries in each row are the\\nrelative frequency of each of a predeﬁned set of terms. These terms could\\nbe a large set of dictionary entries (50,000 words), or an even larger set\\nof bigrams (word pairs), or subsets of these. These matrices are typically\\nvery sparse, and so often some preprocessing is done to reduce the number\\nof features (columns). Sometimes the SVD (next section) is used to reduce\\nthe matrix; Kohonen et al. (2000) use a randomized variant thereof. These\\nreduced vectors are then the input to the SOM.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 551}, page_content='14.4 Self-Organizing Maps 533\\nFIGURE 14.19. Heatmap representation of the SOM model ﬁt to a corpus\\nof12,088 newsgroup comp.ai.neural-nets contributions (courtesy WEBSOM\\nhomepage). The lighter areas indicate higher-density areas. Populated nodes are\\nautomatically labeled according to typical content.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 552}, page_content='534 14. Unsupervised Learning\\n•••\\n•\\n••\\n••••\\n•••\\n••\\n• ••\\n••\\n•••\\n•\\n••\\n••••\\n•••\\n••\\n• ••\\n•v1v1v1v1v1v1v1v1\\nui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1\\nxixixixixixixixi\\nFIGURE 14.20. The ﬁrst linear principal component of a set of data. The line\\nminimizes the total squared distance from each point to its orth ogonal projection\\nonto the line.\\nIn this application the authors have developed a “zoom” feature, which\\nallows one to interact with the map in order to get more detail. The ﬁnal\\nlevel of zooming retrieves the actual news articles, which can then be read.\\n14.5 Principal Components, Curves and Surfaces\\nPrincipal components are discussed in Sections 3.4.1, where they shed light\\non the shrinkage mechanism of ridge regression. Principal components are\\na sequence of projections of the data, mutually uncorrelated and ordered\\nin variance. In the next section we present principal components as linear\\nmanifolds approximating a set of Npoints xi∈IRp. We then present\\nsome nonlinear generalizations in Section 14.5.2. Other recent proposals\\nfor nonlinear approximating manifolds are discussed in Section 14.9.\\n14.5.1 Principal Components\\nThe principal components of a set of data in IRpprovide a sequence of best\\nlinear approximations to that data, of all ranks q≤p.\\nDenote the observations by x1,x2,... ,x N, and consider the rank- qlinear\\nmodel for representing them'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 553}, page_content='14.5 Principal Components, Curves and Surfaces 535\\nf(λ) =θ+Vqλ, (14.49)\\nwhere θis a location vector in IRp,Vqis ap×qmatrix with qorthogonal\\nunit vectors as columns, and λis aqvector of parameters. This is the\\nparametric representation of an aﬃne hyperplane of rank q. Figures 14.20\\nand 14.21 illustrate for q= 1 and q= 2, respectively. Fitting such a model\\nto the data by least squares amounts to minimizing the reconstruction error\\nmin\\nθ,{λi},VqN∑\\ni=1∥xi−θ−Vqλi∥2. (14.50)\\nWe can partially optimize for θand the λi(Exercise 14.7) to obtain\\nˆθ= ¯x, (14.51)\\nˆλi=VT\\nq(xi−¯x). (14.52)\\nThis leaves us to ﬁnd the orthogonal matrix Vq:\\nmin\\nVqN∑\\ni=1||(xi−¯x)−VqVT\\nq(xi−¯x)||2. (14.53)\\nFor convenience we assume that ¯ x= 0 (otherwise we simply replace the\\nobservations by their centered versions ˜ xi=xi−¯x). The p×pmatrix\\nHq=VqVT\\nqis aprojection matrix , and maps each point xionto its rank-\\nqreconstruction Hqxi, the orthogonal projection of xionto the subspace\\nspanned by the columns of Vq. The solution can be expressed as follows.\\nStack the (centered) observations into the rows of an N×pmatrix X. We\\nconstruct the singular value decomposition ofX:\\nX=UDVT. (14.54)\\nThis is a standard decomposition in numerical analysis, and many algo-\\nrithms exist for its computation (Golub and Van Loan, 1983, for example).\\nHereUis anN×porthogonal matrix ( UTU=Ip) whose columns ujare\\ncalled the left singular vectors ;Vis ap×porthogonal matrix ( VTV=Ip)\\nwith columns vjcalled the right singular vectors , andDis ap×pdiagonal\\nmatrix, with diagonal elements d1≥d2≥ ≤≤≤ ≥ dp≥0 known as the sin-\\ngular values . For each rank q, the solution Vqto (14.53) consists of the ﬁrst\\nqcolumns of V. The columns of UDare called the principal components\\nofX(see Section 3.5.1). The Noptimal ˆλiin (14.52) are given by the ﬁrst\\nqprincipal components (the Nrows of the N×qmatrix UqDq).\\nThe one-dimensional principal component line in IR2is illustrated in Fig-\\nure 14.20. For each data point xi, there is a closest point on the line, given\\nbyui1d1v1. Here v1is the direction of the line and ˆλi=ui1d1measures\\ndistance along the line from the origin. Similarly Figure 14.21 shows the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 554}, page_content='536 14. Unsupervised Learning\\nFirst principal componentSecond principal component\\n−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0•\\n•••\\n•••\\n•\\n••\\n•••\\n••••\\n•\\n•••\\n••••••\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n••••••\\n•••\\n••\\n••\\n•••\\n•\\n••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n•••\\n••••\\n•••\\n••\\nFIGURE 14.21. The best rank-two linear approximation to the half-sphere data .\\nThe right panel shows the projected points with coordinates giv en byU2D2, the\\nﬁrst two principal components of the data.\\ntwo-dimensional principal component surface ﬁt to the half-sphere data\\n(left panel). The right panel shows the projection of the data onto the\\nﬁrst two principal components. This projection was the basis for the initial\\nconﬁguration for the SOM method shown earlier. The procedure is quite\\nsuccessful at separating the clusters. Since the half-sphere is nonlinear, a\\nnonlinear projection will do a better job, and this is the topic of the next\\nsection.\\nPrincipal components have many other nice properties, for example, the\\nlinear combination Xv1has the highest variance among all linear com-\\nbinations of the features; Xv2has the highest variance among all linear\\ncombinations satisfying v2orthogonal to v1, and so on.\\nExample: Handwritten Digits\\nPrincipal components are a useful tool for dimension reduction and com-\\npression. We illustrate this feature on the handwritten digits data described\\nin Chapter 1. Figure 14.22 shows a sample of 130 handwritten 3’s, each a\\ndigitized 16 ×16 grayscale image, from a total of 658 such 3’s. We see\\nconsiderable variation in writing styles, character thickness and orienta-\\ntion. We consider these images as points xiin IR256, and compute their\\nprincipal components via the SVD (14.54).\\nFigure 14.23 shows the ﬁrst two principal components of these data. For\\neach of these ﬁrst two principal components ui1andui2, we computed the\\n5%, 25%, 50%, 75% and 95% quantile points, and used them to deﬁne\\nthe rectangular grid superimposed on the plot. The circled points indicate'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 555}, page_content='14.5 Principal Components, Curves and Surfaces 537\\nFIGURE 14.22. A sample of 130handwritten 3’s shows a variety of writing\\nstyles.\\nthose images close to the vertices of the grid, where the distance measure\\nfocuses mainly on these projected coordinates, but gives some weight to the\\ncomponents in the orthogonal subspace. The right plot shows the images\\ncorresponding to these circled points. This allows us to visualize the nature\\nof the ﬁrst two principal components. We see that the v1(horizontal move-\\nment) mainly accounts for the lengthening of the lower tail of the three,\\nwhile v2(vertical movement) accounts for character thickness. In terms of\\nthe parametrized model (14.49), this two-component model has the form\\nˆf(λ) = ¯ x+λ1v1+λ2v2\\n= +λ1≤ +λ2≤ . (14.55)\\nHere we have displayed the ﬁrst two principal component directions, v1\\nandv2, as images. Although there are a possible 256 principal components,\\napproximately 50 account for 90% of the variation in the threes, 12 ac-\\ncount for 63%. Figure 14.24 compares the singular values to those obtained\\nfor equivalent uncorrelated data, obtained by randomly scrambling each\\ncolumn of X. The pixels in a digitized image are inherently correlated,\\nand since these are all the same digit the correlations are even stronger.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 556}, page_content='538 14. Unsupervised Learning\\nFirst Principal ComponentSecond Principal Component\\n-6 -4 -2 0 2 4 6 8-5 0 5••\\n••\\n••\\n••\\n• ••••\\n••\\n•••\\n••\\n••••\\n••\\n•\\n•••\\n•\\n••\\n••••\\n•••\\n••\\n•\\n••\\n••\\n•\\n• •\\n••\\n•••\\n••\\n••••\\n•••\\n•••••\\n••\\n•\\n••\\n••\\n•\\n•••\\n•\\n••\\n••••\\n•\\n•\\n•\\n••••••\\n•\\n•••\\n•••\\n•\\n•••••\\n•\\n•••\\n•\\n••\\n••\\n••\\n••\\n•••\\n•••\\n•\\n•••\\n••\\n••••\\n•\\n•\\n••\\n••\\n••\\n••\\n•\\n•••\\n•\\n••\\n•\\n•• •\\n••\\n••\\n•••\\n••\\n•••\\n•\\n•••••\\n••\\n•\\n••\\n••\\n•\\n•••\\n•\\n••\\n•••\\n•••••\\n•\\n••\\n••\\n•\\n••\\n••\\n•\\n•• ••\\n••••\\n••\\n••\\n•••\\n••\\n•\\n•••\\n•••\\n•\\n•\\n•\\n•• •\\n•\\n•••\\n• •• ••\\n••••\\n•\\n•\\n••\\n•• •\\n•\\n•\\n••\\n•\\n••\\n•••\\n••\\n••\\n••\\n• •••\\n•\\n•\\n•••\\n••••\\n•••\\n••\\n••••\\n••\\n•\\n• ••\\n•••\\n•••\\n••\\n••••\\n••••\\n••\\n• ••\\n••\\n•••••\\n•\\n•\\n••••\\n•\\n••••\\n•\\n••\\n••\\n•\\n• •••\\n••\\n••••\\n•\\n•••\\n•\\n•\\n••\\n••••\\n••••\\n•••\\n•\\n•••\\n•\\n•••\\n••\\n•• •\\n•\\n•\\n••\\n•••\\n••\\n••\\n•\\n••\\n•\\n••\\n•••\\n•••\\n•\\n•••\\n•\\n•••\\n•••••\\n••••\\n•• •\\n•\\n•••\\n•\\n••\\n••\\n•••\\n•\\n••\\n•• ••\\n••\\n••\\n••••\\n••\\n•\\n•••\\n••\\n•\\n•\\n•••\\n•\\n•\\n••\\n••\\n••\\n••\\n••\\n••\\n•\\n••\\n•\\n•\\n••\\n•\\n•\\n••••\\n•\\n••\\n•••\\n•••\\n•••• ••\\n••\\n••••\\n••\\n•••\\n••••\\n•\\n••\\n•••\\n••\\n••\\n••\\n•\\n•••\\n••\\n••\\n••••\\n•\\n•\\n•••••\\n•••\\n••\\n••••\\n•••••\\n••\\n•••\\n••\\n••\\n••••\\n•\\n••\\n•• •••\\n•••\\n••\\n•\\n••••\\n••• ••\\n•\\n••\\n•••\\n•••\\n•••\\n•\\n•\\n•••\\n•••••\\n••\\n•\\n••\\n••\\n••\\n•\\n••••••\\n••\\n•\\n••\\n•\\n••\\nO O O OOO O OO\\nOOO\\nOO OO O O OOO O OOO\\nFIGURE 14.23. (Left panel:) the ﬁrst two principal components of the hand-\\nwritten threes. The circled points are the closest projected images to the vertices\\nof a grid, deﬁned by the marginal quantiles of the principal compone nts. (Right\\npanel:) The images corresponding to the circled points. These sh ow the nature of\\nthe ﬁrst two principal components.\\nDimensionSingular Values\\n0 50 100 150 200 2500 20 40 60 80•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\\n•••\\n•\\n••\\n••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Real Trace\\n•Randomized Trace\\nFIGURE 14.24. The256singular values for the digitized threes, compared to\\nthose for a randomized version of the data (each column of Xwas scrambled).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 557}, page_content='14.5 Principal Components, Curves and Surfaces 539\\nA relatively small subset of the principal components serve as excellent\\nlower-dimensional features for representing the high-dimensional data.\\nExample: Procrustes Transformations and Shape Averaging\\nFIGURE 14.25. (Left panel:) Two diﬀerent digitized handwritten Ss, each rep-\\nresented by 96 corresponding points in I R2. The green S has been deliberately\\nrotated and translated for visual eﬀect. (Right panel:) A Procr ustes transforma-\\ntion applies a translation and rotation to best match up the two set of points.\\nFigure 14.25 represents two sets of points, the orange and green, in the\\nsame plot. In this instance these points represent two digitized versions\\nof a handwritten S, extracted from the signature of a subject “Suresh.”\\nFigure 14.26 shows the entire signatures from which these were extracted\\n(third and fourth panels). The signatures are recorded dynamically using\\ntouch-screen devices, familiar sights in modern supermarkets. There are\\nN= 96 points representing each S, which we denote by the N×2 matrices\\nX1andX2. There is a correspondence between the points—the ith rows\\nofX1andX2are meant to represent the same positions along the two S’s.\\nIn the language of morphometrics, these points represent landmarks on\\nthe two objects. How one ﬁnds such corresponding landmarks is in general\\ndiﬃcult and subject speciﬁc. In this particular case we used dynamic time\\nwarping of the speed signal along each signature (Hastie et al., 1992), but\\nwill not go into details here.\\nIn the right panel we have applied a translation and rotation to the green\\npoints so as best to match the orange—a so-called Procrustes3transforma-\\ntion (Mardia et al., 1979, for example).\\nConsider the problem\\nmin\\nθ,R||X2−(X1R+1θT)||F, (14.56)\\n3Procrustes was an African bandit in Greek mythology, who str etched or squashed\\nhis visitors to ﬁt his iron bed (eventually killing them).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 558}, page_content='540 14. Unsupervised Learning\\nwithX1andX2bothN×pmatrices of corresponding points, Ran or-\\nthonormal p×pmatrix4, and θap-vector of location coordinates. Here\\n||X||2\\nF= trace( XTX) is the squared Frobenius matrix norm.\\nLet ¯x1and ¯x2be the column mean vectors of the matrices, and ˜X1and\\n˜X2be the versions of these matrices with the means removed. Consider\\nthe SVD ˜XT\\n1˜X2=UDVT. Then the solution to (14.56) is given by (Exer-\\ncise 14.8)\\nˆR=UVT\\nˆθ= ¯x2−ˆR¯x1,(14.57)\\nand the minimal distances is referred to as the Procrustes distance . From\\nthe form of the solution, we can center each matrix at its column centroid,\\nand then ignore location completely. Hereafter we assume this is the case.\\nTheProcrustes distance with scaling solves a slightly more general\\nproblem,\\nmin\\nβ,R||X2−βX1R||F, (14.58)\\nwhere β >0 is a positive scalar. The solution for Ris as before, with\\nˆβ= trace( D)/||X1||2\\nF.\\nRelated to Procrustes distance is the Procrustes average of a collection\\nofLshapes, which solves the problem\\nmin\\n{Rℓ}L\\n1,ML∑\\nℓ=1||XℓRℓ−M||2\\nF; (14.59)\\nthat is, ﬁnd the shape Mclosest in average squared Procrustes distance to\\nall the shapes. This is solved by a simple alternating algorithm:\\n0. Initialize M=X1(for example).\\n1. Solve the LProcrustes rotation problems with Mﬁxed, yielding\\nX′\\nℓ←XˆRℓ.\\n2. Let M←1\\nL∑L\\nℓ=1X′\\nℓ.\\nSteps 1. and 2. are repeated until the criterion (14.59) converges.\\nFigure 14.26 shows a simple example with three shapes. Note that we can\\nonly expect a solution up to a rotation; alternatively, we can impose a\\nconstraint, such as that Mbe upper-triangular, to force uniqueness. One\\ncan easily incorporate scaling in the deﬁnition (14.59); see Exercise 14.9.\\nMost generally we can deﬁne the aﬃne-invariant average of a set of\\nshapes via\\n4To simplify matters, we consider only orthogonal matrices w hich include reﬂections\\nas well as rotations [the O(p) group]; although reﬂections are unlikely here, these meth ods\\ncan be restricted further to allow only rotations [ SO(p) group].'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 559}, page_content='14.5 Principal Components, Curves and Surfaces 541\\nFIGURE 14.26. The Procrustes average of three versions of the leading S in\\nSuresh’s signatures. The left panel shows the preshape average, with each of the\\nshapes X′\\nℓin preshape space superimposed. The right three panels map th e pre-\\nshapeMseparately to match each of the original S’s.\\nmin\\n{Aℓ}L\\n1,ML∑\\nℓ=1||XℓAℓ−M||2\\nF, (14.60)\\nwhere the Aℓare any p×pnonsingular matrices. Here we require a stan-\\ndardization, such as MTM=I, to avoid a trivial solution. The solution is\\nattractive, and can be computed without iteration (Exercise 14.10):\\n1. Let Hℓ=Xℓ(XT\\nℓXℓ)−1Xℓbe the rank- pprojection matrix deﬁned\\nbyXℓ.\\n2.Mis the N×p matrix formed from the plargest eigenvectors of ¯H=\\n1\\nL∑L\\nℓ=1Hℓ.\\n14.5.2 Principal Curves and Surfaces\\nPrincipal curves generalize the principal component line, providing a smooth\\none-dimensional curved approximation to a set of data points in IRp. A prin-\\ncipal surface is more general, providing a curved manifold approximation\\nof dimension 2 or more.\\nWe will ﬁrst deﬁne principal curves for random variables X∈IRp, and\\nthen move to the ﬁnite data case. Let f(λ) be a parameterized smooth\\ncurve in IRp. Hence f(λ) is a vector function with pcoordinates, each a\\nsmooth function of the single parameter λ. The parameter λcan be chosen,\\nfor example, to be arc-length along the curve from some ﬁxed origin. For\\neach data value x, letλf(x) deﬁne the closest point on the curve to x. Then\\nf(λ) is called a principal curve for the distribution of the random vector\\nXif\\nf(λ) = E( X|λf(X) =λ). (14.61)\\nThis says f(λ) is the average of all data points that project to it, that is, the\\npoints for which it is “responsible.” This is also known as a self-consistency\\nproperty. Although in practice, continuous multivariate distributes have\\ninﬁnitely many principal curves (Duchamp and Stuetzle, 1996), we are'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 560}, page_content='542 14. Unsupervised Learning\\n....\\n•••\\n•••\\n••••\\n•• ••\\n•\\n••\\n•••\\n•\\n•••\\n•\\n•\\n••••\\n••••\\n•••\\n••••\\n••••\\n•\\n••\\n•••\\n•\\n•••\\n•\\n•\\n••••\\n•.....\\nf(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)] f(λ) = [f1(λ), f2(λ)]\\nFIGURE 14.27. The principal curve of a set of data. Each point on the curve\\nis the average of all data points that project there.\\ninterested mainly in the smooth ones. A principal curve is illustrated in\\nFigure 14.27.\\nPrincipal points are an interesting related concept. Consider a set of k\\nprototypes and for each point xin the support of a distribution, identify\\nthe closest prototype, that is, the prototype that is responsible for it. T his\\ninduces a partition of the feature space into so-called Voronoi regions. The\\nset of kpoints that minimize the expected distance from Xto its prototype\\nare called the principal points of the distribution. Each principal point is\\nself-consistent, in that it equals the mean of Xin its Voronoi region. For\\nexample, with k= 1, the principal point of a circular normal distribution is\\nthe mean vector; with k= 2 they are a pair of points symmetrically placed\\non a ray through the mean vector. Principal points are the distributional\\nanalogs of centroids found by K-means clustering. Principal curves can be\\nviewed as k=∞principal points, but constrained to lie on a smooth curve,\\nin a similar way that a SOM constrains K-means cluster centers to fall on\\na smooth manifold.\\nTo ﬁnd a principal curve f(λ) of a distribution, we consider its coordinate\\nfunctions f(λ) = [f1(λ),f2(λ),... ,f p(λ)] and let XT= (X1,X2,... ,X p).\\nConsider the following alternating steps:\\n(a) ˆfj(λ)←E(Xj|λ(X) =λ);j= 1,2,... ,p,\\n(b) ˆλf(x)←argminλ′||x−ˆf(λ′)||2.(14.62)\\nThe ﬁrst equation ﬁxes λand enforces the self-consistency requirement\\n(14.61). The second equation ﬁxes the curve and ﬁnds the closest point on'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 561}, page_content='14.5 Principal Components, Curves and Surfaces 543\\n-0.1 0.0 0.1 0.2-0.2 -0.1 0.0 0.1 0.2••••\\n•••\\n•\\n••\\n•••\\n•••••\\n•••\\n••••••\\n••\\n• ••\\n••\\n•••\\n•••\\n•\\n••••••\\n•••\\n••\\n••\\n•••\\n• •••\\n•\\n•••\\n•••\\n•\\n••\\n•\\n•••••\\n••••\\n••••\\n•••\\n••\\nλ1λ2\\nFIGURE 14.28. Principal surface ﬁt to half-sphere data. (Left panel:) ﬁtted\\ntwo-dimensional surface. (Right panel:) projections of data po ints onto the sur-\\nface, resulting in coordinates ˆλ1,ˆλ2.\\nthe curve to each data point. With ﬁnite data, the principal curve algorithm\\nstarts with the linear principal component, and iterates the two steps in\\n(14.62) until convergence. A scatterplot smoother is used to estimate the\\nconditional expectations in step (a) by smoothing each Xjas a function of\\nthe arc-length ˆλ(X), and the projection in (b) is done for each of the ob-\\nserved data points. Proving convergence in general is diﬃcult, but one can\\nshow that if a linear least squares ﬁt is used for the scatterplot smoothing,\\nthen the procedure converges to the ﬁrst linear principal component, and\\nis equivalent to the power method for ﬁnding the largest eigenvector of a\\nmatrix.\\nPrincipal surfaces have exactly the same form as principal curves, but\\nare of higher dimension. The mostly commonly used is the two-dimensional\\nprincipal surface, with coordinate functions\\nf(λ1,λ2) = [f1(λ1,λ2),... ,f p(λ1,λ2)].\\nThe estimates in step (a) above are obtained from two-dimensional surface\\nsmoothers. Principal surfaces of dimension greater than two are rarely used,\\nsince the visualization aspect is less attractive, as is smoothing in high\\ndimensions.\\nFigure 14.28 shows the result of a principal surface ﬁt to the half-sphere\\ndata. Plotted are the data points as a function of the estimated nonlinear\\ncoordinates ˆλ1(xi),ˆλ2(xi). The class separation is evident.\\nPrincipal surfaces are very similar to self-organizing maps. If we use a\\nkernel surface smoother to estimate each coordinate function fj(λ1,λ2),\\nthis has the same form as the batch version of SOMs (14.48). The SOM\\nweights wkare just the weights in the kernel. There is a diﬀerence, however:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 562}, page_content='544 14. Unsupervised Learning\\nthe principal surface estimates a separate prototype f(λ1(xi),λ2(xi)) for\\neach data point xi, while the SOM shares a smaller number of prototypes\\nfor all data points. As a result, the SOM and principal surface will agree\\nonly as the number of SOM prototypes grows very large.\\nThere also is a conceptual diﬀerence between the two. Principal sur-\\nfaces provide a smooth parameterization of the entire manifold in terms\\nof its coordinate functions, while SOMs are discrete and produce only the\\nestimated prototypes for approximating the data. The smooth parameter-\\nization in principal surfaces preserves distance locally: in Figure 14.28 it\\nreveals that the red cluster is tighter than the green or blue clusters. In\\nsimple examples the estimates coordinate functions themselves can be in-\\nformative: see Exercise14.13.\\n14.5.3 Spectral Clustering\\nTraditional clustering methods like K-means use a spherical or elliptical\\nmetric to group data points. Hence they will not work well when the clus-\\nters are non-convex, such as the concentric circles in the top left panel of\\nFigure 14.29. Spectral clustering is a generalization of standard clustering\\nmethods, and is designed for these situations. It has close connections with\\nthe local multidimensional-scaling techniques (Section 14.9) that generalize\\nMDS.\\nThe starting point is a N×Nmatrix of pairwise similarities sii′≥0 be-\\ntween all observation pairs. We represent the observations in an undirected\\nsimilarity graph G=⟨V, E⟩. The Nvertices virepresent the observations,\\nand pairs of vertices are connected by an edge if their similarity is positive\\n(or exceeds some threshold). The edges are weighted by the sii′. Clustering\\nis now rephrased as a graph-partition problem, where we identify connected\\ncomponents with clusters. We wish to partition the graph, such that edges\\nbetween diﬀerent groups have low weight, and within a group have high\\nweight. The idea in spectral clustering is to construct similarity graphs that\\nrepresent the local neighborhood relationships between observations.\\nTo make things more concrete, consider a set of Npoints xi∈IRp, and let\\ndii′be the Euclidean distance between xiandxi′. We will use as similarity\\nmatrix the radial-kernel gram matrix; that is, sii′= exp( −d2\\nii′/c), where\\nc >0 is a scale parameter.\\nThere are many ways to deﬁne a similarity matrix and its associated\\nsimilarity graph that reﬂect local behavior. The most popular is the mutual\\nK-nearest-neighbor graph . Deﬁne NKto be the symmetric set of nearby\\npairs of points; speciﬁcally a pair ( i,i′) is in NKif point iis among the\\nK-nearest neighbors of i′, or vice-versa. Then we connect all symmetric\\nnearest neighbors, and give them edge weight wii′=sii′; otherwise the\\nedge weight is zero. Equivalently we set to zero all the pairwise similarit ies\\nnot in NK, and draw the graph for this modiﬁed similarity matrix.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 563}, page_content='14.5 Principal Components, Curves and Surfaces 545\\nAlternatively, a fully connected graph includes all pairwise edges with\\nweights wii′=sii′, and the local behavior is controlled by the scale param-\\neterc.\\nThe matrix of edge weights W={wii′}from a similarity graph is called\\ntheadjacency matrix . The degree of vertex iisgi=∑\\ni′wii′, the sum of\\nthe weights of the edges connected to it. Let Gbe a diagonal matrix with\\ndiagonal elements gi.\\nFinally, the graph Laplacian is deﬁned by\\nL=G−W (14.63)\\nThis is called the unnormalized graph Laplacian ; a number of normalized\\nversions have been proposed—these standardize the Laplacian with respect\\nto the node degrees gi, for example, ˜L=I−G−1W.\\nSpectral clustering ﬁnds the meigenvectors ZN×mcorresponding to the\\nmsmallest eigenvalues of L(ignoring the trivial constant eigenvector).\\nUsing a standard method like K-means, we then cluster the rows of Zto\\nyield a clustering of the original data points.\\nAn example is presented in Figure 14.29. The top left panel shows 450\\nsimulated data points in three circular clusters indicated by the colors. K-\\nmeans clustering would clearly have diﬃculty identifying the outer clusters.\\nWe applied spectral clustering using a 10-nearest neighbor similarity graph,\\nand display the eigenvector corresponding to the second and third smallest\\neigenvalue of the graph Laplacian in the lower left. The 15 smallest eigen-\\nvalues are shown in the top right panel. The two eigenvectors shown have\\nidentiﬁed the three clusters, and a scatterplot of the rows of the eigenvector\\nmatrix Yin the bottom right clearly separates the clusters. A procedure\\nsuch as K-means clustering applied to these transformed points would eas-\\nily identify the three groups.\\nWhy does spectral clustering work? For any vector fwe have\\nfTLf=N∑\\ni=1gif2\\ni−N∑\\ni=1N∑\\ni′=1fifi′wii′\\n=1\\n2N∑\\ni=1N∑\\ni′=1wii′(fi−fi′)2. (14.64)\\nFormula 14.64 suggests that a small value of fTLfwill be achieved if pairs\\nof points with large adjacencies have coordinates fiandfi′close together.\\nSince1TL1= 0 for any graph, the constant vector is a trivial eigenvector\\nwith eigenvalue zero. Not so obvious is the fact that if the graph is con-\\nnected5, it is the onlyzero eigenvector (Exercise 14.21). Generalizing this\\nargument, it is easy to show that for a graph with mconnected components,\\n5A graph is connected if any two nodes can be reached via a path o f connected nodes.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 564}, page_content='546 14. Unsupervised Learning\\n−4 −2 0 2 4−4 −2 0 2 4\\nx1x2\\n0.0 0.1 0.2 0.3 0.4 0.5\\nNumberEigenvalue\\n1 3 5 10 15\\n0 100 200 300 400Eigenvectors\\nIndex2nd Smallest 3rd Smallest\\n−0.05  0.05 −0.05  0.05\\n−0.04 −0.02 0.00 0.02−0.06 −0.02 0.02 0.06\\nSecond Smallest EigenvectorThird Smallest EigenvectorSpectral Clustering\\nFIGURE 14.29. Toy example illustrating spectral clustering. Data in top left are\\n450points falling in three concentric clusters of 150points each. The points are\\nuniformly distributed in angle, with radius 1,2.8and5in the three groups, and\\nGaussian noise with standard deviation 0.25added to each point. Using a k= 10\\nnearest-neighbor similarity graph, the eigenvector correspo nding to the second and\\nthird smallest eigenvalues of Lare shown in the bottom left; the smallest eigen-\\nvector is constant. The data points are colored in the same way as in the top left.\\nThe 15 smallest eigenvalues are shown in the top right panel. Th e coordinates of\\nthe 2nd and 3rd eigenvectors (the 450rows of Z) are plotted in the bottom right\\npanel. Spectral clustering does standard (e.g., K-means) clustering of these points\\nand will easily recover the three original clusters.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 565}, page_content='14.5 Principal Components, Curves and Surfaces 547\\nthe nodes can be reordered so that Lis block diagonal with a block for each\\nconnected component. Then Lhasmeigenvectors of eigenvalue zero, and\\nthe eigenspace of eigenvalue zero is spanned by the indicator vectors of the\\nconnected components. In practice one has strong and weak connections,\\nso zero eigenvalues are approximated by small eigenvalues.\\nSpectral clustering is an interesting approach for ﬁnding non-convex clus-\\nters. When a normalized graph Laplacian is used, there is another way to\\nview this method. Deﬁning P=G−1W, we consider a random walk on\\nthe graph with transition probability matrix P. Then spectral clustering\\nyields groups of nodes such that the random walk seldom transitions from\\none group to another.\\nThere are a number of issues that one must deal with in applying spec-\\ntral clustering in practice. We must choose the type of similarity graph—eg.\\nfully connected or nearest neighbors, and associated parameters such as the\\nnumber of nearest of neighbors kor the scale parameter of the kernel c. We\\nmust also choose the number of eigenvectors to extract from Land ﬁnally,\\nas with all clustering methods, the number of clusters. In the toy example\\nof Figure 14.29 we obtained good results for k∈[5,200], the value 200 cor-\\nresponding to a fully connected graph. With k <5 the results deteriorated.\\nLooking at the top-right panel of Figure 14.29, we see no strong separation\\nbetween the smallest three eigenvalues and the rest. Hence it is not clear\\nhow many eigenvectors to select.\\n14.5.4 Kernel Principal Components\\nSpectral clustering is related to kernel principal components , a non-linear\\nversion of linear principal components. Standard linear principal compo-\\nnents (PCA) are obtained from the eigenvectors of the covariance matrix,\\nand give directions in which the data have maximal variance. Kernel PCA\\n(Sch¨ olkopf et al., 1999) expand the scope of PCA, mimicking what we would\\nobtain if we were to expand the features by non-linear transformations, and\\nthen apply PCA in this transformed feature space.\\nWe show in Section 18.5.2 that the principal components variables Zof\\na data matrix Xcan be computed from the inner-product (gram) matrix\\nK=XXT. In detail, we compute the eigen-decomposition of the double-\\ncentered version of the gram matrix\\n˜K= (I−M)K(I−M) =UD2UT, (14.65)\\nwithM=11T/N, and then Z=UD. Exercise 18.15 shows how to com-\\npute the projections of new observations in this space.\\nKernel PCA simply mimics this procedure, interpreting the kernel ma-\\ntrixK={K(xi,xi′)}as an inner-product matrix of the implicit fea-\\ntures⟨φ(xi),φ(xi′)⟩and ﬁnding its eigenvectors. The elements of the mth\\ncomponent zm(mth column of Z) can be written (up to centering) as\\nzim=∑N\\nj=1αjmK(xi,xj), where αjm=ujm/dm(Exercise 14.16).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 566}, page_content='548 14. Unsupervised Learning\\nWe can gain more insight into kernel PCA by viewing the zmas sam-\\nple evaluations of principal component functions gm∈ H K, with HKthe\\nreproducing kernel Hilbert space generated by K(see Section 5.8.1). The\\nﬁrst principal component function g1solves\\nmax\\ng1∈HKVarTg1(X) subject to ||g1||HK= 1 (14.66)\\nHere Var Trefers to the sample variance over training data T. The norm\\nconstraint ||g1||HK= 1 controls the size and roughness of the function g1,\\nas dictated by the kernel K. As in the regression case it can be shown that\\nthe solution to (14.66) is ﬁnite dimensional with representation g1(x) =∑N\\nj=1cjK(x,xj). Exercise 14.17 shows that the solution is deﬁned by ˆ cj=\\nαj1, j= 1,... ,N above. The second principal component function is de-\\nﬁned in a similar way, with the additional constraint that ⟨g1,g2⟩HK= 0,\\nand so on.6\\nSch¨ olkopf et al. (1999) demonstrate the use of kernel principal compo-\\nnents as features for handwritten-digit classiﬁcation, and show that they\\ncan improve the performance of a classiﬁer when these are used instead of\\nlinear principal components.\\nNote that if we use the radial kernel\\nK(x,x′) = exp( −∥x−x′∥2/c), (14.67)\\nthen the kernel matrix Khas the same form as the similarity matrix Sin\\nspectral clustering. The matrix of edge weights Wis a localized version of\\nK, setting to zero all similarities for pairs of points that are not nearest\\nneighbors.\\nKernel PCA ﬁnds the eigenvectors corresponding to the largest eigenval-\\nues of ˜K; this is equivalent to ﬁnding the eigenvectors corresponding to the\\nsmallest eigenvalues of\\nI−˜K. (14.68)\\nThis is almost the same as the Laplacian (14.63), the diﬀerences being the\\ncentering of ˜Kand the fact that Ghas the degrees of the nodes along the\\ndiagonal.\\nFigure 14.30 examines the performance of kernel principal components\\nin the toy example of Figure 14.29. In the upper left panel we used the ra-\\ndial kernel with c= 2, the same value that was used in spectral clustering.\\nThis does not separate the groups, but with c= 10 (upper right panel), the\\nﬁrst component separates the groups well. In the lower-left panel we ap-\\nplied kernel PCA using the nearest-neighbor radial kernel Wfrom spectral\\nclustering. In the lower right panel we use the kernel matrix itself as the\\n6This section beneﬁted from helpful discussions with Jonath an Taylor.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 567}, page_content='14.5 Principal Components, Curves and Surfaces 549\\n−0.10 −0.06 −0.02 0.02−0.10 −0.05 0.00 0.05 0.10\\nFirst Largest EigenvectorSecond Largest EigenvectorRadial Kernel (c=2)\\n−0.06 −0.02 0.02 0.06−0.05 0.00 0.05\\nFirst Largest EigenvectorSecond Largest EigenvectorRadial Kernel (c=10)\\n0.00 0.05 0.10 0.15−0.2 −0.1 0.0 0.1 0.2\\nFirst Largest EigenvectorSecond Largest EigenvectorNN Radial Kernel (c=2)\\n−0.05 0.00 0.05 0.10 0.15−0.10 0.00 0.05 0.10 0.15\\nSecond Smallest EigenvectorThird Smallest EigenvectorRadial Kernel Laplacian (c=2)\\nFIGURE 14.30. Kernel principal components applied to the toy example of Fig-\\nure 14.29, using diﬀerent kernels. (Top left:) Radial kernel (14 .67) with c= 2.\\n(Top right:) Radial kernel with c= 10. (Bottom left): Nearest neighbor radial ker-\\nnelWfrom spectral clustering. (Bottom right:) Spectral clusteri ng with Laplacian\\nconstructed from the radial kernel.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 568}, page_content='550 14. Unsupervised Learning\\nsimilarity matrix for constructing the Laplacian (14.63) in spectral cl uster-\\ning. In neither case do the projections separate the two groups. Adjusting\\ncdid not help either.\\nIn this toy example, we see that kernel PCA is quite sensitive to the scale\\nand nature of the kernel. We also see that the nearest-neighbor truncation\\nof the kernel is important for the success of spectral clustering.\\n14.5.5 Sparse Principal Components\\nWe often interpret principal components by examining the direction vectors\\nvj, also known as loadings , to see which variables play a role. We did this\\nwith the image loadings in (14.55). Often this interpretation is made eas ier\\nif the loadings are sparse. In this section we brieﬂy discuss some methods\\nfor deriving principal components with sparse loadings. They are all based\\non lasso ( L1) penalties.\\nWe start with an N×pdata matrix X, with centered columns. The\\nproposed methods focus on either the maximum-variance property of prin-\\ncipal components, or the minimum reconstruction error. The SCoTLASS\\nprocedure of Joliﬀe et al. (2003) takes the ﬁrst approach, by solving\\nmaxvT(XTX)v,subject to∑p\\nj=1|vj| ≤t,vTv= 1. (14.69)\\nThe absolute-value constraint encourages some of the loadings to be zero\\nand hence vto be sparse. Further sparse principal components are found\\nin the same way, by forcing the kth component to be orthogonal to the\\nﬁrstk−1 components. Unfortunately this problem is not convex and the\\ncomputations are diﬃcult.\\nZou et al. (2006) start instead with the regression/reconstruction prop-\\nerty of PCA, similar to the approach in Section 14.5.1. Let xibe the ith row\\nofX. For a single component, their sparse principal component technique\\nsolves\\nmin\\nθ,vN∑\\ni=1||xi−θvTxi||2\\n2+λ||v||2\\n2+λ1||v||1 (14.70)\\nsubject to ||θ||2= 1.\\nLets examine this formulation in more detail.\\n•If both λandλ1are zero and N > p , it is easy to show that v=θ\\nand is the largest principal component direction.\\n•When p≫Nthe solution is not necessarily unique unless λ >0. For\\nanyλ >0 and λ1= 0 the solution for vis proportional to the largest\\nprincipal component direction.\\n•The second penalty on vencourages sparseness of the loadings.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 569}, page_content='14.5 Principal Components, Curves and Surfaces 551\\nWalking Speed\\nVerbal Fluency\\nPrincipal Components Sparse Principal Components\\nFIGURE 14.31. Standard and sparse principal components from a study of\\nthe corpus callosum variation. The shape variations correspo nding to signiﬁcant\\nprincipal components (red curves) are overlaid on the mean CC sh ape (black\\ncurves).\\nFor multiple components, the sparse principal components procedures\\nminimizes\\nN∑\\ni=1||xi−ΘVTxi||2+λK∑\\nk=1||vk||2\\n2+K∑\\nk=1λ1k||vk||1, (14.71)\\nsubject to ΘTΘ=IK. Here Vis ap×Kmatrix with columns vkandΘ\\nis also p×K.\\nCriterion (14.71) is not jointly convex in VandΘ, but it is convex in\\neach parameter with the other parameter ﬁxed7. Minimization over Vwith\\nΘﬁxed is equivalent to Kelastic net problems (Section 18.4) and can be\\ndone eﬃciently. On the other hand, minimization over ΘwithVﬁxed is a\\nversion of the Procrustes problem (14.56), and is solved by a simple SVD\\ncalculation (Exercise 14.12). These steps are alternated until convergence.\\nFigure 14.31 shows an example of sparse principal components analysis\\nusing (14.71), taken from Sj¨ ostrand et al. (2007). Here the shape of the\\nmid-sagittal cross-section of the corpus callosum (CC) is related to various\\nclinical parameters in a study involving 569 elderly persons8. In this exam-\\n7Note that the usual principal component criterion, for exam ple (14.50), is not jointly\\nconvex in the parameters either. Nevertheless, the solutio n is well deﬁned and an eﬃcient\\nalgorithm is available.\\n8We thank Rasmus Larsen and Karl Sj¨ ostrand for suggesting th is application, and\\nsupplying us with the postscript ﬁgures reproduced here.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 570}, page_content='552 14. Unsupervised Learning\\nFIGURE 14.32. An example of a mid-saggital brain slice, with the corpus col-\\nlosum annotated with landmarks.\\nple PCA is applied to shape data, and is a popular tool in morphometrics.\\nFor such applications, a number of landmarks are identiﬁed along the cir-\\ncumference of the shape; an example is given in Figure 14.32. These are\\naligned by Procrustes analysis to allow for rotations, and in this case s cal-\\ning as well (see Section 14.5.1). The features used for PCA are the sequence\\nof coordinate pairs for each landmark, unpacked into a single vector.\\nIn this analysis, both standard and sparse principal components were\\ncomputed, and components that were signiﬁcantly associated with various\\nclinical parameters were identiﬁed. In the ﬁgure, the shape variations cor-\\nresponding to signiﬁcant principal components (red curves) are overlaid on\\nthe mean CC shape (black curves). Low walking speed relates to CCs that\\nare thinner (displaying atrophy) in regions connecting the motor control\\nand cognitive centers of the brain. Low verbal ﬂuency relates to CCs that\\nare thinner in regions connecting auditory/visual/cognitive centers. The\\nsparse principal components procedure gives a more parsimonious, and po-\\ntentially more informative picture of the important diﬀerences.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 571}, page_content='14.6 Non-negative Matrix Factorization 553\\n14.6 Non-negative Matrix Factorization\\nNon-negative matrix factorization (Lee and Seung, 1999) is a recent al-\\nternative approach to principal components analysis, in which the data\\nand components are assumed to be non-negative. It is useful for modeling\\nnon-negative data such as images.\\nTheN×pdata matrix Xis approximated by\\nX≈WH (14.72)\\nwhere WisN×randHisr×p,r≤max(N,p). We assume that\\nxij,wik,hkj≥0.\\nThe matrices WandHare found by maximizing\\nL(W,H) =N∑\\ni=1p∑\\nj=1[xijlog(WH)ij−(WH)ij]. (14.73)\\nThis is the log-likelihood from a model in which xijhas a Poisson dis-\\ntribution with mean ( WH)ij—quite reasonable for positive data.\\nThe following alternating algorithm (Lee and Seung, 2001) converges to\\na local maximum of L(W,H):\\nwik←wik∑p\\nj=1hkjxij/(WH)ij∑p\\nj=1hkj\\nhkj←hkj∑N\\ni=1wikxij/(WH)ij∑N\\ni=1wik(14.74)\\nThis algorithm can be derived as a minorization procedure for maximizing\\nL(W,H) (Exercise 14.23) and is also related to the iterative-proportional-\\nscaling algorithm for log-linear models (Exercise 14.24).\\nFigure 14.33 shows an example taken from Lee and Seung (1999)9, com-\\nparing non-negative matrix factorization (NMF), vector quantization (VQ,\\nequivalent to k-means clustering) and principal components analysis (PCA).\\nThe three learning methods were applied to a database of N= 2,429 fa-\\ncial images, each consisting of 19 ×19 pixels, resulting in a 2 ,429×381\\nmatrix X. As shown in the 7 ×7 array of montages (each a 19 ×19 image),\\neach method has learned a set of r= 49 basis images. Positive values are\\nillustrated with black pixels and negative values with red pixels. A par-\\nticular instance of a face, shown at top right, is approximated by a linear\\nsuperposition of basis images. The coeﬃcients of the linear superposition\\nare shown next to each montage, in a 7 ×7 array10, and the resulting su-\\nperpositions are shown to the right of the equality sign. The authors poin t\\n9We thank Sebastian Seung for providing this image.\\n10These 7 ×7 arrangements allow for a compact display, and have no struc tural\\nsigniﬁcance.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 572}, page_content='554 14. Unsupervised Learning\\nout that unlike VQ and PCA, NMF learns to represent faces with a set of\\nbasis images resembling parts of faces.\\nDonoho and Stodden (2004) point out a potentially serious problem with\\nnon-negative matrix factorization. Even in situations where X=WHholds\\nexactly, the decomposition may not be unique. Figure 14.34 illustrates the\\nproblem. The data points lie in p= 2 dimensions, and there is “open space”\\nbetween the data and the coordinate axes. We can choose the basis vectors\\nh1andh2anywhere in this open space, and represent each data point\\nexactly with a nonnegative linear combination of these vectors. This non-\\nuniqueness means that the solution found by the above algorithm depends\\non the starting values, and it would seem to hamper the interpretability of\\nthe factorization. Despite this interpretational drawback, the non-negative\\nmatrix factorization and its applications has attracted a lot of interest .\\n14.6.1 Archetypal Analysis\\nThis method, due to Cutler and Breiman (1994), approximates data points\\nby prototypes that are themselves linear combinations of data points. In\\nthis sense it has a similar ﬂavor to K-means clustering. However, rather\\nthan approximating each data point by a single nearby prototype, archety-\\npal analysis approximates each data point by a convex combination of a\\ncollection of prototypes. The use of a convex combination forces the proto-\\ntypes to lie on the convex hull of the data cloud. In this sense, the prototypes\\nare “pure,”, or “archetypal.”\\nAs in (14.72), the N×pdata matrix Xis modeled as\\nX≈WH (14.75)\\nwhereWisN×randHisr×p. We assume that wik≥0 and∑r\\nk=1wik=\\n1∀i. Hence the Ndata points (rows of X) inp-dimensional space are\\nrepresented by convex combinations of the rarchetypes (rows of H). We\\nalso assume that\\nH=BX (14.76)\\nwhere Bisr×Nwithbki≥0 and∑N\\ni=1bki= 1∀k. Thus the archetypes\\nthemselves are convex combinations of the data points. Using both (14.75)\\nand (14.76) we minimize\\nJ(W,B) = ||X−WH||2\\n=||X−WBX ||2(14.77)\\nover the weights WandB. This function is minimized in an alternating\\nfashion, with each separate minimization involving a convex optimizatio n.\\nThe overall problem is not convex however, and so the algorithm converges\\nto a local minimum of the criterion.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 573}, page_content='14.6 Non-negative Matrix Factorization 555\\nVQ\\n×\\r =NMF\\n= ×\\r\\nPCA\\n= ×\\r\\nOriginal\\nFIGURE 14.33. Non-negative matrix factorization (NMF), vector quantizatio n\\n(VQ, equivalent to k-means clustering) and principal components analysis (PCA)\\napplied to a database of facial images. Details are given in t he text. Unlike VQ\\nand PCA, NMF learns to represent faces with a set of basis images r esembling\\nparts of faces.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 574}, page_content='556 14. Unsupervised Learning\\nh1\\nh2\\nFIGURE 14.34. Non-uniqueness of the non-negative matrix factorization.\\nThere are 11 data points in two dimensions. Any choice of the basis vectors h1\\nandh2in the open space between the coordinate axes and data, gives an e xact\\nreconstruction of the data.\\nFigure 14.35 shows an example with simulated data in two dimensions.\\nThe top panel displays the results of archetypal analysis, while the bottom\\npanel shows the results from K-means clustering. In order to best recon-\\nstruct the data from convex combinations of the prototypes, it pays to\\nlocate the prototypes on the convex hull of the data. This is seen in the top\\npanels of Figure 14.35 and is the case in general, as proven by Cutler and\\nBreiman (1994). K-means clustering, shown in the bottom panels, chooses\\nprototypes in the middle of the data cloud.\\nWe can think of K-means clustering as a special case of the archetypal\\nmodel, in which each row of Whas a single one and the rest of the entries\\nare zero.\\nNotice also that the archetypal model (14.75) has the same general form\\nas the non-negative matrix factorization model (14.72). However, the two\\nmodels are applied in diﬀerent settings, and have somewhat diﬀerent goals.\\nNon-negative matrix factorization aims to approximate the columns of the\\ndata matrix X, and the main output of interest are the columns of W\\nrepresenting the primary non-negative components in the data. Archetypal\\nanalysis focuses instead on the approximation of the rows of Xusing the\\nrows of H, which represent the archetypal data points. Non-negative matrix\\nfactorization also assumes that r≤p. With r=p, we can get an exact\\nreconstruction simply choosing Wto be the data Xwith columns scaled\\nso that they sum to 1. In contrast, archetypal analysis requires r≤N,\\nbut allows r > p . In Figure 14.35, for example, p= 2,N= 50 while\\nr= 2,4 or 8. The additional constraint (14.76) implies that the archetypal\\napproximation will not be perfect, even if r > p.\\nFigure 14.36 shows the results of archetypal analysis applied to the\\ndatabase of 3’s displayed in Figure 14.22. The three rows in Figure 14.36\\nare the resulting archetypes from three runs, specifying two, three and four'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 575}, page_content='14.7 Independent Component Analysisand Exploratory Projection Pursu it 557\\n2 Prototypes 4 Prototypes 8 Prototypes\\nFIGURE 14.35. Archetypal analysis (top panels) and K-means clustering (bot-\\ntom panels) applied to 50data points drawn from a bivariate Gaussian distribu-\\ntion. The colored points show the positions of the prototypes in each case.\\narchetypes, respectively. As expected, the algorithm has produced extreme\\n3’s both in size and shape.\\n14.7 Independent Component Analysis and\\nExploratory Projection Pursuit\\nMultivariate data are often viewed as multiple indirect measurements aris-\\ning from an underlying source, which typically cannot be directly measured.\\nExamples include the following:\\n•Educational and psychological tests use the answers to questionnaires\\nto measure the underlying intelligence and other mental abilities of\\nsubjects.\\n•EEG brain scans measure the neuronal activity in various parts of\\nthe brain indirectly via electromagnetic signals recorded at sensors\\nplaced at various positions on the head.\\n•The trading prices of stocks change constantly over time, and reﬂect\\nvarious unmeasured factors such as market conﬁdence, external in-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 576}, page_content='558 14. Unsupervised Learning\\nFIGURE 14.36. Archetypal analysis applied to the database of digitized 3’s . The\\nrows in the ﬁgure show the resulting archetypes from three runs, specifying two,\\nthree and four archetypes, respectively.\\nﬂuences, and other driving forces that may be hard to identify or\\nmeasure.\\nFactor analysis is a classical technique developed in the statistical liter-\\nature that aims to identify these latent sources. Factor analysis models\\nare typically wed to Gaussian distributions, which has to some extent hin-\\ndered their usefulness. More recently, independent component analysis has\\nemerged as a strong competitor to factor analysis, and as we will see, relies\\non the non-Gaussian nature of the underlying sources for its success.\\n14.7.1 Latent Variables and Factor Analysis\\nThe singular-value decomposition X=UDVT(14.54) has a latent variable\\nrepresentation. Writing S=√\\nNUandAT=DVT/√\\nN, we have X=\\nSAT, and hence each of the columns of Xis a linear combination of the\\ncolumns of S. Now since Uis orthogonal, and assuming as before that the\\ncolumns of X(and hence U) each have mean zero, this implies that the\\ncolumns of Shave zero mean, are uncorrelated and have unit variance. In\\nterms of random variables, we can interpret the SVD, or the corresponding\\nprincipal component analysis (PCA) as an estimate of a latent variable\\nmodel'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 577}, page_content='14.7 Independent Component Analysis and Exploratory Projection Pursu it 559\\nX1=a11S1+a12S2+≤≤≤+a1pSp\\nX2=a21S1+s22S2+≤≤≤+a2pSp\\n......\\nXp=ap1S1+sp2S2+≤≤≤+appSp,(14.78)\\nor simply X=AS. The correlated Xjare each represented as a linear\\nexpansion in the uncorrelated, unit variance variables Sℓ. This is not too\\nsatisfactory, though, because given any orthogonal p×pmatrix R, we can\\nwrite\\nX=AS\\n=ARTRS\\n=A∗S∗, (14.79)\\nand Cov( S∗) =RCov(S)RT=I. Hence there are many such decom-\\npositions, and it is therefore impossible to identify any particular lat ent\\nvariables as unique underlying sources. The SVD decomposition does have\\nthe property that any rank q < p truncated decomposition approximates\\nXin an optimal way.\\nThe classical factor analysis model, developed primarily by researchers in\\npsychometrics, alleviates these problems to some extent; see, for example,\\nMardia et al. (1979). With q < p, a factor analysis model has the form\\nX1=a11S1+≤≤≤+a1qSq+ε1\\nX2=a21S1+≤≤≤+a2qSq+ε2\\n......\\nXp=ap1S1+≤≤≤+apqSq+εp,(14.80)\\norX=AS+ε. Here Sis a vector of q < p underlying latent variables or\\nfactors, Ais ap×qmatrix of factor loadings , and the εjare uncorrelated\\nzero-mean disturbances. The idea is that the latent variables Sℓare com-\\nmon sources of variation amongst the Xj, and account for their correlation\\nstructure, while the uncorrelated εjare unique to each Xjand pick up the\\nremaining unaccounted variation. Typically the Sjand the εjare modeled\\nas Gaussian random variables, and the model is ﬁt by maximum likelihood.\\nThe parameters all reside in the covariance matrix\\nΣ=AAT+Dε, (14.81)\\nwhere Dε= diag[Var( ε1),... ,Var(εp)]. The Sjbeing Gaussian and un-\\ncorrelated makes them statistically independent random variables. Thus a\\nbattery of educational test scores would be thought to be driven by the\\nindependent underlying factors such as intelligence ,drive and so on. The\\ncolumns of Aare referred to as the factor loadings , and are used to name\\nand interpret the factors.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 578}, page_content='560 14. Unsupervised Learning\\nUnfortunately the identiﬁability issue (14.79) remains, since AandART\\nare equivalent in (14.81) for any q×qorthogonal R. This leaves a certain\\nsubjectivity in the use of factor analysis, since the user can search for ro-\\ntated versions of the factors that are more easily interpretable. This aspect\\nhas left many analysts skeptical of factor analysis, and may account for it s\\nlack of popularity in contemporary statistics. Although we will not g o into\\ndetails here, the SVD plays a key role in the estimation of (14.81). For ex-\\nample, if the Var( εj) are all assumed to be equal, the leading qcomponents\\nof the SVD identify the subspace determined by A.\\nBecause of the separate disturbances εjfor each Xj, factor analysis can\\nbe seen to be modeling the correlation structure of the Xjrather than the\\ncovariance structure. This can be easily seen by standardizing the covari-\\nance structure in (14.81) (Exercise 14.14). This is an important disti nction\\nbetween factor analysis and PCA, although not central to the discussion\\nhere. Exercise 14.15 discusses a simple example where the solutions from\\nfactor analysis and PCA diﬀer dramatically because of this distinction.\\n14.7.2 Independent Component Analysis\\nThe independent component analysis (ICA) model has exactly the same\\nform as (14.78), except the Siare assumed to be statistically indepen-\\ndentrather than uncorrelated. Intuitively, lack of correlation determines\\nthe second-degree cross-moments (covariances) of a multivariate distribu-\\ntion, while in general statistical independence determines all of the cross-\\nmoments. These extra moment conditions allow us to identify the elements\\nofAuniquely. Since the multivariate Gaussian distribution is determined\\nby its second moments alone, it is the exception, and any Gaussian inde-\\npendent components can be determined only up to a rotation, as before.\\nHence identiﬁability problems in (14.78) and (14.80) can be avoided if we\\nassume that the Siare independent and non-Gaussian .\\nHere we will discuss the full p-component model as in (14.78), where the\\nSℓare independent with unit variance; ICA versions of the factor analysis\\nmodel (14.80) exist as well. Our treatment is based on the survey article\\nby Hyv¨ arinen and Oja (2000).\\nWe wish to recover the mixing matrix AinX=AS. Without loss\\nof generality, we can assume that Xhas already been whitened to have\\nCov(X) =I; this is typically achieved via the SVD described above. This\\nin turn implies that Ais orthogonal, since Salso has covariance I. So\\nsolving the ICA problem amounts to ﬁnding an orthogonal Asuch that\\nthe components of the vector random variable S=ATXare independent\\n(and non-Gaussian).\\nFigure 14.37 shows the power of ICA in separating two mixed signals.\\nThis is an example of the classical cocktail party problem , where diﬀer-\\nent microphones Xjpick up mixtures of diﬀerent independent sources Sℓ\\n(music, speech from diﬀerent speakers, etc.). ICA is able to perform blind'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 579}, page_content='14.7 Independent Component Analysis and Exploratory Projection Pursu it 561\\nSource Signals Measured Signals\\nPCA Solution ICA Solution\\nFIGURE 14.37. Illustration of ICA vs. PCA on artiﬁcial time-series data. Th e\\nupper left panel shows the two source signals, measured at 1000uniformly spaced\\ntime points. The upper right panel shows the observed mixed signa ls. The lower\\ntwo panels show the principal components and independent component sol utions.\\nsource separation , by exploiting the independence and non-Gaussianity of\\nthe original sources.\\nMany of the popular approaches to ICA are based on entropy. The dif-\\nferential entropy Hof a random variable Ywith density g(y) is given by\\nH(Y) =−∫\\ng(y)logg(y)dy. (14.82)\\nA well-known result in information theory says that among all random\\nvariables with equal variance, Gaussian variables have the maximum en-\\ntropy. Finally, the mutual information I(Y) between the components of the\\nrandom vector Yis a natural measure of dependence:\\nI(Y) =p∑\\nj=1H(Yj)−H(Y). (14.83)\\nThe quantity I(Y) is called the Kullback–Leibler distance between the\\ndensity g(y) ofYand its independence version∏p\\nj=1gj(yj), where gj(yj)\\nis the marginal density of Yj. Now if Xhas covariance I, and Y=ATX\\nwithAorthogonal, then it is easy to show that\\nI(Y) =p∑\\nj=1H(Yj)−H(X)−log|detA| (14.84)\\n=p∑\\nj=1H(Yj)−H(X). (14.85)\\nFinding an Ato minimize I(Y) =I(ATX) looks for the orthogonal trans-\\nformation that leads to the most independence between its components. In'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 580}, page_content='562 14. Unsupervised Learning\\n*\\n**\\n**\\n**\\n**\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n*\\n**\\n**\\n****\\n***\\n*\\n**\\n*\\n**\\n*\\n*\\n**\\n** ***\\n**\\n**\\n**\\n**\\n**\\n**\\n***\\n*\\n***\\n*****\\n**\\n**\\n*\\n**\\n**\\n*\\n**\\n*\\n***\\n*****\\n**\\n* **\\n*\\n***\\n* **\\n***\\n** **\\n**\\n****\\n*\\n******\\n***\\n*\\n**\\n*\\n* ***\\n***\\n**\\n**\\n**\\n**\\n**\\n***\\n*\\n***\\n** *\\n**\\n***\\n****\\n** *\\n**\\n*\\n**\\n**\\n*\\n**\\n**\\n*\\n* **\\n***\\n***\\n******\\n***\\n**\\n**\\n******\\n*\\n*****\\n***\\n**\\n*\\n*****\\n**\\n***\\n*\\n***\\n*\\n***\\n**\\n***\\n*\\n***\\n*\\n**\\n*\\n*\\n**\\n*\\n***\\n**\\n*\\n**\\n****\\n***\\n*\\n*\\n**\\n***\\n**\\n**\\n***\\n***\\n*\\n****\\n**\\n***\\n*\\n**\\n**\\n*\\n*\\n*\\n****\\n**\\n***\\n**\\n**\\n***\\n*\\n***\\n****\\n**\\n**\\n** *\\n***\\n**\\n* ***\\n***\\n***\\n*\\n**\\n**\\n****\\n**\\n*\\n*\\n***\\n*\\n***\\n**\\n**\\n**\\n***\\n* **\\n**\\n**\\n*\\n***\\n*\\n*\\n***\\n*\\n***\\n**\\n****\\n*\\n***\\n*\\n**\\n*\\n**\\n****\\n**\\n*\\n***\\n*****\\n**\\n**\\n** **\\n**\\n***\\n*** **\\n***\\n***\\n***\\n***\\n**\\n**\\n**\\n*\\n** **\\n*\\n**\\n**\\n**\\n**\\n*\\n**\\n*\\n***\\n***\\n***\\n**Source S\\n**\\n*\\n**\\n**\\n****\\n**\\n*\\n*****\\n****\\n*\\n**\\n***\\n***\\n**\\n***\\n*\\n**\\n*\\n**\\n****\\n*\\n**\\n*\\n***\\n*\\n***\\n**\\n***\\n*\\n***\\n***\\n****\\n****\\n*\\n**\\n*\\n**\\n*****\\n*\\n*\\n****\\n***\\n*\\n*\\n***\\n**\\n****\\n**\\n***\\n***\\n*\\n*\\n***\\n*\\n**\\n*\\n**\\n**\\n***\\n***\\n***\\n****\\n****\\n**\\n***\\n**\\n**\\n**\\n***\\n***\\n****\\n***\\n**\\n**\\n* **\\n*\\n****\\n***\\n***\\n*\\n***\\n*\\n**\\n***\\n***\\n**\\n**\\n**\\n****\\n**\\n*\\n****\\n*\\n*\\n**\\n**\\n**\\n***\\n*\\n***\\n**\\n**\\n****\\n*\\n*\\n***\\n****\\n*\\n***\\n****\\n*\\n***\\n**\\n**\\n*\\n**\\n*\\n***\\n**\\n**\\n*****\\n*\\n***\\n*\\n****\\n******\\n*\\n**\\n**\\n**\\n*\\n*\\n**\\n****\\n***\\n**\\n****\\n***\\n**\\n**\\n****\\n*\\n***\\n***\\n***\\n**\\n*\\n***\\n***\\n***\\n*\\n***\\n****\\n*\\n***\\n**\\n******\\n*\\n*****\\n**\\n*\\n*\\n**\\n*\\n*\\n**\\n*\\n***\\n**\\n***\\n***\\n*\\n**\\n**\\n**\\n***\\n*\\n******\\n****\\n*\\n**\\n***\\n****\\n*\\n**\\n**\\n***\\n* *\\n**\\n***\\n****\\n******\\n**\\n***\\n*\\n**\\n**\\n***\\n****\\n*\\n**\\n**\\n**\\n**\\n***\\n****\\n*\\n**\\n*\\n**\\n**Data X\\n*\\n***\\n***\\n*\\n*\\n***\\n***\\n**\\n**\\n***\\n**\\n*** ***\\n*\\n**\\n* **\\n***\\n*\\n***\\n**\\n***\\n****\\n**\\n**\\n***\\n***\\n**\\n***\\n*\\n****\\n****\\n*\\n** *\\n***\\n**\\n**\\n*****\\n*\\n***\\n***\\n***\\n***\\n***\\n**\\n**\\n*\\n*\\n*****\\n*\\n*\\n****\\n**\\n*\\n*\\n**\\n*\\n**\\n****\\n***\\n***\\n*\\n***\\n*\\n**\\n**\\n****\\n**\\n**\\n**\\n**\\n*** **\\n*\\n***\\n**\\n***\\n**\\n***\\n*\\n**\\n*\\n**\\n* *\\n*\\n**\\n**\\n**\\n**\\n**\\n***\\n***\\n****\\n****\\n*\\n***\\n***\\n**\\n**\\n*\\n**\\n*\\n***\\n**\\n**\\n*\\n*\\n***\\n*\\n** *\\n***\\n***\\n**\\n**\\n*\\n***\\n**\\n**\\n*\\n**\\n**\\n* *\\n**\\n** *\\n***\\n**\\n**\\n*\\n***\\n**\\n*\\n****\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n*******\\n** *\\n**\\n***\\n***\\n***\\n**\\n**\\n**\\n*\\n* **\\n**\\n***\\n*\\n***\\n*\\n*******\\n**\\n**\\n**\\n**\\n**\\n*\\n*\\n** *\\n*\\n*****\\n**\\n**\\n*\\n**\\n***\\n**\\n***\\n**\\n**\\n** ***\\n****\\n*\\n****\\n*\\n*\\n***\\n**\\n*\\n****\\n*\\n***\\n**\\n***\\n****\\n*\\n*\\n*\\n***\\n**\\n*****\\n***\\n*****\\n*\\n***\\n**\\n*\\n**\\n*\\n*****\\n**\\n*\\n*****\\n**\\n*\\n*\\n***\\n***\\n***\\n***\\n***\\n**PCA Solution\\n*\\n**\\n**\\n**\\n**\\n**\\n* *\\n*\\n*\\n**\\n**\\n**\\n*\\n**\\n**\\n****\\n***\\n*\\n**\\n*\\n**\\n*\\n*\\n**\\n*****\\n**\\n**\\n**\\n**\\n**\\n**\\n***\\n*\\n***\\n*** **\\n**\\n**\\n*\\n**\\n**\\n*\\n**\\n*\\n***\\n**** *\\n**\\n*\\n**\\n*\\n***\\n* **\\n***\\n** **\\n**\\n****\\n*\\n** ****\\n***\\n*\\n**\\n*\\n** **\\n***\\n**\\n**\\n*\\n*\\n**\\n**\\n**\\n*\\n*\\n***\\n** *\\n**\\n***\\n****\\n***\\n**\\n*\\n**\\n**\\n*\\n**\\n**\\n*\\n* **\\n***\\n***\\n*** ***\\n***\\n**\\n**\\n******\\n*\\n** ***\\n***\\n***\\n*****\\n**\\n***\\n*\\n***\\n*\\n***\\n**\\n***\\n*\\n***\\n*\\n**\\n*\\n*\\n**\\n*\\n***\\n**\\n*\\n**\\n* ***\\n***\\n*\\n*\\n**\\n***\\n**\\n**\\n***\\n***\\n*\\n****\\n**\\n***\\n*\\n**\\n**\\n*\\n*\\n*\\n****\\n**\\n***\\n**\\n**\\n***\\n*\\n***\\n****\\n**\\n**\\n** *\\n***\\n* *\\n****\\n***\\n***\\n*\\n**\\n**\\n*** *\\n**\\n*\\n*\\n***\\n*\\n***\\n**\\n**\\n**\\n***\\n* **\\n**\\n**\\n*\\n***\\n*\\n* ***\\n*\\n*****\\n****\\n*\\n***\\n*\\n**\\n*\\n**\\n****\\n**\\n*\\n***\\n*\\n****\\n**\\n**\\n****\\n**\\n***\\n* ** **\\n***\\n***\\n** *\\n*\\n**\\n**\\n**\\n**\\n*\\n****\\n*\\n**\\n**\\n**\\n**\\n*\\n**\\n*\\n***\\n***\\n***\\n**ICA Solution\\nFIGURE 14.38. Mixtures of independent uniform random variables. The upper\\nleft panel shows 500realizations from the two independent uniform sources, the\\nupper right panel their mixed versions. The lower two panels show the PCA and\\nICA solutions, respectively.\\nlight of (14.84) this is equivalent to minimizing the sum of the entropies of\\nthe separate components of Y, which in turn amounts to maximizing their\\ndepartures from Gaussianity.\\nFor convenience, rather than using the entropy H(Yj), Hyv¨ arinen and\\nOja (2000) use the negentropy measure J(Yj) deﬁned by\\nJ(Yj) =H(Zj)−H(Yj), (14.86)\\nwhere Zjis a Gaussian random variable with the same variance as Yj. Ne-\\ngentropy is non-negative, and measures the departure of Yjfrom Gaussian-\\nity. They propose simple approximations to negentropy which can be com-\\nputed and optimized on data. The ICA solutions shown in Figures 14.37–\\n14.39 use the approximation\\nJ(Yj)≈[EG(Yj)−EG(Zj)]2, (14.87)\\nwhere G(u) =1\\nalog cosh( au) for 1 ≤a≤2. When applied to a sample'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 580}, page_content='puted and optimized on data. The ICA solutions shown in Figures 14.37–\\n14.39 use the approximation\\nJ(Yj)≈[EG(Yj)−EG(Zj)]2, (14.87)\\nwhere G(u) =1\\nalog cosh( au) for 1 ≤a≤2. When applied to a sample\\nofxi, the expectations are replaced by data averages. This is one of the\\noptions in the FastICA software provided by these authors. More classical\\n(and less robust) measures are based on fourth moments, and hence look for\\ndepartures from the Gaussian via kurtosis. See Hyv¨ arinen and Oja (2000)\\nfor more details. In Section 14.7.4 we describe their approximate Newton\\nalgorithm for ﬁnding the optimal directions.\\nIn summary then, ICA applied to multivariate data looks for a sequence\\nof orthogonal projections such that the projected data look as far from'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 581}, page_content='14.7 Independent Component Analysis and Exploratory Projection Pursu it 563\\nComponent\\n 1oooooo o\\nooo\\noo\\noo\\noo\\no\\noooooo\\nooo\\noooo ooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\nooo ooooo\\nooooooooo\\nooooooooooo\\noooo\\noo o\\noo\\noooooooooo\\noo\\no\\noooooooo\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\nooooooooooo\\noooo\\nooooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\noooooooooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\noooooo\\nooooo\\noooooooo\\nooooooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\noooo ooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\nooo\\noooo\\nooo\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\noooooooooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\noooo\\nooooo o\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\noooo\\no\\nooo\\noooooo\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\nooo\\nooooooooooo\\nooooooo\\noo\\noo\\noo\\nooo oo\\no\\nooooo\\noo\\noooo\\noooooooo\\nooo\\noo\\noo\\noo\\no\\noooooo\\nooo\\nooooooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\nooo ooooo\\nooooooooo\\nooooooooooo\\noooo\\noo o\\noo\\noooooooooo\\noo\\no\\noooooooo\\nooo\\noooooooooo\\noooooo\\noooooo\\noooooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\nooooooooooo\\noooo\\noooo o\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\noooooo oooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\noooooo\\nooooo\\noooooooo\\nooooooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\nooooooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\nooo\\noooo\\noo o\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\noooo o ooooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\noooo\\nooooo o\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\noooo\\no\\nooo\\noo oooo\\no oo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\nooo\\noo ooooooooo\\nooooooo\\noo\\noo\\noo\\nooooo\\no\\nooooo\\noo\\noooo\\noooooooo\\nooo\\noo\\noo\\noo\\no\\noooooo\\noo o\\nooooooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\noooooooo\\nooooooooo\\nooooooooooo\\noooo\\noo o\\noo\\nooooo o oooo\\noo\\no\\noooooooo\\nooo\\noooo oooooo\\noooooo\\noooooo\\noooooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\noooooooo ooo\\noooo\\nooooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\noooooo oooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\nooo ooo\\nooooo\\noooooooo\\nooo oooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\noooo ooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\noooo\\nooo\\noooo\\nooo\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\nooooo o oooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\noooo\\noooooo\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\no ooo\\no\\nooo\\noo oooo\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\nooo\\nooooooooooo\\nooooooo\\noo\\noo\\noo\\nooooo\\no\\nooooo\\noo\\noooo\\noooooooo\\nooo\\noo\\noo\\noo\\no\\noooooo\\noo o\\nooooooo\\nooooooo\\noo\\no\\noo\\noooooo\\noo\\nooo ooooo\\nooooooooo\\nooooooooooo\\noooo\\nooo\\noo\\noooooooooo\\noo\\no\\noooooooo\\nooo\\noooo oooooo\\noooo oo\\noooooo\\nooo ooo\\noo\\nooo\\nooo ooo\\nooooo\\no\\noo\\nooooooooooo\\noooo\\nooooo\\noooooooo\\nooo\\noo\\nooooo\\noo\\nooo\\nooooo\\nooooooo\\nooo\\noooo\\noo\\noooo\\noooooo\\noo\\nooo\\noooo\\nooo ooooooooooo\\noooooooooooo\\noooooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooooooo\\nooo\\noooo\\nooooo\\noo\\noooo\\noooooo\\nooooo\\noooooooo\\nooooooooooo\\nooo\\nooo\\noooo\\noo\\noo\\nooooo\\nooooo\\nooo\\noo\\noo\\noo\\nooooooooo\\nooooooo\\nooooo\\noo\\nooooooooo\\noo\\no\\noooo\\nooo\\noo\\noooooooo\\nooo\\noo\\nooo\\nooo\\no ooo\\noo o\\noooo\\nooo\\noooooooooo\\nooooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\no\\noooooooo\\nooo\\noooooooooooooo\\nooooooo\\nooooo oo\\noo\\noooo\\no ooo\\noooooo\\noooooooo\\nooooo\\noo\\nooooooo\\nooo\\noooo\\noooo\\no\\nooo\\noo oooo\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\noo\\no\\noo o\\nooooooooooo\\nooooooo\\noo\\noo\\noo\\nooooo\\no\\nooooo\\noo\\noooo\\no\\noo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooo\\noooo\\noo\\noooo\\nooo\\nooo\\noo\\noo\\nooooo\\nooo\\noo\\noooo\\nooo\\nooooo\\noo\\no\\noo\\noooooo\\no\\noo\\noo oo\\no\\noo\\noooooo\\no\\nooo\\noooo\\nooooo\\no\\nooo\\no\\noo\\noo\\noo\\noo\\nooo\\noooo\\nooo\\noo\\nooooo\\no\\noooo\\noo\\nooo\\nooo\\no\\nooo\\nooo\\noo\\noo\\nooo\\noo\\nooooo oooo\\noo\\nooo\\nooo\\nooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\noo\\noo\\no\\noo oo\\noooo\\noooo\\nooo\\noo\\no\\nooo\\nooo\\no\\no\\no\\nooo\\no\\nooo\\nooooo\\noooo\\noo\\noooo o\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\noooo\\noooo\\nooo\\noo\\noooo\\noo\\no\\nooo\\nooo\\nooo\\noo\\noooo\\noooo\\nooooo\\noo\\noooooo\\no\\nooooo\\nooooo\\nooooo\\noooo\\noo\\noooo\\noooo\\no\\no\\noooo oooooo\\nooo\\no\\noooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\noo\\no\\noo\\no\\noo\\nooo\\nooo\\noooo\\noooo\\nooooo\\noooo\\nooo\\no\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noooo\\noo\\no'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 581}, page_content='oooo\\nooooo\\noo\\noooooo\\no\\nooooo\\nooooo\\nooooo\\noooo\\noo\\noooo\\noooo\\no\\no\\noooo oooooo\\nooo\\no\\noooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\noo\\no\\noo\\no\\noo\\nooo\\nooo\\noooo\\noooo\\nooooo\\noooo\\nooo\\no\\nooo\\no\\noo\\noo\\nooo\\no\\noo\\noooo\\noo\\noo\\noooo\\noo\\no\\nooo\\noo\\no\\noooo\\noo\\noo\\noo\\noo\\noo\\noo\\noo\\noooo\\no\\noooo\\noooo\\nooo\\nooo\\nooo\\noooooo\\noooooo\\noo\\nooo\\noooo\\no\\nooooooo\\noo\\noo\\no\\nooo\\noo\\noooooo\\no\\no\\nooooo\\nooo\\noo\\noooo\\nooooo\\noo\\nooooooo\\noooo\\nooo\\noo ooo\\nooo\\nooo\\noooo\\nooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooooo\\noo\\no\\noo\\noo\\nooo\\noooooo\\noo\\no\\noo\\nooo\\nComponent\\n 2\\noooo\\nooooo\\nooo\\noooo\\nooooooo\\noo\\noooo\\noooo\\noooo\\noooo\\nooooooo ooooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\noooo\\noo oooo o\\noo\\noooooooooo\\nooo\\no\\noo\\nooooooooooooooo\\no o\\nooo\\nooooo\\noooo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\no\\no oooooooooooo\\noooo\\no\\nooo\\noo\\nooooo oo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\no\\noooo\\noooooo\\nooooo\\noooo\\noooooo\\nooooo\\nooo\\noooo\\nooooo\\nooooo\\noooo\\no oooo\\nooo\\nooooooo\\noo\\no\\noooo\\nooooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo ooooo\\noooooo\\noo\\nooooo\\nooooo\\nooo\\noo\\nooo\\nooooo\\noooooo\\nooooo\\nooooo\\noooooo\\noooo\\noooooooo\\no\\noo oooooo\\no\\nooooooooo\\noooooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\noo\\noooooo\\noo\\nooooo\\nooooo\\noooo oooo\\noo\\noo\\nooooo\\noo\\noooooooo\\noooo\\noo\\noooo\\no\\noo\\no oooooo\\noooooooo\\nooooooooooo\\noo\\noooo\\no\\nooooo\\nooo\\no\\noooooo\\noooooo\\nooo\\nooo\\noo ooooooooo\\nooo\\no\\nooo\\nooo\\noooooo\\noo\\noooo\\nooo\\noo oooo\\nooo\\noooooo\\noooo\\nooooo\\noooooo\\nooo\\nooooo\\noooo\\no\\noooo\\noo\\noo\\nooooo\\noo\\nooooo oooo\\no\\no\\noo\\noooo oo\\nooooo\\nooo\\noooo\\nooooooo\\noo\\noooo\\noooo\\noooo\\noooo\\noooo ooo ooooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\noooo\\noo ooooo\\noo\\nooooo ooooo\\nooo\\no\\noo\\noo ooooooooooooo\\no o\\noo o\\nooooo\\noooo\\nooo\\noooooo\\no\\noo\\noo\\nooo\\no\\nooooooooooooo\\noooo\\no\\nooo\\noo\\nooooooo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\no\\noooo\\noooooo\\nooooo\\noooo\\noooooo\\nooooo\\nooo\\noooo\\nooooo\\nooo oo\\noooo\\nooooo\\nooo\\nooooooo\\noo\\no\\noooo\\nooooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\nooo ooooo\\noooooo\\noo\\nooooo\\nooooo\\nooo\\noo\\nooo\\nooooo\\noo o ooo\\nooooo\\nooo oo\\noooooo\\noooo\\noooooooo\\no\\noooooooo\\no\\nooooooooo\\noooooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\noo\\noooooo\\noo\\nooooo\\no oooo\\noooooooo\\noo\\noo\\nooooo\\noo\\noooooooo\\noooo\\noo\\noooo\\no\\noo\\no oooooo\\noooooooo\\nooooooooooo\\noo\\noooo\\no\\nooooo\\nooo\\no\\noooooo\\noooooo\\nooo\\nooo\\nooooooooooo\\nooo\\no\\nooo\\nooo\\noooooo\\noo\\noooo\\nooo\\noooooo\\nooo\\noooooo\\noooo\\nooooo\\noooooo\\nooo\\nooooo\\noooo\\no\\noooo\\noo\\noo\\nooooo\\noo\\nooooooooo\\no\\no\\noo\\noooo oo\\nooooo\\nooo\\noooo\\nooooooo\\no o\\noooo\\noooo\\noooo\\noooo\\noooo ooo ooooooo\\nooooooooooo\\nooo\\noo\\noooo\\noo\\noooo\\noooo ooo\\noo\\noooooooooo\\nooo\\no\\noo\\noo ooooooooooooo\\no o\\noo o\\nooooo\\noooo\\nooo\\noo oooo\\no\\noo\\noo\\nooo\\no\\nooooo oooooooo\\noooo\\no\\nooo\\noo\\nooooooo\\noo\\no\\nooo\\noooo\\nooo\\noo\\noo\\no\\noooo\\noooooo\\nooooo\\noooo\\noooooo\\nooooo\\nooo\\noooo\\nooooo\\nooooo\\noooo\\nooooo\\nooo\\nooooooo\\noo\\no\\noooo\\nooooo\\noo\\noo\\nooo\\noo\\noooo\\noo\\noo\\noooooooo\\noooooo\\noo\\nooooo\\nooooo\\nooo\\noo\\nooo\\nooooo\\noooooo\\nooooo\\nooo oo\\noooooo\\noooo\\noooooooo\\no\\noooooooo\\no\\nooooooooo\\noooooooo\\noooo\\nooo\\noo\\noo\\noooo\\nooo\\nooo\\no\\nooo\\noo\\nooo\\nooo\\noo\\noooooo\\noo\\nooooo\\noo oo o\\noooooooo\\noo\\noo\\nooooo\\noo\\noooooooo\\noooo\\noo\\noooo\\no\\noo\\no oooooo\\noooooooo\\nooooooooooo\\noo\\noooo\\no\\nooooo\\nooo\\no\\nooo ooo\\noooooo\\nooo\\nooo\\nooooooooooo\\nooo\\no\\nooo\\nooo\\noooooo\\noo\\noooo\\nooo\\noooooo\\nooo\\noooooo\\noooo\\nooooo\\noooooo\\no oo\\nooooo\\noooo\\no\\noooo\\noo\\noo\\nooooo\\noo\\nooooooooo\\no\\no\\noo\\noo\\no\\noooooo\\nooo\\nooo\\no\\nooo\\nooo\\no\\nooo oooo\\noo\\nooo\\nooooo\\noo\\noo\\no\\noooooo\\no\\nooo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooo\\noo\\nooooooo\\no o\\noo\\no\\nooo\\noo\\noo\\noo\\no\\nooo\\nooooo\\noo\\noo\\nooo\\no\\nooo\\nooo o\\nooooo\\noo\\no\\noo\\noo\\no\\nooo\\no\\nooo\\noo\\noooo\\nooo ooo\\nooo\\no\\noo\\noo\\nooo\\nooo o\\noo\\noo\\no\\nooo\\no\\noo\\noooooo\\nooo\\noo\\nooo\\no\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooooo\\no\\no\\noooo\\nooo\\noooo\\noo\\nooo\\noo\\no\\nooooo\\nooo\\noo\\no\\no\\noo\\noo\\no\\noo\\noo\\noooooooo\\nooooo\\nooooo\\no\\noooo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\noooo\\noo\\nooo\\no\\nooo\\noo\\noooooo\\noo\\noooo\\noo\\nooo\\noooo\\nooooo\\noooo\\no\\noooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\no\\no\\noooo\\noo\\nooo\\no\\noo\\noo\\no\\noooooo\\no\\no\\nooooo\\nooo\\noooo\\no\\no\\nooo\\nooo\\noooooooooo\\noo\\nooo\\noo\\noooooo\\nooo\\noooo\\noo\\nooo\\nooo\\noooooo\\noo\\no\\noooooo\\no\\noo\\no\\nooo\\no\\nooo\\no\\noo oo\\noo\\nooo\\nooo\\no\\noo\\nooo\\nooooo\\noo\\nooo\\no\\noooo\\no\\noooo\\noo\\no\\noo\\nooo\\nooo\\no\\nooooo\\noo\\noo\\nooo\\noo\\nooo\\nooo\\nooooo\\no\\nooooooo\\noooo\\nooo\\nooooo\\noo\\noo\\nooooooo\\noo o\\noo\\no\\nooo\\noo o\\noo\\no\\noo\\noooooo\\noo\\no\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\no\\noooo\\noooooo\\nooo\\nooo\\no\\nooo\\nooo\\no\\nooooooo\\noo\\noo o\\nooooo\\noo\\noo\\no\\nooooooo\\nooo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooo\\noo'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 581}, page_content='ooooo\\noo\\noo\\nooooooo\\noo o\\noo\\no\\nooo\\noo o\\noo\\no\\noo\\noooooo\\noo\\no\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\no\\noooo\\noooooo\\nooo\\nooo\\no\\nooo\\nooo\\no\\nooooooo\\noo\\noo o\\nooooo\\noo\\noo\\no\\nooooooo\\nooo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\noo\\nooo\\noo\\nooooooo\\noo\\noo\\no\\nooo\\noo\\noo\\noo\\no\\nooo\\noo ooo\\noo\\noo\\nooo\\no\\nooo\\nooo o\\no oooo\\noo\\no\\noo\\noo\\no\\nooo\\no\\nooo\\noo\\noooo\\noooooo\\nooo\\no\\noo\\noo\\nooo\\noooo\\noo\\noo\\no\\nooo\\no\\noo\\noooooo\\nooo\\noo\\nooo\\no\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooooo\\no\\no\\noooo\\nooo\\noooo\\noo\\nooo\\noo\\no\\nooooo\\nooo\\noo\\no\\no\\noo\\noo\\no\\noo\\noo\\noooooo oo\\nooooo\\nooooo\\no\\noooo\\noo\\nooooo\\noo\\nooooo\\noooo\\nooo\\noo\\nooo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\noooo\\noo\\nooo\\no\\nooo\\noo\\noooooo\\noo\\noooo\\noo\\nooo\\noooo\\nooooo\\noooo\\no\\no ooo\\nooooo\\noo\\no\\noooo\\noo\\noo\\noo\\noooo\\noo\\nooo\\no\\noo\\noo\\no\\noooooo\\no\\no\\nooo oo\\nooo\\noooo\\no\\no\\nooo\\nooo\\noooooooooo\\noo\\nooo\\noo\\noooooo\\nooo\\noooo\\noo\\nooo\\nooo\\noooooo\\noo\\no\\noooooo\\no\\noo\\no\\nooo\\no\\nooo\\no\\noooo\\noo\\nooo\\nooo\\no\\noo\\nooo\\nooooo\\noo\\nooo\\no\\noooo\\no\\noooo\\noo\\no\\noo\\nooo\\nooo\\no\\nooooo\\noo\\noo\\nooo\\no o\\nooo\\nooo\\nooooo\\no\\nooooooo\\noooo\\nooo\\nooooo\\noo\\noo\\nooooooo\\nooo\\noo\\no\\nooo\\nooo\\noo\\no\\noo\\noooooo\\noo\\no\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\no\\noooComponent\\n 3oo\\nooooooooo\\noooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\nooooooo\\nooooo\\noooo\\nooooooo\\nooo\\nooooo\\nooo\\nooooo\\no ooooooo\\nooo\\no\\noo\\no o\\nooo\\noo\\nooooo\\no ooo\\noooooo\\nooo\\noooooooo\\noo\\nooo\\nooooo\\noooooooooo\\noo\\noooooooo\\noo\\nooooo o ooooooooo\\nooooo\\nooooo\\nooo\\nooo\\noo\\nooooooo\\nooooooo\\no\\no\\noo\\noooo\\noo\\noooo\\noo\\noo\\nooooooo\\nooo\\noooooo\\nooo\\nooo\\nooo\\noooooo\\nooo oo\\noooo\\noo\\nooooo\\no\\noo\\noooooooo\\noo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooooooooo\\noo\\noooo\\noooo\\nooo\\noooo\\nooo\\noo\\noooooo\\no\\noooo\\noooo\\noooooo o\\nooo\\noooooo\\nooo\\noooo\\no\\noo\\noo\\noo\\noooooooooo\\no\\noooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\nooo\\noooooo\\nooo\\noooooooo\\noooooo\\no\\noooo\\noooooooooooo\\noo\\noo\\noooooooo o\\noo\\nooo\\noooooooo\\noooooooo\\noooo\\noo\\noooooooo\\noo\\noo\\noooooo\\noooooooo\\noooo\\nooo\\noooooooo\\noooooo\\no\\noo\\no ooo\\noooo\\no\\nooooo\\noooo\\noooo\\nooooo\\noooooooo\\noo\\nooo ooo\\nooooooo\\nooooooo\\nooooo\\noooo\\nooooooooo\\noooo oo\\noooo\\noooo\\noooooooo\\nooo o\\nooo\\no\\nooo\\noo\\nooooooo\\nooooooooo\\noooo\\noo\\noo\\noo\\nooo\\nooo\\noo\\nooooooo\\nooooo\\noooo\\nooooooo\\nooo\\nooooo\\nooo\\nooooo\\noooooooo\\nooo\\no\\noo\\noo\\nooo\\noo\\nooooo\\noooo\\noooooo\\nooo\\noooooooo\\noo\\nooo\\nooooo\\noooooooooo\\noo\\noooooooo\\noo\\nooooooooooo o ooo\\nooooo\\nooo oo\\nooo\\nooo\\noo\\nooooo oo\\nooooooo\\no\\no\\noo\\noooo\\noo\\noooo\\noo\\noo\\nooooooo\\nooo\\noooooo\\nooo\\nooo\\nooo\\noooooo\\nooooo\\noooo\\noo\\nooooo\\no\\noo\\noooooooo\\noo\\noo\\noo\\noo\\noo\\noo\\nooo\\nooooooooo\\noo\\noooo\\noooo\\nooo\\noooo\\nooo\\noo\\noooooo\\no\\noooo\\noooo\\nooooooo\\nooo\\noooooo\\nooo\\noooo\\no\\noo\\noo\\noo\\noooooooooo\\no\\noooo\\nooo\\nooooo\\noooo\\noo\\nooo\\nooo\\noo\\nooooooooo\\nooo\\nooo\\noooooo\\nooo\\noooooooo\\noooooo\\no\\noooo\\nooooo o oooooo\\noo\\noo\\nooooo oooo\\noo\\nooo\\noooooooo\\noooooooo\\noooo\\noo\\noooooooo\\noo\\noo\\noooooo\\noooooooo\\noooo\\nooo\\noooooooo\\noooooo\\no\\noo\\no ooo\\noooo\\no\\nooooo\\noooo\\noooo\\nooooo\\noooooooo\\noo\\noooooo\\nooooooo\\nooooooo\\nooooo\\noooo\\noooo oo ooo\\noooo oo\\noooo\\noooo\\noooooooo\\noo oo\\nooo\\no\\nooo\\noo\\nooooo\\noooo\\noooooooo\\noo\\noo\\noo\\no\\noooo\\noo\\noo\\no\\noooo\\noo\\nooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noo\\noooo\\no\\nooo\\noo\\no\\noo\\nooo\\noooo\\noooo\\no\\no\\nooo\\no\\noo\\nooo\\noo\\no\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\noooo\\noooooo\\noooo\\noo\\nooo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\noo\\nooo\\nooooo\\no\\noo\\noooo\\nooo\\noo\\no\\nooo\\nooooooooooo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\noo o\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\nooooooooooo\\nooo\\no\\noo\\no\\noooo\\no\\nooooooo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\no\\noooo\\no\\noo\\noo\\nooo\\no\\nooo oo\\noo\\nooo\\noo\\no\\noooo\\noo\\noo\\nooo\\no\\noo\\nooooo\\noooo\\no\\noo\\no\\noooooooo\\noo\\noooooo\\nooo\\nooooooo\\noooo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\noo\\noo\\no\\no\\noooo\\noo\\no\\noooo\\no oooooo\\no\\noooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noooo\\nooo\\no\\nooo\\noooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\no\\noo\\no\\noooo\\nooo\\noooo\\nooo\\nooooooooo\\no\\nooooo\\no\\nooo\\no\\noo\\nooo\\noooo\\nooo\\noooo\\no\\nooooo\\no\\nooo\\nooo\\nooo\\noo\\noooooo\\nooo\\noo\\noo o\\nooooooooo\\noo\\noooo\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooooo\\noooooo\\noo\\no\\noooo\\nooo\\nooo\\nooo\\noo\\no\\nooo\\noo\\noo\\no oooo\\noooooooo\\noo\\noo\\noo\\no\\noooo\\noo\\noo\\no\\noooo\\noo\\nooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noo\\noooo\\no\\nooo\\noo\\no\\noo\\nooo\\noooo\\noooo\\no\\no\\nooo\\no\\noo\\nooo\\noo\\no\\nooooo\\noo\\nooo\\nooo\\noo\\noo\\nooooo\\noooo oo\\noooo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\noo\\nooo\\no\\noooo\\no\\noo\\noooo\\nooo\\noo\\no\\no oo\\noooo ooooo oo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\nooooooooooo\\nooo\\no\\noo\\no\\noooo\\no\\noooo\\nooo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\no\\noooo'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 581}, page_content='ooo\\noo\\no\\no oo\\noooo ooooo oo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\nooooooooooo\\nooo\\no\\noo\\no\\noooo\\no\\noooo\\nooo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\no\\noooo\\no\\noo\\noo\\nooo\\no\\nooooo\\noo\\nooo\\noo\\no\\noooo\\noo\\noo\\nooo\\no\\noo\\nooooo\\noooo\\no\\noo\\no\\noooooooo\\noo\\noooooo\\nooo\\nooooooo\\noooo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\noo\\noo\\no\\no\\noooo\\noo\\no\\noooo\\nooooooo\\no\\noooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noooo\\nooo\\no\\nooo\\noooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\no\\noo\\no\\noooo\\nooo\\noooo\\nooo\\nooooooooo\\no\\nooooo\\no\\nooo\\no\\noo\\nooo\\noooo\\nooo\\noooo\\no\\nooooo\\no\\nooo\\nooo\\nooo\\noo\\noooooo\\nooo\\noo\\nooo\\nooooooooo\\noo\\noooo\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\nooooo\\noo\\nooo ooo\\noooooo\\noo\\no\\noooo\\nooo\\nooo\\nooo\\noo\\no\\nooo\\noo\\noo\\nooooo\\noooooooo\\noo\\noo\\noo\\no\\noooo\\noo\\noo\\no\\noooo\\noo\\nooo\\no\\noooo\\noo\\noo\\nooo\\noo\\noo\\noooo\\no\\nooo\\noo\\no\\noo\\nooo\\noooo\\noooo\\no\\no\\noo o\\no\\noo\\nooo\\noo\\no\\nooooo\\noo\\nooo\\nooo\\noo\\nooo\\noooo\\noooooo\\no ooo\\noo\\noo o\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\noo\\nooo\\nooooo\\no\\noo\\noooo\\nooo\\noo\\no\\nooo\\nooooooooooo\\nooo\\no\\nooo\\noooo\\no\\nooo\\nooo\\no\\nooo\\no\\noo\\nooo\\nooo\\noooo\\nooo\\nooo\\nooooo\\no\\noo\\nooo\\nooo\\nooo\\nooo\\noooooo\\noo\\no\\nooo\\no\\noooo ooooooo\\nooo\\no\\noo\\no\\noooo\\no\\nooooooo\\noo\\noo\\noo\\nooooo\\noo\\nooooo\\no\\noooo\\no\\noo\\noo\\nooo\\no\\nooooo\\noo\\nooo\\noo\\no\\noooo\\noo\\noo\\nooo\\no\\noo\\nooooo\\noooo\\no\\noo\\no\\noooooooo\\noo\\noooooo\\nooo\\nooooooo\\noooo\\no\\noo\\no\\nooo\\no\\noooo\\noooo\\noo\\noo\\noo\\no\\no\\noooo\\noo\\no\\noooo\\nooooooo\\no\\noooo\\noo\\noo\\noooo\\nooo\\nooo\\noo\\noooo\\nooo\\no\\nooo\\noooo\\noo\\nooo\\nooo\\no\\noooo\\noo\\no\\noo\\no\\noooo\\nooo\\noooo\\nooo\\nooooooooo\\no\\nooooo\\no\\nooo\\no\\noo\\nooo\\noooo\\nooo\\noooo\\no\\nooooo\\no\\nooo\\nooo\\nooo\\noo\\noo oooo\\nooo\\noo\\noo o\\nooooooooo\\noo\\noooo\\nooo\\noo\\nooo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\nooooo\\noo\\noooooo\\noooooo\\noo\\no\\noooo\\nooo\\nooo\\nooo\\noo\\no\\nooo\\noo\\noo\\noComponent\\n 4ooo\\nooo\\nooo\\no\\noooo oooooo\\noooooo\\noooooo\\noo\\noo\\nooo\\noo\\noo ooo\\no\\nooo\\noo\\nooooo\\no\\nooo\\no\\noooo\\no\\no\\noooooo ooo\\nooooooo\\no oooo\\nooooo\\no\\noooooo\\noo\\noooooooo\\no\\nooo\\noo\\noooo\\nooo\\noo\\nooo\\noooooo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\noo\\noooooo o\\nooo\\nooooo\\noo\\no ooo\\no\\noo\\nooooooo\\nooo\\nooo\\no\\nooo\\nooo\\noo\\noooo\\nooooooooo\\nooooooo\\no\\nooooo\\noo\\noooo\\noooo\\noo\\no\\nooooooo\\noo\\nooooooooooo\\no\\noooooo\\noo\\noooo\\nooo o\\nooooooo\\nooo\\nooo\\no\\noooo\\noooo\\nooo\\noooooooooooo\\nooooooooooo\\noooo\\nooooo\\nooo\\no\\no\\no\\nooooo\\noooooooooo\\noooooo\\noo o\\no\\noooo\\noooooo\\no\\noooooooooo\\noo\\noooooooooo\\nooooooo\\nooo oo\\noo\\no\\noooo\\nooo\\nooo oo\\nooooooo\\no\\nooo\\noo\\no\\no\\nooooo\\noo\\noo\\nooo\\no\\noo\\noooo\\nooo\\nooo\\noo\\nooo\\noo\\noo\\noo\\noo\\nooo\\nooooooo\\noo\\noo\\noooooooo\\nooo\\noo\\noo\\noo\\noooooooo\\nooo\\nooooooo\\nooo\\nooooooooooo\\noooo\\noooo\\no\\noooooooo\\noo\\noooooo\\no\\noo\\nooooooo\\nooo\\nooooooooo\\nooo\\noo\\no\\noooooo\\noooo\\noooooo\\noo\\noo\\noo oo o\\no\\no\\noo oooooo\\nooo\\no\\noo\\noo\\no\\noooo\\no\\noooo\\nooooo\\noo\\no\\noo\\noo\\noo\\no\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\no ooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\noooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooooo\\nooo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\noooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\nooooooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noo oo\\noo\\noo\\noo\\noo\\noo\\noooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\noooooooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\noooooo\\no\\no\\nooooooo\\no\\noooo\\noo\\noooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooooo oooooo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\noooo\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noooo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\nooo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooo\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\noooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\noooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooooo\\nooooo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\no\\nooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\nooooooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noooo\\noo\\noo\\noo\\noo\\noo\\no ooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\noooooooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\nooo ooo\\no\\no'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 581}, page_content='oo\\no\\nooooo\\noo\\noooooooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\nooo ooo\\no\\no\\nooooooo\\no\\noooo\\noo\\noooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooooooooooo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\nooo o\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noooo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\no oo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooo\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\noooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\no ooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooo oo\\nooo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\noooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\noooo ooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noooo\\noo\\noo\\noo\\noo\\noo\\noooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\nooooo ooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooooo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\noooooo\\no\\no\\nooooooo\\no\\noooo\\noo\\noooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooooo oooo oo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\noooo\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noooo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noo\\noo\\noo\\nooooo\\nooo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooo\\no\\noo\\noooo\\no\\no\\noo\\noo\\nooo\\nooo\\nooo\\noooo\\nooooo\\no\\no\\noo\\noo\\no\\noo\\noo\\no\\nooooo\\nooooo\\noo\\noo\\nooo\\noo\\no\\nooo\\noooo\\noooo\\noooo\\noooo\\no\\noo\\no\\no\\noooooo\\nooooooooo\\nooo\\noo\\noo\\noo\\no\\noo\\noo\\noo\\no\\noo\\no\\noo\\no\\noooo\\nooo\\noo\\noo\\no\\noo\\nooooo\\noo\\nooo\\noo\\nooo\\no\\no\\nooooooooo\\noo\\no\\noooo\\noooo\\noo\\no\\nooo\\noo\\noo\\noooooo\\nooo\\noo\\nooo\\noooo\\nooo\\noo\\noooo ooo\\noo\\nooo\\no\\noo\\nooooo\\no\\nooooo\\no\\noooo\\noo\\nooo\\noooo\\noooo\\noo\\no\\no\\noo\\nooo\\nooooo\\noo\\noo\\no\\no\\nooo\\nooo\\noo\\nooooooooo\\no\\noooo\\no\\noo\\nooo\\noo\\noooo\\noooo\\noo\\noo\\noo\\noo\\noo\\noooo\\nooo\\nooooo\\noo\\noo\\nooo\\noo\\noooooo\\noo\\no\\nooooo\\noo\\noooooooooo\\noooooo\\noo\\noooo\\no\\nooo\\noo\\no\\noooooo\\nooo\\nooo\\nooo\\nooo\\noo\\nooo\\nooo\\nooooo\\noooooo\\nooooo\\nooo\\nooo\\noo\\nooo\\no\\nooo\\nooo\\nooooo oo\\no\\nooooo\\noo\\no\\noo\\noo\\nooo\\noo\\noo\\nooo\\no\\noooo\\no\\nooo\\noooooo\\no\\no\\nooooooo\\no\\noooo\\noooooo\\nooo\\nooo\\no\\nooooo\\noooo\\noooo\\noo oooooo\\noo\\noooo\\noo\\nooooo\\nooo\\noo\\nooo\\noo\\noooo\\nooo\\noo\\no\\noooo\\nooooo\\no\\noo\\no\\noo\\noo\\noo\\no\\no\\nooo\\noo\\no\\nooo\\noo\\noooo\\noo\\no\\noo\\noo\\noo\\noooooo\\noooo\\noo\\nooooo\\nooo\\noooo\\noo\\no\\noo\\no\\nooooo\\nooooComponent\\n 5PCA ComponentsICA Components\\nFIGURE 14.39. A comparison of the ﬁrst ﬁve ICA components computed using\\nFastICA (above diagonal) with the ﬁrst ﬁve PCA components(below diagonal) .\\nEach component is standardized to have unit variance.\\nGaussian as possible. With pre-whitened data, this amounts to looking for\\ncomponents that are as independent as possible.\\nICA starts from essentially a factor analysis solution, and looks fo r rota-\\ntions that lead to independent components. From this point of view, ICA is\\njust another factor rotation method, along with the traditional “varimax ”\\nand “quartimax” methods used in psychometrics.\\nExample: Handwritten Digits\\nWe revisit the handwritten threes analyzed by PCA in Section 14.5.1. Fig-\\nure 14.39 compares the ﬁrst ﬁve (standardized) principal components with\\nthe ﬁrst ﬁve ICA components, all shown in the same standardized units.\\nNote that each plot is a two-dimensional projection from a 256-dimensional'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 582}, page_content='564 14. Unsupervised Learning\\nMean ICA 1 ICA 2 ICA 3 ICA 4 ICA 5\\nFIGURE 14.40. The highlighted digits from Figure 14.39. By comparing with\\nthe mean digits, we see the nature of the ICA component.\\nspace. While the PCA components all appear to have joint Gaussian distri-\\nbutions, the ICA components have long-tailed distributions. This is not too\\nsurprising, since PCA focuses on variance, while ICA speciﬁcally looks for\\nnon-Gaussian distributions. All the components have been standardized,\\nso we do not see the decreasing variances of the principal components.\\nFor each ICA component we have highlighted two of the extreme digits,\\nas well as a pair of central digits and displayed them in Figure 14.40.\\nThis illustrates the nature of each of the components. For example, ICA\\ncomponent ﬁve picks up the long sweeping tailed threes.\\nExample: EEG Time Courses\\nICA has become an important tool in the study of brain dynamics—the\\nexample we present here uses ICA to untangle the components of signals\\nin multi-channel electroencephalographic (EEG) data (Onton and Makeig,\\n2006).\\nSubjects wear a cap embedded with a lattice of 100 EEG electrodes,\\nwhich record brain activity at diﬀerent locations on the scalp. Figure 14.4111\\n(top panel) shows 15 seconds of output from a subset of nine of these elec-\\ntrodes from a subject performing a standard “two-back” learning task over\\na 30 minute period. The subject is presented with a letter (B, H, J, C, F, or\\nK) at roughly 1500-ms intervals, and responds by pressing one of two but-\\ntons to indicate whether the letter presented is the same or diﬀerent from\\nthat presented two steps back. Depending on the answer, the subject earns\\nor loses points, and occasionally earns bonus or loses penalty points. The\\ntime-course data show spatial correlation in the EEG signals—the signals\\nof nearby sensors look very similar.\\nThe key assumption here is that signals recorded at each scalp electrode\\nare a mixture of independent potentials arising from diﬀerent cortical ac-\\n11Reprinted from Progress in Brain Research , Vol. 159, Julie Onton and Scott Makeig,\\n“Information based modeling of event-related brain dynami cs,” Page 106 , Copyright\\n(2006), with permission from Elsevier. We thank Julie Onton and Scott Makeig for\\nsupplying an electronic version of the image.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 583}, page_content='14.7 Independent Component Analysis and Exploratory Projection Pursu it 565\\ntivities, as well as non-cortical artifact domains; see the reference for a\\ndetailed overview of ICA in this domain.\\nThe lower part of Figure 14.41 shows a selection of ICA components.\\nThe colored images represent the estimated unmixing coeﬃcient vectors ˆ aj\\nas heatmap images superimposed on the scalp, indicating the location of\\nactivity. The corresponding time-courses show the activity of the learned\\nICA components.\\nFor example, the subject blinked after each performance feedback signal\\n(colored vertical lines), which accounts for the location and artifact signa l\\nin IC1 and IC3. IC12 is an artifact associated with the cardiac pulse. IC4\\nand IC7 account for frontal theta-band activities, and appear after a stretch\\nof correct performance. See Onton and Makeig (2006) for a more detailed\\ndiscussion of this example, and the use of ICA in EEG modeling.\\n14.7.3 Exploratory Projection Pursuit\\nFriedman and Tukey (1974) proposed exploratory projection pursuit, a\\ngraphical exploration technique for visualizing high-dimensional data. Their\\nview was that most low (one- or two-dimensional) projections of high-\\ndimensional data look Gaussian. Interesting structure, such as clusters or\\nlong tails, would be revealed by non-Gaussian projections. They proposed\\na number of projection indices for optimization, each focusing on a diﬀer-\\nent departure from Gaussianity. Since their initial proposal, a variety of\\nimprovements have been suggested (Huber, 1985; Friedman, 1987), and a\\nvariety of indices, including entropy, are implemented in the interactive\\ngraphics package Xgobi (Swayne et al., 1991, now called GGobi). These\\nprojection indices are exactly of the same form as J(Yj) above, where\\nYj=aT\\njX, a normalized linear combination of the components of X. In\\nfact, some of the approximations and substitutions for cross-entropy coin-\\ncide with indices proposed for projection pursuit. Typically with projection\\npursuit, the directions ajare not constrained to be orthogonal. Friedman\\n(1987) transforms the data to look Gaussian in the chosen projection, and\\nthen searches for subsequent directions. Despite their diﬀerent origins, ICA\\nand exploratory projection pursuit are quite similar, at least in the repre-\\nsentation described here.\\n14.7.4 A Direct Approach to ICA\\nIndependent components have by deﬁnition a joint product density\\nfS(s) =p∏\\nj=1fj(sj), (14.88)\\nso here we present an approach that estimates this density directly us-\\ning generalized additive models (Section 9.1). Full details can be found in'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 584}, page_content='566 14. Unsupervised Learning\\nFIGURE 14.41. Fifteen seconds of EEG data (of 1917seconds) at nine (of\\n100) scalp channels (top panel), as well as nine ICA components (lower pa nel).\\nWhile nearby electrodes record nearly identical mixtures of bra in and non-brain\\nactivity, ICA components are temporally distinct. The colored scalps represent the\\nICA unmixing coeﬃcients ˆajas a heatmap, showing brain or scalp location of the\\nsource.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 585}, page_content='14.7 Independent Component Analysis and Exploratory Projection Pursu it 567\\nHastie and Tibshirani (2003), and the method is implemented in the R\\npackageProDenICA , available from CRAN.\\nIn the spirit of representing departures from Gaussianity, we represent\\neachfjas\\nfj(sj) =φ(sj)egj(sj), (14.89)\\natilted Gaussian density. Here φis the standard Gaussian density, and\\ngjsatisﬁes the normalization conditions required of a density. Assuming\\nas before that Xis pre-whitened, the log-likelihood for the observed data\\nX=ASis\\nℓ(A,{gj}p\\n1;X) =N∑\\ni=1p∑\\nj=1[\\nlogφj(aT\\njxi) +gj(aT\\njxi)\\n], (14.90)\\nwhich we wish to maximize subject to the constraints that Ais orthogonal\\nand that the gjresult in densities in (14.89). Without imposing any further\\nrestrictions on gj, the model (14.90) is over-parametrized, so we instead\\nmaximize a regularized version\\np∑\\nj=1[\\n1\\nNN∑\\ni=1[\\nlogφ(aT\\njxi) +gj(aT\\njxi)]\\n−∫\\nφ(t)egj(t)dt−λj∫\\n{g′′′\\nj(t)}2(t)dt]\\n.\\n(14.91)\\nWe have subtracted two penalty terms (for each j) in (14.91), inspired by\\nSilverman (1986, Section 5.4.4):\\n•The ﬁrst enforces the density constraint∫\\nφ(t)eˆgj(t)dt= 1 on any\\nsolution ˆ gj.\\n•The second is a roughness penalty, which guarantees that the solution\\nˆgjis a quartic-spline with knots at the observed values of sij=aT\\njxi.\\nIt can further be shown that the solution densities ˆfj=φeˆgjeach have\\nmean zero and variance one (Exercise 14.18). As we increase λj, these\\nsolutions approach the standard Gaussian φ.\\nAlgorithm 14.3 Product Density ICA Algorithm: ProDenICA\\n1. Initialize A(random Gaussian matrix followed by orthogonalization).\\n2. Alternate until convergence of A:\\n(a) Given A, optimize (14.91) w.r.t. gj(separately for each j).\\n(b) Given gj, j= 1,... ,p , perform one step of a ﬁxed point algo-\\nrithm towards ﬁnding the optimal A.\\nWe ﬁt the functions gjand directions ajby optimizing (14.91) in an\\nalternating fashion, as described in Algorithm 14.3.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 586}, page_content='568 14. Unsupervised Learning\\nStep 2(a) amounts to a semi-parametric density estimation, which can\\nbe solved using a novel application of generalized additive models. For\\nconvenience we extract one of the pseparate problems,\\n1\\nNN∑\\ni=1[logφ(si) +g(si)]−∫\\nφ(t)eg(t)dt−λ∫\\n{g′′′(t)}2(t)dt. (14.92)\\nAlthough the second integral in (14.92) leads to a smoothing spline, the\\nﬁrst integral is problematic, and requires an approximation. We construct\\na ﬁne grid of Lvalues s∗\\nℓin increments ∆ covering the observed values si,\\nand count the number of siin the resulting bins:\\ny∗\\nℓ=#si∈(s∗\\nℓ−∆/2,s∗\\nℓ+ ∆/2)\\nN. (14.93)\\nTypically we pick Lto be 1000, which is more than adequate. We can then\\napproximate (14.92) by\\nL∑\\nℓ=1{\\ny∗\\ni[log(φ(s∗\\nℓ)) +g(s∗\\nℓ)]−∆φ(s∗\\nℓ)eg(s∗\\nℓ)}\\n−λ∫\\ng′′′2(s)ds. (14.94)\\nThis last expression can be seen to be proportional to a penalized Poisson\\nlog-likelihood with response y∗\\nℓ/∆ and penalty parameter λ/∆, and mean\\nθ(s) =φ(s)eg(s). This is a generalized additive spline model (Hastie and\\nTibshirani, 1990; Efron and Tibshirani, 1996), with an oﬀset term log φ(s),\\nand can be ﬁt using a Newton algorithm in O(L) operations. Although\\na quartic spline is called for, we ﬁnd in practice that a cubic spline is\\nadequate. We have ptuning parameters λjto set; in practice we make\\nthem all the same, and specify the amount of smoothing via the eﬀective\\ndegrees-of-freedom df( λ). Our software uses 5df as a default value.\\nStep 2(b) in Algorithm 14.3 requires optimizing (14.92) with respect to\\nA, holding the ˆ gjﬁxed. Only the ﬁrst terms in the sum involve A, and\\nsinceAis orthogonal, the collection of terms involving φdo not depend on\\nA(Exercise 14.19). Hence we need to maximize\\nC(A) =1\\nNp∑\\nj=1N∑\\ni=1ˆgj(aT\\njxi) (14.95)\\n=p∑\\nj=1Cj(aj)\\nC(A) is a log-likelihood ratio between the ﬁtted density and a Gaussian,\\nand can be seen as an estimate of negentropy (14.86), with each ˆ gja con-\\ntrast function as in (14.87). The ﬁxed point update in step 2(b) is a modiﬁed\\nNewton step (Exercise 14.20)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 587}, page_content='14.7 Independent Component Analysis and Exploratory Projection Pursu it 569\\n1. For each jupdate\\naj←E{\\nXˆg′\\nj(aT\\njX)−E[ˆg′′\\nj(aT\\njX)]aj}\\n, (14.96)\\nwhere E represents expectation w.r.t the sample xi. Since ˆ gjis a ﬁtted\\nquartic (or cubic) spline, the ﬁrst and second derivatives are readily\\navailable.\\n2. Orthogonalize Ausing the symmetric square-root transformation\\n(AAT)−1\\n2A. IfA=UDVTis the SVD of A, it is easy to show that\\nthis leads to the update A←UVT.\\nOurProDenICA algorithm works as well as FastICA on the artiﬁcial time\\nseries data of Figure 14.37, the mixture of uniforms data of Figure 14.38 ,\\nand the digit data in Figure 14.39.\\nExample: Simulations\\na b c\\nd e f\\ng h i\\nj k l\\nm n o\\np q r\\nDistributionAmari Distance from True A\\na b c d e f g h i j k l m n o p q r0.01 0.02 0.05 0.10 0.20 0.50FastICA\\nKernelICA\\nProdDenICA\\nFIGURE 14.42. The left panel shows 18distributions used for comparisons.\\nThese include the “t”, uniform, exponential, mixtures of exponential s, symmetric\\nand asymmetric Gaussian mixtures. The right panel shows (on the log scale)\\nthe average Amari metric for each method and each distributio n, based on 30\\nsimulations in I R2for each distribution.\\nFigure 14.42 shows the results of a simulation comparing ProDenICA to\\nFastICA , and another semi-parametric competitor KernelICA (Bach and\\nJordan, 2002). The left panel shows the 18 distributions used as a basis\\nof comparison. For each distribution, we generated a pair of independent\\ncomponents ( N= 1024), and a random mixing matrix in IR2with condition\\nnumber between 1 and 2. We used our R implementations of FastICA , using\\nthe negentropy criterion (14.87), and ProDenICA . ForKernelICA we used'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 588}, page_content='570 14. Unsupervised Learning\\nthe authors MATLAB code.12Since the search criteria are nonconvex, we\\nused ﬁve random starts for each method. Each of the algorithms delivers\\nan orthogonal mixing matrix A(the data were pre-whitened ), which is\\navailable for comparison with the generating orthogonalized mixing matri x\\nA0. We used the Amari metric (Bach and Jordan, 2002) as a measure of\\nthe closeness of the two frames:\\nd(A0,A) =1\\n2pp∑\\ni=1(∑p\\nj=1|rij|\\nmax j|rij|−1)\\n+1\\n2pp∑\\nj=1(∑p\\ni=1|rij|\\nmax i|rij|−1)\\n,(14.97)\\nwhere rij= (AoA−1)ij. The right panel in Figure 14.42 compares the\\naverages (on the log scale) of the Amari metric between the truth and the\\nestimated mixing matrices. ProDenICA is competitive with FastICA and\\nKernelICA in all situations, and dominates most of the mixture simulations.\\n14.8 Multidimensional Scaling\\nBoth self-organizing maps and principal curves and surfaces map data\\npoints in IRpto a lower-dimensional manifold. Multidimensional scaling\\n(MDS) has a similar goal, but approaches the problem in a somewhat dif-\\nferent way.\\nWe start with observations x1,x2,... ,x N∈IRp, and let dijbe the dis-\\ntance between observations iandj. Often we choose Euclidean distance\\ndij=||xi−xj||, but other distances may be used. Further, in some ap-\\nplications we may not even have available the data points xi, but only\\nhave some dissimilarity measure dij(see Section 14.3.10). For example, in\\na wine tasting experiment, dijmight be a measure of how diﬀerent a sub-\\nject judged wines iandj, and the subject provides such a measure for all\\npairs of wines i,j. MDS requires only the dissimilarities dij, in contrast to\\nthe SOM and principal curves and surfaces which need the data points xi.\\nMultidimensional scaling seeks values z1,z2,... ,z N∈IRkto minimize\\nthe so-called stress function13\\nSM(z1,z2,... ,z N) =∑\\ni̸=i′(dii′− ||zi−zi′||)2. (14.98)\\nThis is known as least squares orKruskal–Shephard scaling. The idea is\\nto ﬁnd a lower-dimensional representation of the data that preserves the\\npairwise distances as well as possible. Notice that the approximation is\\n12Francis Bach kindly supplied this code, and helped us set up th e simulations.\\n13Some authors deﬁne stress as the square-root of SM; since it does not aﬀect the\\noptimization, we leave it squared to make comparisons with o ther criteria simpler.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 589}, page_content='14.8 Multidimensional Scaling 571\\nin terms of the distances rather than squared distances (which results in\\nslightly messier algebra). A gradient descent algorithm is used to minimize\\nSM.\\nA variation on least squares scaling is the so-called Sammon mapping\\nwhich minimizes\\nSSm(z1,z2,... ,z N) =∑\\ni̸=i′(dii′− ||zi−zi′||)2\\ndii′. (14.99)\\nHere more emphasis is put on preserving smaller pairwise distances.\\nInclassical scaling , we instead start with similarities sii′: often we use\\nthe centered inner product sii′=⟨xi−¯x,xi′−¯x⟩. The problem then is to\\nminimize\\nSC(z1,z2,... ,z N) =∑\\ni,i′(sii′− ⟨zi−¯z,zi′−¯z⟩)2(14.100)\\noverz1,z2,... ,z N∈IRk. This is attractive because there is an explicit\\nsolution in terms of eigenvectors: see Exercise 14.11. If we have distances\\nrather than inner-products, we can convert them to centered inner-products\\nif the distances are Euclidean ;14see (18.31) on page 671 in Chapter 18.\\nIf the similarities are in fact centered inner-products, classical scaling is\\nexactly equivalent to principal components, an inherently linear dimension-\\nreduction technique. Classical scaling is not equivalent to least squares\\nscaling; the loss functions are diﬀerent, and the mapping can be nonlinear.\\nLeast squares and classical scaling are referred to as metric scaling meth-\\nods, in the sense that the actual dissimilarities or similarities are appro x-\\nimated. Shephard–Kruskal nonmetric scaling eﬀectively uses only ranks.\\nNonmetric scaling seeks to minimize the stress function\\nSNM(z1,z2,... ,z N) =∑\\ni̸=i′[\\n||zi−zi′|| −θ(dii′)]2\\n∑\\ni̸=i′||zi−zi′||2(14.101)\\nover the ziand an arbitrary increasing function θ. With θﬁxed, we min-\\nimize over ziby gradient descent. With the ziﬁxed, the method of iso-\\ntonic regression is used to ﬁnd the best monotonic approximation θ(dii′)\\nto||zi−zi′||. These steps are iterated until the solutions stabilize.\\nLike the self-organizing map and principal surfaces, multidimensional\\nscaling represents high-dimensional data in a low-dimensional coordinate\\nsystem. Principal surfaces and SOMs go a step further, and approximate\\nthe original data by a low-dimensional manifold, parametrized in the low\\ndimensional coordinate system. In a principal surface and SOM, points\\n14AnN×Ndistance matrix is Euclidean if the entries represent pairw ise Euclidean\\ndistances between Npoints in some dimensional space.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 590}, page_content='572 14. Unsupervised Learning\\nFirst MDS CoordinateSecond MDS Coordinate\\n-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•\\n•••\\n•••\\n•\\n••\\n•••\\n••••\\n•\\n•••\\n••••••\\n••\\n•••\\n••\\n•\\n••\\n•••\\n•\\n••••••\\n•••\\n••\\n••\\n•••\\n•\\n••••\\n•••\\n••\\n••\\n••\\n•\\n••\\n•••\\n•\\n•••\\n••••\\n•••\\n••\\nFIGURE 14.43. First two coordinates for half-sphere data, from classical m ulti-\\ndimensional scaling.\\nclose together in the original feature space should map close together on\\nthe manifold, but points far apart in feature space might also map close\\ntogether. This is less likely in multidimensional scaling since it explicitly\\ntries to preserve all pairwise distances.\\nFigure 14.43 shows the ﬁrst two MDS coordinates from classical scaling\\nfor the half-sphere example. There is clear separation of the clusters, and\\nthe tighter nature of the red cluster is apparent.\\n14.9 Nonlinear Dimension Reduction and Local\\nMultidimensional Scaling\\nSeveral methods have been recently proposed for nonlinear dimension re-\\nduction, similar in spirit to principal surfaces. The idea is that the data lie\\nclose to an intrinsically low-dimensional nonlinear manifold embedded in a\\nhigh-dimensional space. These methods can be thought of as “ﬂattening”\\nthe manifold, and hence reducing the data to a set of low-dimensional co-\\nordinates that represent their relative positions in the manifold. They are\\nuseful for problems where signal-to-noise ratio is very high (e.g., physical\\nsystems), and are probably not as useful for observational data with lower\\nsignal-to-noise ratios.\\nThe basic goal is illustrated in the left panel of Figure 14.44. The data\\nlie near a parabola with substantial curvature. Classical MDS does not pre-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 591}, page_content='14.9 Nonlinear Dimension Reduction and Local Multidimensional Scaling 573\\n−5 0 5−15 −10 −5 0Classical MDS\\n−5 0 5−15 −10 −5 0Local MDS\\nx1 x1\\nx2x2\\nFIGURE 14.44. The orange points show data lying on a parabola, while the blue\\npoints shows multidimensional scaling representations in one dime nsion. Classical\\nmultidimensional scaling (left panel) does not preserve the orde ring of the points\\nalong the curve, because it judges points on opposite ends of the curve to be close\\ntogether. In contrast, local multidimensional scaling (right p anel) does a good job\\nof preserving the ordering of the points along the curve.\\nserve the ordering of the points along the curve, because it judges points\\non opposite ends of the curve to be close together. The right panel shows\\nthe results of local multi-dimensional scaling , one of the three methods for\\nnon-linear multi-dimensional scaling that we discuss below. These meth-\\nods use only the coordinates of the points in pdimensions, and have no\\nother information about the manifold. Local MDS has done a good job of\\npreserving the ordering of the points along the curve.\\nWe now brieﬂy describe three new approaches to nonlinear dimension\\nreduction and manifold mapping.\\nIsometric feature mapping (ISOMAP) (Tenenbaum et al., 2000) con-\\nstructs a graph to approximate the geodesic distance between points along\\nthe manifold. Speciﬁcally, for each data point we ﬁnd its neighbors—points\\nwithin some small Euclidean distance of that point. We construct a graph\\nwith an edge between any two neighboring points. The geodesic distance\\nbetween any two points is then approximated by the shortest path be-\\ntween points on the graph. Finally, classical scaling is applied to the graph\\ndistances, to produce a low-dimensional mapping.\\nLocal linear embedding (Roweis and Saul, 2000) takes a very diﬀerent ap-\\nproach, trying to preserve the local aﬃne structure of the high-dimensional\\ndata. Each data point is approximated by a linear combination of neigh-\\nboring points. Then a lower dimensional representation is constructed that'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 592}, page_content='574 14. Unsupervised Learning\\nbest preserves these local approximations. The details are interesting, so\\nwe give them here.\\n1. For each data point xiinpdimensions, we ﬁnd its K-nearest neigh-\\nborsN(i) in Euclidean distance.\\n2. We approximate each point by an aﬃne mixture of the points in its\\nneighborhood:\\nmin\\nWik||xi−∑\\nk∈N(i)wikxk||2(14.102)\\nover weights wiksatisfying wik= 0, k /∈ N(i),∑N\\nk=1wik= 1.wik\\nis the contribution of point kto the reconstruction of point i. Note\\nthat for a hope of a unique solution, we must have K < p .\\n3. Finally, we ﬁnd points yiin a space of dimension d < p to minimize\\nN∑\\ni=1||yi−N∑\\nk=1wikyk||2(14.103)\\nwithwikﬁxed.\\nIn step 3, we minimize\\ntr[(Y−WY)T(Y−WY)] = tr[ YT(I−W)T(I−W)Y] (14.104)\\nwhere WisN×N;YisN×d, for some small d < p . The solutions ˆY\\nare the trailing eigenvectors of M= (I−W)T(I−W). Since 1is a trivial\\neigenvector with eigenvalue 0, we discard it and keep the next d. This has\\nthe side eﬀect that 1TY= 0, and hence the embedding coordinates are\\nmean centered.\\nLocal MDS (Chen and Buja, 2008) takes the simplest and arguably the\\nmost direct approach. We deﬁne Nto be the symmetric set of nearby pairs\\nof points; speciﬁcally a pair ( i,i′) is in Nif point iis among the K-nearest\\nneighbors of i′, or vice-versa. Then we construct the stress function\\nSL(z1,z2,... ,z N) =∑\\n(i,i′)∈N(dii′− ||zi−zi′||)2\\n+∑\\n(i,i′)/∈Nw≤(D− ||zi−zi′||)2.(14.105)\\nHereDis some large constant and wis a weight. The idea is that points\\nthat are not neighbors are considered to be very far apart; such pairs are\\ngiven a small weight wso that they don’t dominate the overall stress func-\\ntion. To simplify the expression, we take w∼1/D, and let D→ ∞ .\\nExpanding (14.105), this gives'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 593}, page_content='14.9 Nonlinear Dimension Reduction and Local Multidimensional Scaling 575\\nFIGURE 14.45. Images of faces mapped into the embedding space described by\\nthe ﬁrst two coordinates of LLE. Next to the circled points, repre sentative faces\\nare shown in diﬀerent parts of the space. The images at the bott om of the plot\\ncorrespond to points along the top right path (linked by solid line ), and illustrate\\none particular mode of variability in pose and expression.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 594}, page_content='576 14. Unsupervised Learning\\nSL(z1,z2,... ,z N) =∑\\n(i,i′)∈N(dii′− ||zi−zi′||)2−τ∑\\n(i,i′)/∈N||zi−zi′||,\\n(14.106)\\nwhere τ= 2wD. The ﬁrst term in (14.106) tries to preserve local structure\\nin the data, while the second term encourages the representations zi,zi′\\nfor pairs ( i,i′) that are non-neighbors to be farther apart. Local MDS\\nminimizes the stress function (14.106) over zi, for ﬁxed values of the number\\nof neighbors Kand the tuning parameter τ.\\nThe right panel of Figure 14.44 shows the result of local MDS, using k= 2\\nneighbors and τ= 0.01. We used coordinate descent with multiple starting\\nvalues to ﬁnd a good minimum of the (nonconvex) stress function (14.106).\\nThe ordering of the points along the curve has been largely preserved,\\nFigure 14.45 shows a more interesting application of one of these meth-\\nods (LLE)15. The data consist of 1965 photographs, digitized as 20 ×28\\ngrayscale images. The result of the ﬁrst two-coordinates of LLE are shown\\nand reveal some variability in pose and expression. Similar pictures were\\nproduced by local MDS.\\nIn experiments reported in Chen and Buja (2008), local MDS shows su-\\nperior performance, as compared to ISOMAP and LLE. They also demon-\\nstrate the usefulness of local MDS for graph layout. There are also close\\nconnections between the methods discussed here, spectral clustering (Sec-\\ntion 14.5.3) and kernel PCA (Section 14.5.4).\\n14.10 The Google PageRank Algorithm\\nIn this section we give a brief description of the original PageRank algo-\\nrithm used by the Google search engine, an interesting recent application\\nof unsupervised learning methods.\\nWe suppose that we have Nweb pages and wish to rank them in terms\\nof importance. For example, the Npages might all contain a string match\\nto “statistical learning” and we might wish to rank the pages in terms of\\ntheir likely relevance to a websurfer.\\nThePageRank algorithm considers a webpage to be important if many\\nother webpages point to it. However the linking webpages that point to a\\ngiven page are not treated equally: the algorithm also takes into account\\nboth the importance ( PageRank ) of the linking pages and the number of\\noutgoing links that they have. Linking pages with higher PageRank are\\ngiven more weight, while pages with more outgoing links are given less\\nweight. These ideas lead to a recursive deﬁnition for PageRank , detailed\\nnext.\\n15Sam Roweis and Lawrence Saul kindly provided this ﬁgure.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 595}, page_content='14.10 The Google PageRank Algorithm 577\\nLetLij= 1 if page jpoints to page i, and zero otherwise. Let cj=∑N\\ni=1Lijequal the number of pages pointed to by page j(number of out-\\nlinks). Then the Google PageRanks piare deﬁned by the recursive rela-\\ntionship\\npi= (1−d) +dN∑\\nj=1(Lij\\ncj)\\npj (14.107)\\nwhere dis a positive constant (apparently set to 0.85).\\nThe idea is that the importance of page iis the sum of the importances of\\npages that point to that page. The sums are weighted by 1 /cj, that is, each\\npage distributes a total vote of 1 to other pages. The constant densures\\nthat each page gets a PageRank of at least 1 −d. In matrix notation\\np= (1−d)e+d≤LD−1\\ncp (14.108)\\nwhere eis a vector of Nones and Dc= diag( c) is a diagonal matrix with\\ndiagonal elements cj. Introducing the normalization eTp=N(i.e., the\\naverage PageRank is 1), we can write (14.108) as\\np=[\\n(1−d)eeT/N+dLD−1\\nc]\\np\\n=Ap (14.109)\\nwhere the matrix Ais the expression in square braces.\\nExploiting a connection with Markov chains (see below), it can be shown\\nthat the matrix Ahas a real eigenvalue equal to one, and one is its largest\\neigenvalue. This means that we can ﬁnd ˆpby the power method: starting\\nwith some p=p0we iterate\\npk←Apk−1;pk←Npk\\neTpk. (14.110)\\nThe ﬁxed points ˆpare the desired PageRanks .\\nIn the original paper of Page et al. (1998), the authors considered PageR-\\nankas a model of user behavior, where a random web surfer clicks on links\\nat random, without regard to content. The surfer does a random walk on\\nthe web, choosing among available outgoing links at random. The factor\\n1−dis the probability that he does not click on a link, but jumps instead\\nto a random webpage.\\nSome descriptions of PageRank have (1 −d)/Nas the ﬁrst term in def-\\ninition (14.107), which would better coincide with the random surfer in-\\nterpretation. Then the page rank solution (divided by N) is the stationary\\ndistribution of an irreducible, aperiodic Markov chain over the Nwebpages.\\nDeﬁnition (14.107) also corresponds to an irreducible, aperiodic Markov\\nchain, with diﬀerent transition probabilities than those from he (1 −d)/N\\nversion. Viewing PageRank as a Markov chain makes clear why the matrix\\nAhas a maximal real eigenvalue of 1. Since Ahas positive entries with'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 596}, page_content='578 14. Unsupervised Learning\\nPage 2\\nPage 3Page 4Page 1\\nFIGURE 14.46. PageRank algorithm: example of a small network\\neach column summing to one, Markov chain theory tells us that it has a\\nunique eigenvector with eigenvalue one, corresponding to the stationary\\ndistribution of the chain (Bremaud, 1999).\\nA small network is shown for illustration in Figure 14.46. The link ma trix\\nis\\nL=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed0 0 1 0\\n1 0 0 0\\n1 1 0 1\\n0 0 0 0\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8(14.111)\\nand the number of outlinks is c= (2,1,1,1).\\nThePageRank solution is ˆp= (1.49,0.78,1.58,0.15). Notice that page 4\\nhas no incoming links, and hence gets the minimum PageRank of 0.15.\\nBibliographic Notes\\nThere are many books on clustering, including Hartigan (1975), Gordon\\n(1999) and Kaufman and Rousseeuw (1990). K-means clustering goes back\\nat least to Lloyd (1957), Forgy (1965), Jancey (1966) and MacQueen (1967 ).\\nApplications in engineering, especially in image compression via vector\\nquantization, can be found in Gersho and Gray (1992). The k-medoid pro-\\ncedure is described in Kaufman and Rousseeuw (1990). Association rules\\nare outlined in Agrawal et al. (1995). The self-organizing map was propos ed\\nby Kohonen (1989) and Kohonen (1990); Kohonen et al. (2000) give a more\\nrecent account. Principal components analysis and multidimensional scal-\\ning are described in standard books on multivariate analysis, for exampl e,\\nMardia et al. (1979). Buja et al. (2008) have implemented a powerful en-\\nvironment called Ggvis for multidimensional scaling, and the user manual'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 597}, page_content='Exercises 579\\ncontains a lucid overview of the subject. Figures 14.17, 14.21 (left panel)\\nand 14.28 (left panel) were produced in Xgobi, a multidimensional data\\nvisualization package by the same authors. GGobi is a more recent im-\\nplementation (Cook and Swayne, 2007). Goodall (1991) gives a technical\\noverview of Procrustes methods in statistics, and Ramsay and Silverman\\n(1997) discuss the shape registration problem. Principal curves and surfaces\\nwere proposed in Hastie (1984) and Hastie and Stuetzle (1989). The idea of\\nprincipal points was formulated in Flury (1990), Tarpey and Flury (1996 )\\ngive an exposition of the general concept of self-consistency. An excellent\\ntutorial on spectral clustering can be found in von Luxburg (2007); this was\\nthe main source for Section 14.5.3. Luxborg credits Donath and Hoﬀman\\n(1973) and Fiedler (1973) with the earliest work on the subject. A history\\nof spectral clustering my be found in Spielman and Teng (1996). Indepen-\\ndent component analysis was proposed by Comon (1994), with subsequent\\ndevelopments by Bell and Sejnowski (1995); our treatment in Section 14.7\\nis based on Hyv¨ arinen and Oja (2000). Projection pursuit was proposed by\\nFriedman and Tukey (1974), and is discussed in detail in Huber (1985). A\\ndynamic projection pursuit algorithm is implemented in GGobi.\\nExercises\\nEx. 14.1 Weights for clustering . Show that weighted Euclidean distance\\nd(w)\\ne(xi,xi′) =∑p\\nl=1wl(xil−xi′l)2\\n∑p\\nl=1wl\\nsatisﬁes\\nd(w)\\ne(xi,xi′) =de(zi,zi′) =p∑\\nl=1(zil−zi′l)2, (14.112)\\nwhere\\nzil=xil≤(wl∑p\\nl=1wl)1/2\\n. (14.113)\\nThus weighted Euclidean distance based on xis equivalent to unweighted\\nEuclidean distance based on z.\\nEx. 14.2 Consider a mixture model density in p-dimensional feature space,\\ng(x) =K∑\\nk=1πkgk(x), (14.114)\\nwhere gk=N(θk,L≤σ2) and πk≥0∀kwith∑\\nkπk= 1. Here {θk,πk},k=\\n1,... ,K andσ2are unknown parameters.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 598}, page_content='580 14. Unsupervised Learning\\nSuppose we have data x1,x2,... ,x N∼g(x) and we wish to ﬁt the mix-\\nture model.\\n1. Write down the log-likelihood of the data\\n2. Derive an EM algorithm for computing the maximum likelihood es-\\ntimates (see Section 8.1).\\n3. Show that if σhas a known value in the mixture model and we take\\nσ→0, then in a sense this EM algorithm coincides with K-means\\nclustering.\\nEx. 14.3 In Section 14.2.6 we discuss the use of CART or PRIM for con-\\nstructing generalized association rules. Show that a problem occurs with ei-\\nther of these methods when we generate the random data from the product-\\nmarginal distribution; i.e., by randomly permuting the values for each of\\nthe variables. Propose ways to overcome this problem.\\nEx. 14.4 Cluster the demographic data of Table 14.1 using a classiﬁcation\\ntree. Speciﬁcally, generate a reference sample of the same size of the train-\\ning set, by randomly permuting the values within each feature. Build a\\nclassiﬁcation tree to the training sample (class 1) and the reference sample\\n(class 0) and describe the terminal nodes having highest estimated class 1\\nprobability. Compare the results to the PRIM results near Table 14.1 and\\nalso to the results of K-means clustering applied to the same data.\\nEx. 14.5 Generate data with three features, with 30 data points in each of\\nthree classes as follows:\\nθ1=U(−π/8, π/8)\\nφ1=U(0,2π)\\nx1= sin( θ1)cos(φ1) +W11\\ny1= sin( θ1)sin(φ1) +W12\\nz1= cos( θ1) +W13\\nθ2=U(π/2−π/4, π/2 +π/4)\\nφ2=U(−π/4, π/4)\\nx2= sin( θ2)cos(φ2) +W21\\ny2= sin( θ2)sin(φ2) +W22\\nz2= cos( θ2) +W23\\nθ3=U(π/2−π/4, π/2 +π/4)\\nφ3=U(π/2−π/4, π/2 +π/4)\\nx3= sin( θ3)cos(φ3) +W31\\ny3= sin( θ3)sin(φ3) +W32\\nz3= cos( θ3) +W33\\nHereU(a,b) indicates a uniform variate on the range [ a,b] and Wjkare\\nindependent normal variates with standard deviation 0 .6. Hence the data'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 599}, page_content='Exercises 581\\nlie near the surface of a sphere in three clusters centered at (1 ,0,0), (0,1,0)\\nand (0 ,0,1).\\nWrite a program to ﬁt a SOM to these data, using the learning rates\\ngiven in the text. Carry out a K-means clustering of the same data, and\\ncompare the results to those in the text.\\nEx. 14.6 Write programs to implement K-means clustering and a self-\\norganizing map (SOM), with the prototype lying on a two-dimensional\\ngrid. Apply them to the columns of the human tumor microarray data, us-\\ningK= 2,5,10,20 centroids for both. Demonstrate that as the size of the\\nSOM neighborhood is taken to be smaller and smaller, the SOM solution\\nbecomes more similar to the K-means solution.\\nEx. 14.7 Derive (14.51) and (14.52) in Section 14.5.1. Show that ˆ θis not\\nunique, and characterize the family of equivalent solutions.\\nEx. 14.8 Derive the solution (14.57) to the Procrustes problem (14.56).\\nDerive also the solution to the Procrustes problem with scaling (14.58).\\nEx. 14.9 Write an algorithm to solve\\nmin\\n{βℓ,Rℓ}L\\n1,ML∑\\nℓ=1||XℓRℓ−M||2\\nF. (14.115)\\nApply it to the three S’s, and compare the results to those shown in Fig-\\nure 14.26.\\nEx. 14.10 Derive the solution to the aﬃne-invariant average problem (14.60).\\nApply it to the three S’s, and compare the results to those computed in\\nExercise 14.9.\\nEx. 14.11 Classical multidimensional scaling. LetSbe the centered in-\\nner product matrix with elements ⟨xi−¯x,xj−¯x⟩. Let λ1> λ2>≤≤≤>\\nλkbe the klargest eigenvalues of S, with associated eigenvectors Ek=\\n(e1,e2,... ,ek). Let Dkbe a diagonal matrix with diagonal entries√λ1,√λ2,... ,√λk. Show that the solutions zito the classical scaling problem\\n(14.100) are the rowsofEkDk.\\nEx. 14.12 Consider the sparse PCA criterion (14.71).\\n1. Show that with Θﬁxed, solving for Vamounts to Kseparate elastic-\\nnet regression problems, with responses the Kelements of ΘTxi.\\n2. Show that with Vﬁxed, solving for Θamounts to a reduced-rank\\nversion of the Procrustes problem, which reduces to\\nmax\\nΘtrace(ΘTM) subject to ΘTΘ=IK, (14.116)\\nwhere MandΘare both p×KwithK≤p. IfM=UDQTis the\\nSVD of M, show that the optimal Θ=UQT.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 600}, page_content='582 14. Unsupervised Learning\\nEx. 14.13 Generate 200 data points with three features, lying close to a\\nhelix. In detail, deﬁne X1= cos( s) + 0.1≤Z1,X2= sin( s) + 0.1≤Z2,X3=\\ns+ 0.1≤Z3where stakes on 200 equally spaced values between 0 and 2 π,\\nandZ1,Z2,Z3are independent and have standard Gaussian distributions.\\n(a) Fit a principal curve to the data and plot the estimated coordinate\\nfunctions. Compare them to the underlying functions cos( s),sin(s)\\nands.\\n(b) Fit a self-organizing map to the same data, and see if you can discover\\nthe helical shape of the original point cloud.\\nEx. 14.14 Pre- and post-multiply equation (14.81) by a diagonal matrix\\ncontaining the inverse variances of the Xj. Hence obtain an equivalent\\ndecomposition for the correlation matrix, in the sense that a simple scali ng\\nis applied to the matrix A.\\nEx. 14.15 Generate 200 observations of three variates X1,X2,X3according\\nto\\nX1∼Z1\\nX2=X1+ 0.001≤Z2\\nX3= 10 ≤Z3 (14.117)\\nwhere Z1,Z2,Z3are independent standard normal variates. Compute the\\nleading principal component and factor analysis directions. Hence show\\nthat the leading principal component aligns itself in the maximal variance\\ndirection X3, while the leading factor essentially ignores the uncorrelated\\ncomponent X3, and picks up the correlated component X2+X1(Geoﬀrey\\nHinton, personal communication).\\nEx. 14.16 Consider the kernel principal component procedure outlined in\\nSection 14.5.4. Argue that the number Mof principal components is equal\\nto the rank of K, which is the number of non-zero elements in D. Show\\nthat the mth component zm(mth column of Z) can be written (up to\\ncentering) as zim=∑N\\nj=1αjmK(xi,xj), where αjm=ujm/dm. Show that\\nthe mapping of a new observation x0to the mth component is given by\\nz0m=∑N\\nj=1αjmK(x0,xj).\\nEx. 14.17 Show that with g1(x) =∑N\\nj=1cjK(x,xj), the solution to (14.66)\\nis given by ˆ cj=uj1/d1, where u1is the ﬁrst column of Uin (14.65), and\\nd1the ﬁrst diagonal element of D. Show that the second and subsequent\\nprincipal component functions are deﬁned in a similar manner ( hint: see\\nSection 5.8.1.)\\nEx. 14.18 Consider the regularized log-likelihood for the density estimation\\nproblem arising in ICA,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 601}, page_content='Exercises 583\\n1\\nNN∑\\ni=1[logφ(si) +g(si)]−∫\\nφ(t)eg(t)dt−λ∫\\n{g′′′(t)}2(t)dt.(14.118)\\nThe solution ˆ gis a quartic smoothing spline, and can be written as ˆ g(s) =\\nˆq(s) + ˆq⊥(s), where qis a quadratic function (in the null space of the\\npenalty). Let q(s) =θ0+θ1s+θ2s2. By examining the stationarity condi-\\ntions for ˆθk, k= 1,2,3, show that the solution ˆf=φeˆgis a density, and\\nhas mean zero and variance one. If we used a second-derivative penalty∫\\n{g′′(t)}2(t)dtinstead, what simple modiﬁcation could we make to the\\nproblem to maintain the three moment conditions?\\nEx. 14.19 IfAisp×porthogonal, show that the ﬁrst term in (14.92) on\\npage 568\\np∑\\nj=1N∑\\ni=1logφ(aT\\njxi),\\nwithajthejth column of A, does not depend on A.\\nEx. 14.20 Fixed point algorithm for ICA (Hyv¨ arinen et al., 2001). Consider\\nmaximizing C(a) =E{g(aTX)}with respect to a, with ||a||= 1 and\\nCov(X) =I. Use a Lagrange multiplier to enforce the norm constraint,\\nand write down the ﬁrst two derivatives of the modiﬁed criterion. Use the\\napproximation\\nE{XXTg′′(aTX)} ≈E{XXT}E{g′′(aTX)}\\nto show that the Newton update can be written as the ﬁxed-point update\\n(14.96).\\nEx. 14.21 Consider an undirected graph with non-negative edge weights\\nwii′and graph Laplacian L. Suppose there are mconnected components\\nA1,A2,... ,A min the graph. Show that there are meigenvectors of Lcorre-\\nsponding to eigenvalue zero, and the indicator vectors of these components\\nIA1,IA2,... ,I Amspan the zero eigenspace.\\nEx. 14.22\\n(a) Show that deﬁnition (14.108) implies that the sum of the PageRanks\\npiisN, the number of web pages.\\n(b) Write a program to compute the PageRank solutions by the power\\nmethod using formulation (14.107). Apply it to the network of Fig-\\nure 14.47.\\nEx. 14.23 Algorithm for non-negative matrix factorization (Wu and Lange,\\n2007). A function g(x,y) to said to minorize a function f(x) if'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 602}, page_content='584 14. Unsupervised Learning\\nPage 2Page 1\\nPage 3\\nPage 4\\nPage 5Page 6\\nFIGURE 14.47. Example of a small network.\\ng(x,y)≤f(x), g(x,x) =f(x) (14.119)\\nfor all x,yin the domain. This is useful for maximizing f(x) since it is easy\\nto show that f(x) is nondecreasing under the update\\nxs+1= argmaxxg(x,xs) (14.120)\\nThere are analogous deﬁnitions for majorization , for minimizing a function\\nf(x). The resulting algorithms are known as MMalgorithms, for “minorize-\\nmaximize” or “majorize-minimize” (Lange, 2004). It also can be shown\\nthat the EM algorithm (8.5) is an example of an MM algorithm: see Sec-\\ntion 8.5.3 and Exercise 8.2 for details.\\n(a) Consider maximization of the function L(W,H) in (14.73), written\\nhere without the matrix notation\\nL(W,H) =N∑\\ni=1p∑\\nj=1[\\nxijlog(r∑\\nk=1wikhkj)\\n−r∑\\nk=1wikhkj]\\n.\\nUsing the concavity of log( x), show that for any set of rvalues yk≥0\\nand 0 ≤ck≤1 with∑r\\nk=1ck= 1,\\nlog(r∑\\nk=1yk)\\n≥r∑\\nk=1cklog(yk/ck)\\nHence\\nlog(r∑\\nk=1wikhkj)\\n≥r∑\\nk=1as\\nikj\\nbs\\nijlog(\\nbs\\nij\\nas\\nikjwikhkj)\\n,\\nwhere\\nas\\nikj=ws\\nikhs\\nkjandbs\\nij=r∑\\nk=1ws\\nikhs\\nkj,\\nandsindicates the current iteration.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 603}, page_content='Exercises 585\\n(b) Hence show that, ignoring constants, the function\\ng(W,H|Ws,Hs) =N∑\\ni=1p∑\\nj=1r∑\\nk=1uijas\\nikj\\nbs\\nij(\\nlogwik+ loghkj)\\n−N∑\\ni=1p∑\\nj=1r∑\\nk=1wikhkj\\nminorizes L(W,H).\\n(c) Set the partial derivatives of g(W,H|Ws,Hs) to zero and hence\\nderive the updating steps (14.74).\\nEx. 14.24 Consider the non-negative matrix factorization (14.72) in the\\nrank one case ( r= 1).\\n(a) Show that the updates (14.74) reduce to\\nwi←wi∑p\\nj=1xij∑p\\nj=1wihj\\nhj←hj∑N\\ni=1xij∑N\\ni=1wihj(14.121)\\nwhere wi=wi1,hj=h1j. This is an example of the iterative pro-\\nportional scaling procedure, applied to the independence model for a\\ntwo-way contingency table (Fienberg, 1977, for example).\\n(b) Show that the ﬁnal iterates have the explicit form\\nwi=c≤∑p\\nj=1xij∑N\\ni=1∑p\\nj=1xij, h k=1\\nc≤∑N\\ni=1xik∑N\\ni=1∑p\\nj=1xij(14.122)\\nfor any constant c >0. These are equivalent to the usual row and\\ncolumn estimates for a two-way independence model.\\nEx. 14.25 Fit a non-negative matrix factorization model to the collection\\nof two’s in the digits database. Use 25 basis elements, and compare with a\\n24- component (plus mean) PCA model. In both cases display the Wand\\nHmatrices as in Figure 14.33.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 604}, page_content='586 14. Unsupervised Learning'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 605}, page_content='This is page 587\\nPrinter: Opaque this\\n15\\nRandom Forests\\n15.1 Introduction\\nBagging or bootstrap aggregation (section 8.7) is a technique for reducing\\nthe variance of an estimated prediction function. Bagging seems to work\\nespecially well for high-variance, low-bias procedures, such as trees. For\\nregression, we simply ﬁt the same regression tree many times to bootstra p-\\nsampled versions of the training data, and average the result. For classiﬁ-\\ncation, a committee of trees each cast a vote for the predicted class.\\nBoosting in Chapter 10 was initially proposed as a committee method as\\nwell, although unlike bagging, the committee of weak learners evolves over\\ntime, and the members cast a weighted vote. Boosting appears to dominate\\nbagging on most problems, and became the preferred choice.\\nRandom forests (Breiman, 2001) is a substantial modiﬁcation of bagging\\nthat builds a large collection of de-correlated trees, and then averages them.\\nOn many problems the performance of random forests is very similar to\\nboosting, and they are simpler to train and tune. As a consequence, random\\nforests are popular, and are implemented in a variety of packages.\\n15.2 Deﬁnition of Random Forests\\nThe essential idea in bagging (Section 8.7) is to average many noisy but\\napproximately unbiased models, and hence reduce the variance. Trees are\\nideal candidates for bagging, since they can capture complex interaction'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 606}, page_content='588 15. Random Forests\\nAlgorithm 15.1 Random Forest for Regression or Classiﬁcation.\\n1. For b= 1 to B:\\n(a) Draw a bootstrap sample Z∗of size Nfrom the training data.\\n(b) Grow a random-forest tree Tbto the bootstrapped data, by re-\\ncursively repeating the following steps for each terminal node of\\nthe tree, until the minimum node size nminis reached.\\ni. Select mvariables at random from the pvariables.\\nii. Pick the best variable/split-point among the m.\\niii. Split the node into two daughter nodes.\\n2. Output the ensemble of trees {Tb}B\\n1.\\nTo make a prediction at a new point x:\\nRegression: ˆfB\\nrf(x) =1\\nB∑B\\nb=1Tb(x).\\nClassiﬁcation: LetˆCb(x) be the class prediction of the bth random-forest\\ntree. Then ˆCB\\nrf(x) =majority vote {ˆCb(x)}B\\n1.\\nstructures in the data, and if grown suﬃciently deep, have relatively low\\nbias. Since trees are notoriously noisy, they beneﬁt greatly from the averag-\\ning. Moreover, since each tree generated in bagging is identically distributed\\n(i.d.), the expectation of an average of Bsuch trees is the same as the ex-\\npectation of any one of them. This means the bias of bagged trees is the\\nsame as that of the individual trees, and the only hope of improvement is\\nthrough variance reduction. This is in contrast to boosting, where the trees\\nare grown in an adaptive way to remove bias, and hence are not i.d.\\nAn average of Bi.i.d. random variables, each with variance σ2, has vari-\\nance1\\nBσ2. If the variables are simply i.d. (identically distributed, but not\\nnecessarily independent) with positive pairwise correlation ρ, the variance\\nof the average is (Exercise 15.1)\\nρσ2+1−ρ\\nBσ2. (15.1)\\nAsBincreases, the second term disappears, but the ﬁrst remains, and\\nhence the size of the correlation of pairs of bagged trees limits the beneﬁts\\nof averaging. The idea in random forests (Algorithm 15.1) is to improve\\nthe variance reduction of bagging by reducing the correlation between the\\ntrees, without increasing the variance too much. This is achieved in the\\ntree-growing process through random selection of the input variables.\\nSpeciﬁcally, when growing a tree on a bootstrapped dataset:\\nBefore each split, select m≤pof the input variables at random\\nas candidates for splitting.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 607}, page_content='15.2 Deﬁnition of Random Forests 589\\nTypically values for mare√por even as low as 1.\\nAfter Bsuch trees {T(x;Θb)}B\\n1are grown, the random forest (regression)\\npredictor is\\nˆfB\\nrf(x) =1\\nBB∑\\nb=1T(x;Θb). (15.2)\\nAs in Section 10.9 (page 356), Θ bcharacterizes the bth random forest tree in\\nterms of split variables, cutpoints at each node, and terminal-node values.\\nIntuitively, reducing mwill reduce the correlation between any pair of trees\\nin the ensemble, and hence by (15.1) reduce the variance of the average.\\n0 500 1000 1500 2000 25000.040 0.045 0.050 0.055 0.060 0.065 0.070Spam Data\\nNumber of TreesTest ErrorBagging\\nRandom Forest\\nGradient Boosting (5 Node)\\nFIGURE 15.1. Bagging, random forest, and gradient boosting, applied to the\\nspam data. For boosting, 5-node trees were used, and the number of trees were\\nchosen by 10-fold cross-validation ( 2500trees). Each “step” in the ﬁgure corre-\\nsponds to a change in a single misclassiﬁcation (in a test set of 1536).\\nNot all estimators can be improved by shaking up the data like this.\\nIt seems that highly nonlinear estimators, such as trees, beneﬁt the most.\\nFor bootstrapped trees, ρis typically small (0 .05 or lower is typical; see\\nFigure 15.9), while σ2is not much larger than the variance for the original\\ntree. On the other hand, bagging does not change linear estimates, such\\nas the sample mean (hence its variance either); the pairwise correlation\\nbetween bootstrapped means is about 50% (Exercise 15.4).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 608}, page_content='590 15. Random Forests\\nRandom forests are popular. Leo Breiman’s1collaborator Adele Cutler\\nmaintains a random forest website2where the software is freely available,\\nwith more than 3000 downloads reported by 2002. There is a randomForest\\npackage in R, maintained by Andy Liaw, available from the CRANwebsite.\\nThe authors make grand claims about the success of random forests:\\n“most accurate,” “most interpretable,” and the like. In our experience ran-\\ndom forests do remarkably well, with very little tuning required. A ran-\\ndom forest classiﬁer achieves 4 .88% misclassiﬁcation error on the spamtest\\ndata, which compares well with all other methods, and is not signiﬁcantly\\nworse than gradient boosting at 4 .5%. Bagging achieves 5 .4% which is\\nsigniﬁcantly worse than either (using the McNemar test outlined in Ex-\\nercise 10.6), so it appears on this example the additional randomization\\nhelps.\\nRF−1 RF−3 Bagging GBM−1 GBM−60.00 0.05 0.10 0.15Nested SpheresTest Misclassification Error\\nBayes Error\\nFIGURE 15.2. The results of 50simulations from the “nested spheres” model in\\nI R10. The Bayes decision boundary is the surface of a sphere (addit ive). “RF-3”\\nrefers to a random forest with m= 3, and “GBM-6” a gradient boosted model\\nwith interaction order six; similarly for “RF-1” and “GBM-1.” Th e training sets\\nwere of size 2000, and the test sets 10,000.\\nFigure 15.1 shows the test-error progression on 2500 trees for the three\\nmethods. In this case there is some evidence that gradient boosting has\\nstarted to overﬁt, although 10-fold cross-validation chose all 2500 tr ees.\\n1Sadly, Leo Breiman died in July, 2005.\\n2http://www.math.usu.edu/ ∼adele/forests/'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 609}, page_content='15.2 Deﬁnition of Random Forests 591\\n0 200 400 600 800 10000.32 0.34 0.36 0.38 0.40 0.42 0.44California Housing Data\\nNumber of TreesTest Average Absolute ErrorRF m=2\\nRF m=6\\nGBM depth=4\\nGBM depth=6\\nFIGURE 15.3. Random forests compared to gradient boosting on the California\\nhousing data. The curves represent mean absolute error on the t est data as a\\nfunction of the number of trees in the models. Two random forests are shown, with\\nm= 2andm= 6. The two gradient boosted models use a shrinkage parameter\\nν= 0.05in (10.41), and have interaction depths of 4and6. The boosted models\\noutperform random forests.\\nFigure 15.2 shows the results of a simulation3comparing random forests\\nto gradient boosting on the nested spheres problem [Equation (10.2) in\\nChapter 10]. Boosting easily outperforms random forests here. Notice that\\nsmaller mis better here, although part of the reason could be that the true\\ndecision boundary is additive.\\nFigure 15.3 compares random forests to boosting (with shrinkage) in a\\nregression problem, using the California housing data (Section 10.14.1).\\nTwo strong features that emerge are\\n•Random forests stabilize at about 200 trees, while at 1000 trees boost-\\ning continues to improve. Boosting is slowed down by the shrinkage,\\nas well as the fact that the trees are much smaller.\\n•Boosting outperforms random forests here. At 1000 terms, the weaker\\nboosting model (GBM depth 4) has a smaller error than the stronger\\n3Details: The random forests were ﬁt using the R package randomForest 4.5-11 ,\\nwith 500 trees. The gradient boosting models were ﬁt using R p ackagegbm 1.5 , with\\nshrinkage parameter set to 0.05, and 2000 trees.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 610}, page_content='592 15. Random Forests\\n0 500 1000 1500 2000 25000.045 0.055 0.065 0.075\\nNumber of TreesMisclassification ErrorOOB Error\\nTest Error\\nFIGURE 15.4. ooberror computed on the spamtraining data, compared to the\\ntest error computed on the test set.\\nrandom forest (RF m= 6); a Wilcoxon test on the mean diﬀerences\\nin absolute errors has a p-value of 0 .007. For larger mthe random\\nforests performed no better.\\n15.3 Details of Random Forests\\nWe have glossed over the distinction between random forests for classiﬁca-\\ntion versus regression. When used for classiﬁcation, a random forest obtains\\na class vote from each tree, and then classiﬁes using majority vote (see Sec-\\ntion 8.7 on bagging for a similar discussion). When used for regression, the\\npredictions from each tree at a target point xare simply averaged, as in\\n(15.2). In addition, the inventors make the following recommendations:\\n•For classiﬁcation, the default value for mis⌊√p⌋and the minimum\\nnode size is one.\\n•For regression, the default value for mis⌊p/3⌋and the minimum\\nnode size is ﬁve.\\nIn practice the best values for these parameters will depend on the problem,\\nand they should be treated as tuning parameters. In Figure 15.3 the m= 6\\nperforms much better than the default value ⌊8/3⌋= 2.\\n15.3.1 Out of Bag Samples\\nAn important feature of random forests is its use of out-of-bag (oob) sam-\\nples:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 611}, page_content='15.3 Details of Random Forests 593\\nFor each observation zi= (xi,yi), construct its random forest\\npredictor by averaging onlythose trees corresponding to boot-\\nstrap samples in which zidid not appear.\\nAnooberror estimate is almost identical to that obtained by N-fold cross-\\nvalidation; see Exercise 15.2. Hence unlike many other nonlinear estimators,\\nrandom forests can be ﬁt in one sequence, with cross-validation being per-\\nformed along the way. Once the ooberror stabilizes, the training can be\\nterminated.\\nFigure 15.4 shows the oobmisclassiﬁcation error for the spamdata, com-\\npared to the test error. Although 2500 trees are averaged here, it appears\\nfrom the plot that about 200 would be suﬃcient.\\n15.3.2 Variable Importance\\nVariable importance plots can be constructed for random forests in exactly\\nthe same way as they were for gradient-boosted models (Section 10.13).\\nAt each split in each tree, the improvement in the split-criterion is the\\nimportance measure attributed to the splitting variable, and is accumulated\\nover all the trees in the forest separately for each variable. The left plot\\nof Figure 15.5 shows the variable importances computed in this way for\\nthespamdata; compare with the corresponding Figure 10.6 on page 354 for\\ngradient boosting. Boosting ignores some variables completely, while the\\nrandom forest does not. The candidate split-variable selection increases\\nthe chance that any single variable gets included in a random forest, while\\nno such selection occurs with boosting.\\nRandom forests also use the oobsamples to construct a diﬀerent variable-\\nimportance measure, apparently to measure the prediction strength of each\\nvariable. When the bth tree is grown, the oobsamples are passed down\\nthe tree, and the prediction accuracy is recorded. Then the values for the\\njth variable are randomly permuted in the oobsamples, and the accuracy\\nis again computed. The decrease in accuracy as a result of this permuting\\nis averaged over all trees, and is used as a measure of the importance of\\nvariable jin the random forest. These are expressed as a percent of the\\nmaximum in the right plot in Figure 15.5. Although the rankings of the\\ntwo methods are similar, the importances in the right plot are more uni-\\nform over the variables. The randomization eﬀectively voids the eﬀect of\\na variable, much like setting a coeﬃcient to zero in a linear model (Exer-\\ncise 15.7). This does not measure the eﬀect on prediction were this variable\\nnot available, because if the model was reﬁtted without the variable, other\\nvariables could be used as surrogates.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 612}, page_content='594 15. Random Forests\\n!$removefreeCAPAVEyourCAPMAXhpCAPTOTmoneyouryougeorge000eduhplbusiness1999internet(willallemailrereceiveovermail;650meetinglabsorderaddresspmpeoplemake#creditfontdatatechnology85[labtelnetreportoriginalprojectconferencedirect415857addresses3dcspartstableGini\\n0 20 40 60 80 100\\nVariable Importance!remove$CAPAVEhpfreeCAPMAXedugeorgeCAPTOTyourour1999reyouhplbusiness000meetingmoney(willinternet650pmreceiveoveremail;fontmailtechnologyorderalllabs[85addressoriginallabtelnetpeopleprojectdatacreditconference857#415makecsreportdirectaddresses3dpartstableRandomization\\n0 20 40 60 80 100\\nVariable Importance\\nFIGURE 15.5. Variable importance plots for a classiﬁcation random forest\\ngrown on the spamdata. The left plot bases the importance on the Gini split-\\nting index, as in gradient boosting. The rankings compare well with t he rankings\\nproduced by gradient boosting (Figure 10.6 on page 354). The ri ght plot uses oob\\nrandomization to compute variable importances, and tends to spr ead the impor-\\ntances more uniformly.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 613}, page_content='15.3 Details of Random Forests 595\\nProximity Plot\\n12\\n34\\n5\\n6Random Forest Classifier\\n123456\\nDimension 1Dimension 2\\nX1X2\\nFIGURE 15.6. (Left): Proximity plot for a random forest classiﬁer grown to\\nthe mixture data. (Right): Decision boundary and training data fo r random forest\\non mixture data. Six points have been identiﬁed in each plot.\\n15.3.3 Proximity Plots\\nOne of the advertised outputs of a random forest is a proximity plot . Fig-\\nure 15.6 shows a proximity plot for the mixture data deﬁned in Section 2.3.3\\nin Chapter 2. In growing a random forest, an N×Nproximity matrix is\\naccumulated for the training data. For every tree, any pair of oobobser-\\nvations sharing a terminal node has their proximity increased by one. This\\nproximity matrix is then represented in two dimensions using multidimen-\\nsional scaling (Section 14.8). The idea is that even though the data may be\\nhigh-dimensional, involving mixed variables, etc., the proximity plot gives\\nan indication of which observations are eﬀectively close together in the eyes\\nof the random forest classiﬁer.\\nProximity plots for random forests often look very similar, irrespect ive of\\nthe data, which casts doubt on their utility. They tend to have a star shape,\\none arm per class, which is more pronounced the better the classiﬁcation\\nperformance.\\nSince the mixture data are two-dimensional, we can map points from the\\nproximity plot to the original coordinates, and get a better understanding of\\nwhat they represent. It seems that points in pure regions class-wise map to\\nthe extremities of the star, while points nearer the decision boundaries map\\nnearer the center. This is not surprising when we consider the construction\\nof the proximity matrices. Neighboring points in pure regions will often\\nend up sharing a bucket, since when a terminal node is pure, it is no longer'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 614}, page_content='596 15. Random Forests\\nsplit by a random forest tree-growing algorithm. On the other hand, pairs\\nof points that are close but belong to diﬀerent classes will sometimes share\\na terminal node, but not always.\\n15.3.4 Random Forests and Overﬁtting\\nWhen the number of variables is large, but the fraction of relevant variables\\nsmall, random forests are likely to perform poorly with small m. At each\\nsplit the chance can be small that the relevant variables will be selected.\\nFigure 15.7 shows the results of a simulation that supports this claim. De-\\ntails are given in the ﬁgure caption and Exercise 15.3. At the top of each\\npair we see the hyper-geometric probability that a relevant variable will be\\nselected at any split by a random forest tree (in this simulation, the relevant\\nvariables are all equal in stature). As this probability gets small, the ga p\\nbetween boosting and random forests increases. When the number of rele-\\nvant variables increases, the performance of random forests is surprisingly\\nrobust to an increase in the number of noise variables. For example, with 6\\nrelevant and 100 noise variables, the probability of a relevant variable bei ng\\nselected at any split is 0.46, assuming m=√\\n(6 + 100) ≈10. According to\\nFigure 15.7, this does not hurt the performance of random forests compared\\nwith boosting. This robustness is largely due to the relative insensitivity of\\nmisclassiﬁcation cost to the bias and variance of the probability estimates\\nin each tree. We consider random forests for regression in the next section.\\nAnother claim is that random forests “cannot overﬁt” the data. It is\\ncertainly true that increasing Bdoes not cause the random forest sequence\\nto overﬁt; like bagging, the random forest estimate (15.2) approximates t he\\nexpectation\\nˆfrf(x) = E ΘT(x;Θ) = lim\\nB→∞ˆf(x)B\\nrf (15.3)\\nwith an average over Brealizations of Θ. The distribution of Θ here is con-\\nditional on the training data. However, this limit can overﬁt the data ; the\\naverage of fully grown trees can result in too rich a model, and incur unnec-\\nessary variance. Segal (2004) demonstrates small gains in performance by\\ncontrolling the depths of the individual trees grown in random forests. Our\\nexperience is that using full-grown trees seldom costs much, and results in\\none less tuning parameter.\\nFigure 15.8 shows the modest eﬀect of depth control in a simple regression\\nexample. Classiﬁers are less sensitive to variance, and this eﬀect of over-\\nﬁtting is seldom seen with random-forest classiﬁcation.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 615}, page_content='15.4 Analysis of Random Forests 597Test Misclassification Error\\n0.10 0.15 0.20 0.25 0.30Bayes Error\\n(2, 5) (2, 25) (2, 50) (2, 100) (2, 150)\\nNumber of (Relevant, Noise) Variables0.52 0.34 0.25 0.19 0.15\\nRandom Forest\\nGradient Boosting\\nFIGURE 15.7. A comparison of random forests and gradient boosting on prob-\\nlems with increasing numbers of noise variables. In each case the true decision\\nboundary depends on two variables, and an increasing number of noise variables\\nare included. Random forests uses its default value m=√p. At the top of each\\npair is the probability that one of the relevant variables is ch osen at any split.\\nThe results are based on 50simulations for each pair, with a training sample of\\n300, and a test sample of 500.\\n15.4 Analysis of Random Forests\\nIn this section we analyze the mechanisms at play with the additional\\nrandomization employed by random forests. For this discussion we focus\\non regression and squared error loss, since this gets at the main points,\\nand bias and variance are more complex with 0–1 loss (see Section 7.3.1).\\nFurthermore, even in the case of a classiﬁcation problem, we can consider\\nthe random-forest average as an estimate of the class posterior probabilit ies,\\nfor which bias and variance are appropriate descriptors.\\n15.4.1 Variance and the De-Correlation Eﬀect\\nThe limiting form ( B→ ∞) of the random forest regression estimator is\\nˆfrf(x) = E Θ|ZT(x;Θ(Z)), (15.4)\\nwhere we have made explicit the dependence on the training data Z. Here\\nwe consider estimation at a single target point x. From (15.1) we see that'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 616}, page_content='598 15. Random Forests\\n50 30 20 10 51.00 1.05 1.10\\nMinimum Node SizeMean Squared Test ErrorShallow Deep\\nFIGURE 15.8. The eﬀect of tree size on the error in random forest regres-\\nsion. In this example, the true surface was additive in two of th e12variables,\\nplus additive unit-variance Gaussian noise. Tree depth is contr olled here by the\\nminimum node size; the smaller the minimum node size, the deeper t he trees.\\nVarˆfrf(x) =ρ(x)σ2(x). (15.5)\\nHere\\n•ρ(x) is the sampling correlation between any pair of trees used in the\\naveraging:\\nρ(x) = corr[ T(x;Θ1(Z)),T(x;Θ2(Z))], (15.6)\\nwhere Θ 1(Z) and Θ 2(Z) are a randomly drawn pair of random forest\\ntrees grown to the randomly sampled Z;\\n•σ2(x) is the sampling variance of any single randomly drawn tree,\\nσ2(x) = Var T(x;Θ(Z)). (15.7)\\nIt is easy to confuse ρ(x) with the average correlation between ﬁtted trees\\nin agiven random-forest ensemble; that is, think of the ﬁtted trees as N-\\nvectors, and compute the average pairwise correlation between these vec-\\ntors, conditioned on the data. This is notthe case; this conditional corre-\\nlation is not directly relevant in the averaging process, and the dependence\\nonxinρ(x) warns us of the distinction. Rather, ρ(x) is the theoretical\\ncorrelation between a pair of random-forest trees evaluated at x, induced\\nby repeatedly making training sample draws Zfrom the population, and\\nthen drawing a pair of random forest trees. In statistical jargon, this is the\\ncorrelation induced by the sampling distribution ofZand Θ.\\nMore precisely, the variability averaged over in the calculations in (15.6)\\nand (15.7) is both'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 617}, page_content='15.4 Analysis of Random Forests 599\\n•conditional on Z: due to the bootstrap sampling and feature sampling\\nat each split, and\\n•a result of the sampling variability of Zitself.\\nIn fact, the conditional covariance of a pair of tree ﬁts at xis zero, because\\nthe bootstrap and feature sampling is i.i.d; see Exercise 15.5.\\n1 4 7 13 19 25 31 37 43 490.00 0.02 0.04 0.06 0.08\\nNumber of Randomly Selected Splitting Variables mCorrelation between Trees\\nFIGURE 15.9. Correlations between pairs of trees drawn by a random-forest\\nregression algorithm, as a function of m. The boxplots represent the correlations\\nat600randomly chosen prediction points x.\\nThe following demonstrations are based on a simulation model\\nY=1√\\n5050∑\\nj=1Xj+ε, (15.8)\\nwith all the Xjandεiid Gaussian. We use 500 training sets of size 100, and\\na single set of test locations of size 600. Since regression trees are nonlinear\\ninZ, the patterns we see below will diﬀer somewhat depending on the\\nstructure of the model.\\nFigure 15.9 shows how the correlation (15.6) between pairs of trees de-\\ncreases as mdecreases: pairs of tree predictions at xfor diﬀerent training\\nsetsZare likely to be less similar if they do not use the same splitting\\nvariables.\\nIn the left panel of Figure 15.10 we consider the variances of single tree\\npredictors, Var T(x;Θ(Z)) (averaged over 600 prediction points xdrawn\\nrandomly from our simulation model). This is the total variance, and can be'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 618}, page_content='600 15. Random Forests\\ndecomposed into two parts using standard conditional variance arguments\\n(see Exercise 15.5):\\nVarΘ,ZT(x;Θ(Z)) = Var ZEΘ|ZT(x;Θ(Z)) + E ZVarΘ|ZT(x;Θ(Z))\\nTotal Variance = Var Zˆfrf(x) + within- ZVariance\\n(15.9)\\nThe second term is the within- Zvariance—a result of the randomization,\\nwhich increases as mdecreases. The ﬁrst term is in fact the sampling vari-\\nance of the random forest ensemble (shown in the right panel), which de-\\ncreases as mdecreases. The variance of the individual trees does not change\\nappreciably over much of the range of m, hence in light of (15.5), the vari-\\nance of the ensemble is dramatically lower than this tree variance.\\n0 10 20 30 40 501.80 1.85 1.90 1.95Single Tree\\nmVariance\\nWithin Z\\nTotal\\n0 10 20 30 40 500.65 0.70 0.75 0.80 0.85Random Forest Ensemble\\nmMean Squared Error and Squared Bias\\nVariance\\nMean Squared Error\\nSquared Bias\\nVariance\\n0.0 0.05 0.10 0.15 0.20\\nFIGURE 15.10. Simulation results. The left panel shows the average variance o f\\na single random forest tree, as a function of m. “Within Z” refers to the average\\nwithin-sample contribution to the variance, resulting from the bootstrap sampling\\nand split-variable sampling (15.9). “Total” includes the samp ling variability of\\nZ. The horizontal line is the average variance of a single fully gro wn tree (with-\\nout bootstrap sampling). The right panel shows the average mea n-squared error,\\nsquared bias and variance of the ensemble, as a function of m. Note that the\\nvariance axis is on the right (same scale, diﬀerent level). The h orizontal line is\\nthe average squared-bias of a fully grown tree.\\n15.4.2 Bias\\nAs in bagging, the bias of a random forest is the same as the bias of any\\nof the individual sampled trees T(x;Θ(Z)):'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 619}, page_content='15.4 Analysis of Random Forests 601\\nBias(x) = θ(x)−EZˆfrf(x)\\n=θ(x)−EZEΘ|ZT(x;Θ(Z)). (15.10)\\nThis is also typically greater (in absolute terms) than the bias of an un-\\npruned tree grown to Z, since the randomization and reduced sample space\\nimpose restrictions. Hence the improvements in prediction obtained by bag-\\nging or random forests are solely a result of variance reduction .\\nAny discussion of bias depends on the unknown true function. Fig-\\nure 15.10 (right panel) shows the squared bias for our additive model simu-\\nlation (estimated from the 500 realizations). Although for diﬀerent model s\\nthe shape and rate of the bias curves may diﬀer, the general trend is that\\nasmdecreases, the bias increases. Shown in the ﬁgure is the mean-squared\\nerror, and we see a classical bias-variance trade-oﬀ in the choice of m. For\\nallmthe squared bias of the random forest is greater than that for a single\\ntree (horizontal line).\\nThese patterns suggest a similarity with ridge regression (Section 3.4.1) .\\nRidge regression is useful (in linear models) when one has a large number\\nof variables with similarly sized coeﬃcients; ridge shrinks their coeﬃcients\\ntoward zero, and those of strongly correlated variables toward each other.\\nAlthough the size of the training sample might not permit all the variables\\nto be in the model, this regularization via ridge stabilizes the model and al-\\nlows all the variables to have their say (albeit diminished). Random forests\\nwith small mperform a similar averaging. Each of the relevant variables\\nget their turn to be the primary split, and the ensemble averaging reduces\\nthe contribution of any individual variable. Since this simulation exam-\\nple (15.8) is based on a linear model in all the variables, ridge regression\\nachieves a lower mean-squared error (about 0 .45 with df( λopt)≈29).\\n15.4.3 Adaptive Nearest Neighbors\\nThe random forest classiﬁer has much in common with the k-nearest neigh-\\nbor classiﬁer (Section 13.3); in fact a weighted version thereof. Since each\\ntree is grown to maximal size, for a particular Θ∗,T(x;Θ∗(Z)) is the re-\\nsponse value for one of the training samples4. The tree-growing algorithm\\nﬁnds an “optimal” path to that observation, choosing the most informative\\npredictors from those at its disposal. The averaging process assigns weight s\\nto these training responses, which ultimately vote for the prediction. Hence\\nvia the random-forest voting mechanism, those observations close to the\\ntarget point get assigned weights—an equivalent kernel—which combine to\\nform the classiﬁcation decision.\\nFigure 15.11 demonstrates the similarity between the decision boundary\\nof 3-nearest neighbors and random forests on the mixture data.\\n4We gloss over the fact that pure nodes are not split further, a nd hence there can be\\nmore than one observation in a terminal node'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 620}, page_content='602 15. Random Forests\\nRandom Forest Classifier\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\nooooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo oooo\\noo\\noo\\noooo\\no\\noo\\noo\\nooooo\\nooo\\no\\no\\nooo\\noooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.000\\nTest Error:       0.238\\nBayes Error:    0.2103−Nearest Neighbors\\no\\noooo\\nooo\\noo\\no\\noo\\noo\\nooo\\noo\\nooo\\noo\\noo\\no\\noo\\no\\noooo\\noo\\no\\noo\\noo\\nooo\\noo\\no\\nooo\\no\\noo\\noo\\no\\nooo\\noo\\no\\no\\noo\\no\\noo\\nooooo\\no\\noo\\no oo\\noo\\nooo\\noo\\noo\\noo\\noo\\noo\\nooo\\no\\nooooo\\noooo\\nooo\\noo\\nooo\\noo\\no\\noo\\no\\nooo\\noo ooo\\noo\\nooooo\\noo\\noo\\noo\\nooo\\nooooooo\\noo oooo\\noo\\noo\\noooo\\no\\noo\\noo\\nooooo\\nooo\\no\\no\\nooo\\noooooo\\noo\\no\\noo\\nooo\\no\\nTraining Error: 0.130\\nTest Error:       0.242\\nBayes Error:    0.210\\nFIGURE 15.11. Random forests versus 3-NN on the mixture data. The axis-ori-\\nented nature of the individual trees in a random forest lead to dec ision regions\\nwith an axis-oriented ﬂavor.\\nBibliographic Notes\\nRandom forests as described here were introduced by Breiman (2001), al-\\nthough many of the ideas had cropped up earlier in the literature in dif-\\nferent forms. Notably Ho (1995) introduced the term “random forest,” and\\nused a consensus of trees grown in random subspaces of the features. The\\nidea of using stochastic perturbation and averaging to avoid overﬁtting was\\nintroduced by Kleinberg (1990), and later in Kleinberg (1996). Amit and\\nGeman (1997) used randomized trees grown on image features for image\\nclassiﬁcation problems. Breiman (1996a) introduced bagging, a precursor\\nto his version of random forests. Dietterich (2000b) also proposed an im -\\nprovement on bagging using additional randomization. His approach was\\nto rank the top 20 candidate splits at each node, and then select from the\\nlist at random. He showed through simulations and real examples that this\\nadditional randomization improved over the performance of bagging. Fried-\\nman and Hall (2007) showed that sub-sampling (without replacement) is\\nan eﬀective alternative to bagging. They showed that growing and aver-\\naging trees on samples of size N/2 is approximately equivalent (in terms\\nbias/variance considerations) to bagging, while using smaller fractions of\\nNreduces the variance even further (through decorrelation).\\nThere are several free software implementations of random forests. In\\nthis chapter we used the randomForest package in R, maintained by Andy\\nLiaw, available from the CRANwebsite. This allows both split-variable se-\\nlection, as well as sub-sampling. Adele Cutler maintains a random forest\\nwebsitehttp://www.math.usu.edu/ ∼adele/forests/ where (as of Au-\\ngust 2008) the software written by Leo Breiman and Adele Cutler is freely'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 621}, page_content='Exercises 603\\navailable. Their code, and the name “random forests”, is exclusively li-\\ncensed to Salford Systems for commercial release. The Wekamachine learn-\\ning archive http://www.cs.waikato.ac.nz/ml/weka/ at Waikato Univer-\\nsity, New Zealand, oﬀers a free javaimplementation of random forests.\\nExercises\\nEx. 15.1 Derive the variance formula (15.1). This appears to fail if ρis\\nnegative; diagnose the problem in this case.\\nEx. 15.2 Show that as the number of bootstrap samples Bgets large, the\\nooberror estimate for a random forest approaches its N-fold CV error\\nestimate, and that in the limit, the identity is exact.\\nEx. 15.3 Consider the simulation model used in Figure 15.7 (Mease and\\nWyner, 2008). Binary observations are generated with probabilities\\nPr(Y= 1|X) =q+ (1−2q)≤1\\uf8ee\\n\\uf8f0J∑\\nj=1Xj> J/2\\uf8f9\\n\\uf8fb, (15.11)\\nwhere X∼U[0,1]p, 0≤q≤1\\n2, and J≤pis some predeﬁned (even)\\nnumber. Describe this probability surface, and give the Bayes error rate.\\nEx. 15.4 Suppose xi, i= 1,... ,N are iid ( θ,σ2). Let ¯ x∗\\n1and ¯x∗\\n2be two\\nbootstrap realizations of the sample mean. Show that the sampling cor-\\nrelation corr(¯ x∗\\n1,¯x∗\\n2) =n\\n2n−1≈50%. Along the way, derive var(¯ x∗\\n1) and\\nthe variance of the bagged mean ¯ xbag. Here ¯ xis alinear statistic; bagging\\nproduces no reduction in variance for linear statistics.\\nEx. 15.5 Show that the sampling correlation between a pair of random-\\nforest trees at a point xis given by\\nρ(x) =VarZ[EΘ|ZT(x;Θ(Z))]\\nVarZ[EΘ|ZT(x;Θ(Z))] + E ZVarΘ|Z[T(x,Θ(Z)]. (15.12)\\nThe term in the numerator is Var Z[ˆfrf(x)], and the second term in the\\ndenominator is the expected conditional variance due to the randomization\\nin random forests.\\nEx. 15.6 Fit a series of random-forest classiﬁers to the spamdata, to explore\\nthe sensitivity to the parameter m. Plot both the ooberror as well as the\\ntest error against a suitably chosen range of values for m.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 622}, page_content='604 15. Random Forests\\nEx. 15.7 Suppose we ﬁt a linear regression model to Nobservations with\\nresponse yiand predictors xi1,... ,x ip. Assume that all variables are stan-\\ndardized to have mean zero and standard deviation one. Let RSSbe the\\nmean-squared residual on the training data, and ˆβthe estimated coeﬃcient.\\nDenote by RSS∗\\njthe mean-squared residual on the training data using the\\nsame ˆβ, but with the Nvalues for the jth variable randomly permuted\\nbefore the predictions are calculated. Show that\\nEP[RSS∗\\nj−RSS] = 2ˆβ2\\nj, (15.13)\\nwhere E Pdenotes expectation with respect to the permutation distribution.\\nArgue that this is approximately true when the evaluations are done using\\nan independent test set.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 623}, page_content='This is page 605\\nPrinter: Opaque this\\n16\\nEnsemble Learning\\n16.1 Introduction\\nThe idea of ensemble learning is to build a prediction model by combining\\nthe strengths of a collection of simpler base models. We have already seen\\na number of examples that fall into this category.\\nBagging in Section 8.7 and random forests in Chapter 15 are ensemble\\nmethods for classiﬁcation, where a committee of trees each cast a vote for\\nthe predicted class. Boosting in Chapter 10 was initially proposed as a\\ncommittee method as well, although unlike random forests, the committee\\nofweak learners evolves over time, and the members cast a weighted vote.\\nStacking (Section 8.8) is a novel approach to combining the strengths of\\na number of ﬁtted models. In fact one could characterize any dictionary\\nmethod, such as regression splines, as an ensemble method, with the basis\\nfunctions serving the role of weak learners.\\nBayesian methods for nonparametric regression can also be viewed as\\nensemble methods: a large number of candidate models are averaged with\\nrespect to the posterior distribution of their parameter settings (e.g. (Neal\\nand Zhang, 2006)).\\nEnsemble learning can be broken down into two tasks: developing a pop-\\nulation of base learners from the training data, and then combining them\\nto form the composite predictor. In this chapter we discuss boosting tech-\\nnology that goes a step further; it builds an ensemble model by conducting\\na regularized and supervised search in a high-dimensional space of weak\\nlearners.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 624}, page_content='606 16. Ensemble Learning\\nAn early example of a learning ensemble is a method designed for multi-\\nclass classiﬁcation using error-correcting output codes (Dietterich and Bakiri,\\n1995, ECOC). Consider the 10-class digit classiﬁcation problem, and the\\ncoding matrix Cgiven in Table 16.1.\\nTABLE 16.1. Part of a 15-bit error-correcting coding matrix Cfor the 10-class\\ndigit classiﬁcation problem. Each column deﬁnes a two-class cl assiﬁcation prob-\\nlem.\\nDigit C1C2C3C4C5C6· · · C15\\n0 1 1 0 0 0 0 · · · 1\\n1 0 0 1 1 1 1 · · · 0\\n2 1 0 0 1 0 0 · · · 1\\n.....................· · ·...\\n8 1 1 0 1 0 1 · · · 1\\n9 0 1 1 1 0 0 · · · 0\\nNote that the ℓth column of the coding matrix Cℓdeﬁnes a two-class\\nvariable that merges all the original classes into two groups. The method\\nworks as follows:\\n1. Learn a separate classiﬁer for each of the L= 15 two class problems\\ndeﬁned by the columns of the coding matrix.\\n2. At a test point x, let ˆpℓ(x) be the predicted probability of a one for\\ntheℓth response.\\n3. Deﬁne δk(x) =∑L\\nℓ=1|Ckℓ−ˆpℓ(x)|, the discriminant function for the\\nkth class, where Ckℓis the entry for row kand column ℓin Table 16.1.\\nEach row of Cis a binary code for representing that class. The rows have\\nmore bits than is necessary, and the idea is that the redundant “error-\\ncorrecting” bits allow for some inaccuracies, and can improve performance.\\nIn fact, the full code matrix Cabove has a minimum Hamming distance1\\nof 7 between any pair of rows. Note that even the indicator response coding\\n(Section 4.2) is redundant, since 10 classes require only ⌈log210 = 4 bits for\\ntheir unique representation. Dietterich and Bakiri (1995) showed impressive\\nimprovements in performance for a variety of multiclass problems when\\nclassiﬁcation trees were used as the base classiﬁer.\\nJames and Hastie (1998) analyzed the ECOC approach, and showed\\nthat random code assignment worked as well as the optimally constructed\\nerror-correcting codes. They also argued that the main beneﬁt of the coding\\nwas in variance reduction (as in bagging and random forests), because the\\ndiﬀerent coded problems resulted in diﬀerent trees, and the decoding step\\n(3) above has a similar eﬀect as averaging.\\n1The Hamming distance between two vectors is the number of mis matches between\\ncorresponding entries.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 625}, page_content='16.2 Boosting and Regularization Paths 607\\n16.2 Boosting and Regularization Paths\\nIn Section 10.12.2 of the ﬁrst edition of this book, we suggested an analogy\\nbetween the sequence of models produced by a gradient boosting algorithm\\nand regularized model ﬁtting in high-dimensional feature spaces. This was\\nprimarily motivated by observing the close connection between a boosted\\nversion of linear regression and the lasso (Section 3.4.2). These connec-\\ntions have been pursued by us and others, and here we present our current\\nthinking in this area. We start with the original motivation, which ﬁts m ore\\nnaturally in this chapter on ensemble learning.\\n16.2.1 Penalized Regression\\nIntuition for the success of the shrinkage strategy (10.41) of gradient bo ost-\\ning (page 364 in Chapter 10) can be obtained by drawing analogies with\\npenalized linear regression with a large basis expansion. Consider the dic-\\ntionary of all possible J-terminal node regression trees T={Tk}that could\\nbe realized on the training data as basis functions in IRp. The linear model\\nis\\nf(x) =K∑\\nk=1αkTk(x), (16.1)\\nwhere K= card( T). Suppose the coeﬃcients are to be estimated by least\\nsquares. Since the number of such trees is likely to be much larger than\\neven the largest training data sets, some form of regularization is required.\\nLet ˆα(λ) solve\\nmin\\nα\\uf8f1\\n\\uf8f2\\n\\uf8f3N∑\\ni=1(\\nyi−K∑\\nk=1αkTk(xi))2\\n+λ≤J(α)\\uf8fc\\n\\uf8fd\\n\\uf8fe, (16.2)\\nJ(α) is a function of the coeﬃcients that generally penalizes larger values.\\nExamples are\\nJ(α) =K∑\\nk=1|αk|2ridge regression , (16.3)\\nJ(α) =K∑\\nk=1|αk|lasso, (16.4)\\n(16.5)\\nboth covered in Section 3.4. As discussed there, the solution to the lasso\\nproblem with moderate to large λtends to be sparse; many of the ˆ αk(λ) =\\n0. That is, only a small fraction of all possible trees enter the model (16. 1).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 626}, page_content='608 16. Ensemble Learning\\nAlgorithm 16.1 Forward Stagewise Linear Regression.\\n1. Initialize ˇ αk= 0, k= 1,... ,K . Setε >0 to some small constant,\\nandMlarge.\\n2. For m= 1 to M:\\n(a) (β∗,k∗) = arg min β,k∑N\\ni=1(\\nyi−∑K\\nl=1ˇαlTl(xi)−βTk(xi))2\\n.\\n(b) ˇαk∗←ˇαk∗+ε≤sign(β∗).\\n3. Output fM(x) =∑K\\nk=1ˇαkTk(x).\\nThis seems reasonable since it is likely that only a small fraction of all po s-\\nsible trees will be relevant in approximating any particular target function.\\nHowever, the relevant subset will be diﬀerent for diﬀerent targets. Those\\ncoeﬃcients that are not set to zero are shrunk by the lasso in that their\\nabsolute values are smaller than their corresponding least squares values2:\\n|ˆαk(λ)|<|ˆαk(0)|. Asλincreases, the coeﬃcients all shrink, each one\\nultimately becoming zero.\\nOwing to the very large number of basis functions Tk, directly solving\\n(16.2) with the lasso penalty (16.4) is not possible. However, a feasibl e\\nforward stagewise strategy exists that closely approximates the eﬀect of\\nthe lasso, and is very similar to boosting and the forward stagewise A lgo-\\nrithm 10.2. Algorithm 16.1 gives the details. Although phrased in terms\\nof tree basis functions Tk, the algorithm can be used with any set of ba-\\nsis functions. Initially all coeﬃcients are zero in line 1; this corresponds\\ntoλ=∞in (16.2). At each successive step, the tree Tk∗is selected that\\nbest ﬁts the current residuals in line 2(a). Its corresponding coeﬃcient ˇ αk∗\\nis then incremented or decremented by an inﬁnitesimal amount in 2(b),\\nwhile all other coeﬃcients ˇ αk, k̸=k∗are left unchanged. In principle, this\\nprocess could be iterated until either all the residuals are zero, or β∗= 0.\\nThe latter case can occur if K < N , and at that point the coeﬃcient values\\nrepresent a least squares solution. This corresponds to λ= 0 in (16.2).\\nAfter applying Algorithm 16.1 with M <∞iterations, many of the coef-\\nﬁcients will be zero, namely, those that have yet to be incremented. The oth-\\ners will tend to have absolute values smaller than their corresponding least\\nsquares solution values, |ˇαk(M)|<|ˆαk(0)|. Therefore this M-iteration\\nsolution qualitatively resembles the lasso, with Minversely related to λ.\\nFigure 16.1 shows an example, using the prostate data studied in Chap-\\nter 3. Here, instead of using trees Tk(X) as basis functions, we use the origi-\\n2IfK > N , there is in general no unique “least squares value,” since i nﬁnitely many\\nsolutions will exist that ﬁt the data perfectly. We can pick t he minimum L1-norm solution\\namongst these, which is the unique lasso solution.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 627}, page_content='16.2 Boosting and Regularization Paths 609−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0.0 0.5 1.0 1.5 2.0\\n−0.2 0.0 0.2 0.4 0.6lcavol\\nlweight\\nagelbphsvi\\nlcpgleasonpgg45\\n0 50 100 150 200\\nt=P\\nk|αk|\\nCoeﬃcientsCoeﬃcientsLasso Forward Stagewise\\nIteration\\nFIGURE 16.1. Proﬁles of estimated coeﬃcients from linear regression, for the\\nprostate data studied in Chapter 3. The left panel shows the re sults from the lasso,\\nfor diﬀerent values of the bound parameter t=P\\nk|αk|. The right panel shows\\nthe results of the stagewise linear regression Algorithm 16. 1, using M= 220\\nconsecutive steps of size ε=.01.\\nnal variables Xkthemselves; that is, a multiple linear regression model. The\\nleft panel displays the proﬁles of estimated coeﬃcients from the lasso, for\\ndiﬀerent values of the bound parameter t=∑\\nk|αk|. The right panel shows\\nthe results of the stagewise Algorithm 16.1, with M= 250 and ε= 0.01.\\n[The left and right panels of Figure 16.1 are the same as Figure 3.10 and\\nthe left panel of Figure 3.19, respectively.] The similarity between the two\\ngraphs is striking.\\nIn some situations the resemblance is more than qualitative. For example,\\nif all of the basis functions Tkare mutually uncorrelated, then as ε↓0,M↑\\nsuch that Mǫ→t, Algorithm 16.1 yields exactly the same solution as the\\nlasso for bound parameter t=∑\\nk|αk|(and likewise for all solutions along\\nthe path). Of course, tree-based regressors are not uncorrelated. However,\\nthe solution sets are also identical if the coeﬃcients ˆ αk(λ) are all monotone\\nfunctions of λ. This is often the case when the correlation between the\\nvariables is low. When the ˆ αk(λ) are not monotone in λ, then the solution\\nsets are not identical. The solution sets for Algorithm 16.1 tend to change\\nless rapidly with changing values of the regularization parameter than those\\nof the lasso.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 628}, page_content='610 16. Ensemble Learning\\nEfron et al. (2004) make the connections more precise, by characterizing\\nthe exact solution paths in the ε-limiting case. They show that the coeﬃ-\\ncient paths are piece-wise linear functions, both for the lasso and forward\\nstagewise. This facilitates eﬃcient algorithms which allow the entire pat hs\\nto be computed with the same cost as a single least-squares ﬁt. This least\\nangle regression algorithm is described in more detail in Section 3.8.1.\\nHastie et al. (2007) show that this inﬁnitesimal forward stagewise alg o-\\nrithm (FS 0) ﬁts a monotone version of the lasso, which optimally reduces\\nat each step the loss function for a given increase in the arc length of the\\ncoeﬃcient path (see Sections 16.2.3 and 3.8.1). The arc-length for the ǫ >0\\ncase is Mǫ, and hence proportional to the number of steps.\\nTree boosting (Algorithm 10.3) with shrinkage (10.41) closely resembl es\\nAlgorithm 16.1, with the learning rate parameter νcorresponding to ε. For\\nsquared error loss, the only diﬀerence is that the optimal tree to be selected\\nat each iteration Tk∗is approximated by the standard top-down greedy\\ntree-induction algorithm. For other loss functions, such as the exponential\\nloss of AdaBoost and the binomial deviance, Rosset et al. (2004a) show\\nsimilar results to what we see here. Thus, one can view tree boosting with\\nshrinkage as a form of monotone ill-posed regression on all possible ( J-\\nterminal node) trees, with the lasso penalty (16.4) as a regularizer. We\\nreturn to this topic in Section 16.2.3.\\nThe choice of no shrinkage [ ν= 1 in equation (10.41)] is analogous to\\nforward-stepwise regression, and its more aggressive cousin best-subset se-\\nlection, which penalizes the number of non zero coeﬃcients J(α) =∑\\nk|αk|0.\\nWith a small fraction of dominant variables, best subset approaches often\\nwork well. But with a moderate fraction of strong variables, it is well k nown\\nthat subset selection can be excessively greedy (Copas, 1983), often yielding\\npoor results when compared to less aggressive strategies such as the lasso\\nor ridge regression. The dramatic improvements often seen when shrinkage\\nis used with boosting are yet another conﬁrmation of this approach.\\n16.2.2 The “Bet on Sparsity” Principle\\nAs shown in the previous section, boosting’s forward stagewise strategy\\nwith shrinkage approximately minimizes the same loss function with a\\nlasso-style L1penalty. The model is built up slowly, searching through\\n“model space” and adding shrunken basis functions derived from impor-\\ntant predictors. In contrast, the L2penalty is computationally much easier\\nto deal with, as shown in Section 12.3.7. With the basis functions and L2\\npenalty chosen to match a particular positive-deﬁnite kernel, one can solve\\nthe corresponding optimization problem without explicitly searching over\\nindividual basis functions.\\nHowever, the sometimes superior performance of boosting over proce-\\ndures such as the support vector machine may be largely due to the im-\\nplicit use of the L1versus L2penalty. The shrinkage resulting from the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 629}, page_content='16.2 Boosting and Regularization Paths 611\\nL1penalty is better suited to sparse situations, where there are few basis\\nfunctions with nonzero coeﬃcients (among all possible choices).\\nWe can strengthen this argument through a simple example, taken from\\nFriedman et al. (2004). Suppose we have 10 ,000 data points and our model\\nis a linear combination of a million trees. If the true population coeﬃcients\\nof these trees arose from a Gaussian distribution, then we know that in a\\nBayesian sense the best predictor is ridge regression (Exercise 3.6). That is ,\\nwe should use an L2rather than an L1penalty when ﬁtting the coeﬃcients.\\nOn the other hand, if there are only a small number (e.g., 1000) coeﬃcients\\nthat are nonzero, the lasso ( L1penalty) will work better. We think of this\\nas asparse scenario, while the ﬁrst case (Gaussian coeﬃcients) is dense.\\nNote however that in the dense scenario, although the L2penalty is best,\\nneither method does very well since there is too little data from which to\\nestimate such a large number of nonzero coeﬃcients. This is the curse of\\ndimensionality taking its toll. In a sparse setting, we can potentially do\\nwell with the L1penalty, since the number of nonzero coeﬃcients is small.\\nTheL2penalty fails again.\\nIn other words, use of the L1penalty follows what we call the “bet on\\nsparsity” principle for high-dimensional problems:\\nUse a procedure that does well in sparse problems, since no pr o-\\ncedure does well in dense problems.\\nThese comments need some qualiﬁcation:\\n•For any given application, the degree of sparseness/denseness depends\\non the unknown true target function, and the chosen dictionary T.\\n•The notion of sparse versus dense is relative to the size of the train-\\ning data set and/or the noise-to-signal ratio (NSR). Larger training\\nsets allow us to estimate coeﬃcients with smaller standard errors.\\nLikewise in situations with small NSR, we can identify more nonzero\\ncoeﬃcients with a given sample size than in situations where the NSR\\nis larger.\\n•The size of the dictionary plays a role as well. Increasing the size of the\\ndictionary may lead to a sparser representation for our function, but\\nthe search problem becomes more diﬃcult leading to higher variance.\\nFigure 16.2 illustrates these points in the context of linear models us-\\ning simulation. We compare ridge regression and lasso, both for classiﬁ-\\ncation and regression problems. Each run has 50 observations with 300\\nindependent Gaussian predictors. In the top row all 300 coeﬃcients are\\nnonzero, generated from a Gaussian distribution. In the middle row, only\\n10 are nonzero and generated from a Gaussian, and the last row has 30\\nnon zero Gaussian coeﬃcients. For regression, standard Gaussian noise is'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 630}, page_content='612 16. Ensemble Learning\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Gaussian0.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Gaussian\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 100.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 10\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 300.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 30Percentage Squared Prediction Error Explained\\nNoise−to−Signal RatioRegression\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Gaussian0.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Gaussian\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 100.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 10\\n0.1 0.2 0.3 0.4 0.50.0 0.2 0.4 0.6 0.8 1.0Lasso/Subset 300.0 0.2 0.4 0.6 0.8 1.0\\n0.1 0.2 0.3 0.4 0.5Ridge/Subset 30Percentage Misclassification Error Explained\\nNoise−to−Signal RatioClassification\\nFIGURE 16.2. Simulations that show the superiority of the L1(lasso) penalty\\noverL2(ridge) in regression and classiﬁcation. Each run has 50observations\\nwith300independent Gaussian predictors. In the top row all 300coeﬃcients are\\nnonzero, generated from a Gaussian distribution. In the middle r ow, only 10are\\nnonzero, and the last row has 30nonzero. Gaussian errors are added to the linear\\npredictor η(X)for the regression problems, and binary responses generated via the\\ninverse-logit transform for the classiﬁcation problems. Scali ng of η(X)resulted in\\nthe noise-to-signal ratios shown. Lasso is used in the left sub- columns, ridge in the\\nright. We report the optimal percentage of error explained on te st data (relative\\nto the error of a constant model), displayed as boxplots over 20realizations for\\neach combination. In the only situation where ridge beats lasso (top row), neither\\ndo well.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 631}, page_content='16.2 Boosting and Regularization Paths 613\\nadded to the linear predictor η(X) =XTβto produce a continuous re-\\nsponse. For classiﬁcation the linear predictor is transformed via the inverse-\\nlogit to a probability, and a binary response is generated. Five diﬀer-\\nent noise-to-signal ratios are presented, obtained by scaling η(X) prior\\nto generating the response. In both cases this is deﬁned to be NSR =\\nVar(Y|η(X))/Var(η(X)). Both the ridge regression and lasso coeﬃcient\\npaths were ﬁt using a series of 50 values of λcorresponding to a range of\\ndffrom 1 to 50 (see Chapter 3 for details). The models were evaluated on\\na large test set (inﬁnite for Gaussian, 5000 for binary), and in each case the\\nvalue for λwas chosen to minimize the test-set error. We report percentage\\nvariance explained for the regression problems, and percentage misclassiﬁ-\\ncation error explained for the classiﬁcation problems (relative to a baseline\\nerror of 0 .5). There are 20 simulation runs for each scenario.\\nNote that for the classiﬁcation problems, we are using squared-error loss\\nto ﬁt the binary response. Note also that we do not using the training\\ndata to select λ, but rather are reporting the best possible behavior for\\neach method in the diﬀerent scenarios. The L2penalty performs poorly\\neverywhere. The Lasso performs reasonably well in the only two situations\\nwhere it can (sparse coeﬃcients). As expected the performance gets worse\\nas the NSR increases (less so for classiﬁcation), and as the model becomes\\ndenser. The diﬀerences are less marked for classiﬁcation than for regression.\\nThese empirical results are supported by a large body of theoretical\\nresults (Donoho and Johnstone, 1994; Donoho and Elad, 2003; Donoho,\\n2006b; Candes and Tao, 2007) that support the superiority of L1estimation\\nin sparse settings.\\n16.2.3 Regularization Paths, Over-ﬁtting and Margins\\nIt has often been observed that boosting “does not overﬁt,” or more as-\\ntutely is “slow to overﬁt.” Part of the explanation for this phenomenon was\\nmade earlier for random forests — misclassiﬁcation error is less sensitive to\\nvariance than is mean-squared error, and classiﬁcation is the major focus\\nin the boosting community. In this section we show that the regulariza-\\ntion paths of boosted models are “well behaved,” and that for certain loss\\nfunctions they have an appealing limiting form.\\nFigure 16.3 shows the coeﬃcient paths for lasso and inﬁnitesimal forward\\nstagewise (FS 0) in a simulated regression setting. The data consists of a\\ndictionary of 1000 Gaussian variables, strongly correlated ( ρ= 0.95) within\\nblocks of 20, but uncorrelated between blocks. The generating model has\\nnonzero coeﬃcients for 50 variables, one drawn from each block, and the\\ncoeﬃcient values are drawn from a standard Gaussian. Finally, Gaussian\\nnoise is added, with a noise-to-signal ratio of 0 .72 (Exercise 16.1.) The\\nFS0algorithm is a limiting form of algorithm 16.1, where the step size ε\\nis shrunk to zero (Section 3.8.1). The grouping of the variables is intended\\nto mimic the correlations of nearby trees, and with the forward-stagewise'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 632}, page_content='614 16. Ensemble Learning\\n0.0 0.2 0.4 0.6 0.8 1.0−20 −10 0 10 20 30Standardized CoefficientsLASSO\\n0.0 0.2 0.4 0.6 0.8 1.0−20 −10 0 10 20 30Standardized CoefficientsForward Stagewise\\n|α(m)|/|α(∞)| |α(m)|/|α(∞)|\\nFIGURE 16.3. Comparison of lasso and inﬁnitesimal forward stagewise paths\\non simulated regression data. The number of samples is 60and the number of\\nvariables is 1000. The forward-stagewise paths ﬂuctuate less than those of la sso\\nin the ﬁnal stages of the algorithms.\\nalgorithm, this setup is intended as an idealized version of gradient boosting\\nwith shrinkage. For both these algorithms, the coeﬃcient paths can be\\ncomputed exactly, since they are piecewise linear (see the LARS algorithm\\nin Section 3.8.1).\\nHere the coeﬃcient proﬁles are similar only in the early stages of the\\npaths. For the later stages, the forward stagewise paths tend to be mono-\\ntone and smoother, while those for the lasso ﬂuctuate widely. This is due\\nto the strong correlations among subsets of the variables —lasso suﬀers\\nsomewhat from the multi-collinearity problem (Exercise 3.28).\\nThe performance of the two models is rather similar (Figure 16.4), and\\nthey achieve about the same minimum. In the later stages forward stagewise\\ntakes longer to overﬁt, a likely consequence of the smoother paths.\\nHastie et al. (2007) show that FS 0solves a monotone version of the lasso\\nproblem for squared error loss. Let Ta=T ∪ {−T } be the augmented\\ndictionary obtained by including a negative copy of every basis element\\ninT. We consider models f(x) =∑\\nTk∈TaαkTk(x) with non-negative co-\\neﬃcients αk≥0. In this expanded space, the lasso coeﬃcient paths are\\npositive, while those of FS 0are monotone nondecreasing.\\nThe monotone lasso path is characterized by a diﬀerential equation\\n∂α\\n∂ℓ=ρml(α(ℓ)), (16.6)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 633}, page_content='16.2 Boosting and Regularization Paths 615\\no\\noo\\no\\no\\noo\\noo\\noo\\noooo\\nooo\\nooooooooooooooooooooooooooooooo ooooooo ooooooooooooooooooooooooooooooooooooooooo ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo oo ooo\\noo\\no\\no o\\noo\\nooo\\nooo oo\\no ooooo\\noooooooo ooooo ooo ooooooooooo oooooo o ooo oooo oo ooooo oo oo ooooooo oo oo ooooo oo oo oo oo ooo oo oo oo ooooooo oo ooo oooooo oo ooo oo oooooooo oooo ooo ooooooo ooo ooo ooooo oooo ooo oooooo ooooooo oooooooooooooo ooooo ooooooooooo ooooooooo oo oooo oooooo oo ooooooooooo ooo oo ooo oo ooo oooooooooo ooooo ooooooo ooooooooo oooo oooooo oooo ooo oooooo oooo oo oo oooo ooooo ooooo ooooooo ooooo o o oooooooooooooooooo oo ooo oo oo oo oooo o oo o o o o o oo o o oooo oo o oo o o oo oo o ooo o o o ooo o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o o\\n0 10 20 30 40 50 60 7025 30 35 40 45 50 55Lasso\\nForward Stagewise\\n|α(m)|Mean Squared Error\\nFIGURE 16.4. Mean squared error for lasso and inﬁnitesimal forward stagewise\\non the simulated data. Despite the diﬀerence in the coeﬃcient p aths, the two\\nmodels perform similarly over the critical part of the regul arization path. In the\\nright tail, lasso appears to overﬁt more rapidly.\\nwith initial condition α(0) = 0, where ℓis the L1arc-length of the path\\nα(ℓ) (Exercise 16.2). The monotone lasso move direction (velocity vector)\\nρml(α(ℓ)) decreases the loss at the optimal quadratic rate per unit increase\\nin the L1arc-length of the path. Since ρml\\nk(α(ℓ))≥0∀k,ℓ, the solution\\npaths are monotone.\\nThe lasso can similarly be characterized as the solution to a diﬀerential\\nequation as in (16.6), except that the move directions decrease the loss\\noptimally per unit increase in the L1norm of the path. As a consequence,\\nthey are not necessarily positive, and hence the lasso paths need not be\\nmonotone.\\nIn this augmented dictionary, restricting the coeﬃcients to be positive is\\nnatural, since it avoids an obvious ambiguity. It also ties in more natura lly\\nwith tree boosting—we always ﬁnd trees positively correlated with the\\ncurrent residual.\\nThere have been suggestions that boosting performs well (for two-class\\nclassiﬁcation) because it exhibits maximal-margin properties, much like the\\nsupport-vector machines of Chapters 4.5.2 and 12. Schapire et al. (1998)\\ndeﬁne the normalized L1margin of a ﬁtted model f(x) =∑\\nkαkTk(x) as\\nm(f) = min\\niyif(xi)∑K\\nk=1|αk|. (16.7)\\nHere the minimum is taken over the training sample, and yi∈ {−1,+1}.\\nUnlike the L2margin (4.40) of support vector machines, the L1margin\\nm(f) measures the distance to the closest training point in L∞units (max-\\nimum coordinate distance).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 634}, page_content='616 16. Ensemble Learning\\n−0.3 −0.2 −0.1 0.0 0.1\\nNumber of TreesMargin\\n0 2K 4K 6K 8K 10K\\n0.25 0.26 0.27 0.28\\nNumber of TreesTest Error\\n0 2K 4K 6K 8K 10K\\nFIGURE 16.5. The left panel shows the L1margin m(f)for the Adaboost clas-\\nsiﬁer on the mixture data, as a function of the number of 4-node trees. The model\\nwas ﬁt using the R package gbm, with a shrinkage factor of 0.02. After 10,000\\ntrees, m(f)has settled down. Note that when the margin crosses zero, the t raining\\nerror becomes zero. The right panel shows the test error, whic h is minimized at\\n240trees. In this case, Adaboost overﬁts dramatically if run to c onvergence.\\nSchapire et al. (1998) prove that with separable data, Adaboost in-\\ncreases m(f) with each iteration, converging to a margin-symmetric so-\\nlution. R¨ atsch and Warmuth (2002) prove the asymptotic convergence of\\nAdaboost with shrinkage to a L1-margin-maximizing solution. Rosset et\\nal. (2004a) consider regularized models of the form (16.2) for general loss\\nfunctions. They show that as λ↓0, for particular loss functions the solution\\nconverges to a margin-maximizing conﬁguration. In particular they show\\nthis to be the case for the exponential loss of Adaboost, as well as binomia l\\ndeviance.\\nCollecting together the results of this section, we reach the following\\nsummary for boosted classiﬁers:\\nThe sequence of boosted classiﬁers form an L1-regularized mono-\\ntone path to a margin-maximizing solution.\\nOf course the margin-maximizing end of the path can be a very poor, overﬁt\\nsolution, as it is in the example in Figure 16.5. Early stopping amounts\\nto picking a point along the path, and should be done with the aid of a\\nvalidation dataset.\\n16.3 Learning Ensembles\\nThe insights learned from the previous sections can be harnessed to produce\\na more eﬀective and eﬃcient ensemble model. Again we consider functions'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 635}, page_content='16.3 Learning Ensembles 617\\nof the form\\nf(x) =α0+∑\\nTk∈TαkTk(x), (16.8)\\nwhere Tis a dictionary of basis functions, typically trees. For gradient\\nboosting and random forests, |T |is very large, and it is quite typical for the\\nﬁnal model to involve many thousands of trees. In the previous section we\\nargue that gradient boosting with shrinkage ﬁts an L1regularized monotone\\npath in this space of trees.\\nFriedman and Popescu (2003) propose a hybrid approach which breaks\\nthis process down into two stages:\\n•A ﬁnite dictionary TL={T1(x),T2(x),... ,T M(x)}of basis functions\\nis induced from the training data;\\n•A family of functions fλ(x) is built by ﬁtting a lasso path in this\\ndictionary:\\nα(λ) = arg min\\nαN∑\\ni=1L[yi,α0+M∑\\nm=1αmTm(xi)] +λM∑\\nm=1|αm|.(16.9)\\nIn its simplest form this model could be seen as a way of post-processing\\nboosting or random forests, taking for TLthe collection of trees produced\\nby the gradient boosting or random forest algorithms. By ﬁtting the lasso\\npath to these trees, we would typically use a much reduced set, which would\\nsave in computations and storage for future predictions. In the next section\\nwe describe modiﬁcations of this prescription that reduce the correlations in\\nthe ensemble TL, and improve the performance of the lasso post processor.\\nAs an initial illustration, we apply this procedure to a random forest\\nensemble grown on the spam data.\\nFigure 16.6 shows that a lasso post-processing oﬀers modest improve-\\nment over the random forest (blue curve), and reduces the forest to about\\n40 trees, rather than the original 1000. The post-processed performance\\nmatches that of gradient boosting. The orange curves represent a modiﬁed\\nversion of random forests, designed to reduce the correlations between trees\\neven more. Here a random sub-sample (without replacement) of 5% of the\\ntraining sample is used to grow each tree, and the trees are restricted to be\\nshallow (about six terminal nodes). The post-processing oﬀers more dra-\\nmatic improvements here, and the training costs are reduced by a factor\\nof about 100. However, the performance of the post-processed model falls\\nsomewhat short of the blue curves.\\n16.3.1 Learning a Good Ensemble\\nNot all ensembles TLwill perform well with post-processing. In terms of\\nbasis functions, we want a collection that covers the space well in places'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 636}, page_content='618 16. Ensemble Learning\\n0 100 200 300 400 5000.04 0.05 0.06 0.07 0.08 0.09Spam Data\\nNumber of TreesTest ErrorRandom Forest\\nRandom Forest (5%, 6)\\nGradient Boost (5 node)\\nFIGURE 16.6. Application of the lasso post-processing (16.9) to the spam d ata.\\nThe horizontal blue line is the test error of a random forest ﬁt to t he spam data,\\nusing 1000trees grown to maximum depth (with m= 7; see Algorithm 15.1).\\nThe jagged blue curve is the test error after post-processing the ﬁrst 500trees\\nusing the lasso, as a function of the number of trees with nonzero co eﬃcients.\\nThe orange curve/line use a modiﬁed form of random forest, where a random\\ndraw of 5% of the data are used to grow each tree, and the trees are forced to\\nbe shallow (typically six terminal nodes). Here the post-proc essing oﬀers much\\ngreater improvement over the random forest that generated the e nsemble.\\nwhere they are needed, and are suﬃciently diﬀerent from each other for\\nthe post-processor to be eﬀective.\\nFriedman and Popescu (2003) gain insights from numerical quadrature\\nand importance sampling. They view the unknown function as an integral\\nf(x) =∫\\nβ(γ)b(x;γ)dγ, (16.10)\\nwhere γ∈Γ indexes the basis functions b(x;γ). For example, if the basis\\nfunctions are trees, then γindexes the splitting variables, the split-points\\nand the values in the terminal nodes. Numerical quadrature amounts to\\nﬁnding a set of Mevaluation points γm∈Γ and corresponding weights\\nαmso that fM(x) =α0+∑M\\nm=1αmb(x;γm) approximates f(x) well over\\nthe domain of x. Importance sampling amounts to sampling γat random,\\nbut giving more weight to relevant regions of the space Γ. Friedman and\\nPopescu (2003) suggest a measure of (lack of) relevance that uses the loss\\nfunction (16.9):'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 637}, page_content='16.3 Learning Ensembles 619\\nQ(γ) = min\\nc0,c1N∑\\ni=1L(yi,c0+c1b(xi;γ)), (16.11)\\nevaluated on the training data.\\nIf a single basis function were to be selected (e.g., a tree), it would be\\nthe global minimizer γ∗= arg min γ∈ΓQ(γ). Introducing randomness in the\\nselection of γwould necessarily produce less optimal values with Q(γ)≥\\nQ(γ∗). They propose a natural measure of the characteristic width σof the\\nsampling scheme S,\\nσ= ES[Q(γ)−Q(γ∗)]. (16.12)\\n•σtoo narrow suggests too many of the b(x;γm) look alike, and similar\\ntob(x;γ∗);\\n•σtoo wide implies a large spread in the b(x;γm), but possibly con-\\nsisting of many irrelevant cases.\\nFriedman and Popescu (2003) use sub-sampling as a mechanism for intro-\\nducing randomness, leading to their ensemble-generation algorithm 16.2.\\nAlgorithm 16.2 ISLE Ensemble Generation.\\n1.f0(x) = arg min c∑N\\ni=1L(yi,c)\\n2. For m= 1 to Mdo\\n(a)γm= arg min γ∑\\ni∈Sm(η)L(yi,fm−1(xi) +b(xi;γ))\\n(b)fm(x) =fm−1(x) +νb(x;γm)\\n3.TISLE={b(x;γ1),b(x;γ2),... ,b (x;γM)}.\\nSm(η) refers to a subsample of N≤η(η∈(0,1]) of the training obser-\\nvations, typically without replacement. Their simulations suggest picking\\nη≤1\\n2, and for large Npicking η∼1/√\\nN. Reducing ηincreases the\\nrandomness, and hence the width σ. The parameter ν∈[0,1] introduces\\nmemory into the randomization process; the larger ν, the more the pro-\\ncedure avoids b(x;γ) similar to those found before. A number of familiar\\nrandomization schemes are special cases of Algorithm 16.2:\\nBagging hasη= 1, but samples with replacement, and has ν= 0. Fried-\\nman and Hall (2007) argue that sampling without replacement with\\nη= 1/2 is equivalent to sampling with replacement with η= 1, and\\nthe former is much more eﬃcient.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 638}, page_content='620 16. Ensemble Learning\\nRandom forest sampling is similar, with more randomness introduced by\\nthe selection of the splitting variable. Reducing η <1/2 in algo-\\nrithm 16.2 has a similar eﬀect to reducing min random forests, but\\ndoes not suﬀer from the potential biases discussed in Section 15.4.2.\\nGradient boosting with shrinkage (10.41) uses η= 1, but typically does\\nnot produce suﬃcient width σ.\\nStochastic gradient boosting (Friedman, 1999) follows the recipe exactly.\\nThe authors recommend values ν= 0.1 and η≤1\\n2, and call their combined\\nprocedure (ensemble generation and post processing) Importance sampled\\nlearning ensemble (ISLE).\\nFigure 16.7 shows the performance of an ISLE on the spam data. It does\\n0 500 1000 1500 2000 25000.040 0.045 0.050 0.055 0.060Spam Data\\nNumber of TreesTest ErrorGradient Boosting (5 Node)\\nLasso Post−processed\\nFIGURE 16.7. Importance sampling learning ensemble (ISLE) ﬁt to the spam\\ndata. Here we used η= 1/2,ν= 0.05, and trees with ﬁve terminal nodes. The\\nlasso post-processed ensemble does not improve the predictio n error in this case,\\nbut it reduces the number of trees by a factor of ﬁve.\\nnot improve the predictive performance, but is able to produce a more\\nparsimonious model. Note that in practice the post-processing includes\\nthe selection of the regularization parameter λin (16.9), which would be'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 639}, page_content='16.3 Learning Ensembles 621\\nchosen by cross-validation. Here we simply demonstrate the eﬀects of post-\\nprocessing by showing the entire path on the test data.\\nFigure 16.8 shows various ISLEs on a regression example. The generating\\n0 500 1000 1500 2000 25001.0 1.5 2.0 2.5 3.0 3.5\\nNumber of TreesMean Squared ErrorGBM (1, 0.01)\\nGBM (0.1, 0.01)\\nISLE  GB\\nISLE RF\\nRandom Forest\\nFIGURE 16.8. Demonstration of ensemble methods on a regression simulation\\nexample. The notation GBM (0.1, 0.01) refers to a gradient boost ed model, with\\nparameters (η, ν). We report mean-squared error from the true (known) function.\\nNote that the sub-sampled GBM model (green) outperforms the f ull GBM model\\n(orange). The lasso post-processed version achieves simila r error. The random\\nforest is outperformed by its post-processed version, but bo th fall short of the\\nother models.\\nfunction is\\nf(X) = 10 ≤5∏\\nj=1e−2X2\\nj+35∑\\nj=6Xj, (16.13)\\nwhere X∼U[0,1]100(the last 65 elements are noise variables). The re-\\nsponse Y=f(X) +εwhere ε∼N(0,σ2); we chose σ= 1.3 resulting in a\\nsignal-to-noise ratio of approximately 2. We used a training sample of size\\n1000, and estimated the mean squared error E( ˆf(X)−f(X))2by averaging\\nover a test set of 500 samples. The sub-sampled GBM curve (light blue)\\nis an instance of stochastic gradient boosting (Friedman, 1999) discussed in\\nSection 10.12, and it outperforms gradient boosting on this example.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 640}, page_content='622 16. Ensemble Learning\\n16.3.2 Rule Ensembles\\nHere we describe a modiﬁcation of the tree-ensemble method that focuses\\non individual rules (Friedman and Popescu, 2003). We encountered rules\\nin Section 9.3 in the discussion of the PRIM method. The idea is to enlarge\\nan ensemble of trees by constructing a set of rules from each of the trees\\nin the collection.\\n1 2\\n30\\n4\\n5 6X1<2.1 X1≥2.1\\nX3∈ {M,L} X3∈ {S}\\nX7<4.5 X7≥4.5\\nFIGURE 16.9. A typical tree in an ensemble, from which rules can be derived.\\nFigure 16.9 depicts a small tree, with numbered nodes. The following\\nrules can be derived from this tree:\\nR1(X) = I(X1<2.1)\\nR2(X) = I(X1≥2.1)\\nR3(X) = I(X1≥2.1)≤I(X3∈ {S})\\nR4(X) = I(X1≥2.1)≤I(X3∈ {M, L})\\nR5(X) = I(X1≥2.1)≤I(X3∈ {S})≤I(X7<4.5)\\nR6(X) = I(X1≥2.1)≤I(X3∈ {S})≤I(X7≥4.5)(16.14)\\nA linear expansion in rules 1, 4, 5 and 6 is equivalent to the tree itself\\n(Exercise 16.3); hence (16.14) is an over-complete basis for the tree.\\nFor each tree Tmin an ensemble T, we can construct its mini-ensemble\\nof rules Tm\\nRULE, and then combine them all to form a larger ensemble\\nTRULE=M⋃\\nm=1Tm\\nRULE. (16.15)\\nThis is then treated like any other ensemble, and post-processed via the\\nlasso or similar regularized procedure.\\nThere are several advantages to this approach of deriving rules from the\\nmore complex trees:\\n•The space of models is enlarged, and can lead to improved perfor-\\nmance.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 641}, page_content='16.3 Learning Ensembles 623\\nRules Rules + Linear0.9 1.0 1.1 1.2 1.3Mean Squared Error\\nFIGURE 16.10. Mean squared error for rule ensembles, using 20realizations\\nof the simulation example (16.13).\\n•Rules are easier to interpret than trees, so there is the potential for\\na simpliﬁed model.\\n•It is often natural to augment TRULEby including each variable Xj\\nseparately as well, thus allowing the ensemble to model linear func-\\ntions well.\\nFriedman and Popescu (2008) demonstrate the power of this procedure on a\\nnumber of illustrative examples, including the simulation example (16.13).\\nFigure 16.10 shows boxplots of the mean-squared error from the true model\\nfor twenty realizations from this model. The models were all ﬁt using the\\nRulefit software, available on the ESL homepage3, which runs in an auto-\\nmatic mode.\\nOn the same training set as used in Figure 16.8, the rule based model\\nachieved a mean-squared error of 1.06. Although slightly worse than the\\nbest achieved in that ﬁgure, the results are not comparable because cross-\\nvalidation was used here to select the ﬁnal model.\\nBibliographic Notes\\nAs noted in the introduction, many of the new methods in machine learning\\nhave been dubbed “ensemble” methods. These include neural networks\\nboosting, bagging and random forests; Dietterich (2000a) gives a survey o f\\ntree-based ensemble methods. Neural networks (Chapter 11) are perhaps\\nmore deserving of the name, since they simultaneously learn the parameters\\n3ESL homepage: www-stat.stanford.edu/ElemStatLearn'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 642}, page_content='624 16. Ensemble Learning\\nof the hidden units (basis functions), along with how to combine them.\\nBishop (2006) discusses neural networks in some detail, along with the\\nBayesian perspective (MacKay, 1992; Neal, 1996). Support vector machines\\n(Chapter 12) can also be regarded as an ensemble method; they perform\\nL2regularized model ﬁtting in high-dimensional feature spaces. Boosting\\nand lasso exploit sparsity through L1regularization to overcome the high-\\ndimensionality, while SVMs rely on the “kernel trick” characteristic of L2\\nregularization.\\nC5.0 (Quinlan, 2004) is a commercial tree and rule generation package,\\nwith some goals in common with Rulefit .\\nThere is a vast and varied literature often referred to as “combining clas-\\nsiﬁers” which abounds in ad-hoc schemes for mixing methods of diﬀerent\\ntypes to achieve better performance. For a principled approach, see Kittler\\net al. (1998).\\nExercises\\nEx. 16.1 Describe exactly how to generate the block correlated data used\\nin the simulation in Section 16.2.3.\\nEx. 16.2 Letα(t)∈IRpbe a piecewise-diﬀerentiable and continuous coef-\\nﬁcient proﬁle, with α(0) = 0. The L1arc-length of αfrom time 0 to tis\\ndeﬁned by\\nΛ(t) =∫t\\n0|˙α(t)|1dt. (16.16)\\nShow that Λ( t)≥ |α(t)|1, with equality iﬀ α(t) is monotone.\\nEx. 16.3 Show that ﬁtting a linear regression model using rules 1, 4, 5 and\\n6 in equation (16.14) gives the same ﬁt as the regression tree corresponding\\nto this tree. Show the same is true for classiﬁcation, if a logistic regressi on\\nmodel is ﬁt.\\nEx. 16.4 Program and run the simulation study described in Figure 16.2.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 643}, page_content='This is page 625\\nPrinter: Opaque this\\n17\\nUndirected Graphical Models\\n17.1 Introduction\\nA graph consists of a set of vertices (nodes), along with a set of edges join-\\ning some pairs of the vertices. In graphical models, each vertex represents\\na random variable, and the graph gives a visual way of understanding the\\njoint distribution of the entire set of random variables. They can be use-\\nful for either unsupervised or supervised learning. In an undirected graph ,\\nthe edges have no directional arrows. We restrict our discussion to undi-\\nrected graphical models, also known as Markov random ﬁelds orMarkov\\nnetworks . In these graphs, the absence of an edge between two vertices has\\na special meaning: the corresponding random variables are conditionally\\nindependent, given the other variables.\\nFigure 17.1 shows an example of a graphical model for a ﬂow-cytometry\\ndataset with p= 11 proteins measured on N= 7466 cells, from Sachs\\net al. (2003). Each vertex in the graph corresponds to the real-valued ex-\\npression level of a protein. The network structure was estimated assuming\\na multivariate Gaussian distribution, using the graphical lasso procedure\\ndiscussed later in this chapter.\\nSparse graphs have a relatively small number of edges, and are convenient\\nfor interpretation. They are useful in a variety of domains, including ge-\\nnomics and proteomics, where they provide rough models of cell pathways.\\nMuch work has been done in deﬁning and understanding the structure of\\ngraphical models; see the Bibliographic Notes for references.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 644}, page_content='626 17. Undirected Graphical Models\\nRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38Jnk\\nFIGURE 17.1. Example of a sparse undirected graph, estimated from a ﬂow\\ncytometry dataset, with p= 11proteins measured on N= 7466 cells. The net-\\nwork structure was estimated using the graphical lasso proce dure discussed in this\\nchapter.\\nAs we will see, the edges in a graph are parametrized by values or po-\\ntentials that encode the strength of the conditional dependence between\\nthe random variables at the corresponding vertices. The main challenges in\\nworking with graphical models are model selection (choosing the structure\\nof the graph), estimation of the edge parameters from data, and compu-\\ntation of marginal vertex probabilities and expectations, from their joint\\ndistribution. The last two tasks are sometimes called learning andinference\\nin the computer science literature.\\nWe do not attempt a comprehensive treatment of this interesting area.\\nInstead, we introduce some basic concepts, and then discuss a few sim-\\nple methods for estimation of the parameters and structure of undirected\\ngraphical models; methods that relate to the techniques already discussed\\nin this book. The estimation approaches that we present for continuous\\nand discrete-valued vertices are diﬀerent, so we treat them separately. Sec-\\ntions 17.3.1 and 17.3.2 may be of particular interest, as they describe new,\\nregression-based procedures for estimating graphical models.\\nThere is a large and active literature on directed graphical models or\\nBayesian networks ; these are graphical models in which the edges have\\ndirectional arrows (but no directed cycles). Directed graphical models rep-\\nresent probability distributions that can be factored into products of condi-\\ntional distributions, and have the potential for causal interpretations. We\\nrefer the reader to Wasserman (2004) for a brief overview of both undi-\\nrected and directed graphs; the next section follows closely his Chapter 18.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 645}, page_content='17.2 Markov Graphs and Their Properties 627\\nX\\nXXX\\nYYYY\\nZ\\nZZ\\nZ\\nWWW\\n(a) (b)\\n(c) (d)\\nFIGURE 17.2. Examples of undirected graphical models or Markov networks.\\nEach node or vertex represents a random variable, and the lack of a n edge between\\ntwo nodes indicates conditional independence. For example, in graph (a),Xand\\nZare conditionally independent, given Y. In graph (b), Zis independent of each\\nofX,Y, and W.\\nA longer list of useful references is given in the Bibliographic Notes on\\npage 645.\\n17.2 Markov Graphs and Their Properties\\nIn this section we discuss the basic properties of graphs as models for the\\njoint distribution of a set of random variables. We defer discussion of (a)\\nparametrization and estimation of the edge parameters from data, and (b)\\nestimation of the topology of a graph, to later sections.\\nFigure 17.2 shows four examples of undirected graphs. A graph Gconsists\\nof a pair ( V,E), where Vis a set of vertices and Ethe set of edges (deﬁned\\nby pairs of vertices). Two vertices XandYare called adjacent if there\\nis a edge joining them; this is denoted by X∼Y. ApathX1,X2,... ,X n\\nis a set of vertices that are joined, that is Xi−1∼Xifori= 2,... ,n . A\\ncomplete graph is a graph with every pair of vertices joined by an edge.\\nAsubgraph U∈Vis a subset of vertices together with their edges. For\\nexample, ( X,Y,Z ) in Figure 17.2(a) form a path but not a complete graph.\\nSuppose that we have a graph Gwhose vertex set Vrepresents a set of\\nrandom variables having joint distribution P. In a Markov graph G, the\\nabsence of an edge implies that the corresponding random variables are\\nconditionally independent given the variables at the other vertices. This is\\nexpressed with the following notation:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 646}, page_content='628 17. Undirected Graphical Models\\nNo edge joining XandY⇐⇒X⊥Y|rest (17.1)\\nwhere “rest” refers to all of the other vertices in the graph. For example\\nin Figure 17.2(a) X⊥Z|Y. These are known as the pairwise Markov\\nindependencies ofG.\\nIfA,BandCare subgraphs, then Cis said to separate AandBif every\\npath between AandBintersects a node in C. For example, Yseparates\\nXandZin Figures 17.2(a) and (d), and Zseparates YandWin (d). In\\nFigure 17.2(b) Zis not connected to X,Y,W so we say that the two sets\\nare separated by the empty set. In Figure 17.2(c), C={X,Z}separates\\nYandW.\\nSeparators have the nice property that they break the graph into con-\\nditionally independent pieces. Speciﬁcally, in a Markov graph Gwith sub-\\ngraphs A,BandC,\\nifCseparates AandBthenA⊥B|C. (17.2)\\nThese are known as the global Markov properties ofG. It turns out that the\\npairwise and global Markov properties of a graph are equivalent (for gra phs\\nwith positive distributions). That is, the set of graphs with associated prob-\\nability distributions that satisfy the pairwise Markov independencies and\\nglobal Markov assumptions are the same. This result is useful for inferring\\nglobal independence relations from simple pairwise properties. For example\\nin Figure 17.2(d) X⊥Z|{Y,W}since it is a Markov graph and there is no\\nlink joining XandZ. But Yalso separates XfromZandWand hence by\\nthe global Markov assumption we conclude that X⊥Z|YandX⊥W|Y.\\nSimilarly we have Y⊥W|Z.\\nThe global Markov property allows us to decompose graphs into smaller\\nmore manageable pieces and thus leads to essential simpliﬁcations in com-\\nputation and interpretation. For this purpose we separate the graph into\\ncliques. A clique is a complete subgraph— a set of vertices that are all\\nadjacent to one another; it is called maximal if it is a clique and no other\\nvertices can be added to it and still yield a clique. The maximal cliques for\\nthe graphs of Figure 17.2 are\\n(a){X,Y},{Y,Z},\\n(b){X,Y,W },{Z},\\n(c){X,Y},{Y,Z},{Z,W},{X,W}, and\\n(d){X,Y},{Y,Z},{Z,W}.\\nAlthough the following applies to both continuous and discrete distri-\\nbutions, much of the development has been for the latter. A probability\\ndensity function fover a Markov graph Gcan be can represented as'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 647}, page_content='17.2 Markov Graphs and Their Properties 629\\nf(x) =1\\nZ∏\\nC∈CψC(xC) (17.3)\\nwhere Cis the set of maximal cliques, and the positive functions ψC(≤) are\\ncalled clique potentials . These are not in general density functions1, but\\nrather are aﬃnities that capture the dependence in XCby scoring certain\\ninstances xChigher than others. The quantity\\nZ=∑\\nx∈X∏\\nC∈CψC(xC) (17.4)\\nis the normalizing constant, also known as the partition function. Alterna-\\ntively, the representation (17.3) implies a graph with independence prop-\\nerties deﬁned by the cliques in the product. This result holds for Markov\\nnetworks Gwith positive distributions, and is known as the Hammersley-\\nCliﬀord theorem (Hammersley and Cliﬀord, 1971; Cliﬀord, 1990).\\nMany of the methods for estimation and computation on graphs ﬁrst de-\\ncompose the graph into its maximal cliques. Relevant quantities are com-\\nputed in the individual cliques and then accumulated across the entire\\ngraph. A prominent example is the join tree orjunction tree algorithm for\\ncomputing marginal and low order probabilities from the joint distribution\\non a graph. Details can be found in Pearl (1986), Lauritzen and Spiegel-\\nhalter (1988), Pearl (1988), Shenoy and Shafer (1988), Jensen et al. (1990),\\nor Koller and Friedman (2007).\\nXY\\nZ\\nFIGURE 17.3. A complete graph does not uniquely specify the higher-order\\ndependence structure in the joint distribution of the variable s.\\nA graphical model does not always uniquely specify the higher-order\\ndependence structure of a joint probability distribution. Consider the com-\\nplete three-node graph in Figure 17.3. It could represent the dependence\\nstructure of either of the following distributions:\\nf(2)(x,y,z) =1\\nZψ(x,y)ψ(x,z)ψ(y,z);\\nf(3)(x,y,z) =1\\nZψ(x,y,z).(17.5)\\nThe ﬁrst speciﬁes only second order dependence (and can be represented\\nwith fewer parameters). Graphical models for discrete data are a special\\n1If the cliques are separated, then the potentials can be dens ities, but this is in general\\nnot the case.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 648}, page_content='630 17. Undirected Graphical Models\\ncase of loglinear models for multiway contingency tables (Bishop et al.,\\n1975, e.g.); in that language f(2)is referred to as the “no second-order\\ninteraction” model.\\nFor the remainder of this chapter we focus on pairwise Markov graphs\\n(Koller and Friedman, 2007). Here there is a potential function for each\\nedge (pair of variables as in f(2)above), and at most second–order interac-\\ntions are represented. These are more parsimonious in terms of parameters,\\neasier to work with, and give the minimal complexity implied by the graph\\nstructure. The models for both continuous and discrete data are functions\\nof only the pairwise marginal distributions of the variables represented in\\nthe edge set.\\n17.3 Undirected Graphical Models for Continuous\\nVariables\\nHere we consider Markov networks where all the variables are continuous.\\nThe Gaussian distribution is almost always used for such graphical models,\\nbecause of its convenient analytical properties. We assume that the observa-\\ntions have a multivariate Gaussian distribution with mean θand covariance\\nmatrix Σ. Since the Gaussian distribution represents at most second-order\\nrelationships, it automatically encodes a pairwise Markov graph. The graph\\nin Figure 17.1 is an example of a Gaussian graphical model.\\nThe Gaussian distribution has the property that all conditional distri-\\nbutions are also Gaussian. The inverse covariance matrix Σ−1contains\\ninformation about the partial covariances between the variables; that is,\\nthe covariances between pairs iandj, conditioned on all other variables.\\nIn particular, if the ijth component of Θ=Σ−1is zero, then variables iand\\njare conditionally independent, given the other variables (Exercise 17.3).\\nIt is instructive to examine the conditional distribution of one variable\\nversus the rest, where the role of Θis explicit. Suppose we partition X=\\n(Z,Y) where Z= (X1,... ,X p−1) consists of the ﬁrst p−1 variables and\\nY=Xpis the last. Then we have the conditional distribution of YgiveZ\\n(Mardia et al., 1979, e.g.)\\nY|Z=z∼N(\\nθY+ (z−θZ)TΣ−1\\nZZσZY, σY Y−σT\\nZYΣ−1\\nZZσZY)\\n,(17.6)\\nwhere we have partitioned Σas\\nΣ=(ΣZZσZY\\nσT\\nZYσY Y)\\n. (17.7)\\nThe conditional mean in (17.6) has exactly the same form as the pop-\\nulation multiple linear regression of YonZ, with regression coeﬃcient\\nβ=Σ−1\\nZZσZY[see (2.16) on page 19]. If we partition Θin the same way,\\nsinceΣΘ=Istandard formulas for partitioned inverses give'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 649}, page_content='17.3 Undirected Graphical Models for Continuous Variables 631\\nθZY=−θY Y≤Σ−1\\nZZσZY, (17.8)\\nwhere 1 /θY Y=σY Y−σT\\nZYΣ−1\\nZZσZY>0. Hence\\nβ=Σ−1\\nZZσZY\\n=−θZY/θY Y.(17.9)\\nWe have learned two things here:\\n•The dependence of YonZin (17.6) is in the mean term alone. Here\\nwe see explicitly that zero elements in βand hence θZYmean that\\nthe corresponding elements of Zare conditionally independent of Y,\\ngiven the rest.\\n•We can learn about this dependence structure through multiple linear\\nregression.\\nThusΘcaptures all the second-order information (both structural and\\nquantitative) needed to describe the conditional distribution of each node\\ngiven the rest, and is the so-called “natural” parameter for the Gaussian\\ngraphical model2.\\nAnother (diﬀerent) kind of graphical model is the covariance graph orrel-\\nevance network , in which vertices are connected by bidirectional edges if the\\ncovariance (rather than the partial covariance) between the corresponding\\nvariables is nonzero. These are popular in genomics, see especially Butte\\net al. (2000). The negative log-likelihood from these models is not convex,\\nmaking the computations more challenging (Chaudhuri et al., 2007).\\n17.3.1 Estimation of the Parameters when the Graph\\nStructure is Known\\nGiven some realizations of X, we would like to estimate the parameters\\nof an undirected graph that approximates their joint distribution. Suppose\\nﬁrst that the graph is complete (fully connected). We assume that we have\\nNmultivariate normal realizations xi, i= 1,... ,N with population mean\\nθand covariance Σ. Let\\nS=1\\nNN∑\\ni=1(xi−¯x)(xi−¯x)T(17.10)\\nbe the empirical covariance matrix, with ¯ xthe sample mean vector. Ignoring\\nconstants, the log-likelihood of the data can be written as\\n2The distribution arising from a Gaussian graphical model is a Wishart distribution.\\nThis is a member of the exponential family, with canonical or “natural” parameter\\nΘ=Σ−1. Indeed, the partially maximized log-likelihood (17.11) i s (up to constants)\\nthe Wishart log-likelihood.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 650}, page_content='632 17. Undirected Graphical Models\\nℓ(Θ) = log det Θ−trace(SΘ). (17.11)\\nIn (17.11) we have partially maximized with respect to the mean parameter\\nθ. The quantity −ℓ(Θ) is a convex function of Θ. It is easy to show that\\nthe maximum likelihood estimate of Σis simply S.\\nNow to make the graph more useful (especially in high-dimensional set-\\ntings) let’s assume that some of the edges are missing; for example, the\\nedge between PIP3andErkis one of several missing in Figure 17.1. As we\\nhave seen, for the Gaussian distribution this implies that the correspond-\\ning entries of Θ=Σ−1are zero. Hence we now would like to maximize\\n(17.11) under the constraints that some pre-deﬁned subset of the parame-\\nters are zero. This is an equality-constrained convex optimization problem,\\nand a number of methods have been proposed for solving it, in particular\\nthe iterative proportional ﬁtting procedure (Speed and Kiiveri, 1986). This\\nand other methods are summarized for example in Whittaker (1990) and\\nLauritzen (1996). These methods exploit the simpliﬁcations that arise from\\ndecomposing the graph into its maximal cliques, as described in the previ-\\nous section. Here we outline a simple alternate approach, that exploits the\\nsparsity in a diﬀerent way. The fruits of this approach will become apparent\\nlater when we discuss the problem of estimation of the graph structure.\\nThe idea is based on linear regression, as inspired by (17.6) and (17.9).\\nIn particular, suppose that we want to estimate the edge parameters θijfor\\nthe vertices that are joined to a given vertex i, restricting those that are not\\njoined to be zero. Then it would seem that the linear regression of the node\\nivalues on the other relevant vertices might provide a reasonable estimate.\\nBut this ignores the dependence structure among the predictors in this\\nregression. It turns out that if instead we use our current (model-based)\\nestimate of the cross-product matrix of the predictors when we perform\\nour regressions, this gives the correct solutions and solves the constrained\\nmaximum-likelihood problem exactly. We now give details.\\nTo constrain the log-likelihood (17.11), we add Lagrange constants for\\nall missing edges\\nℓC(Θ) = log det Θ−trace(SΘ)−∑\\n(j,k)̸∈Eγjkθjk. (17.12)\\nThe gradient equation for maximizing (17.12) can be written as\\nΘ−1−S−Γ=0, (17.13)\\nusing the fact that the derivative of log det Θequals Θ−1(Boyd and Van-\\ndenberghe, 2004, for example, page 641). Γis a matrix of Lagrange param-\\neters with nonzero values for all pairs with edges absent.\\nWe will show how we can use regression to solve for Θand its inverse\\nW=Θ−1one row and column at a time. For simplicity let’s focus on the\\nlast row and column. Then the upper right block of equation (17.13) can\\nbe written as'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 651}, page_content='17.3 Undirected Graphical Models for Continuous Variables 633\\nw12−s12−γ12= 0. (17.14)\\nHere we have partitioned the matrices into two parts as in (17.7): part 1\\nbeing the ﬁrst p−1 rows and columns, and part 2 the pth row and column.\\nWithWand its inverse Θpartitioned in a similar fashion, we have\\n(W11w12\\nwT\\n12w22)(Θ11θ12\\nθT\\n12θ22)\\n=(I0\\n0T1)\\n. (17.15)\\nThis implies\\nw12=−W11θ12/θ22 (17.16)\\n=W11β (17.17)\\nwhere β=−θ12/θ22as in (17.9). Now substituting (17.17) into (17.14)\\ngives\\nW11β−s12−γ12= 0. (17.18)\\nThese can be interpreted as the p−1 estimating equations for the con-\\nstrained regression of Xpon the other predictors, except that the observed\\nmean cross-products matrix S11is replaced by W11, the current estimated\\ncovariance matrix from the model.\\nNow we can solve (17.18) by simple subset regression. Suppose there are\\np−qnonzero elements in γ12—i.e., p−qedges constrained to be zero. These\\np−qrows carry no information and can be removed. Furthermore we can\\nreduce βtoβ∗by removing its p−qzero elements, yielding the reduced\\nq×qsystem of equations\\nW∗\\n11β∗−s∗\\n12= 0, (17.19)\\nwith solution ˆβ∗=W∗\\n11−1s∗\\n12. This is padded with p−qzeros to give ˆβ.\\nAlthough it appears from (17.16) that we only recover the elements θ12\\nup to a scale factor 1 /θ22, it is easy to show that\\n1\\nθ22=w22−wT\\n12β (17.20)\\n(using partitioned inverse formulas). Also w22=s22, since the diagonal of\\nΓin (17.13) is zero.\\nThis leads to the simple iterative procedure given in Algorithm 17.1 for\\nestimating both ˆWand its inverse ˆΘ, subject to the constraints of the\\nmissing edges.\\nNote that this algorithm makes conceptual sense. The graph estimation\\nproblem is not pseparate regression problems, but rather pcoupled prob-\\nlems. The use of the common Win step (b), in place of the observed\\ncross-products matrix, couples the problems together in the appropriate\\nfashion. Surprisingly, we were not able to ﬁnd this procedure in the lit-\\nerature. However it is related to the covariance selection procedures of'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 652}, page_content='634 17. Undirected Graphical Models\\nAlgorithm 17.1 A Modiﬁed Regression Algorithm for Estimation of an\\nUndirected Gaussian Graphical Model with Known Structure.\\n1. Initialize W=S.\\n2. Repeat for j= 1,2,... ,p, 1,2,... ,p,... until convergence:\\n(a) Partition the matrix Winto part 1: all but the jth row and\\ncolumn, and part 2: the jth row and column.\\n(b) Solve W∗\\n11β∗−s∗\\n12= 0 for the unconstrained edge parameters\\nβ∗, using the reduced system of equations as in (17.19). Obtain\\nˆβby padding ˆβ∗with zeros in the appropriate positions.\\n(c) Update w12=W11ˆβ\\n3. In the ﬁnal cycle (for each j) solve for ˆθ12=−ˆβ≤ˆθ22, with 1 /ˆθ22=\\ns22−wT\\n12ˆβ.\\nX1X2 X3\\nX4S=\\uf8eb\\n\\uf8ec\\uf8ec\\uf8ed10 1 5 4\\n1 10 2 6\\n5 2 10 3\\n4 6 3 10\\uf8f6\\n\\uf8f7\\uf8f7\\uf8f8\\nFIGURE 17.4. A simple graph for illustration, along with the empirical cova ri-\\nance matrix.\\nDempster (1972), and is similar in ﬂavor to the iterative conditional ﬁtti ng\\nprocedure for covariance graphs, proposed by Chaudhuri et al. (2007).\\nHere is a little example, borrowed from Whittaker (1990). Suppose that\\nour model is as depicted in Figure 17.4, along with its empirical covariance\\nmatrix S. We apply algorithm (17.1) to this problem; for example, in the\\nmodiﬁed regression for variable 1 in step (b), variable 3 is left out. The\\nprocedure quickly converged to the solutions:\\nˆΣ=\\uf8eb\\n\\uf8ec\\uf8ed10.00 1 .00 1.31 4.00\\n1.00 10 .00 2 .00 0.87\\n1.31 2.00 10 .00 3 .00\\n4.00 0.87 3.00 10 .00\\uf8f6\\n\\uf8f7\\uf8f8,ˆΣ−1=\\uf8eb\\n\\uf8ec\\uf8ed0.12−0.010.00−0.05\\n−0.01 0 .11−0.020.00\\n0.00−0.02 0 .11−0.03\\n−0.050.00−0.03 0 .13\\uf8f6\\n\\uf8f7\\uf8f8.\\nNote the zeroes in ˆΣ−1, corresponding to the missing edges (1,3) and (2,4).\\nNote also that the corresponding elements in ˆΣare the only elements dif-\\nferent from S. The estimation of ˆΣis an example of what is sometimes\\ncalled the positive deﬁnite “completion” of S.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 653}, page_content='17.3 Undirected Graphical Models for Continuous Variables 635\\n17.3.2 Estimation of the Graph Structure\\nIn most cases we do not know which edges to omit from our graph, and\\nso would like to try to discover this from the data itself. In recent years a\\nnumber of authors have proposed the use of L1(lasso) regularization for\\nthis purpose.\\nMeinshausen and B¨ uhlmann (2006) take a simple approach to the prob-\\nlem: rather than trying to fully estimate ΣorΘ=Σ−1, they only estimate\\nwhich components of θijare nonzero. To do this, they ﬁt a lasso regression\\nusing each variable as the response and the others as predictors. The com-\\nponent θijis then estimated to be nonzero if either the estimated coeﬃcient\\nof variable ionjis nonzero, orthe estimated coeﬃcient of variable jon\\niis nonzero (alternatively they use an andrule). They show that asymp-\\ntotically this procedure consistently estimates the set of nonzero elements\\nofΘ.\\nWe can take a more systematic approach with the lasso penalty, follow ing\\nthe development of the previous section. Consider maximizing the penalized\\nlog-likelihood\\nlog det Θ−trace(SΘ)−λ||Θ||1, (17.21)\\nwhere ||Θ||1is the L1norm—the sum of the absolute values of the elements\\nofΣ−1, and we have ignored constants. The negative of this penalized\\nlikelihood is a convex function of Θ.\\nIt turns out that one can adapt the lasso to give the exact maximizer of\\nthe penalized log-likelihood. In particular, we simply replace the modiﬁed\\nregression step (b) in Algorithm 17.1 by a modiﬁed lasso step. Here are the\\ndetails.\\nThe analog of the gradient equation (17.13) is now\\nΘ−1−S−λ≤Sign(Θ) =0. (17.22)\\nHere we use sub-gradient notation, with Sign( θjk) = sign( θjk) ifθjk̸= 0,\\nelse Sign( θjk)∈[−1,1] ifθjk= 0. Continuing the development in the\\nprevious section, we reach the analog of (17.18)\\nW11β−s12+λ≤Sign(β) = 0 (17.23)\\n(recall that βandθ12have opposite signs). We will now see that this system\\nis exactly equivalent to the estimating equations for a lasso regression.\\nConsider the usual regression setup with outcome variables yand pre-\\ndictor matrix Z. There the lasso minimizes\\n1\\n2(y−Zβ)T(y−Zβ) +λ≤ ||β||1 (17.24)\\n[see (3.52) on page 68; here we have added a factor1\\n2for convenience]. The\\ngradient of this expression is'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 654}, page_content='636 17. Undirected Graphical Models\\nAlgorithm 17.2 Graphical Lasso.\\n1. Initialize W=S+λI. The diagonal of Wremains unchanged in\\nwhat follows.\\n2. Repeat for j= 1,2,... p, 1,2,... p,... until convergence:\\n(a) Partition the matrix Winto part 1: all but the jth row and\\ncolumn, and part 2: the jth row and column.\\n(b) Solve the estimating equations W11β−s12+λ≤Sign(β) = 0\\nusing the cyclical coordinate-descent algorithm (17.26) for the\\nmodiﬁed lasso.\\n(c) Update w12=W11ˆβ\\n3. In the ﬁnal cycle (for each j) solve for ˆθ12=−ˆβ≤ˆθ22, with 1 /ˆθ22=\\nw22−wT\\n12ˆβ.\\nZTZβ−ZTy+λ≤Sign(β) = 0 (17.25)\\nSo up to a factor 1 /N,ZTyis the analog of s12, and we replace ZTZby\\nW11, the estimated cross-product matrix from our current model.\\nThe resulting procedure is called the graphical lasso , proposed by Fried-\\nman et al. (2008b) building on the work of Banerjee et al. (2008). It is\\nsummarized in Algorithm 17.2.\\nFriedman et al. (2008b) use the pathwise coordinate descent method\\n(Section 3.8.6) to solve the modiﬁed lasso problem at each stage. Here are\\nthe details of pathwise coordinate descent for the graphical lasso algorithm.\\nLetting V=W11, the update has the form\\nˆβj←S(\\ns12j−∑\\nk̸=jVkjˆβk,λ)\\n/Vjj (17.26)\\nforj= 1,2,... ,p −1,1,2,... ,p −1,..., where Sis the soft-threshold\\noperator:\\nS(x,t) = sign( x)(|x| −t)+. (17.27)\\nThe procedure cycles through the predictors until convergence.\\nIt is easy to show that the diagonal elements wjjof the solution matrix\\nWare simply sjj+λ, and these are ﬁxed in step 1 of Algorithm 17.23.\\nThe graphical lasso algorithm is extremely fast, and can solve a moder-\\nately sparse problem with 1000 nodes in less than a minute. It is easy to\\nmodify the algorithm to have edge-speciﬁc penalty parameters λjk; since\\n3An alternative formulation of the problem (17.21) can be pos ed, where we don’t\\npenalize the diagonal of Θ. Then the diagonal elements wjjof the solution matrix are\\nsjj, and the rest of the algorithm is unchanged.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 655}, page_content='17.3 Undirected Graphical Models for Continuous Variables 637\\nλjk=∞will force ˆθjkto be zero, this algorithm subsumes Algorithm 17.1.\\nBy casting the sparse inverse-covariance problem as a series of regressions,\\none can also quickly compute and examine the solution paths as a function\\nof the penalty parameter λ. More details can be found in Friedman et al.\\n(2008b).\\nRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38JnkRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38Jnk\\nRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38JnkRaf\\nMek\\nPlcg\\nPIP2\\nPIP3\\nErk AktPKAPKCP38Jnkλ= 0 λ= 7λ= 27 λ= 36\\nFIGURE 17.5. Four diﬀerent graphical-lasso solutions for the ﬂow-cytomet ry\\ndata.\\nFigure 17.1 shows the result of applying the graphical lasso to the ﬂow-\\ncytometry dataset. Here the lasso penalty parameter λwas set at 14. In\\npractice it is informative to examine the diﬀerent sets of graphs that are\\nobtained as λis varied. Figure 17.5 shows four diﬀerent solutions. The\\ngraph becomes more sparse as the penalty parameter is increased.\\nFinally note that the values at some of the nodes in a graphical model can\\nbe unobserved; that is, missing or hidden. If only some values are missing\\nat a node, the EM algorithm can be used to impute the missing values'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 656}, page_content='638 17. Undirected Graphical Models\\n(Exercise 17.9). However, sometimes the entire node is hidden or latent.\\nIn the Gaussian model, if a node has all missing values, due to linearity\\none can simply average over the missing nodes to yield another Gaussian\\nmodel over the observed nodes. Hence the inclusion of hidden nodes does\\nnot enrich the resulting model for the observed nodes; in fact, it imposes\\nadditional structure on its covariance matrix. However in the discrete model\\n(described next) the inherent nonlinearities make hidden units a powerful\\nway of expanding the model.\\n17.4 Undirected Graphical Models for Discrete\\nVariables\\nUndirected Markov networks with all discrete variables are popular, and\\nin particular pairwise Markov networks with binary variables being the\\nmost common. They are sometimes called Ising models in the statistical\\nmechanics literature, and Boltzmann machines in the machine learning lit-\\nerature, where the vertices are referred to as “nodes” or “units” and are\\nbinary-valued.\\nIn addition, the values at each node can be observed (“visible”) or un-\\nobserved (“hidden”). The nodes are often organized in layers, similar to a\\nneural network. Boltzmann machines are useful both for unsupervised and\\nsupervised learning, especially for structured input data such as images,\\nbut have been hampered by computational diﬃculties. Figure 17.6 shows\\na restricted Boltzmann machine (discussed later), in which some variables\\nare hidden, and only some pairs of nodes are connected. We ﬁrst consider\\nthe simpler case in which all pnodes are visible with edge pairs ( j,k) enu-\\nmerated in E.\\nDenoting the binary valued variable at node jbyXj, the Ising model\\nfor their joint probabilities is given by\\np(X,Θ) = exp[∑\\n(j,k)∈EθjkXjXk−Φ(Θ)]\\nforX∈ X, (17.28)\\nwithX={0,1}p. As with the Gaussian model of the previous section,\\nonly pairwise interactions are modeled. The Ising model was developed in\\nstatistical mechanics, and is now used more generally to model the joint\\neﬀects of pairwise interactions. Φ( Θ) is the log of the partition function,\\nand is deﬁned by\\nΦ(Θ) = log∑\\nx∈X[\\nexp(∑\\n(j,k)∈Eθjkxjxk)]\\n. (17.29)\\nThe partition function ensures that the probabilities add to one over the\\nsample space. The terms θjkXjXkrepresent a particular parametrization'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 657}, page_content='17.4 Undirected Graphical Models for Discrete Variables 639\\nof the (log) potential functions (17.5), and for technical reasons requires\\naconstant nodeX0≡1 to be included (Exercise 17.10), with “edges” to\\nall the other nodes. In the statistics literature, this model is equivalent\\nto a ﬁrst-order-interaction Poisson log-linear model for multiway tables of\\ncounts (Bishop et al., 1975; McCullagh and Nelder, 1989; Agresti, 2002).\\nThe Ising model implies a logistic form for each node conditional on the\\nothers (exercise 17.11):\\nPr(Xj= 1|X−j=x−j) =1\\n1 + exp( −θj0−∑\\n(j,k)∈Eθjkxk),(17.30)\\nwhere X−jdenotes all of the nodes except j. Hence the parameter θjk\\nmeasures the dependence of XjonXk, conditional on the other nodes.\\n17.4.1 Estimation of the Parameters when the Graph\\nStructure is Known\\nGiven some data from this model, how can we estimate the parameters?\\nSuppose we have observations xi= (xi1,xi2,... ,x ip)∈ {0,1}p, i= 1,... ,N .\\nThe log-likelihood is\\nℓ(Θ) =N∑\\ni=1log Pr Θ(Xi=xi)\\n=N∑\\ni=1\\uf8ee\\n\\uf8f0∑\\n(j,k)∈Eθjkxijxik−Φ(Θ)\\uf8f9\\n\\uf8fb (17.31)\\nThe gradient of the log-likelihood is\\n∂ℓ(Θ)\\n∂θjk=N∑\\ni=1xijxik−N∂Φ(Θ)\\n∂θjk(17.32)\\nand\\n∂Φ(Θ)\\n∂θjk=∑\\nx∈Xxjxk≤p(x,Θ)\\n= E Θ(XjXk) (17.33)\\nSetting the gradient to zero gives\\nˆE(XjXk)−EΘ(XjXk) = 0 (17.34)\\nwhere we have deﬁned'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 658}, page_content='640 17. Undirected Graphical Models\\nˆE(XjXk) =1\\nNN∑\\ni=1xijxik, (17.35)\\nthe expectation taken with respect to the empirical distribution of the data.\\nLooking at (17.34), we see that the maximum likelihood estimates simply\\nmatch the estimated inner products between the nodes to their observed\\ninner products. This is a standard form for the score (gradient) equation\\nfor exponential family models, in which suﬃcient statistics are set equal t o\\ntheir expectations under the model.\\nTo ﬁnd the maximum likelihood estimates, we can use gradient search\\nor Newton methods. However the computation of E Θ(XjXk) involves enu-\\nmeration of p(X,Θ) over 2p−2of the |X|= 2ppossible values of X, and is\\nnot generally feasible for large p(e.g., larger than about 30). For smaller\\np, a number of standard statistical approaches are available:\\nPoisson log-linear modeling , where we treat the problem as a large regres-\\nsion problem (Exercise 17.12). The response vector yis the vector of\\n2pcounts in each of the cells of the multiway tabulation of the data4.\\nThe predictor matrix Zhas 2prows and up to 1+ p+p2columns that\\ncharacterize each of the cells, although this number depends on the\\nsparsity of the graph. The computational cost is essentially that of a\\nregression problem of this size, which is O(p42p) and is manageable\\nforp <20. The Newton updates are typically computed by iteratively\\nreweighted least squares, and the number of steps is usually in the\\nsingle digits. See Agresti (2002) and McCullagh and Nelder (1989) for\\ndetails. Standard software (such as the Rpackageglm) can be used\\nto ﬁt this model.\\nGradient descent requires at most O(p22p−2) computations to compute\\nthe gradient, but may require many more gradient steps than the\\nsecond–order Newton methods. Nevertheless, it can handle slightly\\nlarger problems with p≤30. These computations can be reduced\\nby exploiting the special clique structure in sparse graphs, using the\\njunction-tree algorithm. Details are not given here.\\nIterative proportional ﬁtting (IPF) performs cyclical coordinate descent on\\nthe gradient equations (17.34). At each step a parameter is updated\\nso that its gradient equation is exactly zero. This is done in a cyclical\\nfashion until all the gradients are zero. One complete cycle costs the\\nsame as a gradient evaluation, but may be more eﬃcient. Jirou´ sek and\\nPˇ reuˇ cil (1995) implement an eﬃcient version of IPF, using junction\\ntrees.\\n4Each of the cell counts is treated as an independent Poisson v ariable. We get the\\nmultinomial model corresponding to (17.28) by conditionin g on the total count N(which\\nis also Poisson under this framework).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 659}, page_content='17.4 Undirected Graphical Models for Discrete Variables 641\\nWhen pis large ( >30) other approaches have been used to approximate\\nthe gradient.\\n•The mean ﬁeld approximation (Peterson and Anderson, 1987) esti-\\nmates E Θ(XjXk) by E Θ(Xj)EΘ(Xj), and replaces the input vari-\\nables by their means, leading to a set of nonlinear equations for the\\nparameters θjk.\\n•To obtain near-exact solutions, Gibbs sampling (Section 8.6) is used\\nto approximate E Θ(XjXk) by successively sampling from the esti-\\nmated model probabilities Pr Θ(Xj|X−j) (see e.g. Ripley (1996)).\\nWe have not discussed decomposable models , for which the maximum\\nlikelihood estimates can be found in closed form without any iteration\\nwhatsoever. These models arise, for example, in trees: special graphs with\\ntree-structured topology. When computational tractability is a concern,\\ntrees represent a useful class of models and they sidestep the computational\\nconcerns raised in this section. For details, see for example Chapter 12 of\\nWhittaker (1990).\\n17.4.2 Hidden Nodes\\nWe can increase the complexity of a discrete Markov network by including\\nlatent or hidden nodes. Suppose that a subset of the variables XHare\\nunobserved or “hidden”, and the remainder XVare observed or “visible.”\\nThen the log-likelihood of the observed data is\\nℓ(Θ) =N∑\\ni=1log[Pr Θ(XV=xiV)]\\n=N∑\\ni=1[\\nlog∑\\nxH∈XHexp∑\\n(j,k)∈E(θjkxijxik−Φ(Θ))]\\n.(17.36)\\nThe sum over xHmeans that we are summing over all possible {0,1}values\\nfor the hidden units. The gradient works out to be\\ndℓ(Θ)\\ndθjk=ˆEVEΘ(XjXk|XV)−EΘ(XjXk) (17.37)\\nThe ﬁrst term is an empirical average of XjXkif both are visible; if one\\nor both are hidden, they are ﬁrst imputed given the visible data, and then\\naveraged over the hidden variables. The second term is the unconditional\\nexpectation of XjXk.\\nThe inner expectation in the ﬁrst term can be evaluated using basic rules\\nof conditional expectation and properties of Bernoulli random variables. In\\ndetail, for observation i'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 660}, page_content='642 17. Undirected Graphical Models\\nEΘ(XjXk|XV=xiV) ={xijxik ifj,k∈ V\\nxijPrΘ(Xk= 1|XV=xiV) if j∈ V,k∈ H\\nPrΘ(Xj= 1,Xk= 1|XV=xiV) ifj,k∈ H.\\n(17.38)\\nNow two separate runs of Gibbs sampling are required; the ﬁrst to estimate\\nEΘ(XjXk) by sampling from the model as above, and the second to esti-\\nmate E Θ(XjXk|XV=xiV). In this latter run, the visible units are ﬁxed\\n(“clamped”) at their observed values and only the hidden variables are\\nsampled. Gibbs sampling must be done for each observation in the training\\nset, at each stage of the gradient search. As a result this procedure can be\\nvery slow, even for moderate-sized models. In Section 17.4.4 we consider\\nfurther model restrictions to make these computations manageable.\\n17.4.3 Estimation of the Graph Structure\\nThe use of a lasso penalty with binary pairwise Markov networks has been\\nsuggested by Lee et al. (2007) and Wainwright et al. (2007). The ﬁrst au-\\nthors investigate a conjugate gradient procedure for exact maximization of\\na penalized log-likelihood. The bottleneck is the computation of E Θ(XjXk)\\nin the gradient; exact computation via the junction tree algorithm is man-\\nageable for sparse graphs but becomes unwieldy for dense graphs.\\nThe second authors propose an approximate solution, analogous to the\\nMeinshausen and B¨ uhlmann (2006) approach for the Gaussian graphical\\nmodel. They ﬁt an L1-penalized logistic regression model to each node as\\na function of the other nodes, and then symmetrize the edge parameter\\nestimates in some fashion. For example if ˜θjkis the estimate of the j-k\\nedge parameter from the logistic model for outcome node j, the “min”\\nsymmetrization sets ˆθjkto either ˜θjkor˜θkj, whichever is smallest in abso-\\nlute value. The “max” criterion is deﬁned similarly. They show that under\\ncertain conditions either approximation estimates the nonzero edges cor-\\nrectly as the sample size goes to inﬁnity. Hoeﬂing and Tibshirani (2008)\\nextend the graphical lasso to discrete Markov networks, obtaining a pro-\\ncedure which is somewhat faster than conjugate gradients, but still must\\ndeal with computation of E Θ(XjXk). They also compare the exact and\\napproximate solutions in an extensive simulation study and ﬁnd the “min”\\nor “max” approximations are only slightly less accurate than the exact pro-\\ncedure, both for estimating the nonzero edges and for estimating the actual\\nvalues of the edge parameters, and are much faster. Furthermore, they can\\nhandle denser graphs because they never need to compute the quantities\\nEΘ(XjXk).\\nFinally, we point out a key diﬀerence between the Gaussian and binary\\nmodels. In the Gaussian case, both Σand its inverse will often be of interest,\\nand the graphical lasso procedure delivers estimates for both of these quan-\\ntities. However, the approximation of Meinshausen and B¨ uhlmann (2006)\\nfor Gaussian graphical models, analogous to the Wainwright et al. (2007 )'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 661}, page_content='17.4 Undirected Graphical Models for Discrete Variables 643\\nXjXk\\nXℓ\\nVisible V1 Visible V2Hidden H\\nθjk\\nFIGURE 17.6. A restricted Boltzmann machine (RBM) in which there are no\\nconnections between nodes in the same layer. The visible units are subdivided to\\nallow the RBM to model the joint density of feature V1and their labels V2.\\napproximation for the binary case, only yields an estimate of Σ−1. In con-\\ntrast, in the Markov model for binary data, Θis the object of interest, and\\nits inverse is not of interest. The approximate method of Wainwright et a l.\\n(2007) estimates Θeﬃciently and hence is an attractive solution for the\\nbinary problem.\\n17.4.4 Restricted Boltzmann Machines\\nIn this section we consider a particular architecture for graphical models\\ninspired by neural networks, where the units are organized in layers. A\\nrestricted Boltzmann machine (RBM) consists of one layer of visible units\\nand one layer of hidden units with no connections within each layer. It\\nis much simpler to compute the conditional expectations (as in 17.37 and\\n17.38) if the connections between hidden units are removed5. Figure 17.6\\nshows an example; the visible layer is divided into input variables V1and\\noutput variables V2, and there is a hidden layer H. We denote such a\\nnetwork by\\nV1↔ H ↔ V 2. (17.39)\\nFor example, V1could be the binary pixels of an image of a handwritten\\ndigit, and V2could have 10 units, one for each of the observed class labels\\n0-9.\\nThe restricted form of this model simpliﬁes the Gibbs sampling for es-\\ntimating the expectations in (17.37), since the variables in each layer are\\nindependent of one another, given the variables in the other layers. Hence\\nthey can be sampled together, using the conditional probabilities given by\\nexpression (17.30).\\nThe resulting model is less general than a Boltzmann machine, but is still\\nuseful; for example it can learn to extract interesting features from images.\\n5We thank Geoﬀrey Hinton for assistance in the preparation of the material on RBMs.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 662}, page_content='644 17. Undirected Graphical Models\\nBy alternately sampling the variables in each layer of the RBM shown\\nin Figure 17.6, it is possible to generate samples from the joint density\\nmodel. If the V1part of the visible layer is clamped at a particular feature\\nvector during the alternating sampling, it is possible to sample from the\\ndistribution over labels given V1. Alternatively classiﬁcation of test items\\ncan also be achieved by comparing the unnormalized joint densities of each\\nlabel category with the observed features. We do not need to compute the\\npartition function as it is the same for all of these combinations.\\nAs noted the restricted Boltzmann machine has the same generic form\\nas a single hidden layer neural network (Section 11.3). The edges in the\\nlatter model are directed, the hidden units are usually real-valued, and the\\nﬁtting criterion is diﬀerent. The neural network minimizes the error (cross-\\nentropy) between the targets and their model predictions, conditional on\\nthe input features. In contrast, the restricted Boltzmann machine maxi-\\nmizes the log-likelihood for the joint distribution of all visible units—tha t\\nis, the features and targets. It can extract information from the input fea-\\ntures that is useful for predicting the labels, but, unlike supervised learning\\nmethods, it may also use some of its hidden units to model structure in the\\nfeature vectors that is not immediately relevant for predicting the labels.\\nThese features may turn out to be useful, however, when combined with\\nfeatures derived from other hidden layers.\\nUnfortunately, Gibbs sampling in a restricted Boltzmann machine can\\nbe very slow, as it can take a long time to reach stationarity. As the net -\\nwork weights get larger, the chain mixes more slowly and we need to run\\nmore steps to get the unconditional estimates. Hinton (2002) noticed em-\\npirically that learning still works well if we estimate the second expectatio n\\nin (17.37) by starting the Markov chain at the data and only running for a\\nfew steps (instead of to convergence). He calls this contrastive divergence :\\nwe sample HgivenV1,V2, then V1,V2givenHand ﬁnally HgivenV1,V2\\nagain. The idea is that when the parameters are far from the solution, it\\nmay be wasteful to iterate the Gibbs sampler to stationarity, as just a si ngle\\niteration will reveal a good direction for moving the estimates.\\nWe now give an example to illustrate the use of an RBM. Using con-\\ntrastive divergence, it is possible to train an RBM to recognize hand-written\\ndigits from the MNIST dataset (LeCun et al., 1998). With 2000 hidden\\nunits, 784 visible units for representing binary pixel intensities and one\\n10-way multinomial visible unit for representing labels, the RBM achieves\\nan error rate of 1.9% on the test set. This is a little higher than the 1.4%\\nachieved by a support vector machine and comparable to the error rate\\nachieved by a neural network trained with backpropagation. The error rate\\nof the RBM, however, can be reduced to 1.25% by replacing the 784 pixel\\nintensities by 500 features that are produced from the images without using\\nany label information. First, an RBM with 784 visible units and 500 hidden\\nunits is trained, using contrastive divergence, to model the set of images.\\nThen the hidden states of the ﬁrst RBM are used as data for training a'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 663}, page_content='Exercises 645\\nFIGURE 17.7. Example of a restricted Boltzmann machine for handwritten\\ndigit classiﬁcation. The network is depicted in the schematic o n the left. Displayed\\non the right are some diﬃcult test images that the model class iﬁes correctly.\\nsecond RBM that has 500 visible units and 500 hidden units. Finally, the\\nhidden states of the second RBM are used as the features for training an\\nRBM with 2000 hidden units as a joint density model. The details and\\njustiﬁcation for learning features in this greedy, layer-by-layer way are de-\\nscribed in Hinton et al. (2006). Figure 17.7 gives a representation of t he\\ncomposite model that is learned in this way and also shows some examples\\nof the types of distortion that it can cope with.\\nBibliographic Notes\\nMuch work has been done in deﬁning and understanding the structure of\\ngraphical models. Comprehensive treatments of graphical models can be\\nfound in Whittaker (1990), Lauritzen (1996), Cox and Wermuth (1996),\\nEdwards (2000), Pearl (2000), Anderson (2003), Jordan (2004), and Kol ler\\nand Friedman (2007). Wasserman (2004) gives a brief introduction, and\\nChapter 8 of Bishop (2006) gives a more detailed overview. Boltzmann\\nmachines were proposed in Ackley et al. (1985). Ripley (1996) has a detailed\\nchapter on topics in graphical models that relate to machine learning. We\\nfound this particularly useful for its discussion of Boltzmann machines.\\nExercises\\nEx. 17.1 For the Markov graph of Figure 17.8, list all of the implied condi-\\ntional independence relations and ﬁnd the maximal cliques.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 664}, page_content='646 17. Undirected Graphical Models\\nX1\\nX2X3X4\\nX5\\nX6\\nFIGURE 17.8.\\nEx. 17.2 Consider random variables X1,X2,X3,X4. In each of the following\\ncases draw a graph that has the given independence relations:\\n(a)X1⊥X3|X2andX2⊥X4|X3.\\n(b)X1⊥X4|X2,X3andX2⊥X4|X1,X3.\\n(c)X1⊥X4|X2,X3,X1⊥X3|X2,X4andX3⊥X4|X1,X2.\\nEx. 17.3 LetΣbe the covariance matrix of a set of pvariables X. Consider\\nthe partial covariance matrix Σa.b=Σaa−ΣabΣ−1\\nbbΣbabetween the two\\nsubsets of variables Xa= (X1,X2) consisting of the ﬁrst two, and Xb\\nthe rest. This is the covariance matrix between these two variables, after\\nlinear adjustment for all the rest. In the Gaussian distribution, this is the\\ncovariance matrix of the conditional distribution of Xa|Xb. The partial\\ncorrelation coeﬃcient ρjk|restbetween the pair Xaconditional on the rest\\nXb, is simply computed from this partial covariance. Deﬁne Θ=Σ−1.\\n1. Show that Σa.b=Θ−1\\naa.\\n2. Show that if any oﬀ-diagonal element of Θ is zero, then the partial\\ncorrelation coeﬃcient between the corresponding variables is zero.\\n3. Show that if we treat Θas if it were a covariance matrix, and compute\\nthe corresponding “correlation” matrix\\nR= diag(Θ)−1/2≤Θ≤diag(Θ)−1/2, (17.40)\\nthenrjk=−ρjk|rest\\nEx. 17.4 Denote by\\nf(X1|X2,X3,... ,X p)\\nthe conditional density of X1given X2,... ,X p. If\\nf(X1|X2,X3,... ,X p) =f(X1|X3,... ,X p),\\nshow that X1⊥X2|X3,... ,X p.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 665}, page_content='Exercises 647\\nEx. 17.5 Consider the setup in Section 17.4.1 with no missing edges. Show\\nthat\\nS11β−s12= 0\\nare the estimating equations for the multiple regression coeﬃcients of the\\nlast variable on the rest.\\nEx. 17.6 Recovery of ˆΘ=ˆΣ−1from Algorithm 17.1 . Use expression (17.16)\\nto derive the standard partitioned inverse expressions\\nθ12=−W−1\\n11w12θ22 (17.41)\\nθ22= 1/(w22−wT\\n12W−1\\n11w12). (17.42)\\nSince ˆβ=W−1\\n11w12, show that ˆθ22= 1/(w22−wT\\n12ˆβ) and ˆθ12=−ˆβˆθ22.\\nThus ˆθ12is a simply rescaling of ˆβby−ˆθ22.\\nEx. 17.7 Write a program to implement the modiﬁed regression procedure\\n(17.1) for ﬁtting the Gaussian graphical model with pre-speciﬁed edges\\nmissing. Test it on the ﬂow cytometry data from the book website, using\\nthe graph of Figure 17.1.\\nEx. 17.8\\n(a) Write a program to ﬁt the lasso using the coordinate descent procedure\\n(17.26). Compare its results to those from the larsprogram or some\\nother convex optimizer, to check that it is working correctly.\\n(b) Using the program from (a), write code to implement the graphical\\nlasso algorithm (17.2). Apply it to the ﬂow cytometry data from the\\nbook website. Vary the regularization parameter and examine the\\nresulting networks.\\nEx. 17.9 Suppose that we have a Gaussian graphical model in which some\\nor all of the data at some vertices are missing.\\n(a) Consider the EM algorithm for a dataset of Ni.i.d. multivariate ob-\\nservations xi∈IRpwith mean θand covariance matrix Σ. For each\\nsample i, letoiandmiindex the predictors that are observed and\\nmissing, respectively. Show that in the E step, the observations are\\nimputed from the current estimates of θandΣ:\\nˆxi,mi= E(xi,mi|xi,oi,θ) = ˆθmi+ˆΣmi,oiˆΣ−1\\noi,oi(xi,oi−ˆθoi)\\n(17.43)\\nwhile in the M step, θandΣare re-estimated from the empirical\\nmean and (modiﬁed) covariance of the imputed data:\\nˆθj=N∑\\ni=1ˆxij/N'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 666}, page_content='648 17. Undirected Graphical Models\\nˆΣjj′=N∑\\ni=1[(ˆxij−ˆθj)(ˆxij;−ˆθj′) +ci,jj′]/N (17.44)\\nwhere ci,jj′=ˆΣjj′ifj,j′∈miand zero otherwise. Explain the reason\\nfor the correction term ci,jj′(Little and Rubin, 2002).\\n(b) Implement the EM algorithm for the Gaussian graphical model using\\nthe modiﬁed regression procedure from Exercise 17.7 for the M-step.\\n(c) For the ﬂow cytometry data on the book website, set the data for the\\nlast protein Jnkin the ﬁrst 1000 observations to missing, ﬁt the model\\nof Figure 17.1, and compare the predicted values to the actual values\\nforJnk. Compare the results to those obtained from a regression of\\nJnkon the other vertices with edges to Jnkin Figure 17.1, using only\\nthe non-missing data.\\nEx. 17.10 Using a simple binary graphical model with just two variables,\\nshow why it is essential to include a constant node X0≡1 in the model.\\nEx. 17.11 Show that the Ising model (17.28) (17.28) for the joint probabili-\\nties in a discrete graphical model implies that the conditional distributions\\nhave the logistic form (17.30).\\nEx. 17.12 Consider a Poisson regression problem with pbinary variables\\nxij, j= 1,... ,p and response variable yiwhich measures the number of\\nobservations with predictor xi∈ {0,1}p. The design is balanced, in that all\\nn= 2ppossible combinations are measured. We assume a log-linear model\\nfor the Poisson mean in each cell\\nlogθ(X) =θ00+∑\\n(j,k)∈Exijxikθjk, (17.45)\\nusing the same notation as in Section 17.4.1 (including the constant variable\\nxi0= 1∀i). We assume the response is distributed as\\nPr(Y=y|X=x) =e−θ(x)θ(x)y\\ny!. (17.46)\\nWrite down the conditional log-likelihood for the observed responses yi,\\nand compute the gradient.\\n(a) Show that the gradient equation for θ00computes the partition func-\\ntion (17.29).\\n(b) Show that the gradient equations for the remainder of the parameters\\nare equivalent to the gradient (17.34).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 667}, page_content='This is page 649\\nPrinter: Opaque this\\n18\\nHigh-Dimensional Problems: p≫N\\n18.1 When pis Much Bigger than N\\nIn this chapter we discuss prediction problems in which the number of\\nfeatures pis much larger than the number of observations N, often written\\np≫N. Such problems have become of increasing importance, especially in\\ngenomics and other areas of computational biology. We will see that high\\nvariance and overﬁtting are a major concern in this setting. As a result,\\nsimple, highly regularized approaches often become the methods of choice.\\nThe ﬁrst part of the chapter focuses on prediction in both the classiﬁcation\\nand regression settings, while the second part discusses the more basic\\nproblem of feature selection and assessment.\\nTo get us started, Figure 18.1 summarizes a small simulation study that\\ndemonstrates the “less ﬁtting is better” principle that applies when p≫N.\\nFor each of N= 100 samples, we generated pstandard Gaussian features\\nXwith pairwise correlation 0 .2. The outcome Ywas generated according\\nto a linear model\\nY=p∑\\nj=1Xjβj+σε (18.1)\\nwhere εwas generated from a standard Gaussian distribution. For each\\ndataset, the set of coeﬃcients βjwere also generated from a standard Gaus-\\nsian distribution. We investigated three cases: p= 20,100,and 1000. The\\nstandard deviation σwas chosen in each case so that the signal-to-noise\\nratio Var[E( Y|X)]/σ2equaled 2. As a result, the number of signiﬁcant uni-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 668}, page_content='650 18. High-Dimensional Problems: p≫N1.0 1.5 2.0 2.5 3.0Relative error\\n20 9 220 featuresTest Error\\n1.0 1.5 2.0 2.5 3.0\\n99 35 7100 features\\n1.0 1.5 2.0 2.5 3.0\\n99 87 431000 features\\nEffective Degrees of Freedom\\nFIGURE 18.1. Test-error results for simulation experiments. Shown are box-\\nplots of the relative test errors over 100simulations, for three diﬀerent values\\nofp, the number of features. The relative error is the test error d ivided by the\\nBayes error, σ2. From left to right, results are shown for ridge regression w ith\\nthree diﬀerent values of the regularization parameter λ:0.001,100and1000. The\\n(average) eﬀective degrees of freedom in the ﬁt is indicated be low each plot.\\nvariate regression coeﬃcients1was 9, 33 and 331, respectively, averaged\\nover the 100 simulation runs. The p= 1000 case is designed to mimic the\\nkind of data that we might see in a high-dimensional genomic or proteomic\\ndataset, for example.\\nWe ﬁt a ridge regression to the data, with three diﬀerent values for the\\nregularization parameter λ: 0.001, 100, and 1000. When λ= 0.001, this\\nis nearly the same as least squares regression, with a little regularizati on\\njust to ensure that the problem is non-singular when p > N . Figure 18.1\\nshows boxplots of the relative test error achieved by the diﬀerent estimator s\\nin each scenario. The corresponding average degrees of freedom used in\\neach ridge-regression ﬁt is indicated (computed using formula (3.50) on\\npage 682). The degrees of freedom is a more interpretable parameter than\\nλ. We see that ridge regression with λ= 0.001 (20 df) wins when p= 20;\\nλ= 100 (35 df) wins when p= 100, and λ= 1000 (43 df) wins when\\np= 1000,\\nHere is an explanation for these results. When p= 20, we ﬁt all the way\\nand we can identify as many of the signiﬁcant coeﬃcients as possible with\\n1We call a regression coeﬃcient signiﬁcant if |bβj/bsej| ≥2, where ˆβjis the estimated\\n(univariate) coeﬃcient and bsejis its estimated standard error.\\n2For a ﬁxed value of the regularization parameter λ, the degrees of freedom depends\\non the observed predictor values in each simulation. Hence w e compute the average\\ndegrees of freedom over simulations.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 669}, page_content='18.2 Nearest Shrunken Centroids 651\\nlow bias. When p= 100, we can identify some non-zero coeﬃcients using\\nmoderate shrinkage. Finally, when p= 1000, even though there are many\\nnonzero coeﬃcients, we don’t have a hope for ﬁnding them and we need\\nto shrink all the way down. As evidence of this, let tj=ˆβj/ˆsej, where ˆβj\\nis the ridge regression estimate and ˆsejits estimated standard error. Then\\nusing the optimal ridge parameter in each of the three cases, the median\\nvalue of |tj|was 2.0, 0.6 and 0.2, and the average number of |tj|values\\nexceeding 2 was equal to 9.8, 1.2 and 0.0.\\nRidge regression with λ= 0.001 successfully exploits the correlation in\\nthe features when p < N , but cannot do so when p≫N. In the latter case\\nthere is not enough information in the relatively small number of samples\\nto eﬃciently estimate the high-dimensional covariance matrix. In that case,\\nmore regularization leads to superior prediction performance.\\nThus it is not surprising that the analysis of high-dimensional data re-\\nquires either modiﬁcation of procedures designed for the N > p scenario, or\\nentirely new procedures. In this chapter we discuss examples of both kinds\\nof approaches for high dimensional classiﬁcation and regression; these meth-\\nods tend to regularize quite heavily, using scientiﬁc contextual knowledge\\nto suggest the appropriate form for this regularization. The chapter ends\\nwith a discussion of feature selection and multiple testing.\\n18.2 Diagonal Linear Discriminant Analysis and\\nNearest Shrunken Centroids\\nGene expression arrays are an important new technology in biology, and\\nare discussed in Chapters 1 and 14. The data in our next example form\\na matrix of 2308 genes (columns) and 63 samples (rows), from a set of\\nmicroarray experiments. Each expression value is a log-ratio log( R/G).R\\nis the amount of gene-speciﬁc RNA in the target sample that hybridizes\\nto a particular (gene-speciﬁc) spot on the microarray, and Gis the corre-\\nsponding amount of RNA from a reference sample. The samples arose from\\nsmall, round blue-cell tumors (SRBCT) found in children, and are classiﬁed\\ninto four major types: BL (Burkitt lymphoma), EWS (Ewing’s sarcoma) ,\\nNB (neuroblastoma), and RMS (rhabdomyosarcoma). There is an addi-\\ntional test data set of 20 observations. We will not go into the scientiﬁc\\nbackground here.\\nSince p≫N, we cannot ﬁt a full linear discriminant analysis (LDA) to\\nthe data; some sort of regularization is needed. The method we describe\\nhere is similar to the methods of Section 4.3.1, but with important modiﬁ-\\ncations that achieve feature selection. The simplest form of regularization\\nassumes that the features are independent within each class, that is, the\\nwithin-class covariance matrix is diagonal. Despite the fact that features\\nwill rarely be independent within a class, when p≫Nwe don’t have'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 670}, page_content='652 18. High-Dimensional Problems: p≫N\\nenough data to estimate their dependencies. The assumption of indepen-\\ndence greatly reduces the number of parameters in the model and often\\nresults in an eﬀective and interpretable classiﬁer.\\nThus we consider the diagonal-covariance LDA rule for classifying the\\nclasses. The discriminant score [see (4.12 on page 110] for class kis\\nδk(x∗) =−p∑\\nj=1(x∗\\nj−¯xkj)2\\ns2\\nj+ 2log πk. (18.2)\\nHerex∗= (x∗\\n1,x∗\\n2,... ,x∗\\np)Tis a vector of expression values for a test ob-\\nservation, sjis the pooled within-class standard deviation of the jth gene,\\nand ¯xkj=∑\\ni∈Ckxij/Nkis the mean of the Nkvalues for gene jin class\\nk, with Ckbeing the index set for class k. We call ˜ xk= (¯xk1,¯xk2,...¯xkp)T\\nthecentroid of class k. The ﬁrst part of (18.2) is simply the (negative)\\nstandardized squared distance of x∗to the kth centroid. The second part\\nis a correction based on the class prior probability πk, where∑K\\nk=1πk= 1.\\nThe classiﬁcation rule is then\\nC(x∗) =ℓifδℓ(x∗) = max kδk(x∗). (18.3)\\nWe see that the diagonal LDA classiﬁer is equivalent to a nearest centroid\\nclassiﬁer after appropriate standardization. It is also a special case of the\\nnaive-Bayes classiﬁer, as described in Section 6.6.3. It assumes that the\\nfeatures in each class have independent Gaussian distributions with the\\nsame variance.\\nThe diagonal LDA classiﬁer is often eﬀective in high dimensional set-\\ntings. It is also called the “independence rule” in Bickel and Levina (2004),\\nwho demonstrate theoretically that it will often outperform standard lin-\\near discriminant analysis in high-dimensional problems. Here the diagonal\\nLDA classiﬁer yielded ﬁve misclassiﬁcation errors for the 20 test samples.\\nOne drawback of the diagonal LDA classiﬁer is that it uses all of the fea-\\ntures (genes), and hence is not convenient for interpretation. With further\\nregularization we can do better—both in terms of test error and inter-\\npretability.\\nWe would like to regularize in a way that automatically drops out fea-\\ntures that are not contributing to the class predictions. We can do this\\nby shrinking the classwise mean toward the overall mean, for each feature\\nseparately. The result is a regularized version of the nearest centroid clas-\\nsiﬁer, or equivalently a regularized version of the diagonal-covariance form\\nof LDA. We call the procedure nearest shrunken centroids (NSC).\\nThe shrinkage procedure is deﬁned as follows. Let\\ndkj=¯xkj−¯xj\\nmk(sj+s0), (18.4)\\nwhere ¯ xjis the overall mean for gene j,m2\\nk= 1/Nk−1/Nands0is a\\nsmall positive constant, typically chosen to be the median of the sjvalues.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 671}, page_content='18.2 Nearest Shrunken Centroids 653\\n(0,0)∆\\nFIGURE 18.2. Soft thresholding function sign(x)(|x|−∆)+is shown in orange,\\nalong with the 45◦line in red.\\nThis constant guards against large dkjvalues that arise from expression\\nvalues near zero. With constant within-class variance σ2, the variance of\\nthe contrast ¯ xkj−¯xjin the numerator is m2\\nkσ2, and hence the form of the\\nstandardization in the denominator. We shrink the dkjtoward zero using\\nsoft thresholding\\nd′\\nkj= sign( dkj)(|dkj| −∆)+; (18.5)\\nsee Figure 18.2. Here ∆ is a parameter to be determined; we used 10-fold\\ncross-validation in the example (see the top panel of Figure 18.4). Each\\ndkjis reduced by an amount ∆ in absolute value, and is set to zero if its\\nabsolute value is less than zero. The soft-thresholding function is shown\\nin Figure 18.2; the same thresholding is applied to wavelet coeﬃcients in\\nSection 5.9. An alternative is to use hard thresholding\\nd′\\nkj=dkj≤I(|dkj| ≥∆); (18.6)\\nwe prefer soft-thresholding, as it is a smoother operation and typically\\nworks better. The shrunken versions of ¯ xkjare then obtained by reversing\\nthe transformation in (18.4):\\n¯x′\\nkj= ¯xj+mk(sj+s0)d′\\nkj. (18.7)\\nWe then use the shrunken centroids ¯ x′\\nkjin place of the original ¯ xkjin the\\ndiscriminant score (18.2). The estimator (18.5) can also be viewed as a\\nlasso-style estimator for the class means (Exercise 18.2).\\nNotice that only the genes that have a nonzero d′\\nkjfor at least one of the\\nclasses play a role in the classiﬁcation rule, and hence the vast majority\\nof genes can often be discarded. In this example, all but 43 genes were\\ndiscarded, leaving a small interpretable set of genes that characterize each\\nclass. Figure 18.3 represents the genes in a heatmap.\\nFigure 18.4 (top panel) demonstrates the eﬀectiveness of the shrinkage.\\nWith no shrinkage we make 5/20 errors on the test data, and several errors'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 672}, page_content='654 18. High-Dimensional Problems: p≫N\\non the training and CV data. The shrunken centroids achieve zero test er-\\nrors for a fairly broad band of values for ∆. The bottom panel of Figure 18. 4\\nshows the four centroids for the SRBCT data (gray), relative to the overall\\ncentroid. The blue bars are shrunken versions of these centroids, obtained\\nby soft-thresholding the gray bars, using ∆ = 4 .3. The discriminant scores\\n(18.2) can be used to construct class probability estimates:\\nˆpk(x∗) =e1\\n2δk(x∗)\\n∑K\\nℓ=1e1\\n2δℓ(x∗). (18.8)\\nThese can be used to rate the classiﬁcations, or to decide not to classify a\\nparticular sample at all.\\nNote that other forms of feature selection can be used in this setting,\\nincluding hard thresholding. Fan and Fan (2008) show theoretically the\\nimportance of carrying out some kind of feature selection with diagonal\\nlinear discriminant analysis in high-dimensional problems.\\n18.3 Linear Classiﬁers with Quadratic\\nRegularization\\nRamaswamy et al. (2001) present a more diﬃcult microarray classiﬁcation\\nproblem, involving a training set of 144 patients with 14 diﬀerent types of\\ncancer, and a test set of 54 patients. Gene expression measurements were\\navailable for 16 ,063 genes.\\nTable 18.1 shows the prediction results from eight diﬀerent classiﬁcation\\nmethods. The data from each patient was ﬁrst standardized to have mean\\n0 and variance 1; this seems to improve prediction accuracy overall this\\nexample, suggesting that the “shape” of each gene-expression proﬁle is\\nimportant, rather than the absolute expression levels. In each case, the\\nBL EWS NB RMS\\nFIGURE 18.3. Heat-map of the chosen 43 genes. Within each of the horizontal\\npartitions, we have ordered the genes by hierarchical cluster ing, and similarly\\nfor the samples within each vertical partition. Yellow repre sents over- and blue\\nunder-expression.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 673}, page_content='18.3 Linear Classiﬁers with Quadratic Regularization 655\\n0 2 4 6Misclassification Error2308 2059 1223 598 284 159 81 43 23 15 10 5 1Number of Genes0.0 0.2 0.4 0.6 0.8Training\\n10−fold CV\\nTest\\nAmount of Shrinkage ∆\\n−1.0 −0.5 0.0 0.5 1.0BL0 500 1000 1500 2000\\n−1.0 −0.5 0.0 0.5 1.0EWS\\n−1.0 −0.5 0.0 0.5 1.0NB\\n−1.0 −0.5 0.0 0.5 1.0RMSGene\\nCentroids: Average Expression Centered at Overall Centroid\\nFIGURE 18.4. (Top): Error curves for the SRBCT data. Shown are the train-\\ning, 10-fold cross-validation, and test misclassiﬁcation erro rs as the threshold\\nparameter ∆is varied. The value ∆ = 4 .34is chosen by CV, resulting in a sub-\\nset of 43selected genes. (Bottom): Four centroids proﬁles dkjfor the SRBCT\\ndata (gray), relative to the overall centroid. Each centroid h as2308components,\\nand we see considerable noise. The blue bars are shrunken versions d′\\nkjof these\\ncentroids, obtained by soft-thresholding the gray bars, using ∆ = 4 .3.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 674}, page_content='656 18. High-Dimensional Problems: p≫N\\nTABLE 18.1. Prediction results for microarray data with 14 cancer classe s.\\nMethod 1 is described in Section 18.2. Methods 2, 3 and 6 are disc ussed in Sec-\\ntion 18.3, while 4, 7 and 8 are discussed in Section 18.4. Method 5 is described in\\nSection 13.3. The elastic-net penalized multinomial does the be st on the test data,\\nbut the standard error of each test-error estimate is about 3, so such comparisons\\nare inconclusive.\\nMethods CV errors (SE) Test errors Number of\\nOut of 144 Out of 54 Genes Used\\n1. Nearest shrunken centroids 35 (5.0) 17 6,520\\n2.L2-penalized discriminant 25 (4.1) 12 16,063\\nanalysis\\n3. Support vector classiﬁer 26 (4.2) 14 16,063\\n4. Lasso regression (one vs all) 30.7 (1.8) 12.5 1,429\\n5.k-nearest neighbors 41 (4.6) 26 16,063\\n6.L2-penalized multinomial 26 (4.2) 15 16,063\\n7.L1-penalized multinomial 17 (2.8) 13 269\\n8. Elastic-net penalized 22 (3.7) 11.8 384\\nmultinomial\\nregularization parameter has been chosen to minimize the cross-validation\\nerror, and the test error at that value of the parameter is shown. When\\nmore than one value of the regularization parameter yields the minimal\\ncross-validation error, the average test error at these values is reported.\\nRDA (regularized discriminant analysis), regularized multinomial logist ic\\nregression, and the support vector machine are more complex methods that\\ntry to exploit multivariate information in the data. We describe each in\\nturn, as well as a variety of regularization methods, including both L1and\\nL2and some in between.\\n18.3.1 Regularized Discriminant Analysis\\nRegularized discriminant analysis (RDA) is described in Section 4.3.1. Lin-\\near discriminant analysis involves the inversion of a p×pwithin-covariance\\nmatrix. When p≫N, this matrix can be huge, has rank at most N < p ,\\nand hence is singular. RDA overcomes the singularity issues by regulariz-\\ning the within-covariance estimate ˆΣ. Here we use a version of RDA that\\nshrinks ˆΣtowards its diagonal:\\nˆΣ(γ) =γˆΣ+ (1−γ)diag( ˆΣ),withγ∈[0,1]. (18.9)\\nNote that γ= 0 corresponds to diagonal LDA, which is the “no shrinkage”\\nversion of nearest shrunken centroids. The form of shrinkage in (18.9) is'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 675}, page_content='18.3 Linear Classiﬁers with Quadratic Regularization 657\\nmuch like ridge regression (Section 3.4.1), which shrinks the total covar iance\\nmatrix of the features towards a diagonal (scalar) matrix. In fact, view ing\\nlinear discriminant analysis as linear regression with optimal scoring of the\\ncategorical response [see (12.58) in Section 12.6], the equivalence becomes\\nmore precise.\\nThe computational burden of inverting this large p×pmatrix is overcome\\nusing the methods discussed in Section 18.3.5. The value of γwas chosen\\nby cross-validation in line 2 of Table 18.1; all values of γ∈(0.002,0.550)\\ngave the same CV and test error. Further development of RDA, including\\nshrinkage of the centroids in addition to the covariance matrix, can be\\nfound in Guo et al. (2006).\\n18.3.2 Logistic Regression with Quadratic Regularization\\nLogistic regression (Section 4.4) can be modiﬁed in a similar way, to deal\\nwith the p≫Ncase. With Kclasses, we use a symmetric version of the\\nmulticlass logistic model (4.17) on page 119:\\nPr(G=k|X=x) =exp(βk0+xTβk)∑K\\nℓ=1exp(βℓ0+xTβℓ). (18.10)\\nThis has Kcoeﬃcient vectors of log-odds parameters β1,β2,... ,β K. We\\nregularize the ﬁtting by maximizing the penalized log-likelihood\\nmax\\n{β0k,βk}K\\n1[N∑\\ni=1log Pr( gi|xi)−λ\\n2K∑\\nk=1||βk||2\\n2]\\n. (18.11)\\nThis regularization automatically resolves the redundancy in the paramet-\\nrization, and forces∑K\\nk=1ˆβkj= 0, j= 1,... ,p (Exercise 18.3). Note that\\nthe constant terms βk0are not regularized (and so one should be set to\\nzero). The resulting optimization problem is convex, and can be solved by\\na Newton algorithm or other numerical techniques. Details are given in Zhu\\nand Hastie (2004). Friedman et al. (2010) provide software for computi ng\\nthe regularization path for the two- and multiclass logistic regression mod-\\nels. Table 18.1, line 6 reports the results for the multiclass logistic r egres-\\nsion model, referred to there as “multinomial”. It can be shown (Rosset\\net al., 2004a) that for separable data, as λ→0, the regularized (two-\\nclass) logistic regression estimate (renormalized) converges to the maxi mal\\nmargin classiﬁer (Section 12.2). This gives an attractive alternative t o the\\nsupport-vector machine, discussed next, especially in the multiclass case.\\n18.3.3 The Support Vector Classiﬁer\\nThe support vector classiﬁer is described for the two-class case in Sec-\\ntion 12.2. When p > N , it is especially attractive because in general the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 676}, page_content='658 18. High-Dimensional Problems: p≫N\\nclasses are perfectly separable by a hyperplane unless there are identical\\nfeature vectors in diﬀerent classes. Without any regularization the support\\nvector classiﬁer ﬁnds the separating hyperplane with the largest margin;\\nthat is, the hyperplane yielding the biggest gap between the classes in\\nthe training data. Somewhat surprisingly, when p≫Nthe unregularized\\nsupport vector classiﬁer often works about as well as the best regularized\\nversion. Overﬁtting often does not seem to be a problem, partly because of\\nthe insensitivity of misclassiﬁcation loss.\\nThere are many diﬀerent methods for generalizing the two-class support-\\nvector classiﬁer to K >2 classes. In the “one versus one” ( ovo) approach,\\nwe compute all(K\\n2)\\npairwise classiﬁers. For each test point, the predicted\\nclass is the one that wins the most pairwise contests. In the “one versus all”\\n(ova) approach, each class is compared to all of the others in Ktwo-class\\ncomparisons. To classify a test point, we compute the conﬁdences (signed\\ndistance from the hyperplane) for each of the Kclassiﬁers. The winner is the\\nclass with the highest conﬁdence. Finally, Vapnik (1998) and Weston and\\nWatkins (1999) suggested (somewhat complex) multiclass criteria which\\ngeneralize the two-class criterion (12.6).\\nTibshirani and Hastie (2007) propose the margin tree classiﬁer, in which\\nsupport-vector classiﬁers are used in a binary tree, much as in CART\\n(Chapter 9). The classes are organized in a hierarchical manner, which can\\nbe useful for classifying patients into diﬀerent cancer types, for example.\\nLine 3 of Table 18.1 shows the results for the support vector classiﬁer\\nusing the ovamethod; Ramaswamy et al. (2001) reported (and we con-\\nﬁrmed) that this approach worked best for this problem. The errors are\\nvery similar to those in line 6, as we might expect from the comments\\nat the end of the previous section. The error rates are insensitive to the\\nchoice of C[the regularization parameter in (12.8) on page 420], for values\\nofC >0.001. Since p > N , the support vector hyperplane can perfectly\\nseparate the training data by setting C=∞.\\n18.3.4 Feature Selection\\nFeature selection is an important scientiﬁc requirement for a classiﬁer when\\npis large. Neither discriminant analysis, logistic regression, nor the suppo rt-\\nvector classiﬁer perform feature selection automatically, because all use\\nquadratic regularization. All features have nonzero weights in both models.\\nAd-hoc methods for feature selection have been proposed, for example,\\nremoving genes with small coeﬃcients, and reﬁtting the classiﬁer. This is\\ndone in a backward stepwise manner, starting with the smallest weights and\\nmoving on to larger weights. This is known as recursive feature elimination\\n(Guyon et al., 2002). It was not successful in this example; Ramaswamy\\net al. (2001) report, for example, that the accuracy of the support-vector\\nclassiﬁer starts to degrade as the number of genes is reduced from the full'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 677}, page_content='18.3 Linear Classiﬁers with Quadratic Regularization 659\\nset of 16 ,063. This is rather remarkable, as the number of training samples\\nis only 144. We do not have an explanation for this behavior.\\nAll three methods discussed in this section (RDA, LR and SVM) can\\nbe modiﬁed to ﬁt nonlinear decision boundaries using kernels. Usually the\\nmotivation for such an approach is to increase the model complexity. With\\np≫Nthe models are already suﬃciently complex and overﬁtting is always\\na danger. Yet despite the high dimensionality, radial kernels (Section 12.3.3)\\nsometimes deliver superior results in these high dimensional problems. The\\nradial kernel tends to dampen inner products between points far away from\\neach other, which in turn leads to robustness to outliers. This occurs often\\nin high dimensions, and may explain the positive results. We tried a radial\\nkernel with the SVM in Table 18.1, but in this case the performance was\\ninferior.\\n18.3.5 Computational Shortcuts When p≫N\\nThe computational techniques discussed in this section apply to any method\\nthat ﬁts a linear model with quadratic regularization on the coeﬃcients.\\nThat includes all the methods discussed in this section, and many more.\\nWhen p > N , the computations can be carried out in an N-dimensional\\nspace, rather than p, via the singular value decomposition introduced in\\nSection 14.5. Here is the geometric intuition: just like two points in thr ee-\\ndimensional space always lie on a line, Npoints in p-dimensional space lie\\nin an ( N−1)-dimensional aﬃne subspace.\\nGiven the N×pdata matrix X, let\\nX=UDVT(18.12)\\n=RVT(18.13)\\nbe the singular-value decomposition (SVD) of X; that is, Visp×Nwith\\northonormal columns, UisN×Northogonal, and Da diagonal matrix\\nwith elements d1≥d2≥dN≥0. The matrix RisN×N, with rows rT\\ni.\\nAs a simple example, let’s ﬁrst consider the estimates from a ridge re-\\ngression:\\nˆβ= (XTX+λI)−1XTy. (18.14)\\nReplacing XbyRVTand after some further manipulations, this can be\\nshown to equal\\nˆβ=V(RTR+λI)−1RTy (18.15)\\n(Exercise 18.4). Thus ˆβ=Vˆθ, where ˆθis the ridge-regression estimate\\nusing the Nobservations ( ri,yi),i= 1,2,... ,N . In other words, we can\\nsimply reduce the data matrix from XtoR, and work with the rows of\\nR. This trick reduces the computational cost from O(p3) toO(pN2) when\\np > N .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 678}, page_content='660 18. High-Dimensional Problems: p≫N\\nThese results can be generalized to allmodels that are linear in the\\nparameters and have quadratic penalties. Consider any supervised learning\\nproblem where we use a linear function f(X) =β0+XTβto model a\\nparameter in the conditional distribution of Y|X. We ﬁt the parameters β\\nby minimizing some loss function∑N\\ni=1L(yi,f(xi)) over the data with a\\nquadratic penalty on β. Logistic regression is a useful example to have in\\nmind. Then we have the following simple theorem:\\nLetf∗(ri) =θ0+rT\\niθwithrideﬁned in (18.13), and consider the pair of\\noptimization problems:\\n(ˆβ0,ˆβ) = arg min\\nβ0,β∈I RpN∑\\ni=1L(yi,β0+xT\\niβ) +λβTβ; (18.16)\\n(ˆθ0,ˆθ) = arg min\\nθ0,θ∈I RNN∑\\ni=1L(yi,θ0+rT\\niθ) +λθTθ. (18.17)\\nThen the ˆβ0=ˆθ0, and ˆβ=Vˆθ.\\nThe theorem says that we can simply replace the pvectors xiby the\\nN-vectors ri, and perform our penalized ﬁt as before, but with far fewer\\npredictors. The N-vector solution ˆθis then transformed back to the p-\\nvector solution via a simple matrix multiplication. This result is part of\\nthe statistics folklore, and deserves to be known more widely—see Hastie\\nand Tibshirani (2004) for further details.\\nGeometrically, we are rotating the features to a coordinate system in\\nwhich all but the ﬁrst Ncoordinates are zero. Such rotations are allowed\\nsince the quadratic penalty is invariant under rotations, and linear models\\nare equivariant.\\nThis result can be applied to many of the learning methods discussed\\nin this chapter, such as regularized (multiclass) logistic regression, linea r\\ndiscriminant analysis (Exercise 18.6), and support vector machines. It als o\\napplies to neural networks with quadratic regularization (Section 11.5.2).\\nNote, however, that it does not apply to methods such as the lasso, which\\nuses nonquadratic ( L1) penalties on the coeﬃcients.\\nTypically we use cross-validation to select the parameter λ. It can be\\nseen (Exercise 18.12) that we only need to construct Ronce, on the original\\ndata, and use it as the data for each of the CV folds.\\nThe support vector “kernel trick” of Section 12.3.7 exploits the same re-\\nduction used in this section, in a slightly diﬀerent context. Suppose we have\\nat our disposal the N×Ngram (inner-product) matrix K=XXT. From\\n(18.12) we have K=UD2UT, and so Kcaptures the same information as\\nR. Exercise 18.13 shows how we can exploit the ideas in this section to ﬁt\\na ridged logistic regression with Kusing its SVD.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 679}, page_content='18.4 Linear Classiﬁers with L1Regularization 661\\n18.4 Linear Classiﬁers with L1Regularization\\nThe methods of the previous chapter use an L2penalty to regularize their\\nparameters, just as in ridge regression. All of the estimated coeﬃcients\\nare nonzero, and hence no feature selection is performed. In this section we\\ndiscuss methods that use L1penalties instead, and hence provide automatic\\nfeature selection.\\nRecall the lasso of Section 3.4.2,\\nmin\\nβ1\\n2N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\n+λp∑\\nj=1|βj|, (18.18)\\nwhich we have written in the Lagrange form (3.52). As discussed there, the\\nuse of the L1penalty causes a subset of the solution coeﬃcients ˆβjto be\\nexactly zero, for a suﬃciently large value of the tuning parameter λ.\\nIn Section 3.8.1 we discussed the LARS algorithm, an eﬃcient procedure\\nfor computing the lasso solution for all λ. When p > N (as in this chapter),\\nasλapproaches zero, the lasso ﬁts the training data exactly. In fact, by\\nconvex duality one can show that when p > N the number of non-zero\\ncoeﬃcients is at most Nfor all values of λ(Rosset and Zhu, 2007, for\\nexample). Thus the lasso provides a (severe) form of feature selection.\\nLasso regression can be applied to a two-class classiﬁcation problem by\\ncoding the outcome ±1, and applying a cutoﬀ (usually 0) to the predictions.\\nFor more than two classes, there are many possible approaches, including\\ntheovaandovomethods discussed in Section 18.3.3. We tried the ova-\\napproach on the cancer data in Section 18.3. The results are shown in\\nline (4) of Table 18.1. Its performance is among the best.\\nA more natural approach for classiﬁcation problems is to use the lasso\\npenalty to regularize logistic regression. Several implementations have been\\nproposed in the literature, including path algorithms similar to LARS (Par k\\nand Hastie, 2007). Because the paths are piecewise smooth but nonlinear,\\nexact methods are slower than the LARS algorithm, and are less feasible\\nwhen pis large.\\nFriedman et al. (2010) provide very fast algorithms for ﬁtting L1-pen-\\nalized logistic and multinomial regression models. They use the symmetric\\nmultinomial logistic regression model as in (18.10) in Section 18.3.2 , and\\nmaximize the penalized log-likelihood\\nmax\\n{β0k,βk∈I Rp}K\\n1\\uf8ee\\n\\uf8f0N∑\\ni=1log Pr( gi|xi)−λK∑\\nk=1p∑\\nj=1|βkj|\\uf8f9\\n\\uf8fb; (18.19)\\ncompare with (18.11). Their algorithm computes the exact solution at a\\npre-chosen sequence of values for λby cyclical coordinate descent (Sec-\\ntion 3.8.6), and exploits the fact that solutions are sparse when p≫N,'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 680}, page_content='662 18. High-Dimensional Problems: p≫N\\nas well as the fact that solutions for neighboring values of λtend to be\\nvery similar. This method was used in line (7) of Table 18.1, with the over -\\nall tuning parameter λchosen by cross-validation. The performance was\\nsimilar to that of the best methods, except here the automatic feature se-\\nlection chose 269 genes altogether. A similar approach is used in Genkin\\net al. (2007); although they present their model from a Bayesian point of\\nview, they in fact compute the posterior mode, which solves the penalized\\nmaximum-likelihood problem.\\n−8 −7 −6 −5 −4 −3 −2 −10.0 0.5 1.0 1.5 2.0\\n−8 −7 −6 −5 −4 −3 −2 −10.0 0.5 1.0 1.5 2.0CoeﬃcientsCoeﬃcients\\nlog(λ) log(λ)Lasso Elastic Net\\nFIGURE 18.5. Regularized logistic regression paths for the leukemia dat a. The\\nleft panel is the lasso path, the right panel the elastic-net pat h with α= 0.8. At\\nthe ends of the path (extreme left), there are 19nonzero coeﬃcients for the lasso,\\nand39for the elastic net. The averaging eﬀect of the elastic net resul ts in more\\nnon-zero coeﬃcients than the lasso, but with smaller magnitudes .\\nIn genomic applications, there are often strong correlations among the\\nvariables; genes tend to operate in molecular pathways. The lasso penalty\\nis somewhat indiﬀerent to the choice among a set of strong but corre-\\nlated variables (Exercise 3.28). The ridge penalty, on the other hand, tends\\nto shrink the coeﬃcients of correlated variables toward each other (Exer-\\ncise 3.29 on page 99). The elastic net penalty (Zou and Hastie, 2005) is a\\ncompromise, and has the form\\np∑\\nj=1(\\nα|βj|+ (1−α)β2\\nj)\\n. (18.20)\\nThe second term encourages highly correlated features to be averaged, while\\nthe ﬁrst term encourages a sparse solution in the coeﬃcients of these aver-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 681}, page_content='18.4 Linear Classiﬁers with L1Regularization 663\\naged features. The elastic net penalty can be used with any linear model,\\nin particular for regression or classiﬁcation.\\nHence the multinomial problem above with elastic-net penalty becomes\\nmax\\n{β0k,βk∈I Rp}K\\n1\\uf8ee\\n\\uf8f0N∑\\ni=1log Pr( gi|xi)−λK∑\\nk=1p∑\\nj=1(\\nα|βkj|+ (1−α)β2\\nkj)\\uf8f9\\n\\uf8fb.\\n(18.21)\\nThe parameter αdetermines the mix of the penalties, and is often pre-\\nchosen on qualitative grounds. The elastic net can yield more that Nnon-\\nzero coeﬃcients when p > N , a potential advantage over the lasso. Line\\n(8) in Table 18.1 uses this model, with αandλchosen by cross-validation.\\nWe used a sequence of 20 values of αbetween 0 .05 and 1 .0, and a 100\\nvalues of λuniform on the log scale covering the entire range. Values of\\nα∈[0.75,0.80] gave the minimum CV error, with values of λ <0.001 for all\\ntied solutions. Although it has the lowest test error among all methods, the\\nmargin is small and not signiﬁcant. Interestingly, when CV is performed\\nseparately for each value of α, a minimum test error of 8 .8 is achieved at\\nα= 0.10, but this is not the value chosen in the two-dimensional CV.\\n−8 −7 −6 −5 −4 −3 −2 −10.0 0.1 0.2 0.3 0.4Misclassification ErrorTraining\\nTest\\n10−fold CV\\n−8 −7 −6 −5 −4 −3 −2 −10 5 10 15 20 25 30Deviance\\nlog(λ) log(λ)\\nFIGURE 18.6. Training, test, and 10-fold cross validation curves for lasso l ogis-\\ntic regression on the leukemia data. The left panel shows misc lassiﬁcation errors,\\nthe right panel shows deviance.\\nFigure 18.5 shows the lasso and elastic-net coeﬃcient paths on the two-\\nclass leukemia data (Golub et al., 1999). There are 7129 gene-expression\\nmeasurements on 38 samples, 27 of them in class ALL (acute lymphocytic\\nleukemia), and 11 in class AML (acute myelogenous leukemia). There is\\nalso a test set with 34 samples (20, 14). Since the data are linearly separa-\\nble, the solution is undeﬁned at λ= 0 (Exercise 18.11), and degrades for\\nvery small values of λ. Hence the paths have been truncated as the ﬁtted\\nprobabilities approach 0 and 1. There are 19 non-zero coeﬃcients in the\\nleft plot, and 39 in the right. Figure 18.6 (left panel) shows the misclas-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 682}, page_content='664 18. High-Dimensional Problems: p≫N\\nsiﬁcation errors for the lasso logistic regression on the training and t est\\ndata, as well as for 10-fold cross-validation on the training data. The ri ght\\npanel uses binomial deviance to measure errors, and is much smoother. The\\nsmall sample sizes lead to considerable sampling variance in these curves,\\neven though individual curves are relatively smooth (see, for example, Fig-\\nure 7.1 on page 220). Both of these plots suggest that the limiting solutio n\\nλ↓0 is adequate, leading to 3/34 misclassiﬁcations in the test set. The\\ncorresponding ﬁgures for the elastic net are qualitatively similar and are\\nnot shown.\\nForp≫N, the limiting coeﬃcients diverge for all regularized logistic\\nregression models, so in practical software implementations a minimum\\nvalue for λ >0 is either explicitly or implicitly set. However, renormalized\\nversions of the coeﬃcients converge, and these limiting solutions can be\\nthought of as interesting alternatives to the linear optimal separating hy-\\nperplane (SVM). With α= 0 the limiting solution coincides with the SVM\\n(see end of Section 18.3.2), but all the 7129 genes are selected. With α= 1,\\nthe limiting solution coincides with an L1separating hyperplane (Rosset\\net al., 2004a), and includes at most 38 genes. As αdecreases from 1, the\\nelastic-net solutions include more genes in the separating hyperplane.\\n18.4.1 Application of Lasso to Protein Mass Spectroscopy\\nProtein mass spectrometry has become a popular technology for analyzing\\nthe proteins in blood, and can be used to diagnose a disease or understand\\nthe processes underlying it.\\nFor each blood serum sample i, we observe the intensity xijfor many\\ntime of ﬂight values tj. This intensity is related to the number of particles\\nobserved to take approximately tjtime to pass from the emitter to the\\ndetector during a cycle of operation of the machine. The time of ﬂight has\\na known relationship to the mass over charge ratio ( m/z) of the constituent\\nproteins in the blood. Hence the identiﬁcation of a peak in the spectrum\\nat a certain tjtells us that there is a protein with a corresponding mass\\nand charge. The identity of this protein can then be determined by other\\nmeans.\\nFigure 18.7 shows an example taken from Adam et al. (2003). It shows\\nthe average spectra for healthy patients and those with prostate cancer.\\nThere are 16,898 m/zsites in total, ranging in value from 2000 to 40,000.\\nThe full dataset consists of 157 healthy patients and 167 with cancer, and\\nthe goal is to ﬁnd m/zsites that discriminate between the two groups.\\nThis is an example of functional data; the predictors can be viewed as a\\nfunction of m/z. There has been much interest in this problem in the past\\nfew years; see e.g. Petricoin et al. (2002).\\nThe data were ﬁrst standardized (baseline subtraction and normaliza-\\ntion), and we restricted attention to m/zvalues between 2000 and 40,000\\n(spectra outside of this range were not of interest). We then applied near-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 683}, page_content='18.4 Linear Classiﬁers with L1Regularization 665\\n2e+03 5e+03 1e+04 2e+04 5e+04 1e+05 2e+0510 20 30 40\\nm/zIntensityNormal\\nCancer\\nFIGURE 18.7. Protein mass spectrometry data: average proﬁles from normal\\nand prostate cancer patients.\\nest shrunken centroids and lasso regression to the data, with the results for\\nboth methods shown in Table 18.2.\\nBy ﬁtting harder to the data, the lasso achieves a considerably lower\\ntest error rate. However, it may not provide a scientiﬁcally useful solu-\\ntion. Ideally, protein mass spectrometry resolves a biological sample int o\\nits constituent proteins, and these should appear as peaks in the spectra.\\nThe lasso doesn’t treat peaks in any special way, so not surprisingly only\\nsome of the non-zero lasso weights were situated near peaks in the spectra.\\nFurthermore, the same protein may yield a peak at slightly diﬀerent m/z\\nvalues in diﬀerent spectra. In order to identify common peaks, some kind\\nofm/zwarping is needed from sample to sample.\\nTo address this, we applied a standard peak-extraction algorithm to each\\nspectrum, yielding a total of 5178 peaks in the 217 training spectra. Our\\nidea was to pool the collection of peaks from all patients, and hence con-\\nstruct a set of common peaks. For this purpose, we applied hierarchical\\nclustering to the positions of these peaks along the log m/zaxis. We cut\\nthe resulting dendrogram horizontally at height log(0 .005)3, and computed\\naverages of the peak positions in each resulting cluster. This process yielded\\n728 common clusters and their corresponding peak centers.\\nGiven these 728 common peaks, we determined which of these were\\npresent in each individual spectrum, and if present, the height of the peak.\\nA peak height of zero was assigned if that peak was not found. This pro-\\nduced a 217 ×728 matrix of peak heights as features, which was used in a\\nlasso regression. We scored the test spectra for the same 728 peaks.\\n3Use of the value 0 .005 means that peaks with positions less than 0.5% apart are\\nconsidered the same peak, a fairly common assumption.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 684}, page_content='666 18. High-Dimensional Problems: p≫N\\nTABLE 18.2. Results for the prostate data example. The standard deviation for\\nthe test errors is about 4.5.\\nMethod Test Errors/108 Number of Sites\\n1. Nearest shrunken centroids 34 459\\n2. Lasso 22 113\\n3. Lasso on peaks 28 35\\nThe prediction results for this application of the lasso to the peaks are\\nshown in the last line of Table 18.2: it does fairly well, but not as well\\nas the lasso on the raw spectra. However, the ﬁtted model may be more\\nuseful to the biologist as it yields 35 peak positions for further study. On\\nthe other hand, the results suggest that there may be useful discriminatory\\ninformation between the peaks of the spectra, and the positions of the lasso\\nsites from line (2) of the table also deserve further examination.\\n18.4.2 The Fused Lasso for Functional Data\\nIn the previous example, the features had a natural order, determined by\\nthe mass-to-charge ratio m/z. More generally, we may have functional fea-\\ntures xi(t) that are ordered according to some index variable t. We have\\nalready discussed several approaches for exploiting such structure.\\nWe can represent xi(t) by their coeﬃcients in a basis of functions in t,\\nsuch as splines, wavelets or Fourier bases, and then apply a regression using\\nthese coeﬃcients as predictors. Equivalently, one can instead represent the\\ncoeﬃcients of the original features in these bases. These approaches are\\ndescribed in Section 5.3.\\nIn the classiﬁcation setting, we discuss the analogous approach of penal-\\nized discriminant analysis in Section 12.6. This uses a penalty that explicitly\\ncontrols the resulting smoothness of the coeﬃcient vector.\\nThe above methods tend to smooth the coeﬃcients uniformly. Here we\\npresent a more adaptive strategy that modiﬁes the lasso penalty to take\\ninto account the ordering of the features. The fused lasso (Tibshirani et\\nal., 2005) solves\\nmin\\nβ∈I Rp{N∑\\ni=1(yi−β0−p∑\\nj=1xijβj)2+λ1p∑\\nj=1|βj|+λ2p−1∑\\nj=1|βj+1−βj|}\\n.(18.22)\\nThis criterion is strictly convex in β, so a unique solution exists. The ﬁrst\\npenalty encourages the solution to be sparse, while the second encourages\\nit to be smooth in the index j.\\nThe diﬀerence penalty in (18.22) assumes an uniformly spaced index j. If\\ninstead the underlying index variable thas nonuniform values tj, a natural\\ngeneralization of (18.22) would be based on divided diﬀerences'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 685}, page_content='18.4 Linear Classiﬁers with L1Regularization 667\\n0 200 400 600 800 1000−2 0 2 4\\nGenome orderlog2 ratio\\nFIGURE 18.8. Fused lasso applied to CGH data. Each point represents the\\ncopy-number of a gene in a tumor sample, relative to that of a cont rol (on the log\\nbase-2 scale).\\nλ2p−1∑\\nj=1|βj+1−βj|\\n|tj+1−tj|. (18.23)\\nThis amounts to having a penalty modiﬁer for each of the terms in the\\nseries.\\nA particularly useful special case arises when the predictor matrix X=\\nIN, theN×Nidentity matrix. This is a special case of the fused lasso,\\nused to approximate a sequence {yi}N\\n1. Thefused lasso signal approximator\\nsolves\\nmin\\nβ∈I RN{N∑\\ni=1(yi−β0−βi)2+λ1N∑\\ni=1|βi|+λ2N−1∑\\ni=1|βi+1−βi|}\\n.(18.24)\\nFigure 18.8 shows an example taken from Tibshirani and Wang (2007). The\\ndata in the panel come from a Comparative Genomic Hybridization (CGH)\\narray, measuring the approximate log (base-two) ratio of the number of\\ncopies of each gene in a tumor sample, as compared to a normal sample.\\nThe horizontal axis represents the chromosomal location of each gene. The\\nidea is that in cancer cells, genes are often ampliﬁed (duplicated) or deleted,\\nand it is of interest to detect these events. Furthermore, these events tend\\nto occur in contiguous regions. The smoothed signal estimate from the\\nfused lasso signal approximator is shown in dark red (with appropriately\\nchosen values for λ1andλ2). The signiﬁcantly nonzero regions can be used\\nto detect locations of gains and losses of genes in the tumor.\\nThere is also a two-dimensional version of the fused lasso, in which the\\nparameters are laid out in a grid of pixels, and a penalty is applied to the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 686}, page_content='668 18. High-Dimensional Problems: p≫N\\nﬁrst diﬀerences to the left, right, above and below the target pixel. This\\ncan be useful for denoising or classifying images. Friedman et al. (2007)\\ndevelop fast generalized coordinate descent algorithms for the one- and\\ntwo-dimensional fused lasso.\\n18.5 Classiﬁcation When Features are Unavailable\\nIn some applications the objects under study are more abstract in nature,\\nand it is not obvious how to deﬁne a feature vector. As long as we can ﬁll\\nin anN×Nproximity matrix of similarities between pairs of objects in our\\ndatabase, it turns out we can put to use many of the classiﬁers in our arsenal\\nby interpreting the proximities as inner-products. Protein structures fall\\ninto this category, and we explore an example in Section 18.5.1 below.\\nIn other applications, such as document classiﬁcation, feature vectors are\\navailable but can be extremely high-dimensional. Here we may not wish\\nto compute with such high-dimensional data, but rather store the inner-\\nproducts between pairs of documents. Often these inner-products can be\\napproximated by sampling techniques.\\nPairwise distances serve a similar purpose, because they can be turned\\ninto centered inner-products. Proximity matrices are discussed in more de-\\ntail in Chapter 14.\\n18.5.1 Example: String Kernels and Protein Classiﬁcation\\nAn important problem in computational biology is to classify proteins int o\\nfunctional and structural classes based on their sequence similarities. Pro-\\ntein molecules are strings of amino acids, diﬀering in both length and com-\\nposition. In the example we consider, the lengths vary between 75–160\\namino-acid molecules, each of which can be one of 20 diﬀerent types, labeled\\nusing letters. Here are two examples, of length 110 and 153, respectively:\\nIPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTLESQTVQGG TV\\nERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDY LQEFLGVMNTEWI\\nPHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAER LQENLQAYRTFHVLLA\\nRLLEDQQVHFTPTEGDFHQAIHTLLLQVAAFAYQIEELMILLEYKIPRNEADGMLFEKK\\nLWGLKV LQELSQWTVRSIHDLRFISSHQTGIP\\nThere have been many proposals for measuring the similarity between a\\npair of protein molecules. Here we focus on a measure based on the count\\nof matching substrings (Leslie et al., 2004), such as the LQEabove.\\nTo construct our features, we count the number of times that a given\\nsequence of length moccurs in our string, and we compute this number'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 687}, page_content='18.5 Classiﬁcation When Features are Unavailable 669\\nfor all possible sequences of length m. Formally, for a string x, we deﬁne a\\nfeature map\\nΦm(x) ={φa(x)}a∈Am (18.25)\\nwhere Amis the set of subsequences of length m, and φa(x) is the number\\nof times that “ a” occurs in our string x. Using this, we deﬁne the inner\\nproduct\\nKm(x1,x2) =⟨Φm(x1),Φm(x2)⟩, (18.26)\\nwhich measures the similarity between the two strings x1, x2.This can be\\nused to drive, for example, a support vector classiﬁer for classifying strings\\ninto diﬀerent protein classes.\\nNow the number of possible sequences ais|Am|= 20m, which can be\\nvery large for moderate m, and the vast majority of the subsequences do\\nnot match the strings in our training set. It turns out that we can compute\\ntheN×Ninner-product matrix or string kernel Km(18.26) eﬃciently\\nusing tree-structures, without actually computing the individual vectors.\\nThis methodology, and the data to follow, come from Leslie et al. (2004).4\\nThe data consist of 1708 proteins in two classes— negative (1663) and\\npositive (45). The two examples above, which we will call “ x1” and “ x2”,\\nare from this set. We have marked the occurrences of subsequence LQE,\\nwhich appears in both proteins. There are 203possible subsequences, so\\nΦ3(x) will be a vector of length 8000. For this example φLQE(x1) = 1 and\\nφLQE(x2) = 2.\\nUsing software from Leslie et al. (2004), we computed the string kernel\\nform= 4, which was then used in a support vector classiﬁer to ﬁnd the\\nmaximal margin solution in this 204= 160 ,000-dimensional feature space.\\nWe used 10-fold cross-validation to compute the SVM predictions on all of\\nthe training data. The orange curve in Figure 18.9 shows the cross-validated\\nROC curve for the support vector classiﬁer, computed by varying the cut-\\npoint on the real-valued predictions from the cross-validated support vector\\nclassiﬁer. The area under the curve is 0.84. Leslie et al. (2004) show that\\nthe string kernel method is competitive with, but perhaps not as accurate\\nas, more specialized methods for protein string matching.\\nMany other classiﬁers can be computed using only the information in the\\nkernel matrix; some details are given in the next section. The results for\\nthe nearest centroid classiﬁer (green), and distance-weighted one-nearest\\nneighbors (blue) are shown in Figure 18.9. Their performance is similar to\\nthat of the support vector classiﬁer.\\n4We thank Christina Leslie for her help and for providing the d ata, which is available\\non our book website.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 688}, page_content='670 18. High-Dimensional Problems: p≫N\\n0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0ROC Curves for String Kernel\\nSpecificitySensitivity\\nSVM  0.84\\nNearest Centroid  0.84\\nOne−Nearest Neighbor 0.86\\nFIGURE 18.9. Cross-validated ROC curves for protein example using the stri ng\\nkernel. The numbers next to each method in the legend give the area u nder the\\ncurve, an overall measure of accuracy. The SVM achieves bette r sensitivities than\\nthe other two, which achieve better speciﬁcities.\\n18.5.2 Classiﬁcation and Other Models Using Inner-Product\\nKernels and Pairwise Distances\\nThere are a number of other classiﬁers, besides the support-vector ma-\\nchine, that can be implemented using only inner-product matrices. This\\nalso implies they can be “kernelized” like the SVM.\\nAn obvious example is nearest-neighbor classiﬁcation, since we can trans-\\nform pairwise inner-products to pairwise distances:\\n||xi−xi′||2=⟨xi,xi⟩+⟨xi′,xi′⟩ −2⟨xi,xi′⟩. (18.27)\\nA variation of 1-NN classiﬁcation is used in Figure 18.9, which produces\\na continuous discriminant score needed to construct a ROC curve. This\\ndistance-weighted 1-NN makes use of the distance of a test points to the\\nclosest member of each class; see Exercise 18.14.\\nNearest-centroid classiﬁcation follows easily as well. For training pair s\\n(xi,gi), i= 1,... ,N , a test point x0, and class centroids ¯ xk,k= 1,... ,K\\nwe can write\\n||x0−¯xk||2=⟨x0,x0⟩ −2\\nNk∑\\ngi=k⟨x0,xi⟩+1\\nN2\\nk∑\\ngi=k∑\\ngi′=k⟨xi,xi′⟩,(18.28)'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 689}, page_content='18.5 Classiﬁcation When Features are Unavailable 671\\nHence we can compute the distance of the test point to each of the cen-\\ntroids, and perform nearest centroid classiﬁcation. This also implies that\\nmethods like K-means clustering can also be implemented, using only the\\ninner products of the data points.\\nLogistic and multinomial regression with quadratic regularization can\\nalso be implemented with inner-product kernels; see Section 12.3.3 and\\nExercise 18.13. Exercise 12.10 derives linear discriminant analysis using a n\\ninner-product kernel.\\nPrincipal components can be computed using inner-product kernels as\\nwell; since this is frequently useful, we give some details. Suppose ﬁrst\\nthat we have a centered data matrix X, and let X=UDVTbe its SVD\\n(18.12). Then Z=UDis the matrix of principal component variables (see\\nSection 14.5.1). But if K=XXT, then it follows that K=UD2UT, and\\nhence we can compute Zfrom the eigen decomposition of K. IfXisnot\\ncentered, then we can center it using ˜X= (I−M)X, where M=1\\nN11T\\nis the mean operator. Thus we compute the eigenvectors of the double-\\ncentered kernel ( I−M)K(I−M) for the principal components from an\\nuncentered inner-product matrix. Exercise 18.15 explores this further, and\\nSection 14.5.4 discusses in more detail kernel PCA for general kernels, such\\nas the radial kernel used in SVMs.\\nIf instead we had available only the pairwise (squared) Euclidean dis-\\ntances between observations,\\n∆2\\nii′=||xi−xi′||2, (18.29)\\nit turns out we can do all of the above as well. The trick is to convert the\\npairwise distances to centered inner-products, and then proceed as before.\\nWe write\\n∆2\\nii′=||xi−¯x||2+||xi′−¯x||2−2⟨xi−¯x,xi′−¯x⟩. (18.30)\\nDeﬁning B={−∆2\\nii′/2}, we double center B:\\n˜K= (I−M)B(I−M); (18.31)\\nit is easy to check that ˜Kii′=⟨xi−¯x,xi′−¯x⟩, the centered inner-product\\nmatrix.\\nDistances and inner-products also allow us to compute the medoid in each\\nclass—the observation with smallest average distance to other observations\\nin that class. This can be used for classiﬁcation (closest medoids), as well as\\nto drive k-medoids clustering (Section 14.3.10). With abstract data objects\\nlike proteins, medoids have a practical advantage over means. The medoid is\\none of the training examples, and can be displayed. We tried closest medoids\\nin the example in the next section (see Table 18.3), and its performance is\\ndisappointing.\\nIt is useful to consider what we cannot do with inner-product kernels and\\ndistances:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 690}, page_content='672 18. High-Dimensional Problems: p≫N\\nTABLE 18.3. Cross-validated error rates for the abstracts example. The ne arest\\nshrunken centroids ended up using no-shrinkage, but does use a word -by-word\\nstandardization (section 18.2). This standardization gives it a distinct advantage\\nover the other methods.\\nMethod CV Error (SE)\\n1. Nearest shrunken centroids 0.17 (0.05)\\n2. SVM 0.23 (0.06)\\n3. Nearest medoids 0.65 (0.07)\\n4. 1-NN 0.44 (0.07)\\n5. Nearest centroids 0.29 (0.07)\\n•We cannot standardize the variables; standardization signiﬁcantly im-\\nproves performance in the example in the next section.\\n•We cannot assess directly the contributions of individual variables.\\nIn particular, we cannot perform individual t-tests, ﬁt the nearest\\nshrunken centroids model, or ﬁt any model that uses the lasso penalty.\\n•We cannot separate the good variables from the noise: all variables get\\nan equal say. If, as is often the case, the ratio of relevant to irrelevant\\nvariables is small, methods that use kernels are not likely to work as\\nwell as methods that do feature selection.\\n18.5.3 Example: Abstracts Classiﬁcation\\nThis somewhat whimsical example serves to illustrate a limitation of ker -\\nnel approaches. We collected the abstracts from 48 papers, 16 each from\\nBradley Efron (BE), Trevor Hastie and Rob Tibshirani (HT) (frequent co-\\nauthors), and Jerome Friedman (JF). We extracted all unique words from\\nthese abstracts, and deﬁned features xijto be the number of times word\\njappears in abstract i. This is the so-called bag of words representation.\\nQuotations, parentheses and special characters were ﬁrst removed from the\\nabstracts, and all characters were converted to lower case. We also removed\\nthe word “we”, which could unfairly discriminate HT abstracts from the\\nothers.\\nThere were 4492 total words, of which p= 1310 were unique. We sought\\nto classify the documents into BE, HT or JF on the basis of the features\\nxij. Although it is artiﬁcial, this example allows us to assess the possible\\ndegradation in performance if information speciﬁc to the raw features is\\nnot used.\\nWe ﬁrst applied the nearest shrunken centroid classiﬁer to the data, using\\n10-fold cross-validation. It essentially chose no shrinkage, and so used all the\\nfeatures; see the ﬁrst line of Table 18.3. The error rate is 17%; the number\\nof features can be reduced to about 500 without much loss in accuracy.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 691}, page_content='18.5 Classiﬁcation When Features are Unavailable 673\\nNote that the nearest shrunken classiﬁer requires the raw feature matrix\\nXin order to standardize the features individually. Figure 18.10 shows the\\npredictivebayesusingthanalgorithmareproceduretechnologyvaluesaccuracyvariableswheninferencethosebayesianfrequentistproposepresentedmethodproblemsBE HT JF\\nFIGURE 18.10. Abstracts example: top 20scores from nearest shrunken cen-\\ntroids. Each score is the standardized diﬀerence in frequency f or the word in the\\ngiven class (BE, HT or JF) versus all classes. Thus a positive score (to the right\\nof the vertical grey zero lines) indicates a higher frequency in that class; a negative\\nscore indicates a lower relative frequency.\\ntop 20 discriminating words, with a positive score indicating that a word\\nappears more in that class than in the other classes.\\nSome of these terms make sense: for example “frequentist” and “Bayesian”\\nreﬂect Efron’s greater emphasis on statistical inference. However, many oth-\\ners are surprising, and reﬂect personal writing styles: for example, Fried-\\nman’s use of “presented” and HT’s use of “propose”.\\nWe then applied the support vector classiﬁer with linear kernel and no\\nregularization, using the “all pairs” ( ovo) method to handle the three\\nclasses (regularization of the SVM did not improve its performance). The\\nresult is shown in Table 18.3. It does somewhat worse than the nearest\\nshrunken centroid classiﬁer.\\nAs mentioned, the ﬁrst line of Table 18.3 represents nearest shrunken cen-\\ntroids (with no shrinkage). Denote by sjthe pooled within-class standard\\ndeviation for feature j, and s0the median of the sjvalues. Then line (1)\\nalso corresponds to nearest centroid classiﬁcation, after ﬁrst standardizing\\neach feature by sj+s0[recall (18.4) on page 652].\\nLine (3) shows that the performance of nearest medoids is very poor,\\nsomething which surprised us. It is perhaps due to the small sample sizes'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 692}, page_content='674 18. High-Dimensional Problems: p≫N\\nand high dimensions, with medoids having much higher variance than\\nmeans. The performance of the one-nearest neighbor classiﬁer is also poor.\\nThe performance of the nearest centroid classiﬁer is also shown in Ta-\\nble 18.3 in line (5): it is better than nearest medoids, but worse than that\\nof nearest shrunken centroids, even with no shrinkage. The diﬀerence seems\\nto be the standardization of each feature that is done in nearest shrunken\\ncentroids. This standardization is important here, and requires access to\\nthe individual feature values. Nearest centroids uses a spherical metric, and\\nrelies on the fact that the features are in similar units. The support vector\\nmachine estimates a linear combination of the features and can better deal\\nwith unstandardized features.\\n18.6 High-Dimensional Regression: Supervised\\nPrincipal Components\\nIn this section we describe a simple approach to regression and generalized\\nregression that is especially useful when p≫N. We illustrate the method\\non another microarray data example. The data is taken from Rosenwald\\net al. (2002) and consists of 240 samples from patients with diﬀuse large\\nB-cell lymphoma (DLBCL), with gene expression measurements for 7399\\ngenes. The outcome is survival time, either observed or right censored. We\\nrandomly divided the lymphoma samples into a training set of size 160 and\\na test set of size 80.\\nAlthough supervised principal components is useful for linear regression,\\nits most interesting applications may be in survival studies, which is the\\nfocus of this example.\\nWe have not yet discussed regression with censored survival data in this\\nbook; it represents a generalized form of regression in which the outcome\\nvariable (survival time) is only partly observed for some individuals. Sup-\\npose for example we carry out a medical study that lasts for 365 days, and\\nfor simplicity all subjects are recruited on day one. We might observe one\\nindividual to die 200 days after the start of the study. Another individ-\\nual might still be alive at 365 days when the study ends. This individual\\nis said to be “right censored” at 365 days. We know only that he or she\\nlivedat least 365 days. Although we do not know how long past 365 days\\nthe individual actually lived, the censored observation is still informative.\\nThis is illustrated in Figure 18.11. Figure 18.12 shows the survival cur ve\\nestimated by the Kaplan–Meier method for the 80 patients in the test set.\\nSee for example Kalbﬂeisch and Prentice (1980) for a description of the\\nKaplan–Meier method.\\nOur objective in this example is to ﬁnd a set of features (genes) that\\ncan predict the survival of an independent set of patients. This could be'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 693}, page_content='18.6 High-Dimensional Regression: Supervised Principal Components 675\\nTime(days)Patient\\n0 100 200 300 365 1234\\nFIGURE 18.11. Censored survival data. For illustration there are four pati ents.\\nThe ﬁrst and third patients die before the study ends. The second pa tient is alive\\nat the end of the study ( 365days), while the fourth patient is lost to follow-up\\nbefore the study ends. For example, this patient might have move d out of the\\ncountry. The survival times for patients two and four are said to be “censored.”\\n0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0l\\nlllllllll lll\\nll ll ll l lllll\\nl ll l l l lPr(T≥t)\\nMonths tSurvival Function\\nFIGURE 18.12. Lymphoma data. The Kaplan–Meier estimate of the survival\\nfunction for the 80patients in the test set, along with one-standard-error curves.\\nThe curve estimates the probability of surviving past tmonths. The ticks indicate\\ncensored observations.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 694}, page_content='676 18. High-Dimensional Problems: p≫Nprobability densityPoor Cell Type Good Cell Type\\nSurvival Time\\nFIGURE 18.13. Underlying conceptual model for supervised principal compo-\\nnents. There are two cell types, and patients with the good cell ty pe live longer on\\nthe average. Supervised principal components estimate the cell type, by averaging\\nthe expression of genes that reﬂect it.\\nuseful as a prognostic indicator to aid in choosing treatments, or to help\\nunderstand the biological basis for the disease.\\nThe underlying conceptual model for supervised principal components\\nis shown in Figure 18.13. We imagine that there are two cell types, and\\npatients with the good cell type live longer on the average. However there\\nis considerable overlap in the two sets of survival times. We might think\\nof survival time as a “noisy surrogate” for cell type. A fully supervised\\napproach would give the most weight to those genes having the strongest\\nrelationship with survival. These genes are partially, but not perfectly, re-\\nlated to cell type. If we could instead discover the underlying cell types of\\nthe patients, often reﬂected by a sizable signature of genes acting together\\nin pathways, then we might do a better job of predicting patient survival.\\nAlthough the cell type in Figure 18.13 is discrete, it is useful to imagine\\na continuous cell type, deﬁne by some linear combination of the features.\\nWe will estimate the cell type as a continuous quantity, and then discretize\\nit for display and interpretation.\\nHow can we ﬁnd the linear combination that deﬁnes the important under-\\nlying cell types? Principal components analysis (Section 14.5) is an eﬀective\\nmethod for ﬁnding linear combinations of features that exhibit large varia-\\ntion in a dataset. But what we seek here are linear combinations with both\\nhigh variance andsigniﬁcant correlation with the outcome. The lower right\\npanel of Figure 18.14 shows the result of applying standard principal com-\\nponents in this example; the leading component does not correlate strongly\\nwith survival (details are given in the ﬁgure caption).\\nHence we want to encourage principal component analysis to ﬁnd linear\\ncombinations of features that have high correlation with the outcome. To\\ndo this, we restrict attention to features which by themselves have a siz-\\nable correlation with the outcome. This is summarized in the supervised\\nprincipal components Algorithm 18.1, and illustrated in Figure 18.14.\\nThe details in steps (1) and (2b) will depend on the type of outcome\\nvariable. For a standard regression problem, we use the univariate linear\\nleast squares coeﬃcients in step (1) and a linear least squares model in'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 695}, page_content='18.6 High-Dimensional Regression: Supervised Principal Components 677\\n7399 7350 50 27 1 Genes\\nPatientsSupervised PC\\n1 80 160\\nAbsolute Cox Score0 2 40 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0Probability of Survivallow score\\nhigh scoreBest Single Gene\\nP=0.15\\n0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0Probability of SurvivalSupervised Principal Component − 27 Genes\\nP=0.006\\n0 5 10 15 200.0 0.2 0.4 0.6 0.8 1.0\\nMonthsProbability of SurvivalPrincipal Component − 7399 Genes\\nP=0.14\\nFIGURE 18.14. Supervised principal components on the lymphoma data. The\\nleft panel shows a heatmap of a subset of the gene-expression tra ining data. The\\nrows are ordered by the magnitude of the univariate Cox-score, s hown in the mid-\\ndle vertical column. The top 50 and bottom 50genes are shown. The supervised\\nprincipal component uses the top 27genes (chosen by 10-fold CV). It is repre-\\nsented by the bar at the top of the heatmap, and is used to order th e columns\\nof the expression matrix. In addition, each row is multiplied by the sign of the\\nCox-score. The middle panel on the right shows the survival cur ves on the test\\ndata when we create a low and high group by splitting this superv ised PC at zero\\n(training data mean). The curves are well separated, as indicate d by the p-value\\nfor the log-rank test. The top panel does the same, using the top- scoring gene on\\nthe training data. The curves are somewhat separated, but not si gniﬁcantly. The\\nbottom panel uses the ﬁrst principal component on all the genes, and t he separa-\\ntion is also poor. Each of the top genes can be interpreted as nois y surrogates for\\na latent underlying cell-type characteristic, and supervised p rincipal components\\nuses them all to estimate this latent factor.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 696}, page_content='678 18. High-Dimensional Problems: p≫N\\nAlgorithm 18.1 Supervised Principal Components.\\n1. Compute the standardized univariate regression coeﬃcients for the\\noutcome as a function of each feature separately.\\n2. For each value of the threshold θfrom the list 0 ≤θ1< θ2<≤≤≤< θK:\\n(a) Form a reduced data matrix consisting of only those features\\nwhose univariate coeﬃcient exceeds θin absolute value, and\\ncompute the ﬁrst mprincipal components of this matrix.\\n(b) Use these principal components in a regression model to predict\\nthe outcome.\\n3. Pick θ(andm) by cross-validation.\\nstep (2b). For survival problems, Cox’s proportional hazards regression\\nmodel is widely used; hence we use the score test from this model in step (1)\\nand the multivariate Cox model in step (2b). The details are not essential\\nfor understanding the basic method; they may be found in Bair et al. (2006).\\nFigure 18.14 shows the results of supervised principal components in this\\nexample. We used a Cox-score cutoﬀ of 3.53, yielding 27 genes, where the\\nvalue 3.53 was found through 10-fold cross-validation. We then computed\\nthe ﬁrst principal component ( m= 1) using just this subset of the data,\\nas well as its value for each of the test observations. We included this as\\na quantitative predictor in a Cox regression model, and its likelihood-rati o\\nsigniﬁcance was p= 0.005. When dichotomized (using the mean score on\\nthe training data as a threshold), it clearly separates the patients in the\\ntest set into low and high risk groups (middle-right panel of Figure 18.14,\\np= 0.006).\\nThe top-right panel of Figure 18.14 uses the top scoring gene (dichot-\\nomized) alone as a predictor of survival. It is not signiﬁcant on the test set.\\nLikewise, the lower-right panel shows the dichotomized principal compo-\\nnent using all the training data, which is also not signiﬁcant.\\nOur procedure allows m >1 principal components in step (2a). However,\\nthe supervision in step (1) encourages the principal components to align\\nwith the outcome, and thus in most cases only the ﬁrst or ﬁrst few com-\\nponents tend to be useful for prediction. In the mathematical development\\nbelow, we consider only the ﬁrst component, but extensions to more than\\none component can be derived in a similar way.\\n18.6.1 Connection to Latent-Variable Modeling\\nA formal connection between supervised principal components and the un-\\nderlying cell type model (Figure 18.13) can be seen through a latent variable\\nmodel for the data. Suppose we have a response variable Ywhich is related'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 697}, page_content='18.6 High-Dimensional Regression: Supervised Principal Components 679\\nto an underlying latent variable Uby a linear model\\nY=β0+β1U+ε. (18.32)\\nIn addition, we have measurements on a set of features Xjindexed by j∈ P\\n(for pathway), for which\\nXj=α0j+α1jU+ǫj, j∈ P. (18.33)\\nThe errors εandǫjare assumed to have mean zero and are independent of\\nall other random variables in their respective models.\\nWe also have many additional features Xk,k̸∈ Pwhich are independent\\nofU. We would like to identify P, estimate U, and hence ﬁt the predic-\\ntion model (18.32). This is a special case of a latent-structure model, or\\nsingle-component factor-analysis model (Mardia et al., 1979, see also Sec-\\ntion 14.7). The latent factor Uis a continuous version of the cell type\\nconceptualized in Figure 18.13.\\nThe supervised principal component algorithm can be seen as a method\\nfor ﬁtting this model:\\n•The screening step (1) estimates the set P.\\n•Given ˆP, the largest principal component in step (2a) estimates the\\nlatent factor U.\\n•Finally, the regression ﬁt in step (2b) estimates the coeﬃcient in\\nmodel (18.32).\\nStep (1) is natural, since on average the regression coeﬃcient is nonzero\\nonly if α1jis non-zero. Hence this step should select the features j∈ P.\\nStep (2a) is natural if we assume that the errors ǫjhave a Gaussian dis-\\ntribution, with the same variance. In this case the principal component is\\nthe maximum likelihood estimate for the single factor model (Mardia et\\nal., 1979). The regression in (2b) is an obvious ﬁnal step.\\nSuppose there are a total of pfeatures, with p1features in the relevant set\\nP. Then if pandp1grow but p1is small relative to p, one can show (under\\nreasonable conditions) that the leading supervised principal component\\nis consistent for the underlying latent factor. The usual leading principal\\ncomponent may not be consistent, since it can be contaminated by the\\npresence of a large number of “noise” features.\\nFinally, suppose that the threshold used in step (1) of the supervised\\nprincipal component procedure yields a large number of features for com-\\nputation of the principal component. Then for interpretational purposes, as\\nwell as for practical uses, we would like some way of ﬁnding a reduced a set\\nof features that approximates the model. Pre-conditioning (Section 18.6.3)\\nis one way of doing this.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 698}, page_content='680 18. High-Dimensional Problems: p≫N\\n18.6.2 Relationship with Partial Least Squares\\nSupervised principal components is closely related to partial least squares\\nregression (Section 3.5.2). Bair et al. (2006) found that the key to the goo d\\nperformance of supervised principal components was the ﬁltering out of\\nnoisy features in step (2a). Partial least squares (Section 3.5.2) downwei ghts\\nnoisy features, but does not throw them away; as a result a large number\\nof noisy features can contaminate the predictions. However, a modiﬁcation\\nof the partial least squares procedure has been proposed that has a similar\\nﬂavor to supervised principal components [Brown et al. (1991),Nadler and\\nCoifman (2005), for example]. We select the features as in steps (1) and\\n(2a) of supervised principal components, but then apply PLS (rather than\\nprincipal components) to these features. For our current discussion, we call\\nthis “thresholded PLS.”\\nThresholded PLS can be viewed as a noisy version of supervised principal\\ncomponents, and hence we might not expect it to work as well in practice.\\nAssume the variables are all standardized. The ﬁrst PLS variate has the\\nform\\nz=∑\\nj∈P⟨y,xj⟩xj, (18.34)\\nand can be thought of as an estimate of the latent factor Uin model (18.33).\\nIn contrast, the supervised principal components direction ˆusatisﬁes\\nˆu=1\\nd2∑\\nj∈P⟨ˆu,xj⟩xj, (18.35)\\nwhere dis the leading singular value of XP. This follows from the deﬁnition\\nof the leading principal component. Hence thresholded PLS uses weights\\nwhich are the inner product of ywith each of the features, while supervised\\nprincipal components uses the features to derive a “self-consistent” estimate\\nˆu. Since many features contribute to the estimate ˆu, rather than just the\\nsingle outcome y, we can expect ˆuto be less noisy than z. In fact, if there\\narep1features in the set P, andN, pandp1go to inﬁnity with p1/N→0,\\nthen it can be shown using the techniques in Bair et al. (2006) that\\nz=u+Op(1)\\nˆu=u+Op(√\\np1/N), (18.36)\\nwhere uis the true (unobservable) latent variable in the model (18.32),\\n(18.33).\\nWe now present a simulation example to compare the methods numeri-\\ncally. There are N= 100 samples and p= 5000 genes. We generated the\\ndata as follows:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 699}, page_content='18.6 High-Dimensional Regression: Supervised Principal Components 681\\nFIGURE 18.15. Heatmap of the outcome (left column) and ﬁrst 500genes from\\na realization from model (18.37). The genes are in the columns, and the samples\\nare in the rows.\\nxij={\\n3 +ǫijifi≤50,\\n4 +ǫijifi >50j= 1,... ,50\\nxij={\\n1.5 +ǫijif 1≤i≤25 or 51 ≤i≤75\\n5.5 +ǫijif 26≤i≤50 or 76 ≤i≤100j= 51,... ,250\\nxij=ǫij j= 251 ,... ,5000\\nyi= 2≤1\\n50∑50\\nj=1xij+εi\\n(18.37)\\nwhere ǫijandεiare independent normal random variables with mean 0 and\\nstandard deviations 1 and 1.5, respectively. Thus in the ﬁrst 50 genes, there\\nis an average diﬀerence of 1 unit between samples 1–50 and 51–100, and this\\ndiﬀerence correlates with the outcome y. The next 200 genes have a large\\naverage diﬀerence of 4 units between samples (1–25, 51–75) and (26–50,\\n76–100), but this diﬀerence is uncorrelated with the outcome. The rest of\\nthe genes are noise. Figure 18.15 shows a heatmap of a typical realization,\\nwith the outcome at the left, and the ﬁrst 500 genes to the right.\\nWe generated 100 simulations from this model, and summarize the test\\nerror results in Figure 18.16. The test errors of principal components and\\npartial least squares are shown at the right of the plot; both are badly\\naﬀected by the noisy features in the data. Supervised principal components\\nand thresholded PLS work best over a wide range of the number of selected\\nfeatures, with the former showing consistently lower test errors.\\nWhile this example seems “tailor-made” for supervised principal com-\\nponents, its good performance seems to hold in other simulated and real\\ndatasets (Bair et al., 2006).\\n18.6.3 Pre-Conditioning for Feature Selection\\nSupervised principal components can yield lower test errors than competing\\nmethods, as shown in Figure 18.16. However, it does not always produce a\\nsparse model involving only a small number of features (genes). Even if the\\nthresholding in Step (1) of the algorithm yields a relatively small number'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 700}, page_content='682 18. High-Dimensional Problems: p≫N1.00 1.05 1.10 1.15 1.20 1.25\\nNumber of FeaturesRelative Root Mean Square Test Error\\n0 50 100 150 200 250 300 ... 5000Thresholded PLS\\nSupervised Principal Components\\nFIGURE 18.16. Root mean squared test error ( ±one standard error), for\\nsupervised principal components and thresholded PLS on 100realizations from\\nmodel (18.37). All methods use one component, and the errors are r elative to\\nthe noise standard deviation (the Bayes error is 1.0). For both methods, diﬀerent\\nvalues for the ﬁltering threshold were tried and the number of fea tures retained\\nis shown on the horizontal axis. The extreme right points corresp ond to regular\\nprincipal components and partial least squares, using all the gene s.\\nof features, it may be that some of the omitted features have sizable inner\\nproducts with the supervised principal component (and could act as a good\\nsurrogate). In addition, highly correlated features will tend to be chosen\\ntogether, and there may be great deal of redundancy in the set of selected\\nfeatures.\\nThe lasso (Sections 18.4 and 3.4.2), on the other hand, produces a sparse\\nmodel from the data. How do the test errors of the two methods compare on\\nthe simulated example of the last section? Figure 18.17 shows the test errors\\nfor one realization from model (18.37) for the lasso, supervised principal\\ncomponents, and the pre-conditioned lasso (described below).\\nWe see that supervised principal components (orange curve) reaches its\\nlowest error when about 50 features are included in the model, which is\\nthe correct number for the simulation. Although a linear model in the ﬁrst\\n50 features is optimal, the lasso (green) is adversely aﬀected by the large\\nnumber of noisy features, and starts overﬁtting when far fewer are in the\\nmodel.\\nCan we get the low test error of supervised principal components along\\nwith the sparsity of the lasso? This is the goal of pre-conditioning (Paul\\net al., 2008). In this approach, one ﬁrst computes the supervised principal\\ncomponent predictor ˆ yifor each observation in the training set (with the'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 701}, page_content='18.7 Feature Assessment and the Multiple-Testing Problem 683\\n0 50 100 150 200 2503.0 3.2 3.4 3.6 3.8 4.0 4.2 4.4\\nNumber of Features in ModelMean Test ErrorLasso\\nSupervised Principal Components\\nPreconditioned Lasso\\nFIGURE 18.17. Test errors for the lasso, supervised principal components,\\nand pre-conditioned lasso, for one realization from model (18.3 7). Each model is\\nindexed by the number of non-zero features. The supervised princip al component\\npath is truncated at 250features. The lasso self-truncates at 100, the sample size\\n(see Section 18.4). In this case, the pre-conditioned lasso ach ieves the lowest error\\nwith about 25features.\\nthreshold selected by cross-validation). Then we apply the lasso with ˆ yias\\nthe outcome variable, in place of the usual outcome yi. All features are used\\nin the lasso ﬁt, not just those that were retained in the thresholding step\\nin supervised principal components. The idea is that by ﬁrst denoising the\\noutcome variable, the lasso should not be as adversely aﬀected by the large\\nnumber of noise features. Figure 18.17 shows that pre-conditioning (purple\\ncurve) has been successful here, yielding much lower test error than the\\nusual lasso, and as low (in this case) as for supervised principal components.\\nIt also can achieve this using less features. The usual lasso, applied to\\nthe raw outcome, starts to overﬁt more quickly than the pre-conditioned\\nversion. Overﬁtting is not a problem, since the outcome variable has been\\ndenoised. We usually select the tuning parameter for the pre-conditioned\\nlasso on more subjective grounds, like parsimony.\\nPre-conditioning can be applied in a variety of settings, using initial\\nestimates other than supervised principal components and post-processors\\nother than the lasso. More details may be found in Paul et al. (2008).\\n18.7 Feature Assessment and the Multiple-Testing\\nProblem\\nIn the ﬁrst part of this chapter we discuss prediction models in the p≫N\\nsetting. Here we consider the more basic problem of assessing the signif-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 702}, page_content='684 18. High-Dimensional Problems: p≫N\\nicance of each of the pfeatures. Consider the protein mass spectrometry\\nexample of Section 18.4.1. In that problem, the scientist might not be inter-\\nested in predicting whether a given patient has prostate cancer. Rather the\\ngoal might be to identify proteins whose abundance diﬀers between nor-\\nmal and cancer samples, in order to enhance understanding of the disease\\nand suggest targets for drug development. Thus our goal is to assess the\\nsigniﬁcance of individual features. This assessment is usually done without\\nthe use of a multivariate predictive model like those in the ﬁrst part of this\\nchapter. The feature assessment problem moves our focus from prediction\\nto the traditional statistical topic of multiple hypothesis testing . For the\\nremainder of this chapter we will use Minstead of pto denote the number\\nof features, since we will frequently be referring to p-values .\\nTABLE 18.4. Subset of the 12,625genes from microarray study of radiation\\nsensitivity. There are a total of 44samples in the normal group and 14in the\\nradiation sensitive group; we only show three samples from eac h group.\\nNormal Radiation Sensitive\\nGene 1 7.85 29.74 29.50 . . . 17.20 -50.75 -18.89 . . .\\nGene 2 15.44 2.70 19.37 . . . 6.57 -7.41 79.18 . . .\\nGene 3 -1.79 15.52 -3.13 . . . -8.32 12.64 4.75 . . .\\nGene 4 -11.74 22.35 -36.11 . . . -52.17 7.24 -2.32 . . .\\n...........................\\nGene 12,625 -14.09 32.77 57.78 . . . -32.84 24.09 -101.44 . . .\\nConsider, for example, the microarray data in Table 18.4, taken from a\\nstudy on the sensitivity of cancer patients to ionizing radiation treatment\\n(Rieger et al., 2004). Each row consists of the expression of genes in 58\\npatient samples: 44 samples were from patients with a normal reaction, and\\n14 from patients who had a severe reaction to radiation. The measurements\\nwere made on oligo-nucleotide microarrays. The object of the experiment\\nwas to ﬁnd genes whose expression was diﬀerent in the radiation sensitive\\ngroup of patients. There are M= 12,625 genes altogether; the table shows\\nthe data for some of the genes and samples for illustration.\\nTo identify informative genes, we construct a two-sample t-statistic for\\neach gene.\\ntj=¯x2j−¯x1j\\nsej, (18.38)\\nwhere ¯ xkj=∑\\ni∈Cℓxij/Nℓ. Here Cℓare the indices of the Nℓsamples in\\ngroup ℓ, where ℓ= 1 is the normal group and ℓ= 2 is the sensitive group.\\nThe quantity se jis the pooled within-group standard error for gene j:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 703}, page_content='18.7 Feature Assessment and the Multiple-Testing Problem 685\\n−4 −2 0 2 40 200 400 600 800\\nt−statistics\\nFIGURE 18.18. Radiation sensitivity microarray example. A histogram of the\\n12,625t-statistics comparing the radiation-sensitive versus insensit ive groups.\\nOverlaid in blue is the histogram of the t-statistics from 1000permutations of the\\nsample labels.\\nsej= ˆσj√\\n1\\nN1+1\\nN2; ˆσ2\\nj=1\\nN1+N2−2(∑\\ni∈C1(xij−¯x1j)2+∑\\ni∈C2(xij−¯x2j)2)\\n.\\n(18.39)\\nA histogram of the 12,625 t-statistics is shown in orange in Figure 18 .18,\\nranging in value from −4.7 to 5.0. If the tjvalues were normally distributed\\nwe could consider any value greater than two in absolute value to be sig-\\nniﬁcantly large. This would correspond to a signiﬁcance level of about 5%.\\nHere there are 1189 genes with |tj| ≥2. However with 12,625 genes we\\nwould expect many large values to occur by chance, even if the group-\\ning is unrelated to any gene. For example, if the genes were independent\\n(which they are surely not), the number of falsely signiﬁcant genes would\\nhave a binomial distribution with mean 12 ,625≤0.05 = 631 .3 and standard\\ndeviation 24.5; the actual 1189 is way out of range.\\nHow do we assess the results for all 12,625 genes? This is called the mul-\\ntiple testing problem. We can start as above by computing a p-value for\\neach gene. This can be done using the theoretical t-distribution probabil-\\nities, which assumes the features are normally distributed. An attractive\\nalternative approach is to use the permutation distribution, since it avoids\\nassumptions about the distribution of the data. We compute (in principle)\\nallK=(58\\n14)\\npermutations of the sample labels, and for each permutation\\nkcompute the t-statistics tk\\nj. Then the p-value for gene jis'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 704}, page_content='686 18. High-Dimensional Problems: p≫N\\npj=1\\nKK∑\\nk=1I(|tk\\nj|>|tj|). (18.40)\\nOf course,(58\\n14)\\nis a large number (around 1013) and so we can’t enumer-\\nate all of the possible permutations. Instead we take a random sample of\\nthe possible permutations; here we took a random sample of K= 1000\\npermutations.\\nTo exploit the fact that the genes are similar (e.g., measured on the\\nsame scale), we can instead pool the results for all genes in computing the\\np-values.\\npj=1\\nMKM∑\\nj′=1K∑\\nk=1I(|tk\\nj′|>|tj|). (18.41)\\nThis also gives more granular p-values than does (18.40), since there many\\nmore values in the pooled null distribution than there are in each individual\\nnull distribution.\\nUsing this set of p-values, we would like to test the hypotheses:\\nH0j= treatment has no eﬀect on gene j\\nversus (18.42)\\nH1j= treatment has an eﬀect on gene j\\nfor all j= 1,2,... ,M . We reject H0jat level αifpj< α. This test has\\ntype-I error equal to α; that is, the probability of falsely rejecting H0jisα.\\nNow with many tests to consider, it is not clear what we should use\\nas an overall measure of error. Let Ajbe the event that H0jis falsely\\nrejected; by deﬁnition Pr( Aj) =α. The family-wise error rate (FWER)\\nis the probability of at least one false rejection, and is a commonly used\\noverall measure of error. In detail, if A=∪M\\nj=1Ajis the event of at least\\none false rejection, then the FWER is Pr( A). Generally Pr( A)≫αfor\\nlargeM, and depends on the correlation between the tests. If the tests are\\nindependent each with type-I error rate α, then the family-wise error rate\\nof the collection of tests is (1 −(1−α)M). On the other hand, if the tests\\nhave positive dependence, that is Pr( Aj|Ak)>Pr(Aj), then the FWER\\nwill be less than (1 −(1−α)M). Positive dependence between tests often\\noccurs in practice, in particular in genomic studies.\\nOne of the simplest approaches to multiple testing is the Bonferroni\\nmethod. It makes each individual test more stringent, in order to make the\\nFWER equal to at most α: we reject H0jifpj< α/M . It is easy to show\\nthat the resulting FWER is ≤α(Exercise 18.16). The Bonferroni method\\ncan be useful if Mis relatively small, but for large Mit is too conservative,\\nthat is, it calls too few genes signiﬁcant.\\nIn our example, if we test at level say α= 0.05, then we must use the\\nthreshold 0 .05/12,625 = 3 .9×10−6. None of the 12 ,625 genes had a p-value\\nthis small.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 705}, page_content='18.7 Feature Assessment and the Multiple-Testing Problem 687\\nThere are variations to this approach that adjust the individual p-values\\nto achieve an FWER of at most α, with some approaches avoiding the\\nassumption of independence; see, e.g., Dudoit et al. (2002b).\\n18.7.1 The False Discovery Rate\\nA diﬀerent approach to multiple testing does not try to control the FWER,\\nbut focuses instead on the proportion of falsely signiﬁcant genes. As we will\\nsee, this approach has a strong practical appeal.\\nTable 18.5 summarizes the theoretical outcomes of Mhypothesis tests.\\nNote that the family-wise error rate is Pr( V≥1). Here we instead focus\\nTABLE 18.5. Possible outcomes from Mhypothesis tests. Note that Vis the\\nnumber of false-positive tests; the type-I error rate is E(V)/M0. The type-II error\\nrate is E(T)/M1, and the power is 1−E(T)/M1.\\nCalled Called\\nNot Signiﬁcant Signiﬁcant Total\\nH0True U V M0\\nH0False T S M1\\nTotal M−R R M\\non the false discovery rate\\nFDR = E( V/R). (18.43)\\nIn the microarray setting, this is the expected proportion of genes that\\nare incorrectly called signiﬁcant, among the Rgenes that are called signif-\\nicant. The expectation is taken over the population from which the data\\nare generated. Benjamini and Hochberg (1995) ﬁrst proposed the notion of\\nfalse discovery rate, and gave a testing procedure (Algorithm 18.2) whose\\nFDR is bounded by a user-deﬁned level α. The Benjamini–Hochberg (BH)\\nprocedure is based on p-values; these can be obtained from an asymptotic\\napproximation to the test statistic (e.g., Gaussian), or a permutatio n dis-\\ntribution, as is done here.\\nIf the hypotheses are independent, Benjamini and Hochberg (1995) show\\nthat regardless of how many null hypotheses are true and regardless of the\\ndistribution of the p-values when the null hypothesis is false, this procedure\\nhas the property\\nFDR≤M0\\nMα≤α. (18.45)\\nFor illustration we chose α= 0.15. Figure 18.19 shows a plot of the or-\\ndered p-values p(j), and the line with slope 0 .15/12625.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 706}, page_content='688 18. High-Dimensional Problems: p≫N\\nAlgorithm 18.2 Benjamini–Hochberg (BH) Method.\\n1. Fix the false discovery rate αand let p(1)≤p(2)≤ ≤≤≤ ≤ p(M)denote\\nthe ordered p-values\\n2. Deﬁne\\nL= max{\\nj:p(j)< α≤j\\nM}\\n. (18.44)\\n3. Reject all hypotheses H0jfor which pj≤p(L), the BH rejection\\nthreshold.\\nGenes ordered by p−valuep−value\\n1 5 10 50 1005*10^−6 5*10^−5 5*10^−4 5*10^−3••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• ••••••••••••••••••••••• •••• ••••••• • •• • ••• •••••••••••••••••••• • ••••••• •••• •••• • ••• •• •••• •• •••••••• ••••••••• • ••••• •••••• ••••• ••••• • ••• •••• • •••••• •• ••• ••••• • •• • •• •• ••••• •• ••• ••••• ••• • •• • •• •• ••• •• • ••• •• ••••••• ••• • • • •• •• •• • • •• •• •••••• • ••• • •\\nFIGURE 18.19. Microarray example continued. Shown is a plot of the ordered\\np-values p(j)and the line 0.15≤(j/12,625), for the Benjamini–Hochberg method.\\nThe largest jfor which the p-value p(j)falls below the line, gives the BH threshold.\\nHere this occurs at j= 11, indicated by the vertical line. Thus the BH method\\ncalls signiﬁcant the 11genes (in red) with smallest p-values.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 707}, page_content='18.7 Feature Assessment and the Multiple-Testing Problem 689\\nAlgorithm 18.3 The Plug-in Estimate of the False Discovery Rate.\\n1. Create Kpermutations of the data, producing t-statistics tk\\njfor fea-\\ntures j= 1,2,... ,M and permutations k= 1,2,... ,K .\\n2. For a range of values of the cut-point C, let\\nRobs=M∑\\nj=1I(|tj|> C),ˆE(V) =1\\nKM∑\\nj=1K∑\\nk=1I(|tk\\nj|> C).(18.46)\\n3. Estimate the FDR by ˆFDR = ˆE(V)/Robs.\\nStarting at the left and moving right, the BH method ﬁnds the last time\\nthat the p-values fall below the line. This occurs at j= 11, so we reject\\nthe 11 genes with smallest p-values. Note that the cutoﬀ occurs at the 11th\\nsmallest p-value, 0.00012, and the 11th largest of the values |tj|is 4.101\\nThus we reject the 11 genes with |tj| ≥4.101.\\nFrom our brief description, it is not clear how the BH procedure works;\\nthat is, why the corresponding FDR is at most 0 .15, the value used for α.\\nIndeed, the proof of this fact is quite complicated (Benjamini and Hochberg,\\n1995).\\nA more direct way to proceed is a plug-in approach. Rather than starting\\nwith a value for α, we ﬁx a cut-point for our t-statistics, say the value\\n4.101 that appeared above. The number of observed values |tj|equal or\\ngreater than 4 .101 is 11. The total number of permutation values |tk\\nj|equal\\nor greater than 4 .101 is 1518, for an average of 1518 /1000 = 1 .518 per\\npermutation. Thus a direct estimate of the false discovery rate is ˆFDR =\\n1.518/11≈14%. Note that 14% is approximately equal to the value of\\nα= 0.15 used above (the diﬀerence is due to discreteness). This procedure\\nis summarized in Algorithm 18.3. To recap:\\nThe plug-in estimate of FDR of Algorithm 18.3 is equivalent to the BH\\nprocedure of Algorithm 18.2, using the permutation p-values (18.40).\\nThis correspondence between the BH method and the plug-in estimate is\\nnot a coincidence. Exercise 18.17 shows that they are equivalent in general.\\nNote that this procedure makes no reference to p-values at all, but rather\\nworks directly with the test statistics.\\nThe plug-in estimate is based on the approximation\\nE(V/R)≈E(V)\\nE(R), (18.47)\\nand in general ˆFDR is a consistent estimate of FDR (Storey, 2002; Storey et\\nal., 2004). Note that the numerator ˆE(V) actually estimates ( M/M 0)E(V),'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 708}, page_content='690 18. High-Dimensional Problems: p≫N\\nsince the permutation distribution uses Mrather M0null hypotheses.\\nHence if an estimate of M0is available, a better estimate of FDR can be\\nobtained from ( ˆM0/M)≤ˆFDR. Exercise 18.19 shows a way to estimate M0.\\nThe most conservative (upwardly biased) estimate of FDR uses M0=M.\\nEquivalently, an estimate of M0can be used to improve the BH method,\\nthrough relation (18.45).\\nThe reader might be surprised that we chose a value as large as 0 .15 for\\nα, the FDR bound. We must remember that the FDR is not the same as\\ntype-I error, for which 0.05 is the customary choice. For the scientist, the\\nfalse discovery rate is the expected proportion of false positive genes am ong\\nthe list of genes that the statistician tells him are signiﬁcant. Microarra y\\nexperiments with FDRs as high as 0.15 might still be useful, especially i f\\nthey are exploratory in nature.\\n18.7.2 Asymmetric Cutpoints and the SAM Procedure\\nIn the testing methods described above, we used the absolute value of the\\ntest statistic tj, and hence applied the same cut-points to both positive and\\nnegative values of the statistic. In some experiments, it might happen that\\nmost or all of the diﬀerentially expressed genes change in the positive direc-\\ntion (or all in the negative direction). For this situation it is advanta geous\\nto derive separate cut-points for the two cases.\\nThesigniﬁcance analysis of microarrays (SAM) approach oﬀers a way of\\ndoing this. The basis of the SAM method is shown in Figure 18.20. On the\\nvertical axis we have plotted the ordered test statistics t(1)≤t(2)≤ ≤≤≤ ≤\\nt(M), while the horizontal axis shows the expected order statistics from the\\npermutations of the data: ˜t(j)= (1/K)∑K\\nk=1tk\\n(j), where tk\\n(1)≤tk\\n(2)≤ ≤≤≤ ≤\\ntk\\n(M)are the ordered test statistics from permutation k.\\nTwo lines are drawn, parallel to the 45◦line, ∆ units away. Starting at\\nthe origin and moving to the right, we ﬁnd the ﬁrst place that the genes\\nleave the band. This deﬁnes the upper cutpoint Chiand all genes beyond\\nthat point are called signiﬁcant (marked red). Similarly we ﬁnd the lower\\ncutpoint Clowfor genes in the bottom left corner. Thus each value of the\\ntuning parameter ∆ deﬁnes upper and lower cutpoints, and the plug-in\\nestimate ˆFDR for each of these cutpoints is estimated as before. Typically\\na range of values of ∆ and associated ˆFDR values are computed, from which\\na particular pair are chosen on subjective grounds.\\nThe advantage of the SAM approach lies in the possible asymmetry of\\nthe cutpoints. In the example of Figure 18.20, with ∆ = 0 .71 we obtain\\n11 signiﬁcant genes; they are all in the upper right. The data points in the\\nbottom left never leave the band, and hence Clow=−∞. Hence for this\\nvalue of ∆, no genes are called signiﬁcant on the left (negative) side. We\\ndo not impose symmetry on the cutpoints, as was done in Section 18.7.1,\\nas there is no reason to assume similar behavior at the two ends.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 709}, page_content='18.7 Feature Assessment and the Multiple-Testing Problem 691\\nExpected Order Statisticst−statistic\\n−4 −2 0 2 4−4 −2 0 2 4\\n•••••••••••••• • •••••••••••• • ••••••• ••••• •• •••••••••••••••• •• • •••••• •••• • ••• • • •••••• • •••• ••••• •••• •• ••• •••• • •••• ••• •• • •• • •• • ••• • ••••• • •• •••• •• •• •••• • ••• •• ••••••• •• •••••••• •• ••• • ••• ••• •••• •• •• •••• •• •• •• •• ••• • •• • •• •• ••• •• •••• ••• ••• •• • •• ••• •• •• •• ••• •• •• •• •• ••• ••• •••• •• •••• •• •• •• •• •• •• •• •• ••• •• •• ••• •• ••• •• ••• ••• •• ••• •••• •••• •• •• •• •• •• •• •• •• • ••••• •• ••• ••• ••• •• •• •• •• •• ••• •• ••• •• •• •• ••• • ••• ••• •• •• •• •• •• •• ••• •• •• •• ••• •• ••• •• •• •• • •• •• •• •••• •• •• •• •• •• •• ••• ••• •• •• ••• •• •• •• • •• •• •• ••• •• •• ••• •• •• •• ••• •• •• •• •• •• ••• •• •• •• •• •• •• ••• •• •• •• •• •• ••• •••• •• •• •• •• ••• •• •••• •• •• •• •• •• ••• •• ••• •• •• •• •• ••• ••• •• ••• •• •• •• •• •• •• •• ••• •• •••• •• ••• ••• •• • •••• •• •• •• •• •• •• •• •• •• •• ••• •• •• •• •• ••• •• •• •• •• ••• •• •• •• ••• •• •• •• ••• • •• •• •• •• •• ••• •• •• •• •• • ••• •• •• •• •• •• •• ••• •• •• ••• •• ••• •• •• •• •• ••• •• •• •• ••• •• •• •• •• •• ••• •• •• •• •• •• •• •• •• ••• •• ••• ••• •• ••• •• ••• •• •• ••• •• •• •• •• ••• •• •• •••• ••• •• •• •• •• ••• •• ••• •• •• •••• •• •• •• ••• •• •• •• •• •• •• •• •• •• ••• •• ••• ••• •• •• •• •• •• •• ••• •• •• ••• •• •• •• •• ••• •• •• •• •• ••• •• •• ••• ••• •• •• •• ••• •• •• •• •• •• •• •• ••• •• ••• •• •• •• ••• •• •• • ••• •• •• •• •• •• •••• •• •• •• ••• •• •• •• •• •• •• ••• •• •• •• •• •• ••• •• ••• •• •• •• •• ••• •• •• •• •• •• ••• •• •• ••• •• ••• •• •• ••• •• ••• •• •• ••• •• ••• •• •• •• •• ••• •• •• •• ••• •• •• •• •• ••• •• •• •• •••• •• •• ••• •• •• • ••• •• •• ••• •• •• •• •• •• •• ••• •• ••• •• •• •• •• •• ••• •• •••• •• •• ••• •• ••• •• •• •• •• •• •• ••• •• •••• •• ••• •• ••• •• •• •• •• •• ••• •• •• •• ••• •• •••••• •• •• ••• ••• •• •• ••• •• •• ••• •• ••• •• ••• ••• ••• •• •• ••• • ••• •• •• •• •••• •• •• •• •• ••• •• •• •• ••• • •••• •• ••• •• •• •• •• ••• •• •• •••• •••• •• •• ••• ••• •• •• •• ••• •• •• •• ••• •• •• •• •• •• •• •• •••• •• •• ••• •• •• ••••• •• •• •• ••• •• •• •••• •• ••• •• •• •• •• •• ••• •• • •• ••• •• ••• •• • ••• ••• •• •• ••• •• • •• •• ••• •• • ••• ••• ••• •• ••• ••• ••• •• • ••• •• •• • •• •• •• ••• ••• • ••• ••• •• •• •• •• •• •• •• •• ••• •• •• •• •• •• •• •• ••• ••• •• ••• ••• •• • •••••••• •• • •••• • ••• •• ••• •• •• •• •• •••• ••• •• •• • ••••••• •• ••• •• • ••••• •••• •• ••••• •• • •••• • ••••• • •• • ••• ••• •• •• • ••••• •• •• •• • •••••• ••• •••• ••••••• •• • ••• • •• • •••• ••• •• ••••••••••• •• • • • • ••••• • • ••••• •••••• •••••••••••• • ••••••• ••••••••••••••••••••••\\n••••••••••••\\n∆\\nChi\\nFIGURE 18.20. SAM plot for the radiation sensitivity microarray data. On the\\nvertical axis we have plotted the ordered test statistics, wh ile the horizontal axis\\nshows the expected order statistics of the test statistics fr om permutations of the\\ndata. Two lines are drawn, parallel to the 45◦line,∆units away from it. Starting\\nat the origin and moving to the right, we ﬁnd the ﬁrst place that the g enes leave\\nthe band. This deﬁnes the upper cut-point Chiand all genes beyond that point are\\ncalled signiﬁcant (marked in red). Similarly we deﬁne a lower cutpo intClow. For\\nthe particular value of ∆ = 0 .71in the plot, no genes are called signiﬁcant in the\\nbottom left.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 710}, page_content='692 18. High-Dimensional Problems: p≫N\\nThere is some similarity between this approach and the asymmetry possi-\\nble with likelihood-ratio tests. Suppose we have a log-likelihood ℓ0(tj) under\\nthe null-hypothesis of no eﬀect, and a log-likelihood ℓ(tj) under the alterna-\\ntive. Then a likelihood ratio test amounts to rejecting the null-hypothesis\\nif\\nℓ(tj)−ℓ0(tj)>∆, (18.48)\\nfor some ∆. Depending on the likelihoods, and particularly their relative\\nvalues, this can result in a diﬀerent threshold for tjthan for −tj. The SAM\\nprocedure rejects the null-hypothesis if\\n|t(j)−˜t(j)|>∆ (18.49)\\nAgain, the threshold for each t(j)depends on the corresponding value of\\nthe null value ˜t(j).\\n18.7.3 A Bayesian Interpretation of the FDR\\nThere is an interesting Bayesian view of the FDR, developed in Storey\\n(2002) and Efron and Tibshirani (2002). First we need to deﬁne the positive\\nfalse discovery rate (pFDR) as\\npFDR = E[V\\nR⏐⏐⏐⏐R >0]\\n. (18.50)\\nThe additional term positive refers to the fact that we are only interested\\nin estimating an error rate where positive ﬁndings have occurred. It is\\nthis slightly modiﬁed version of the FDR that has a clean Bayesian inter-\\npretation. Note that the usual FDR [expression (18.43)] is not deﬁned if\\nPr(R= 0)>0.\\nLet Γ be a rejection region for a single test; in the example above we used\\nΓ = (−∞,−4.10)∪(4.10,∞). Suppose that Midentical simple hypothe-\\nsis tests are performed with the i.i.d. statistics t1,... ,t Mand rejection\\nregion Γ. We deﬁne a random variable Zjwhich equals 0 if the jth null\\nhypothesis is true, and 1 otherwise. We assume that each pair ( tj,Zj) are\\ni.i.d random variables with\\ntj|Zj∼(1−Zj)≤F0+Zj≤F1 (18.51)\\nfor some distributions F0andF1. This says that each test statistic tjcomes\\nfrom one of two distributions: F0if the null hypothesis is true, and F1\\notherwise. Letting Pr( Zj= 0) = π0, marginally we have:\\ntj∼π0≤F0+ (1−π0)≤F1. (18.52)\\nThen it can be shown (Efron et al., 2001; Storey, 2002) that'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 711}, page_content='18.8 Bibliographic Notes 693\\npFDR(Γ) = Pr( Zj= 0|tj∈Γ). (18.53)\\nHence under the mixture model (18.51), the pFDR is the posterior proba-\\nbility that the null hypothesis it true, given that test statistic falls i n the\\nrejection region for the test; that is, given that we reject the null hypothesis\\n(Exercise 18.20).\\nThe false discovery rate provides a measure of accuracy for tests based\\non an entire rejection region, such as |tj| ≥2. But if the FDR of such a test\\nis say 10%, then a gene with say tj= 5 will be more signiﬁcant than a gene\\nwithtj= 2. Thus it is of interest to derive a local (gene-speciﬁc) version\\nof the FDR. The q-value (Storey, 2003) of a test statistic tjis deﬁned to\\nbe the smallest FDR over all rejection regions that reject tj. That is, for\\nsymmetric rejection regions, the q-value for tj= 2 is deﬁned to be the\\nFDR for the rejection region Γ = {−(∞,−2)∪(2,∞)}. Thus the q-value\\nfortj= 5 will be smaller than that for tj= 2, reﬂecting the fact that tj= 5\\nis more signiﬁcant than tj= 2. The local false discovery rate (Efron and\\nTibshirani, 2002) at t=t0is deﬁned to be\\nPr(Zj= 0|tj=t0). (18.54)\\nThis is the (positive) FDR for an inﬁnitesimal rejection region surrounding\\nthe value tj=t0.\\n18.8 Bibliographic Notes\\nMany references were given at speciﬁc points in this chapter; we give some\\nadditional ones here. Dudoit et al. (2002a) give an overview and compar-\\nison of discrimination methods for gene expression data. Levina (2002)\\ndoes some mathematical analysis comparing diagonal LDA to full LDA, as\\np,N→ ∞ withp > N . She shows that with reasonable assumptions diago-\\nnal LDA has a lower asymptotic error rate than full LDA. Tibshirani et al.\\n(2001a) and Tibshirani et al. (2003) proposed the nearest shrunken-centroid\\nclassiﬁer. Zhu and Hastie (2004) study regularized logistic regression. H igh-\\ndimensional regression and the lasso are very active areas of research, and\\nmany references are given in Section 3.8.5. The fused lasso was proposed\\nby Tibshirani et al. (2005), while Zou and Hastie (2005) introduced the\\nelastic net. Supervised principal components is discussed in Bair and Tib-\\nshirani (2004) and Bair et al. (2006). For an introduction to the analys is\\nof censored survival data, see Kalbﬂeisch and Prentice (1980).\\nMicroarray technology has led to a ﬂurry of statistical research: see for\\nexample the books by Speed (2003), Parmigiani et al. (2003), Simon et al.\\n(2004), and Lee (2004).\\nThe false discovery rate was proposed by Benjamini and Hochberg (1995),\\nand studied and generalized in subsequent papers by these authors and'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 712}, page_content='694 18. High-Dimensional Problems: p≫N\\nmany others. A partial list of papers on FDR may be found on Yoav Ben-\\njamini’s homepage. Some more recent papers include Efron and Tibshirani\\n(2002), Storey (2002), Genovese and Wasserman (2004), Storey and Tib-\\nshirani (2003) and Benjamini and Yekutieli (2005). Dudoit et al. (2002b)\\nreview methods for identifying diﬀerentially expressed genes in microarray\\nstudies.\\nExercises\\nEx. 18.1 For a coeﬃcient estimate ˆβj, letˆβj/||ˆβj||2be the normalized ver-\\nsion. Show that as λ→ ∞, the normalized ridge-regression estimates con-\\nverge to the renormalized partial-least-squares one-component estimates.\\nEx. 18.2 Nearest shrunken centroids and the lasso. Consider a (naive Bayes)\\nGaussian model for classiﬁcation in which the features j= 1,2,... ,p are\\nassumed to be independent within each class k= 1,2,... ,K . With ob-\\nservations i= 1,2,... ,N andCkequal to the set of indices of the Nk\\nobservations in class k, we observe xij∼N(θj+θjk,σ2\\nj) fori∈Ckwith∑K\\nk=1θjk= 0. Set ˆ σ2\\nj=s2\\nj, the pooled within-class variance for feature j,\\nand consider the lasso-style minimization problem\\nmin\\n{θj,θjk}\\uf8f1\\n\\uf8f2\\n\\uf8f31\\n2p∑\\nj=1K∑\\nk=1∑\\ni∈Ck(xij−θj−θjk)2\\ns2\\nj+λ√\\nNkp∑\\nj=1K∑\\nk=1|θjk|\\nsj.\\uf8fc\\n\\uf8fd\\n\\uf8fe(18.55)\\nShow that the solution is equivalent to the nearest shrunken centroid es-\\ntimator (18.5), with s0set to zero, and Mkequal to 1 /Nkinstead of\\n1/Nk−1/Nas before.\\nEx. 18.3 Show that the ﬁtted coeﬃcients for the regularized multiclass\\nlogistic regression problem (18.10) satisfy∑K\\nk=1ˆβkj= 0, j= 1,... ,p .\\nWhat about the ˆβk0? Discuss issues with these constant parameters, and\\nhow they can be resolved.\\nEx. 18.4 Derive the computational formula (18.15) for ridge regression.\\n[Hint: Use the ﬁrst derivative of the penalized sum-of-squares criterion to\\nshow that if λ >0, then ˆβ=XTsfor some s∈IRN.]\\nEx. 18.5 Prove the theorem (18.16)–(18.17) in Section 18.3.5, by decom-\\nposing βand the rows of Xinto their projections into the column space of\\nVand its complement in IRp.\\nEx. 18.6 Show how the theorem in Section 18.3.5 can be applied to regu-\\nlarized discriminant analysis [Section 4.14 and Equation (18.9)].'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 713}, page_content='Exercises 695\\nEx. 18.7 Consider a linear regression problem where p≫N, and assume\\nthe rank of XisN. Let the SVD of X=UDVT=RVT, where Ris\\nN×Nnonsingular, and Visp×Nwith orthonormal columns.\\n(a) Show that there are inﬁnitely many least-squares solutions all with\\nzero residuals.\\n(b) Show that the ridge-regression estimate for βcan be written\\nˆβλ=V(RTR+λI)−1RTy (18.56)\\n(c) Show that when λ= 0, the solution ˆβ0=VD−1UTyhas residuals\\nall equal to zero, and is unique in that it has the smallest Euclidean\\nnorm amongst all zero-residual solutions.\\nEx. 18.8 Data Piling . Exercise 4.2 shows that the two-class LDA solution\\ncan be obtained by a linear regression of a binary response vector ycon-\\nsisting of −1s and +1s. The prediction ˆβTxfor any xis (up to a scale and\\nshift) the LDA score δ(x). Suppose now that p≫N.\\n(a) Consider the linear regression model f(x) =α+βTxﬁt to a binary\\nresponse Y∈ {−1,+1}. Using Exercise 18.7, show that there are\\ninﬁnitely many directions deﬁned by ˆβin IRponto which the data\\nproject to exactly two points, one for each class. These are known as\\ndata piling directions (Ahn and Marron, 2005).\\n(b) Show that the distance between the projected points is 2 /||ˆβ||, and\\nhence these directions deﬁne separating hyperplanes with that mar-\\ngin.\\n(c) Argue that there is a single maximal data piling direction for which\\nthis distance is largest, and is deﬁned by ˆβ0=VD−1UTy=X−y,\\nwhere X=UDVTis the SVD of X.\\nEx. 18.9 Compare the data piling direction of Exercise 18.8 to the direction\\nof the optimal separating hyperplane (Section 4.5.2) qualitatively. Which\\nmakes the widest margin, and why? Use a small simulation to demonstrate\\nthe diﬀerence.\\nEx. 18.10 When p≫N, linear discriminant analysis (see Section 4.3) is\\ndegenerate because the within-class covariance matrix Wis singular. One\\nversion of regularized discriminant analysis (4.14) replaces Wby a ridged\\nversion W+λI, leading to a regularized discriminant function δλ(x) =\\nxT(W+λI)−1(¯x1−¯x−1). Show that δ0(x) = lim λ↓0δλ(x) corresponds to\\nthe maximal data piling direction deﬁned in Exercise 18.8.\\nEx. 18.11 Suppose you have a sample of Npairs ( xi,yi), with yibinary\\nandxi∈IR1. Suppose also that the two classes are separable; e.g., for each'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 714}, page_content='696 18. High-Dimensional Problems: p≫N\\npairi,i′withyi= 0 and yi′= 1,xi′−xi≥Cfor some C >0. You wish\\nto ﬁt a linear logistic regression model logitPr( Y= 1|X) =α+βXby\\nmaximum-likelihood. Show that ˆβis undeﬁned.\\nEx. 18.12 Suppose we wish to select the ridge parameter λby 10-fold cross-\\nvalidation in a p≫Nsituation (for any linear model). We wish to use the\\ncomputational shortcuts described in Section 18.3.5. Show that we need\\nonly to reduce the N×pmatrix Xto the N×Nmatrix Ronce, and can\\nuse it in all the cross-validation runs.\\nEx. 18.13 Suppose our p > N predictors are presented as an N×Ninner-\\nproduct matrix K=XXT, and we wish to ﬁt the equivalent of a linear\\nlogistic regression model in the original features with quadratic regular iza-\\ntion. Our predictions are also to be made using inner products; a new x0\\nis presented as k0=Xx0. LetK=UD2UTbe the eigen-decomposition of\\nK. Show that the predictions are given by ˆf0=kT\\n0ˆα, where\\n(a) ˆα=UD−1ˆβ, and\\n(b)ˆβis the ridged logistic regression estimate with input matrix R=\\nUD.\\nArgue that the same approach can be used for any appropriate kernel\\nmatrix K.\\nEx. 18.14 Distance weighted 1-NN classiﬁcation . Consider the 1-nearest-\\nneighbor method (Section 13.3) in a two-class classiﬁcation problem. Let\\nd+(x0) be the shortest distance to a training observation in class +1, and\\nlikewise d−(x0) the shortest distance for class −1. Let N−be the number\\nof samples in class −1,N+the number in class +1, and N=N−+N+.\\n(a) Show that\\nδ(x0) = logd−(x0)\\nd+(x0)(18.57)\\ncan be viewed as a nonparametric discriminant function correspond-\\ning to 1-NN classiﬁcation. [ Hint: Show that ˆf+(x0) =1\\nN+d+(x0)can\\nbe viewed as a nonparametric estimate of the density in class +1 at\\nx0].\\n(b) How would you modify this function to introduce class prior probabil-\\nitiesπ+andπ−diﬀerent from the sample-priors N+/NandN−/N?\\n(c) How would you generalize this approach for K-NN classiﬁcation?\\nEx. 18.15 Kernel PCA. In Section 18.5.2 we show how to compute the\\nprincipal component variables Zfrom an uncentered inner-product matrix\\nK. We compute the eigen-decomposition ( I−M)K(I−M) =UD2UT,\\nwithM=11T/N, and then Z=UD. Suppose we have the inner-product'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 715}, page_content='Exercises 697\\nvector k0, containing the Ninner-products between a new point x0and\\neach of the xiin our training set. Show that the (centered) projections of\\nx0onto the principal-component directions are given by\\nz0=D−1UT(I−M)[k0−K1/N]. (18.58)\\nEx. 18.16 Bonferroni method for multiple comparisons. Suppose we are in\\na multiple-testing scenario with null hypotheses H0j,j= 1,2,... ,M , and\\ncorresponding p-values pj,i= 1,2,... ,M . LetAbe the event that at least\\none null hypothesis is falsely rejected, and let Ajbe the event that the\\njth null hypothesis is falsely rejected. Suppose that we use the Bonferroni\\nmethod, rejecting the jth null hypothesis if pj< α/M .\\n(a) Show that Pr( A)≤α. [Hint: Pr(Aj∪Aj′) = Pr( Aj) + Pr( Aj′)−\\nPr(Aj∩Aj′)]\\n(b) If the hypotheses H0j,j= 1,2,... ,M , are independent, then Pr( A) =\\n1−Pr(AC) = 1−∏M\\nj=1Pr(AC\\nj) = 1−(1−α/M)M. Use this to show\\nthat Pr( A)≈αin this case.\\nEx. 18.17 Equivalence between Benjamini–Hochberg and plug-in method s.\\n(a) In the notation of Algorithm 18.2, show that for rejection threshold\\np0=p(L), a proportion of at most p0of the permuted values tk\\nj\\nexceed |T|(L)where |T|(L)is the Lth largest value among the |tj|.\\nHence show that the plug-in FDR estimate ˆFDR is less than or equal\\ntop0≤M/L=α.\\n(b) Show that the cut-point |T|(L+1)produces a test with estimated FDR\\ngreater than α.\\nEx. 18.18 Use result (18.53) to show that\\npFDR =π0≤ {Type I error of Γ }\\nπ0≤ {Type I error of Γ }+π1{Power of Γ }(18.59)\\n(Storey, 2003).\\nEx. 18.19 Consider the data in Table 18.4 of Section (18.7), available from\\nthe book website.\\n(a) Using a symmetric two-sided rejection region based on the t-statistic,\\ncompute the plug-in estimate of the FDR for various values of the\\ncut-point.\\n(b) Carry out the BH procedure for various FDR levels αand show the\\nequivalence of your results, with those from part (a).'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 716}, page_content='698 18. High-Dimensional Problems: p≫N\\n(c) Let ( q.25,q.75) be the quartiles of the t-statistics from the permuted\\ndatasets. Let ˆ π0={#tj∈(q.25,q.75)}/(.5M), and set ˆ π0= min(ˆ π0,1).\\nMultiply the FDR estimates from (a) by ˆ π0and examine the results.\\n(d) Give a motivation for the estimate in part (c).\\n(Storey, 2003)\\nEx. 18.20 Proof of result 18.20 . Write\\npFDR = E(\\nV\\nR|R >0)\\n(18.60)\\n=M∑\\nk=1E[\\nV\\nR|R=k]\\nPr(R=k|R >0) (18.61)\\nUse the fact that given R=k,Vis a binomial random variable, with k\\ntrials and probability of success Pr( H= 0|T∈Γ), to complete the proof.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 717}, page_content='This is page 699\\nPrinter: Opaque this\\nReferences\\nAbu-Mostafa, Y. (1995). Hints, Neural Computation 7: 639–671.\\nAckley, D. H., Hinton, G. and Sejnowski, T. (1985). A learning algorithm\\nfor Boltzmann machines, Trends in Cognitive Sciences 9: 147–169.\\nAdam, B.-L., Qu, Y., Davis, J. W., Ward, M. D., Clements, M. A.,\\nCazares, L. H., Semmes, O. J., Schellhammer, P. F., Yasui, Y.,\\nFeng, Z. and Wright, G. (2003). Serum protein ﬁngerprinting cou-\\npled with a pattern-matching algorithm distinguishes prostate cancer\\nfrom benign prostate hyperplasia and healthy mean, Cancer Research\\n63(10): 3609–3614.\\nAgrawal, R., Mannila, H., Srikant, R., Toivonen, H. and Verkamo, A. I.\\n(1995). Fast discovery of association rules, Advances in Knowledge\\nDiscovery and Data Mining , AAAI/MIT Press, Cambridge, MA.\\nAgresti, A. (1996). An Introduction to Categorical Data Analysis , Wiley,\\nNew York.\\nAgresti, A. (2002). Categorical Data Analysis (2nd Ed.) , Wiley, New York.\\nAhn, J. and Marron, J. (2005). The direction of maximal data piling in high\\ndimensional space, Technical report , Statistics Department, University\\nof North Carolina, Chapel Hill.\\nAkaike, H. (1973). Information theory and an extension of the maximum\\nlikelihood principle, Second International Symposium on Information\\nTheory , pp. 267–281.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 718}, page_content='700 References\\nAllen, D. (1974). The relationship between variable selection and data\\naugmentation and a method of prediction, Technometrics 16: 125–7.\\nAmbroise, C. and McLachlan, G. (2002). Selection bias in gene extraction\\non the basis of microarray gene-expression data, Proceedings of the\\nNational Academy of Sciences 99: 6562–6566.\\nAmit, Y. and Geman, D. (1997). Shape quantization and recognition with\\nrandomized trees, Neural Computation 9: 1545–1588.\\nAnderson, J. and Rosenfeld, E. (eds) (1988). Neurocomputing: Foundations\\nof Research , MIT Press, Cambridge, MA.\\nAnderson, T. (2003). An Introduction to Multivariate Statistical Analysis,\\n3rd ed. , Wiley, New York.\\nBach, F. and Jordan, M. (2002). Kernel independent component analysis,\\nJournal of Machine Learning Research 3: 1–48.\\nBair, E. and Tibshirani, R. (2004). Semi-supervised methods to predict\\npatient survival from gene expression data, PLOS Biology 2: 511–522.\\nBair, E., Hastie, T., Paul, D. and Tibshirani, R. (2006). Prediction by\\nsupervised principal components, Journal of the American Statistical\\nAssociation 101: 119–137.\\nBakin, S. (1999). Adaptive regression and model selection in data mining\\nproblems, Technical report , PhD. thesis, Australian National Univer-\\nsity, Canberra.\\nBanerjee, O., Ghaoui, L. E. and d’Aspremont, A. (2008). Model selection\\nthrough sparse maximum likelihood estimation for multivariate gaus-\\nsian or binary data, Journal of Machine Learning Research 9: 485–516.\\nBarron, A. (1993). Universal approximation bounds for superpositions o f a\\nsigmoid function, IEEE Transactions on Information Theory 39: 930–\\n945.\\nBartlett, P. and Traskin, M. (2007). Adaboost is consistent, in\\nB. Sch¨ olkopf, J. Platt and T. Hoﬀman (eds), Advances in Neural Infor-\\nmation Processing Systems 19 , MIT Press, Cambridge, MA, pp. 105–\\n112.\\nBecker, R., Cleveland, W. and Shyu, M. (1996). The visual design and con-\\ntrol of trellis display, Journal of Computational and Graphical Statis-\\ntics5: 123–155.\\nBell, A. and Sejnowski, T. (1995). An information-maximization approac h\\nto blind separation and blind deconvolution, Neural Computation\\n7: 1129–1159.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 719}, page_content='References 701\\nBellman, R. E. (1961). Adaptive Control Processes , Princeton University\\nPress.\\nBenjamini, Y. and Hochberg, Y. (1995). Controlling the false discovery\\nrate: a practical and powerful approach to multiple testing, Journal of\\nthe Royal Statistical Society Series B. 85: 289–300.\\nBenjamini, Y. and Yekutieli, Y. (2005). False discovery rate controlling\\nconﬁdence intervals for selected parameters, Journal of the American\\nStatistical Association 100: 71–80.\\nBickel, P. and Levina, E. (2004). Some theory for Fisher’s linear discrim-\\ninant function,“Naive Bayes”, and some alternatives when there are\\nmany more variables than observations, Bernoulli 10: 989–1010.\\nBickel, P. J., Ritov, Y. and Tsybakov, A. (2008). Simultaneous analysi s of\\nlasso and Dantzig selector, Annals of Statistics . to appear.\\nBishop, C. (1995). Neural Networks for Pattern Recognition , Clarendon\\nPress, Oxford.\\nBishop, C. (2006). Pattern Recognition and Machine Learning , Springer,\\nNew York.\\nBishop, Y., Fienberg, S. and Holland, P. (1975). Discrete Multivariate\\nAnalysis , MIT Press, Cambridge, MA.\\nBoyd, S. and Vandenberghe, L. (2004). Convex Optimization , Cambridge\\nUniversity Press.\\nBreiman, L. (1992). The little bootstrap and other methods for dimension-\\nality selection in regression: X-ﬁxed prediction error, Journal of the\\nAmerican Statistical Association 87: 738–754.\\nBreiman, L. (1996a). Bagging predictors, Machine Learning 26: 123–140.\\nBreiman, L. (1996b). Stacked regressions, Machine Learning 24: 51–64.\\nBreiman, L. (1998). Arcing classiﬁers (with discussion), Annals of Statistics\\n26: 801–849.\\nBreiman, L. (1999). Prediction games and arcing algorithms, Neural Com-\\nputation 11(7): 1493–1517.\\nBreiman, L. (2001). Random forests, Machine Learning 45: 5–32.\\nBreiman, L. and Friedman, J. (1997). Predicting multivariate responses\\nin multiple linear regression (with discussion), Journal of the Royal\\nStatistical Society Series B. 59: 3–37.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 720}, page_content='702 References\\nBreiman, L. and Ihaka, R. (1984). Nonlinear discriminant analysis via\\nscaling and ACE, Technical report , University of California, Berkeley.\\nBreiman, L. and Spector, P. (1992). Submodel selection and evaluation\\nin regression: the X-random case, International Statistical Review\\n60: 291–319.\\nBreiman, L., Friedman, J., Olshen, R. and Stone, C. (1984). Classiﬁcation\\nand Regression Trees , Wadsworth, New York.\\nBremaud, P. (1999). Markov Chains: Gibbs Fields, Monte Carlo Simula-\\ntion, and Queues , Springer, New York.\\nBrown, P., Spiegelman, C. and Denham, M. (1991). Chemometrics and\\nspectral frequency selection, Transactions of the Royal Society of Lon-\\ndon Series A. 337: 311–322.\\nBruce, A. and Gao, H. (1996). Applied Wavelet Analysis with S-PLUS ,\\nSpringer, New York.\\nB¨ uhlmann, P. and Hothorn, T. (2007). Boosting algorithms: regulariza-\\ntion, prediction and model ﬁtting (with discussion), Statistical Science\\n22(4): 477–505.\\nBuja, A., Hastie, T. and Tibshirani, R. (1989). Linear smoothers and\\nadditive models (with discussion), Annals of Statistics 17: 453–555.\\nBuja, A., Swayne, D., Littman, M., Hofmann, H. and Chen, L. (2008). Data\\nvizualization with multidimensional scaling, Journal of Computational\\nand Graphical Statistics . to appear.\\nBunea, F., Tsybakov, A. and Wegkamp, M. (2007). Sparsity oracle inequal-\\nities for the lasso, Electronic Journal of Statistics 1: 169–194.\\nBurges, C. (1998). A tutorial on support vector machines for pattern recog-\\nnition, Knowledge Discovery and Data Mining 2(2): 121–167.\\nButte, A., Tamayo, P., Slonim, D., Golub, T. and Kohane, I. (2000).\\nDiscovering functional relationships between RNA expression and\\nchemotherapeutic susceptibility using relevance networks, Proceedings\\nof the National Academy of Sciences pp. 12182–12186.\\nCandes, E. (2006). Compressive sampling, Proceedings of the Interna-\\ntional Congress of Mathematicians , European Mathematical Society,\\nMadrid, Spain.\\nCandes, E. and Tao, T. (2007). The Dantzig selector: Statistical estimati on\\nwhen p is much larger than n, Annals of Statistics 35(6): 2313–2351.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 721}, page_content='References 703\\nChambers, J. and Hastie, T. (1991). Statistical Models in S ,\\nWadsworth/Brooks Cole, Paciﬁc Grove, CA.\\nChaudhuri, S., Drton, M. and Richardson, T. S. (2007). Estimation of a\\ncovariance matrix with zeros, Biometrika 94(1): 1–18.\\nChen, L. and Buja, A. (2008). Local multidimensional scaling for nonlinear\\ndimension reduction, graph drawing and proximity analysis, Journal\\nof the American Statistical Association .\\nChen, S. S., Donoho, D. and Saunders, M. (1998). Atomic decomposition\\nby basis pursuit, SIAM Journal on Scientiﬁc Computing 20(1): 33–61.\\nCherkassky, V. and Ma, Y. (2003). Comparison of model selection for\\nregression, Neural computation 15(7): 1691–1714.\\nCherkassky, V. and Mulier, F. (2007). Learning from Data (2nd Edition) ,\\nWiley, New York.\\nChui, C. (1992). An Introduction to Wavelets , Academic Press, London.\\nCliﬀord, P. (1990). Markov random ﬁelds in statistics, inG. R. Grimmett\\nand D. J. A. Welsh (eds), Disorder in Physical Systems. A Volume in\\nHonour of John M. Hammersley , Clarendon Press, Oxford, pp. 19–32.\\nComon, P. (1994). Independent component analysis—a new concept?, Sig-\\nnal Processing 36: 287–314.\\nCook, D. and Swayne, D. (2007). Interactive and Dynamic Graphics for\\nData Analysis; with R and GGobi , Springer, New York. With con-\\ntributions from A. Buja, D. Temple Lang, H. Hofmann, H. Wickham\\nand M. Lawrence.\\nCook, N. (2007). Use and misuse of the receiver operating characteristic\\ncurve in risk prediction, Circulation 116(6): 928–35.\\nCopas, J. B. (1983). Regression, prediction and shrinkage (with discus-\\nsion),Journal of the Royal Statistical Society, Series B, Methodo logical\\n45: 311–354.\\nCover, T. and Hart, P. (1967). Nearest neighbor pattern classiﬁcation,\\nIEEE Transactions on Information Theory IT-11 : 21–27.\\nCover, T. and Thomas, J. (1991). Elements of Information Theory , Wiley,\\nNew York.\\nCox, D. and Hinkley, D. (1974). Theoretical Statistics , Chapman and Hall,\\nLondon.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 722}, page_content='704 References\\nCox, D. and Wermuth, N. (1996). Multivariate Dependencies: Models,\\nAnalysis and Interpretation , Chapman and Hall, London.\\nCressie, N. (1993). Statistics for Spatial Data (Revised Edition) , Wiley-\\nInterscience, New York.\\nCsiszar, I. and Tusn´ ady, G. (1984). Information geometry and alternat-\\ning minimization procedures, Statistics & Decisions Supplement Issue\\n1: 205–237.\\nCutler, A. and Breiman, L. (1994). Archetypal analysis, Technometrics\\n36(4): 338–347.\\nDasarathy, B. (1991). Nearest Neighbor Pattern Classiﬁcation Techniques ,\\nIEEE Computer Society Press, Los Alamitos, CA.\\nDaubechies, I. (1992). Ten Lectures in Wavelets , Society for Industrial and\\nApplied Mathematics, Philadelphia, PA.\\nDaubechies, I., Defrise, M. and De Mol, C. (2004). An iterative threshold-\\ning algorithm for linear inverse problems with a sparsity constraint,\\nCommunications on Pure and Applied Mathematics 57: 1413–1457.\\nde Boor, C. (1978). A Practical Guide to Splines , Springer, New York.\\nDempster, A. (1972). Covariance selection, Biometrics 28: 157–175.\\nDempster, A., Laird, N. and Rubin, D. (1977). Maximum likelihood from\\nincomplete data via the EM algorithm (with discussion), Journal of\\nthe Royal Statistical Society Series B 39: 1–38.\\nDevijver, P. and Kittler, J. (1982). Pattern Recognition: A Statistical Ap-\\nproach , Prentice-Hall, Englewood Cliﬀs, N.J.\\nDietterich, T. (2000a). Ensemble methods in machine learning, Lecture\\nNotes in Computer Science 1857: 1–15.\\nDietterich, T. (2000b). An experimental comparison of three methods for\\nconstructing ensembles of decision trees: bagging, boosting, and ran-\\ndomization, Machine Learning 40(2): 139–157.\\nDietterich, T. and Bakiri, G. (1995). Solving multiclass learning problems\\nvia error-correcting output codes, Journal of Artiﬁcial Intelligence Re-\\nsearch 2: 263–286.\\nDonath, W. E. and Hoﬀman, A. J. (1973). Lower bounds for the partition-\\ning of graphs, IBM Journal of Research and Development pp. 420–425.\\nDonoho, D. (2006a). Compressed sensing, IEEE Transactions on Informa-\\ntion Theory 52(4): 1289–1306.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 723}, page_content='References 705\\nDonoho, D. (2006b). For most large underdetermined systems of equations,\\nthe minimal ℓ1-norm solution is the sparsest solution, Communications\\non Pure and Applied Mathematics 59: 797–829.\\nDonoho, D. and Elad, M. (2003). Optimally sparse representation from\\novercomplete dictionaries via ℓ1-norm minimization, Proceedings of\\nthe National Academy of Sciences 100: 2197–2202.\\nDonoho, D. and Johnstone, I. (1994). Ideal spatial adaptation by wavelet\\nshrinkage, Biometrika 81: 425–455.\\nDonoho, D. and Stodden, V. (2004). When does non-negative matrix\\nfactorization give a correct decomposition into parts?, inS. Thrun,\\nL. Saul and B. Sch¨ olkopf (eds), Advances in Neural Information Pro-\\ncessing Systems 16 , MIT Press, Cambridge, MA.\\nDuan, N. and Li, K.-C. (1991). Slicing regression: a link-free regression\\nmethod, Annals of Statistics 19: 505–530.\\nDuchamp, T. and Stuetzle, W. (1996). Extremal properties of principal\\ncurves in the plane, Annals of Statistics 24: 1511–1520.\\nDuda, R., Hart, P. and Stork, D. (2000). Pattern Classiﬁcation (2nd Edi-\\ntion), Wiley, New York.\\nDudoit, S., Fridlyand, J. and Speed, T. (2002a). Comparison of discrimi-\\nnation methods for the classiﬁcation of tumors using gene expression\\ndata,Journal of the American Statistical Association 97(457): 77–87.\\nDudoit, S., Yang, Y., Callow, M. and Speed, T. (2002b). Statistical meth-\\nods for identifying diﬀerentially expressed genes in replicated cDNA\\nmicroarray experiments, Statistica Sinica pp. 111–139.\\nEdwards, D. (2000). Introduction to Graphical Modelling, 2nd Edition ,\\nSpringer, New York.\\nEfron, B. (1975). The eﬃciency of logistic regression compared to normal\\ndiscriminant analysis, Journal of the American Statistical Association\\n70: 892–898.\\nEfron, B. (1979). Bootstrap methods: another look at the jackknife, Annals\\nof Statistics 7: 1–26.\\nEfron, B. (1983). Estimating the error rate of a prediction rule: some\\nimprovements on cross-validation, Journal of the American Statistical\\nAssociation 78: 316–331.\\nEfron, B. (1986). How biased is the apparent error rate of a prediction\\nrule?, Journal of the American Statistical Association 81: 461–70.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 724}, page_content='706 References\\nEfron, B. and Tibshirani, R. (1991). Statistical analysis in the computer\\nage,Science 253: 390–395.\\nEfron, B. and Tibshirani, R. (1993). An Introduction to the Bootstrap ,\\nChapman and Hall, London.\\nEfron, B. and Tibshirani, R. (1996). Using specially designed exponential\\nfamilies for density estimation, Annals of Statistics 24(6): 2431–2461.\\nEfron, B. and Tibshirani, R. (1997). Improvements on cross-validation: t he\\n632+ bootstrap: method, Journal of the American Statistical Associ-\\nation92: 548–560.\\nEfron, B. and Tibshirani, R. (2002). Microarrays, empirical Bayes methods ,\\nand false discovery rates, Genetic Epidemiology 1: 70–86.\\nEfron, B., Hastie, T. and Tibshirani, R. (2007). Discussion of “Dantzi g\\nselector” by Candes and Tao, Annals of Statistics 35(6): 2358–2364.\\nEfron, B., Hastie, T., Johnstone, I. and Tibshirani, R. (2004). Least angl e\\nregression (with discussion), Annals of Statistics 32(2): 407–499.\\nEfron, B., Tibshirani, R., Storey, J. and Tusher, V. (2001). Empirical\\nBayes analysis of a microarray experiment, Journal of the American\\nStatistical Association 96: 1151–1160.\\nEvgeniou, T., Pontil, M. and Poggio, T. (2000). Regularization networ ks\\nand support vector machines, Advances in Computational Mathemat-\\nics13(1): 1–50.\\nFan, J. and Fan, Y. (2008). High dimensional classiﬁcation using features\\nannealed independence rules, Annals of Statistics . to appear.\\nFan, J. and Gijbels, I. (1996). Local Polynomial Modelling and Its Appli-\\ncations , Chapman and Hall, London.\\nFan, J. and Li, R. (2005). Variable selection via nonconcave penalized\\nlikelihood and its oracle properties, Journal of the American Statistical\\nAssociation 96: 1348–1360.\\nFiedler, M. (1973). Algebraic connectivity of graphs, Czechoslovak Mathe-\\nmatics Journal 23(98): 298–305.\\nFienberg, S. (1977). The Analysis of Cross-Classiﬁed Categorical Data ,\\nMIT Press, Cambridge.\\nFisher, R. A. (1936). The use of multiple measurements in taxonomic\\nproblems, Eugen. 7: 179–188.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 725}, page_content='References 707\\nFisher, W. (1958). On grouping for maximum homogeniety, Journal of the\\nAmerican Statistical Association 53(284): 789–798.\\nFix, E. and Hodges, J. (1951). Discriminatory analysis—nonparametric\\ndiscrimination: Consistency properties, Technical Report 21-49-004,4 ,\\nU.S. Air Force, School of Aviation Medicine, Randolph Field, TX.\\nFlury, B. (1990). Principal points, Biometrika 77: 33–41.\\nForgy, E. (1965). Cluster analysis of multivariate data: eﬃciency vs. i nter-\\npretability of classiﬁcations, Biometrics 21: 768–769.\\nFrank, I. and Friedman, J. (1993). A statistical view of some chemometri cs\\nregression tools (with discussion), Technometrics 35(2): 109–148.\\nFreund, Y. (1995). Boosting a weak learning algorithm by majority, Infor-\\nmation and Computation 121(2): 256–285.\\nFreund, Y. and Schapire, R. (1996a). Experiments with a new boosting\\nalgorithm, Machine Learning: Proceedings of the Thirteenth Interna-\\ntional Conference , Morgan Kauﬀman, San Francisco, pp. 148–156.\\nFreund, Y. and Schapire, R. (1996b). Game theory, on-line prediction and\\nboosting, Proceedings of the Ninth Annual Conference on Computa-\\ntional Learning Theory , Desenzano del Garda, Italy, pp. 325–332.\\nFreund, Y. and Schapire, R. (1997). A decision-theoretic generalization of\\nonline learning and an application to boosting, Journal of Computer\\nand System Sciences 55: 119–139.\\nFriedman, J. (1987). Exploratory projection pursuit, Journal of the Amer-\\nican Statistical Association 82: 249–266.\\nFriedman, J. (1989). Regularized discriminant analysis, Journal of the\\nAmerican Statistical Association 84: 165–175.\\nFriedman, J. (1991). Multivariate adaptive regression splines (with dis cus-\\nsion), Annals of Statistics 19(1): 1–141.\\nFriedman, J. (1994a). Flexible metric nearest-neighbor classiﬁcation, Tech-\\nnical report , Stanford University.\\nFriedman, J. (1994b). An overview of predictive learning and function\\napproximation, inV. Cherkassky, J. Friedman and H. Wechsler (eds),\\nFrom Statistics to Neural Networks , Vol. 136 of NATO ISI Series F ,\\nSpringer, New York.\\nFriedman, J. (1996). Another approach to polychotomous classiﬁcation,\\nTechnical report , Stanford University.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 726}, page_content='708 References\\nFriedman, J. (1997). On bias, variance, 0-1 loss and the curse of dimen-\\nsionality, Journal of Data Mining and Knowledge Discovery 1: 55–77.\\nFriedman, J. (1999). Stochastic gradient boosting, Technical report , Stan-\\nford University.\\nFriedman, J. (2001). Greedy function approximation: A gradient boosting\\nmachine, Annals of Statistics 29(5): 1189–1232.\\nFriedman, J. and Fisher, N. (1999). Bump hunting in high dimensional\\ndata,Statistics and Computing 9: 123–143.\\nFriedman, J. and Hall, P. (2007). On bagging and nonlinear estimation,\\nJournal of Statistical Planning and Inference 137: 669–683.\\nFriedman, J. and Popescu, B. (2003). Importance sampled learning ensem-\\nbles,Technical report , Stanford University, Department of Statistics.\\nFriedman, J. and Popescu, B. (2008). Predictive learning via rule ensem-\\nbles,Annals of Applied Statistics, to appear .\\nFriedman, J. and Silverman, B. (1989). Flexible parsimonious smoothing\\nand additive modelling (with discussion), Technometrics 31: 3–39.\\nFriedman, J. and Stuetzle, W. (1981). Projection pursuit regression, Jour-\\nnal of the American Statistical Association 76: 817–823.\\nFriedman, J. and Tukey, J. (1974). A projection pursuit algorithm for\\nexploratory data analysis, IEEE Transactions on Computers, Series\\nC23: 881–889.\\nFriedman, J., Baskett, F. and Shustek, L. (1975). An algorithm for ﬁnding\\nnearest neighbors, IEEE Transactions on Computers 24: 1000–1006.\\nFriedman, J., Bentley, J. and Finkel, R. (1977). An algorthm for ﬁnd-\\ning best matches in logarithmic expected time, ACM Transactions on\\nMathematical Software 3: 209–226.\\nFriedman, J., Hastie, T. and Tibshirani, R. (2000). Additive logistic r e-\\ngression: a statistical view of boosting (with discussion), Annals of\\nStatistics 28: 337–307.\\nFriedman, J., Hastie, T. and Tibshirani, R. (2008a). Response to “Mease\\nand Wyner: Evidence contrary to the statistical view of boosting”,\\nJournal of Machine Learning Research 9: 175–180.\\nFriedman, J., Hastie, T. and Tibshirani, R. (2008b). Sparse inverse covari -\\nance estimation with the graphical lasso, Biostatistics 9: 432–441.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 727}, page_content='References 709\\nFriedman, J., Hastie, T. and Tibshirani, R. (2010). Regularization paths f or\\ngeneralized linear models via coordinate descent, Journal of Statistical\\nSoftware 33(1): 1–22.\\nFriedman, J., Hastie, T., Hoeﬂing, H. and Tibshirani, R. (2007). Pathwis e\\ncoordinate optimization, Annals of Applied Statistics 2(1): 302–332.\\nFriedman, J., Hastie, T., Rosset, S., Tibshirani, R. and Zhu, J. (2004).\\nDiscussion of three boosting papers by Jiang, Lugosi and Vayatis, and\\nZhang, Annals of Statistics 32: 102–107.\\nFriedman, J., Stuetzle, W. and Schroeder, A. (1984). Projection pursuit\\ndensity estimation, Journal of the American Statistical Association\\n79: 599–608.\\nFu, W. (1998). Penalized regressions: the bridge vs. the lasso, Journal of\\nComputational and Graphical Statistics 7(3): 397–416.\\nFurnival, G. and Wilson, R. (1974). Regression by leaps and bounds, Tech-\\nnometrics 16: 499–511.\\nGelfand, A. and Smith, A. (1990). Sampling based approaches to calculat-\\ning marginal densities, Journal of the American Statistical Association\\n85: 398–409.\\nGelman, A., Carlin, J., Stern, H. and Rubin, D. (1995). Bayesian Data\\nAnalysis , CRC Press, Boca Raton, FL.\\nGeman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distribu-\\ntions and the Bayesian restoration of images, IEEE Transactions on\\nPattern Analysis and Machine Intelligence 6: 721–741.\\nGenkin, A., Lewis, D. and Madigan, D. (2007). Large-scale Bayesian logis -\\ntic regression for text categorization, Technometrics 49(3): 291–304.\\nGenovese, C. and Wasserman, L. (2004). A stochastic process approach to\\nfalse discovery rates, Annals of Statistics 32(3): 1035–1061.\\nGersho, A. and Gray, R. (1992). Vector Quantization and Signal Compres-\\nsion, Kluwer Academic Publishers, Boston, MA.\\nGirosi, F., Jones, M. and Poggio, T. (1995). Regularization theory and\\nneural network architectures, Neural Computation 7: 219–269.\\nGolub, G. and Van Loan, C. (1983). Matrix Computations , Johns Hopkins\\nUniversity Press, Baltimore.\\nGolub, G., Heath, M. and Wahba, G. (1979). Generalized cross-validation\\nas a method for choosing a good ridge parameter, Technometrics\\n21: 215–224.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 728}, page_content='710 References\\nGolub, T., Slonim, D., Tamayo, P., Huard, C., Gaasenbeek, M., Mesirov,\\nJ., Coller, H., Loh, M., Downing, J., Caligiuri, M., Bloomﬁeld, C. and\\nLander, E. (1999). Molecular classiﬁcation of cancer: Class discovery\\nand class prediction by gene expression monitoring, Science 286: 531–\\n536.\\nGoodall, C. (1991). Procrustes methods in the statistical analysis of s hape,\\nJournal of the Royal Statistical Society, Series B 53: 285–321.\\nGordon, A. (1999). Classiﬁcation (2nd edition) , Chapman and Hall/CRC\\nPress, London.\\nGreen, P. and Silverman, B. (1994). Nonparametric Regression and Gener-\\nalized Linear Models: A Roughness Penalty Approach , Chapman and\\nHall, London.\\nGreenacre, M. (1984). Theory and Applications of Correspondence Analy-\\nsis, Academic Press, New York.\\nGreenshtein, E. and Ritov, Y. (2004). Persistence in high-dimensional lin-\\near predictor selection and the virtue of overparametrization, Bernoulli\\n10: 971–988.\\nGuo, Y., Hastie, T. and Tibshirani, R. (2006). Regularized linear discrim-\\ninant analysis and its application in microarrays, Biostatistics 8: 86–\\n100.\\nGuyon, I., Gunn, S., Nikravesh, M. and Zadeh, L. (eds) (2006). Feature\\nExtraction, Foundations and Applications , Springer, New York.\\nGuyon, I., Weston, J., Barnhill, S. and Vapnik, V. (2002). Gene selection for\\ncancer classiﬁcation using support vector machines, Machine Learning\\n46: 389–422.\\nHall, P. (1992). The Bootstrap and Edgeworth Expansion , Springer, New\\nYork.\\nHammersley, J. M. and Cliﬀord, P. (1971). Markov ﬁeld on ﬁnite graphs\\nand lattices, unpublished.\\nHand, D. (1981). Discrimination and Classiﬁcation , Wiley, Chichester.\\nHanley, J. and McNeil, B. (1982). The meaning and use of the area under\\na receiver operating characteristic (roc) curve, Radiology 143: 29–36.\\nHart, P. (1968). The condensed nearest-neighbor rule, IEEE Transactions\\non Information Theory 14: 515–516.\\nHartigan, J. A. (1975). Clustering Algorithms , Wiley, New York.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 729}, page_content='References 711\\nHartigan, J. A. and Wong, M. A. (1979). [(Algorithm AS 136] A k-means\\nclustering algorithm (AS R39: 81v30 p355-356), Applied Statistics\\n28: 100–108.\\nHastie, T. (1984). Principal Curves and Surfaces , PhD thesis, Stanford\\nUniversity.\\nHastie, T. and Herman, A. (1990). An analysis of gestational age, neona-\\ntal size and neonatal death using nonparametric logistic regression,\\nJournal of Clinical Epidemiology 43: 1179–90.\\nHastie, T. and Simard, P. (1998). Models and metrics for handwritten digit\\nrecognition, Statistical Science 13: 54–65.\\nHastie, T. and Stuetzle, W. (1989). Principal curves, Journal of the Amer-\\nican Statistical Association 84(406): 502–516.\\nHastie, T. and Tibshirani, R. (1987). Nonparametric logistic and propo r-\\ntional odds regression, Applied Statistics 36: 260–276.\\nHastie, T. and Tibshirani, R. (1990). Generalized Additive Models , Chap-\\nman and Hall, London.\\nHastie, T. and Tibshirani, R. (1996a). Discriminant adaptive nearest-\\nneighbor classiﬁcation, IEEE Pattern Recognition and Machine In-\\ntelligence 18: 607–616.\\nHastie, T. and Tibshirani, R. (1996b). Discriminant analysis by Gaussi an\\nmixtures, Journal of the Royal Statistical Society Series B. 58: 155–\\n176.\\nHastie, T. and Tibshirani, R. (1998). Classiﬁcation by pairwise coupling ,\\nAnnals of Statistics 26(2): 451–471.\\nHastie, T. and Tibshirani, R. (2003). Independent components analysis\\nthrough product density estimation, inS. T. S. Becker and K. Ober-\\nmayer (eds), Advances in Neural Information Processing Systems 15 ,\\nMIT Press, Cambridge, MA, pp. 649–656.\\nHastie, T. and Tibshirani, R. (2004). Eﬃcient quadratic regularization f or\\nexpression arrays, Biostatistics 5(3): 329–340.\\nHastie, T. and Zhu, J. (2006). Discussion of “Support vector machines\\nwith applications” by Javier Moguerza and Alberto Munoz, Statistical\\nScience 21(3): 352–357.\\nHastie, T., Botha, J. and Schnitzler, C. (1989). Regression with an ordered\\ncategorical response, Statistics in Medicine 43: 884–889.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 730}, page_content='712 References\\nHastie, T., Buja, A. and Tibshirani, R. (1995). Penalized discriminant\\nanalysis, Annals of Statistics 23: 73–102.\\nHastie, T., Kishon, E., Clark, M. and Fan, J. (1992). A model for\\nsignature veriﬁcation, Technical report , AT&T Bell Laboratories.\\nhttp://www-stat.stanford.edu/ ∼hastie/Papers/signature.pdf .\\nHastie, T., Rosset, S., Tibshirani, R. and Zhu, J. (2004). The entire reg-\\nularization path for the support vector machine, Journal of Machine\\nLearning Research 5: 1391–1415.\\nHastie, T., Taylor, J., Tibshirani, R. and Walther, G. (2007). Forwa rd\\nstagewise regression and the monotone lasso, Electronic Journal of\\nStatistics 1: 1–29.\\nHastie, T., Tibshirani, R. and Buja, A. (1994). Flexible discriminant ana ly-\\nsis by optimal scoring, Journal of the American Statistical Association\\n89: 1255–1270.\\nHastie, T., Tibshirani, R. and Buja, A. (1998). Flexible discriminant and\\nmixture models, inJ. Kay and M. Titterington (eds), Statistics and\\nArtiﬁcial Neural Networks , Oxford University Press.\\nHastie, T., Tibshirani, R. and Friedman, J. (2003). A note on “Compari-\\nson of model selection for regression” by Cherkassky and Ma, Neural\\ncomputation 15(7): 1477–1480.\\nHathaway, R. J. (1986). Another interpretation of the EM algorithm for\\nmixture distributions, Statistics & Probability Letters 4: 53–56.\\nHebb, D. (1949). The Organization of Behavior , Wiley, New York.\\nHertz, J., Krogh, A. and Palmer, R. (1991). Introduction to the Theory of\\nNeural Computation , Addison Wesley, Redwood City, CA.\\nHinton, G. (1989). Connectionist learning procedures, Artiﬁcial Intelli-\\ngence40: 185–234.\\nHinton, G. (2002). Training products of experts by minimizing contrastive\\ndivergence, Neural Computation 14: 1771–1800.\\nHinton, G., Osindero, S. and Teh, Y.-W. (2006). A fast learning algorithm\\nfor deep belief nets, Neural Computation 18: 1527–1554.\\nHo, T. K. (1995). Random decision forests, inM. Kavavaugh and P. Storms\\n(eds), Proc. Third International Conference on Document Analysis\\nand Recognition , Vol. 1, IEEE Computer Society Press, New York,\\npp. 278–282.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 731}, page_content='References 713\\nHoeﬂing, H. and Tibshirani, R. (2008). Estimation of sparse Markov net-\\nworks using modiﬁed logistic regression and the lasso, submitted.\\nHoerl, A. E. and Kennard, R. (1970). Ridge regression: biased estimation\\nfor nonorthogonal problems, Technometrics 12: 55–67.\\nHothorn, T. and B¨ uhlmann, P. (2006). Model-based boosting in high di-\\nmensions, Bioinformatics 22(22): 2828–2829.\\nHuber, P. (1964). Robust estimation of a location parameter, Annals of\\nMathematical Statistics 53: 73–101.\\nHuber, P. (1985). Projection pursuit, Annals of Statistics 13: 435–475.\\nHunter, D. and Lange, K. (2004). A tutorial on MM algorithms, The\\nAmerican Statistician 58(1): 30–37.\\nHyv¨ arinen, A. and Oja, E. (2000). Independent component analysis: algo-\\nrithms and applications, Neural Networks 13: 411–430.\\nHyv¨ arinen, A., Karhunen, J. and Oja, E. (2001). Independent Component\\nAnalysis , Wiley, New York.\\nIzenman, A. (1975). Reduced-rank regression for the multivariate linear\\nmodel, Journal of Multivariate Analysis 5: 248–264.\\nJacobs, R., Jordan, M., Nowlan, S. and Hinton, G. (1991). Adaptive mix-\\ntures of local experts, Neural computation 3: 79–87.\\nJain, A. and Dubes, R. (1988). Algorithms for Clustering Data , Prentice-\\nHall, Englewood Cliﬀs, N.J.\\nJames, G. and Hastie, T. (1998). The error coding method and PICTs,\\nJournal of Computational and Graphical Statistics 7(3): 377–387.\\nJancey, R. (1966). Multidimensional group analysis, Australian Journal of\\nBotany 14: 127–130.\\nJensen, F. V., Lauritzen, S. and Olesen, K. G. (1990). Bayesian updating\\nin recursive graphical models by local computation, Computational\\nStatistics Quarterly 4: 269–282.\\nJiang, W. (2004). Process consistency for Adaboost, Annals of Statistics\\n32(1): 13–29.\\nJirou´ sek, R. and Pˇ reuˇ cil, S. (1995). On the eﬀective implementation of t he\\niterative proportional ﬁtting procedure, Computational Statistics and\\nData Analysis 19: 177–189.\\nJohnson, N. (2008). A study of the NIPS feature selection challenge, Sub-\\nmitted.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 732}, page_content='714 References\\nJoliﬀe, I. T., Trendaﬁlov, N. T., and Uddin, M. (2003). A modiﬁed principal\\ncomponent technique based on the lasso, Journal of Computational\\nand Graphical Statistics 12: 531–547.\\nJones, L. (1992). A simple lemma on greedy approximation in Hilbert space\\nand convergence rates for projection pursuit regression and neural\\nnetwork training, Annals of Statistics 20: 608–613.\\nJordan, M. (2004). Graphical models, Statistical Science (Special Issue on\\nBayesian Statistics) 19: 140–155.\\nJordan, M. and Jacobs, R. (1994). Hierachical mixtures of experts and the\\nEM algorithm, Neural Computation 6: 181–214.\\nKalbﬂeisch, J. and Prentice, R. (1980). The Statistical Analysis of Failure\\nTime Data , Wiley, New York.\\nKaufman, L. and Rousseeuw, P. (1990). Finding Groups in Data: An In-\\ntroduction to Cluster Analysis , Wiley, New York.\\nKearns, M. and Vazirani, U. (1994). An Introduction to Computational\\nLearning Theory , MIT Press, Cambridge, MA.\\nKittler, J., Hatef, M., Duin, R. and Matas, J. (1998). On combining classi-\\nﬁers,IEEE Transaction on Pattern Analysis and Machine Intellige nce\\n20(3): 226–239.\\nKleinberg, E. M. (1990). Stochastic discrimination, Annals of Mathematical\\nArtiﬁcial Intelligence 1: 207–239.\\nKleinberg, E. M. (1996). An overtraining-resistant stochastic modeling\\nmethod for pattern recognition, Annals of Statistics 24: 2319–2349.\\nKnight, K. and Fu, W. (2000). Asymptotics for lasso-type estimators,\\nAnnals of Statistics 28(5): 1356–1378.\\nKoh, K., Kim, S.-J. and Boyd, S. (2007). An interior-point method\\nfor large-scale L1-regularized logistic regression, Journal of Machine\\nLearning Research 8: 1519–1555.\\nKohavi, R. (1995). A study of cross-validation and bootstrap for accu-\\nracy estimation and model selection, International Joint Conference\\non Artiﬁcial Intelligence (IJCAI) , Morgan Kaufmann, pp. 1137–1143.\\nKohonen, T. (1989). Self-Organization and Associative Memory (3rd edi-\\ntion), Springer, Berlin.\\nKohonen, T. (1990). The self-organizing map, Proceedings of the IEEE\\n78: 1464–1479.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 733}, page_content='References 715\\nKohonen, T., Kaski, S., Lagus, K., Saloj¨ arvi, J., Paatero, A. and Saarela,\\nA. (2000). Self-organization of a massive document collection, IEEE\\nTransactions on Neural Networks 11(3): 574–585. Special Issue on\\nNeural Networks for Data Mining and Knowledge Discovery.\\nKoller, D. and Friedman, N. (2007). Structured Probabilistic Models , Stan-\\nford Bookstore Custom Publishing. (Unpublished Draft).\\nKressel, U. (1999). Pairwise classiﬁcation and support vector machines,\\ninB. Sch¨ olkopf, C. Burges and A. Smola (eds), Advances in Ker-\\nnel Methods - Support Vector Learning , MIT Press, Cambridge, MA.,\\npp. 255–268.\\nLambert, D. (1992). Zero-inﬂated Poisson regression, with an applicatio n\\nto defects in manufacturing, Technometrics 34(1): 1–14.\\nLange, K. (2004). Optimization , Springer, New York.\\nLauritzen, S. (1996). Graphical Models , Oxford University Press.\\nLauritzen, S. and Spiegelhalter, D. (1988). Local computations with proba-\\nbilities on graphical structures and their application to expert systems,\\nJ. Royal Statistical Society B. 50: 157–224.\\nLawson, C. and Hansen, R. (1974). Solving Least Squares Problems ,\\nPrentice-Hall, Englewood Cliﬀs, NJ.\\nLe Cun, Y. (1989). Generalization and network design strategies, Techni-\\ncal Report CRG-TR-89-4 , Department of Computer Science, Univ. of\\nToronto.\\nLe Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard,\\nW. and Jackel, L. (1990). Handwritten digit recognition with a back-\\npropogation network, inD. Touretzky (ed.), Advances in Neural In-\\nformation Processing Systems , Vol. 2, Morgan Kaufman, Denver, CO,\\npp. 386–404.\\nLe Cun, Y., Bottou, L., Bengio, Y. and Haﬀner, P. (1998). Gradient-based\\nlearning applied to document recognition, Proceedings of the IEEE\\n86(11): 2278–2324.\\nLeathwick, J., Elith, J., Francis, M., Hastie, T. and Taylor, P. (20 06). Vari-\\nation in demersal ﬁsh species richness in the oceans surrounding new\\nzealand: an analysis using boosted regression trees, Marine Ecology\\nProgress Series 77: 802–813.\\nLeathwick, J., Rowe, D., Richardson, J., Elith, J. and Hastie, T. (200 5).\\nUsing multivariate adaptive regression splines to predict the distribu-\\ntions of New Zealand’s freshwater diadromous ﬁsh, Freshwater Biology\\n50: 2034–2051.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 734}, page_content='716 References\\nLeblanc, M. and Tibshirani, R. (1996). Combining estimates in regres-\\nsion and classiﬁcation, Journal of the American Statistical Association\\n91: 1641–1650.\\nLeCun, Y., Bottou, L., Bengio, Y. and Haﬀner, P. (1998). Gradient-based\\nlearning applied to document recognition, Proceedings of the IEEE\\n86(11): 2278–2324.\\nLee, D. and Seung, H. (1999). Learning the parts of objects by non-negative\\nmatrix factorization, Nature 401: 788.\\nLee, D. and Seung, H. (2001). Algorithms for non-negative matrix factor-\\nization, Advances in Neural Information Processing Systems, (NIPS\\n2001), Vol. 13, Morgan Kaufman, Denver., pp. 556–562.\\nLee, M.-L. (2004). Analysis of Microarray Gene Expression Data , Kluwer\\nAcademic Publishers.\\nLee, S.-I., Ganapathi, V. and Koller, D. (2007). Eﬃcient structure learning\\nof markov networks using l1-regularization, inB. Sch¨ olkopf, J. Platt\\nand T. Hoﬀman (eds), Advances in Neural Information Processing\\nSystems 19 , MIT Press, Cambridge, MA, pp. 817–824.\\nLeslie, C., Eskin, E., Cohen, A., Weston, J. and Noble, W. S. (2004). Mis-\\nmatch string kernels for discriminative protein classiﬁcation, Bioinfor-\\nmatics 20(4): 467–476.\\nLevina, E. (2002). Statistical issues in texture analysis , PhD thesis, De-\\npartment. of Statistics, University of California, Berkeley.\\nLin, H., McCulloch, C., Turnbull, B., Slate, E. and Clark, L. (2000). A\\nlatent class mixed model for analyzing biomarker trajectories in lon-\\ngitudinal data with irregularly scheduled observations, Statistics in\\nMedicine 19: 1303–1318.\\nLin, Y. and Zhang, H. (2006). Component selection and smoothing in\\nsmoothing spline analysis of variance models, Annals of Statistics\\n34: 2272–2297.\\nLittle, R. and Rubin, D. (2002). Statistical Analysis with Missing Data\\n(2nd Edition) , Wiley, New York.\\nLloyd, S. (1957). Least squares quantization in PCM., Technical report , Bell\\nLaboratories. Published in 1982 in IEEE Transactions on Information\\nTheory 28128-137.\\nLoader, C. (1999). Local Regression and Likelihood , Springer, New York.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 735}, page_content='References 717\\nLoh, W. and Vanichsetakul, N. (1988). Tree structured classiﬁcation via\\ngeneralized discriminant analysis, Journal of the American Statistical\\nAssociation 83: 715–728.\\nLugosi, G. and Vayatis, N. (2004). On the bayes-risk consistency of regu-\\nlarized boosting methods, Annals of Statistics 32(1): 30–55.\\nMacnaughton Smith, P., Williams, W., Dale, M. and Mockett, L. (1965).\\nDissimilarity analysis: a new technique of hierarchical subdivision, Na-\\nture202: 1034–1035.\\nMacKay, D. (1992). A practical Bayesian framework for backpropagation\\nneural networks, Neural Computation 4: 448–472.\\nMacQueen, J. (1967). Some methods for classiﬁcation and analysis of mul-\\ntivariate observations, Proceedings of the Fifth Berkeley Symposium\\non Mathematical Statistics and Probability, eds. L.M. LeCa m and J.\\nNeyman , University of California Press, pp. 281–297.\\nMadigan, D. and Raftery, A. (1994). Model selection and accounting for\\nmodel uncertainty using Occam’s window, Journal of the American\\nStatistical Association 89: 1535–46.\\nMardia, K., Kent, J. and Bibby, J. (1979). Multivariate Analysis , Academic\\nPress.\\nMason, L., Baxter, J., Bartlett, P. and Frean, M. (2000). Boosting al go-\\nrithms as gradient descent, 12: 512–518.\\nMassart, D., Plastria, F. and Kaufman, L. (1983). Non-hierarchical clus-\\ntering with MASLOC, The Journal of the Pattern Recognition Society\\n16: 507–516.\\nMcCullagh, P. and Nelder, J. (1989). Generalized Linear Models , Chapman\\nand Hall, London.\\nMcCulloch, W. and Pitts, W. (1943). A logical calculus of the ideas immi-\\nnent in nervous activity, Bulletin of Mathematical Biophysics 5: 115–\\n133. Reprinted in Anderson and Rosenfeld (1988), pp 96-104.\\nMcLachlan, G. (1992). Discriminant Analysis and Statistical Pattern\\nRecognition , Wiley, New York.\\nMease, D. and Wyner, A. (2008). Evidence contrary to the statistical view\\nof boosting (with discussion), Journal of Machine Learning Research\\n9: 131–156.\\nMeinshausen, N. (2007). Lasso with relaxation, Computational Statistics\\nand Data Analysis 52(1): 374–293.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 736}, page_content='718 References\\nMeinshausen, N. and B¨ uhlmann, P. (2006). High-dimensional graphs and\\nvariable selection with the lasso, Annals of Statistics 34: 1436–1462.\\nMeir, R. and R¨ atsch, G. (2003). An introduction to boosting and leverag -\\ning,inS. Mendelson and A. Smola (eds), Lecture notes in Computer\\nScience , Advanced Lectures in Machine Learning, Springer, New York.\\nMichie, D., Spiegelhalter, D. and Taylor, C. (eds) (1994). Machine Learn-\\ning, Neural and Statistical Classiﬁcation , Ellis Horwood Series in Ar-\\ntiﬁcial Intelligence, Ellis Horwood.\\nMorgan, J. N. and Sonquist, J. A. (1963). Problems in the analysis of surv ey\\ndata, and a proposal, Journal of the American Statistical Association\\n58: 415–434.\\nMurray, W., Gill, P., and Wright, M. (1981). Practical Optimization , Aca-\\ndemic Press.\\nMyles, J. and Hand, D. (1990). The multiclass metric problem in nearest\\nneighbor classiﬁcation, Pattern Recognition 23: 1291–1297.\\nNadler, B. and Coifman, R. R. (2005). An exact asymptotic formula for t he\\nerror in CLS and in PLS: The importance of dimensional reduction in\\nmultivariate calibration, Journal of Chemometrics 102: 107–118.\\nNeal, R. (1996). Bayesian Learning for Neural Networks , Springer, New\\nYork.\\nNeal, R. and Hinton, G. (1998). A view of the EM algorithm that justiﬁes\\nincremental, sparse, and other variants; in Learning in Gra phical Mod-\\nels, M. Jordan (ed.) , Dordrecht: Kluwer Academic Publishers, Boston,\\nMA., pp. 355–368.\\nNeal, R. and Zhang, J. (2006). High dimensional classiﬁcation with\\nbayesian neural networks and dirichlet diﬀusion trees, inI. Guyon,\\nS. Gunn, M. Nikravesh and L. Zadeh (eds), Feature Extraction, Foun-\\ndations and Applications , Springer, New York, pp. 265–296.\\nOnton, J. and Makeig, S. (2006). Information-based modeling of event-\\nrelated brain dynamics, inNeuper and Klimesch (eds), Progress in\\nBrain Research , Vol. 159, Elsevier, pp. 99–120.\\nOsborne, M., Presnell, B. and Turlach, B. (2000a). A new approach to\\nvariable selection in least squares problems, IMA Journal of Numerical\\nAnalysis 20: 389–404.\\nOsborne, M., Presnell, B. and Turlach, B. (2000b). On the lasso and its\\ndual,Journal of Computational and Graphical Statistics 9: 319–337.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 737}, page_content='References 719\\nPace, R. K. and Barry, R. (1997). Sparse spatial autoregressions, Statistics\\nand Probability Letters 33: 291–297.\\nPage, L., Brin, S., Motwani, R. and Winograd, T. (1998). The\\npagerank citation ranking: bringing order to the web, Tech-\\nnical report , Stanford Digital Library Technologies Project.\\nhttp://citeseer.ist.psu.edu/page98pagerank.html .\\nPark, M. Y. and Hastie, T. (2007). l1-regularization path algorithm for gen-\\neralized linear models, Journal of the Royal Statistical Society Series\\nB69: 659–677.\\nParker, D. (1985). Learning logic, Technical Report TR-87 , Cambridge MA:\\nMIT Center for Research in Computational Economics and Manage-\\nment Science.\\nParmigiani, G., Garett, E. S., Irizarry, R. A. and Zeger, S. L. (eds) (200 3).\\nThe Analysis of Gene Expression Data , Springer, New York.\\nPaul, D., Bair, E., Hastie, T. and Tibshirani, R. (2008). “Pre-conditioni ng”\\nfor feature selection and regression in high-dimensional problems, An-\\nnals of Statistics 36(4): 1595–1618.\\nPearl, J. (1986). On evidential reasoning in a hierarchy of hypotheses,\\nArtiﬁcial Intelligence 28: 9–15.\\nPearl, J. (1988). Probabilistic reasoning in intelligent systems: networks of\\nplausible inference , Morgan Kaufmann, San Francisco, CA.\\nPearl, J. (2000). Causality: Models, Reasoning and Inference , Cambridge\\nUniversity Press.\\nPeterson and Anderson, J. R. (1987). A mean ﬁeld theory learning algo-\\nrithm for neural networks, Complex Systems 1: 995–1019.\\nPetricoin, E. F., Ardekani, A. M., Hitt, B. A., Levine, P. J., Fusaro, V.,\\nSteinberg, S. M., Mills, G. B., Simone, C., Fishman, D. A., Kohn,\\nE. and Liotta, L. A. (2002). Use of proteomic patterns in serum to\\nidentify ovarian cancer, Lancet 359: 572–577.\\nPlatt, J. (1999). Fast Training of Support Vector Machines using Sequen-\\ntial Minimal Optimization; in Advances in Kernel Methods—S upport\\nVector Learning, B. Sch¨ olkopf and C. J. C. Burges and A. J. Sm ola\\n(eds), MIT Press, Cambridge, MA., pp. 185–208.\\nQuinlan, R. (1993). C4.5: Programs for Machine Learning , Morgan Kauf-\\nmann, San Mateo.\\nQuinlan, R. (2004). C5.0, www.rulequest.com .'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 738}, page_content='720 References\\nRamaswamy, S., Tamayo, P., Rifkin, R., Mukherjee, S., Yeang, C., Angelo,\\nM., Ladd, C., Reich, M., Latulippe, E., Mesirov, J., Poggio, T., Gerald,\\nW., Loda, M., Lander, E. and Golub, T. (2001). Multiclass cancer\\ndiagnosis using tumor gene expression signature, PNAS 98: 15149–\\n15154.\\nRamsay, J. and Silverman, B. (1997). Functional Data Analysis , Springer,\\nNew York.\\nRao, C. R. (1973). Linear Statistical Inference and Its Applications , Wiley,\\nNew York.\\nR¨ atsch, G. and Warmuth, M. (2002). Maximizing the margin with boost -\\ning,Proceedings of the 15th Annual Conference on Computational\\nLearning Theory , pp. 334–350.\\nRavikumar, P., Liu, H., Laﬀerty, J. and Wasserman, L. (2008). Spam:\\nSparse additive models, inJ. Platt, D. Koller, Y. Singer and S. Roweis\\n(eds), Advances in Neural Information Processing Systems 20 , MIT\\nPress, Cambridge, MA, pp. 1201–1208.\\nRidgeway, G. (1999). The state of boosting, Computing Science and Statis-\\ntics31: 172–181.\\nRieger, K., Hong, W., Tusher, V., Tang, J., Tibshirani, R. and Chu, G.\\n(2004). Toxicity from radiation therapy associated with abnormal\\ntranscriptional responses to DNA damage, Proceedings of the National\\nAcademy of Sciences 101: 6634–6640.\\nRipley, B. D. (1996). Pattern Recognition and Neural Networks , Cambridge\\nUniversity Press.\\nRissanen, J. (1983). A universal prior for integers and estimation by mini -\\nmum description length, Annals of Statistics 11: 416–431.\\nRobbins, H. and Munro, S. (1951). A stochastic approximation method,\\nAnnals of Mathematical Statistics 22: 400–407.\\nRoosen, C. and Hastie, T. (1994). Automatic smoothing spline projection\\npursuit, Journal of Computational and Graphical Statistics 3: 235–248.\\nRosenblatt, F. (1958). The perceptron: a probabilistic model for infor-\\nmation storage and organization in the brain, Psychological Review\\n65: 386–408.\\nRosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the\\nTheory of Brain Mechanisms , Spartan, Washington, D.C.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 739}, page_content='References 721\\nRosenwald, A., Wright, G., Chan, W. C., Connors, J. M., Campo, E.,\\nFisher, R. I., Gascoyne, R. D., Muller-Hermelink, H. K., Smeland,\\nE. B. and Staudt, L. M. (2002). The use of molecular proﬁling to\\npredict survival after chemotherapy for diﬀuse large b-cell lymphoma,\\nThe New England Journal of Medicine 346: 1937–1947.\\nRosset, S. and Zhu, J. (2007). Piecewise linear regularized solution paths,\\nAnnals of Statistics 35(3): 1012–1030.\\nRosset, S., Zhu, J. and Hastie, T. (2004a). Boosting as a regularized path to\\na maximum margin classiﬁer, Journal of Machine Learning Research\\n5: 941–973.\\nRosset, S., Zhu, J. and Hastie, T. (2004b). Margin maximizing loss func-\\ntions,inS. Thrun, L. Saul and B. Sch¨ olkopf (eds), Advances in Neural\\nInformation Processing Systems 16 , MIT Press, Cambridge, MA.\\nRousseauw, J., du Plessis, J., Benade, A., Jordaan, P., Kotze, J., Jooste, P.\\nand Ferreira, J. (1983). Coronary risk factor screening in three rural\\ncommunities, South African Medical Journal 64: 430–436.\\nRoweis, S. T. and Saul, L. K. (2000). Locally linear embedding, Science\\n290: 2323–2326.\\nRumelhart, D., Hinton, G. and Williams, R. (1986). Learning internal rep-\\nresentations by error propagation, inD. Rumelhart and J. McClelland\\n(eds), Parallel Distributed Processing: Explorations in the Micr ostruc-\\nture of Cognition , The MIT Press, Cambridge, MA., pp. 318–362.\\nSachs, K., Perez, O., Pe’er, D., Lauﬀenburger, D. and Nolan, G. (2003).\\nCausal protein-signaling networks derived from multiparameter single-\\ncell data, Science 308(5721): 523–529.\\nSchapire, R. (1990). The strength of weak learnability, Machine Learning\\n5(2): 197–227.\\nSchapire, R. (2002). The boosting approach to machine learning: an\\noverview, inD. Denison, M. Hansen, C. Holmes, B. Mallick and B. Yu\\n(eds), MSRI workshop on Nonlinear Estimation and Classiﬁcation ,\\nSpringer, New York.\\nSchapire, R. and Singer, Y. (1999). Improved boosting algorithms using\\nconﬁdence-rated predictions, Machine Learning 37(3): 297–336.\\nSchapire, R., Freund, Y., Bartlett, P. and Lee, W. (1998). Boosting the\\nmargin: a new explanation for the eﬀectiveness of voting methods,\\nAnnals of Statistics 26(5): 1651–1686.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 740}, page_content='722 References\\nSch¨ olkopf, B., Smola, A. and M¨ uller, K.-R. (1999). Kernel principal compo-\\nnent analysis, inB. Sch¨ olkopf, C. Burges and A. Smola (eds), Advances\\nin Kernel Methods—Support Vector Learning , MIT Press, Cambridge,\\nMA, USA, pp. 327–352.\\nSchwarz, G. (1978). Estimating the dimension of a model, Annals of Statis-\\ntics6(2): 461–464.\\nScott, D. (1992). Multivariate Density Estimation: Theory, Practice, and\\nVisualization , Wiley, New York.\\nSeber, G. (1984). Multivariate Observations , Wiley, New York.\\nSegal, M. (2004). Machine learning benchmarks and random forest regres-\\nsion,Technical report , eScholarship Repository, University of Califor-\\nnia.http://repositories.edlib.org/cbmb/bench rfregn.\\nShao, J. (1996). Bootstrap model selection, Journal of the American Sta-\\ntistical Association 91: 655–665.\\nShenoy, P. and Shafer, G. (1988). An axiomatic framework for Bayesian\\nand belief-function propagation, AAAI Workshop on Uncertainty in\\nAI, North-Holland, pp. 307–314.\\nShort, R. and Fukunaga, K. (1981). The optimal distance measure for near-\\nest neighbor classiﬁcation, IEEE Transactions on Information Theory\\n27: 622–627.\\nSilverman, B. (1986). Density Estimation for Statistics and Data Analysis ,\\nChapman and Hall, London.\\nSilvey, S. (1975). Statistical Inference , Chapman and Hall, London.\\nSimard, P., Cun, Y. L. and Denker, J. (1993). Eﬃcient pattern recognition\\nusing a new transformation distance, Advances in Neural Information\\nProcessing Systems , Morgan Kaufman, San Mateo, CA, pp. 50–58.\\nSimon, R. M., Korn, E. L., McShane, L. M., Radmacher, M. D., Wright,\\nG. and Zhao, Y. (2004). Design and Analysis of DNA Microarray\\nInvestigations , Springer, New York.\\nSj¨ ostrand, K., Rostrup, E., Ryberg, C., Larsen, R., Studholme, C., Baezner,\\nH., Ferro, J., Fazekas, F., Pantoni, L., Inzitari, D. and Waldemar,\\nG. (2007). Sparse decomposition and modeling of anatomical shape\\nvariation, IEEE Transactions on Medical Imaging 26(12): 1625–1635.\\nSpeed, T. and Kiiveri, H. T. (1986). Gaussian Markov distributions over\\nﬁnite graphs, Annals of Statistics 14: 138–150.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 741}, page_content='References 723\\nSpeed, T. (ed.) (2003). Statistical Analysis of Gene Expression Microarray\\nData, Chapman and Hall, London.\\nSpiegelhalter, D., Best, N., Gilks, W. and Inskip, H. (1996). Hepatitis\\nB: a case study in MCMC methods, inW. Gilks, S. Richardson and\\nD. Spegelhalter (eds), Markov Chain Monte Carlo in Practice , Inter-\\ndisciplinary Statistics, Chapman and Hall, London, pp. 21–43.\\nSpielman, D. A. and Teng, S.-H. (1996). Spectral partitioning works: Pla-\\nnar graphs and ﬁnite element meshes, IEEE Symposium on Founda-\\ntions of Computer Science , pp. 96–105.\\nStamey, T., Kabalin, J., McNeal, J., Johnstone, I., Freiha, F., Redwine, E.\\nand Yang, N. (1989). Prostate speciﬁc antigen in the diagnosis and\\ntreatment of adenocarcinoma of the prostate II radical prostatectomy\\ntreated patients, Journal of Urology 16: 1076–1083.\\nStone, C., Hansen, M., Kooperberg, C. and Truong, Y. (1997). Polynomial\\nsplines and their tensor products (with discussion), Annals of Statistics\\n25(4): 1371–1470.\\nStone, M. (1974). Cross-validatory choice and assessment of statistica l\\npredictions, Journal of the Royal Statistical Society Series B 36: 111–\\n147.\\nStone, M. (1977). An asymptotic equivalence of choice of model by cross-\\nvalidation and Akaike’s criterion, Journal of the Royal Statistical So-\\nciety Series B. 39: 44–7.\\nStone, M. and Brooks, R. J. (1990). Continuum regression: cross-validated\\nsequentially constructed prediction embracing ordinary least squares,\\npartial least squares and principal components regression (Corr: V54\\np906-907), Journal of the Royal Statistical Society, Series B 52: 237–\\n269.\\nStorey, J. (2002). A direct approach to false discovery rates, Journal of the\\nRoyal Statistical Society B. 64(3): 479–498.\\nStorey, J. (2003). The positive false discovery rate: A Bayesian inter preta-\\ntion and the q-value, Annals of Statistics 31: 2013–2025.\\nStorey, J. and Tibshirani, R. (2003). Statistical signiﬁcance for genomewide\\nstudies, Proceedings of the National Academy of Sciences 100-: 9440–\\n9445.\\nStorey, J., Taylor, J. and Siegmund, D. (2004). Strong control, conservativ e\\npoint estimation, and simultaneous conservative consistency of false\\ndiscovery rates: A uniﬁed approach., Journal of the Royal Statistical\\nSociety, Series B 66: 187–205.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 742}, page_content='724 References\\nSurowiecki, J. (2004). The Wisdom of Crowds: Why the Many are Smarter\\nthan the Few and How Collective Wisdom Shapes Business, Eco-\\nnomics, Societies and Nations. , Little, Brown.\\nSwayne, D., Cook, D. and Buja, A. (1991). Xgobi: Interactive dynamic\\ngraphics in the X window system with a link to S, ASA Proceedings\\nof Section on Statistical Graphics , pp. 1–8.\\nTanner, M. and Wong, W. (1987). The calculation of posterior distribu-\\ntions by data augmentation (with discussion), Journal of the American\\nStatistical Association 82: 528–550.\\nTarpey, T. and Flury, B. (1996). Self-consistency: A fundamental concept\\nin statistics, Statistical Science 11: 229–243.\\nTenenbaum, J. B., de Silva, V. and Langford, J. C. (2000). A global\\ngeometric framework for nonlinear dimensionality reduction, Science\\n290: 2319–2323.\\nTibshirani, R. (1996). Regression shrinkage and selection via the lasso,\\nJournal of the Royal Statistical Society, Series B 58: 267–288.\\nTibshirani, R. and Hastie, T. (2007). Margin trees for high-dimensional\\nclassiﬁcation, Journal of Machine Learning Research 8: 637–652.\\nTibshirani, R. and Knight, K. (1999). Model search and inference by boot-\\nstrap “bumping, Journal of Computational and Graphical Statistics\\n8: 671–686.\\nTibshirani, R. and Wang, P. (2007). Spatial smoothing and hot spot de-\\ntection for CGH data using the fused lasso, Biostatistics 9: 18–29.\\nTibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2001a). Diagno sis\\nof multiple cancer types by shrunken centroids of gene expression,\\nProceedings of the National Academy of Sciences 99: 6567–6572.\\nTibshirani, R., Hastie, T., Narasimhan, B. and Chu, G. (2003). Class\\nprediction by nearest shrunken centroids, with applications to DNA\\nmicroarrays, Statistical Science 18(1): 104–117.\\nTibshirani, R., Saunders, M., Rosset, S., Zhu, J. and Knight, K. (2005).\\nSparsity and smoothness via the fused lasso, Journal of the Royal\\nStatistical Society, Series B 67: 91–108.\\nTibshirani, R., Walther, G. and Hastie, T. (2001b). Estimating the number\\nof clusters in a dataset via the gap statistic, Journal of the Royal\\nStatistical Society, Series B. 32(2): 411–423.\\nTropp, J. (2004). Greed is good: algorithmic results for sparse approxim a-\\ntion,IEEE Transactions on Information Theory 50: 2231– 2242.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 743}, page_content='References 725\\nTropp, J. (2006). Just relax: convex programming methods for identify-\\ning sparse signals in noise, IEEE Transactions on Information Theory\\n52: 1030–1051.\\nValiant, L. G. (1984). A theory of the learnable, Communications of the\\nACM27: 1134–1142.\\nvan der Merwe, A. and Zidek, J. (1980). Multivariate regression analysis\\nand canonical variates, The Canadian Journal of Statistics 8: 27–39.\\nVapnik, V. (1996). The Nature of Statistical Learning Theory , Springer,\\nNew York.\\nVapnik, V. (1998). Statistical Learning Theory , Wiley, New York.\\nVidakovic, B. (1999). Statistical Modeling by Wavelets , Wiley, New York.\\nvon Luxburg, U. (2007). A tutorial on spectral clustering, Statistics and\\nComputing 17(4): 395–416.\\nWahba, G. (1980). Spline bases, regularization, and generalized cross-\\nvalidation for solving approximation problems with large quantities\\nof noisy data, Proceedings of the International Conference on Approx-\\nimation theory in Honour of George Lorenz , Academic Press, Austin,\\nTexas, pp. 905–912.\\nWahba, G. (1990). Spline Models for Observational Data , SIAM, Philadel-\\nphia.\\nWahba, G., Lin, Y. and Zhang, H. (2000). GACV for support vector ma-\\nchines, inA. Smola, P. Bartlett, B. Sch¨ olkopf and D. Schuurmans\\n(eds), Advances in Large Margin Classiﬁers , MIT Press, Cambridge,\\nMA., pp. 297–311.\\nWainwright, M. (2006). Sharp thresholds for noisy and high-dimensional\\nrecovery of sparsity using ℓ1-constrained quadratic programming,\\nTechnical report , Department of Statistics, University of California,\\nBerkeley.\\nWainwright, M. J., Ravikumar, P. and Laﬀerty, J. D. (2007). High-\\ndimensional graphical model selection using ℓ1-regularized logistic re-\\ngression, inB. Sch¨ olkopf, J. Platt and T. Hoﬀman (eds), Advances\\nin Neural Information Processing Systems 19 , MIT Press, Cambridge,\\nMA, pp. 1465–1472.\\nWasserman, L. (2004). All of Statistics: a Concise Course in Statistical\\nInference , Springer, New York.\\nWeisberg, S. (1980). Applied Linear Regression , Wiley, New York.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 744}, page_content='726 References\\nWerbos, P. (1974). Beyond Regression , PhD thesis, Harvard University.\\nWeston, J. and Watkins, C. (1999). Multiclass support vector machines, in\\nM. Verleysen (ed.), Proceedings of ESANN99 , D. Facto Press, Brussels.\\nWhittaker, J. (1990). Graphical Models in Applied Multivariate Statistics ,\\nWiley, Chichester.\\nWickerhauser, M. (1994). Adapted Wavelet Analysis from Theory to Soft-\\nware, A.K. Peters Ltd, Natick, MA.\\nWidrow, B. and Hoﬀ, M. (1960). Adaptive switching circuits, IRE\\nWESCON Convention record , Vol. 4. pp 96-104; Reprinted in An-\\ndersen and Rosenfeld (1988).\\nWold, H. (1975). Soft modelling by latent variables: the nonlinear iterativ e\\npartial least squares (NIPALS) approach, Perspectives in Probability\\nand Statistics, In Honor of M. S. Bartlett , pp. 117–144.\\nWolpert, D. (1992). Stacked generalization, Neural Networks 5: 241–259.\\nWu, T. and Lange, K. (2007). The MM alternative to EM, unpublished.\\nWu, T. and Lange, K. (2008). Coordinate descent procedures for lasso\\npenalized regression, Annals of Applied Statistics 2(1): 224–244.\\nYee, T. and Wild, C. (1996). Vector generalized additive models, Journal\\nof the Royal Statistical Society, Series B. 58: 481–493.\\nYuan, M. and Lin, Y. (2007). Model selection and estimation in regression\\nwith grouped variables, Journal of the Royal Statistical Society, Series\\nB68(1): 49–67.\\nZhang, P. (1993). Model selection via multifold cross-validation, Annals of\\nStatistics 21: 299–311.\\nZhang, T. and Yu, B. (2005). Boosting with early stopping: convergence\\nand consistency, Annals of Statistics 33: 1538–1579.\\nZhao, P. and Yu, B. (2006). On model selection consistency of lasso, Jour-\\nnal of Machine Learning Research 7: 2541–2563.\\nZhao, P., Rocha, G. and Yu, B. (2008). The composite absolute penalties\\nfor grouped and hierarchichal variable selection, Annals of Statistics .\\n(to appear).\\nZhu, J. and Hastie, T. (2004). Classiﬁcation of gene microarrays by pena l-\\nized logistic regression, Biostatistics 5(2): 427–443.\\nZhu, J., Zou, H., Rosset, S. and Hastie, T. (2005). Multiclass adaboost ,\\nUnpublished.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 745}, page_content='References 727\\nZou, H. (2006). The adaptive lasso and its oracle properties, Journal of\\nthe American Statistical Association 101: 1418–1429.\\nZou, H. and Hastie, T. (2005). Regularization and variable selection via\\nthe elastic net, Journal of the Royal Statistical Society Series B.\\n67(2): 301–320.\\nZou, H., Hastie, T. and Tibshirani, R. (2006). Sparse principal com-\\nponent analysis, Journal of Computational and Graphical Statistics\\n15(2): 265–28.\\nZou, H., Hastie, T. and Tibshirani, R. (2007). On the degrees of freedom\\nof the lasso, Annals of Statistics 35(5): 2173–2192.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 746}, page_content='728 References'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 747}, page_content='This is page 729\\nPrinter: Opaque this\\nAuthor Index\\nAbu-Mostafa, Y. 95, 474\\nAckley, D. H. 645\\nAdam, B.-L. 664\\nAgrawal, R. 489–491, 578\\nAgresti, A. 385, 638, 640\\nAhn, J. 695\\nAkaike, H. 257\\nAllen, D. 257\\nAmbroise, C. 247\\nAmit, Y. 602\\nAnderson, J. R. 641\\nAnderson, T. 645\\nAngelo, M. 654, 658\\nArdekani, A. M. 664\\nBach, F. 569\\nBaezner, H. 551\\nBair, E. 676, 679–683, 693\\nBakin, S. 90\\nBakiri, G. 605, 606\\nBanerjee, O. 636\\nBarnhill, S. 658\\nBarron, A. 415\\nBarry, R. 371\\nBartlett, P. 384, 615Baskett, F. 480\\nBaxter, J. 384\\nBecker, R. 369\\nBell, A. 578\\nBellman, R. E. 22\\nBenade, A. 122\\nBengio, Y. 404, 407, 408, 414, 644\\nBenjamini, Y. 687, 689, 693\\nBentley, J. 480\\nBest, N. 292\\nBibby, J. 94, 135, 441, 539, 559,\\n578, 630, 679\\nBickel, P. 652\\nBickel, P. J. 89\\nBishop, C. 38, 233, 414, 623, 645\\nBishop, Y. 629, 638\\nBloomﬁeld, C. 663\\nBoser, B. 404, 414\\nBotha, J. 334\\nBottou, L. 404, 407, 408, 414, 644\\nBoyd, S. 125, 632\\nBreiman, L. 85, 243, 251, 257, 292,\\n308, 310, 334, 339, 367,\\n384, 451, 453, 455, 554,\\n587, 602'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 748}, page_content='730 Author Index\\nBremaud, P. 577\\nBrin, S. 577\\nBrooks, R. J. 81\\nBrown, P. 679\\nBruce, A. 181\\nB¨ uhlmann, P. 87, 361, 384\\nBuja, A. 110, 297, 441, 446, 451,\\n455, 565, 574, 576, 578\\nBunea, F. 91\\nBurges, C. 455\\nButte, A. 631\\nCaligiuri, M. 663\\nCallow, M. 686, 693\\nCampo, E. 674\\nCandes, E. 86, 89, 613\\nCarlin, J. 292\\nCazares, L. H. 664\\nChambers, J. 334\\nChan, W. C. 674\\nChaudhuri, S. 631, 633\\nChen, L. 574, 576, 578\\nChen, S. S. 68, 94\\nCherkassky, V. 38, 239, 257\\nChu, G. 684, 693\\nChui, C. 181\\nClark, L. 331\\nClark, M. 539\\nClements, M. A. 664\\nCleveland, W. 369\\nCliﬀord, P. 629\\nCohen, A. 668, 669\\nCoifman, R. R. 679\\nColler, H. 663\\nComon, P. 578\\nConnors, J. M. 674\\nCook, D. 565, 578\\nCook, N. 317\\nCopas, J. B. 94, 610\\nCover, T. 257, 465, 481\\nCox, D. 292, 645\\nCressie, N. 171\\nCsiszar, I. 292\\nCun, Y. L. 407, 471, 481\\nCutler, A. 554Dale, M. 526\\nDasarathy, B. 480, 481\\nd’Aspremont, A. 636\\nDaubechies, I. 92, 181\\nDavis, J. W. 664\\nde Boor, C. 181\\nDe Mol, C. 92\\nde Silva, V. 573\\nDefrise, M. 92\\nDempster, A. 292, 449, 633\\nDenham, M. 679\\nDenker, J. 404, 407, 414, 471, 481\\nDevijver, P. 480\\nDietterich, T. 286, 602, 605, 606,\\n623\\nDonath, W. E. 578\\nDonoho, D. 68, 86, 91, 94, 179,\\n181, 554, 613\\nDowning, J. 663\\nDrton, M. 631, 633\\ndu Plessis, J. 122\\nDuan, N. 480\\nDubes, R. 508, 522\\nDuchamp, T. 541\\nDuda, R. 38, 135\\nDudoit, S. 686, 693\\nDuin, R. 624\\nEdwards, D. 645\\nEfron, B. 73, 86, 90, 94, 97, 98,\\n128, 231, 254, 257, 292,\\n334, 568, 609, 692, 693\\nElad, M. 613\\nElith, J. 375, 376, 378\\nEskin, E. 668, 669\\nEvgeniou, T. 168, 181, 455\\nFan, J. 92, 216, 539, 654\\nFan, Y. 654\\nFazekas, F. 551\\nFeng, Z. 664\\nFerreira, J. 122\\nFerro, J. 551\\nFiedler, M. 578\\nFienberg, S. 585, 629, 638'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 749}, page_content='Author Index 731\\nFinkel, R. 480\\nFisher, N. 334\\nFisher, R. A. 136, 455\\nFisher, R. I. 674\\nFisher, W. 310\\nFishman, D. A. 664\\nFix, E. 481\\nFlury, B. 578\\nForgy, E. 578\\nFrancis, M. 375, 376, 378\\nFrank, I. 81, 82, 94\\nFrean, M. 384\\nFreiha, F. 3, 49\\nFreund, Y. 337, 383, 384, 615\\nFridlyand, J. 693\\nFriedman, J. 38, 81, 82, 85, 92–94,\\n111, 121, 126, 251, 257,\\n258, 308, 310, 334, 339,\\n345, 365, 367, 384, 391,\\n414, 437, 451, 453, 475,\\n480, 565, 578, 602, 611,\\n617–621, 623, 636, 657,\\n661, 667\\nFriedman, N. 629, 630, 645\\nFu, W. 91, 92\\nFukunaga, K. 475\\nFurnival, G. 57\\nFusaro, V. 664\\nGaasenbeek, M. 663\\nGanapathi, V. 642\\nGao, H. 181\\nGascoyne, R. D. 674\\nGelfand, A. 292\\nGelman, A. 292\\nGeman, D. 292, 602\\nGeman, S. 292\\nGenkin, A. 661\\nGenovese, C. 693\\nGerald, W. 654, 658\\nGersho, A. 514, 515, 526, 578\\nGhaoui, L. E. 636\\nGijbels, I. 216\\nGilks, W. 292\\nGill, P. 96, 421Girosi, F. 168, 174, 181, 415\\nGolub, G. 257, 335, 535\\nGolub, T. 631, 654, 658, 663\\nGoodall, C. 578\\nGordon, A. 578\\nGray, R. 514, 515, 526, 578\\nGreen, P. 181, 183, 334\\nGreenacre, M. 455\\nGreenshtein, E. 91\\nGuo, Y. 657\\nGuyon, I. 658\\nHaﬀner, P. 404, 407, 408, 414, 644\\nHall, P. 292, 602, 619\\nHammersley, J. M. 629\\nHand, D. 135, 475\\nHanley, J. 317\\nHansen, M. 328\\nHansen, R. 93\\nHart, P. 38, 135, 465, 480, 481\\nHartigan, J. A. 510, 578\\nHastie, T. 72, 73, 78, 86, 88, 90,\\n92–94, 97, 98, 110, 121,\\n122, 126, 137, 174, 216,\\n257, 297, 299, 304, 334,\\n339, 345, 348, 349, 375,\\n376, 378, 384, 385, 414,\\n428, 431, 434, 437, 441,\\n446, 451, 455, 475, 478,\\n480, 481, 519, 539, 550,\\n565, 568, 578, 606, 609–\\n611, 614, 615, 636, 657,\\n658, 660–662, 664, 667,\\n676, 679–683, 693\\nHatef, M. 624\\nHathaway, R. J. 292\\nHeath, M. 257\\nHebb, D. 414\\nHenderson, D. 404, 414\\nHerman, A. 334\\nHertz, J. 414\\nHinkley, D. 292\\nHinton, G. 292, 334, 408, 414, 644,\\n645\\nHitt, B. A. 664'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 750}, page_content='732 Author Index\\nHo, T. K. 602\\nHochberg, Y. 687, 689, 693\\nHodges, J. 481\\nHoeﬂing, H. 92, 93, 642, 667\\nHoerl, A. E. 64, 94\\nHoﬀ, M. 396, 414\\nHoﬀman, A. J. 578\\nHofmann, H. 578\\nHolland, P. 629, 638\\nHong, W. 684\\nHothorn, T. 87, 361, 384\\nHoward, R. 404, 414\\nHuard, C. 663\\nHubbard, W. 404, 414\\nHuber, P. 349, 414, 435, 565, 578\\nHunter, D. 294\\nHyv¨ arinen, A. 560, 562, 578, 583\\nIhaka, R. 455\\nInskip, H. 292\\nInzitari, D. 551\\nIzenman, A. 84\\nJackel, L. 404, 414\\nJacobs, R. 334\\nJain, A. 508, 522\\nJames, G. 606\\nJancey, R. 578\\nJensen, F. V. 629\\nJiang, W. 384\\nJirou´ sek, R. 640\\nJohnson, N. 412\\nJohnstone, I. 3, 49, 73, 86, 94, 97,\\n98, 179, 181, 609, 613\\nJoliﬀe, I. T. 550\\nJones, L. 415\\nJones, M. 168, 174, 181, 415\\nJooste, P. 122\\nJordaan, P. 122\\nJordan, M. 334, 569, 645\\nKabalin, J. 3, 49\\nKalbﬂeisch, J. 674, 693\\nKarhunen, J. 583\\nKaski, S. 531, 532, 578Kaufman, L. 517, 526, 578\\nKearns, M. 380\\nKennard, R. 64, 94\\nKent, J. 94, 135, 441, 539, 559,\\n578, 630, 679\\nKiiveri, H. T. 632\\nKim, S.-J. 125\\nKishon, E. 539\\nKittler, J. 480, 624\\nKleinberg, E. M. 602\\nKnight, K. 91, 292, 666, 693\\nKoh, K. 125\\nKohane, I. 631\\nKohavi, R. 243, 257\\nKohn, E. 664\\nKohonen, T. 462, 481, 531, 532,\\n578\\nKoller, D. 629, 630, 642, 645\\nKooperberg, C. 328\\nKorn, E. L. 693\\nKotze, J. 122\\nKressel, U. 437\\nKrogh, A. 414\\nLadd, C. 654, 658\\nLaﬀerty, J. 90, 304\\nLaﬀerty, J. D. 642\\nLagus, K. 531, 532, 578\\nLaird, N. 292, 449\\nLambert, D. 376\\nLander, E. 654, 658, 663\\nLange, K. 92, 294, 583, 584\\nLangford, J. C. 573\\nLarsen, R. 551\\nLatulippe, E. 654, 658\\nLauﬀenburger, D. 625\\nLauritzen, S. 629, 632, 645\\nLawson, C. 93\\nLe Cun, Y. 404, 406–408, 414\\nLeathwick, J. 375, 376, 378\\nLeblanc, M. 292\\nLeCun, Y. 644\\nLee, D. 552, 553\\nLee, M.-L. 693\\nLee, S.-I. 642'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 751}, page_content='Author Index 733\\nLee, W. 384, 615\\nLeslie, C. 668, 669\\nLevina, E. 652, 693\\nLevine, P. J. 664\\nLewis, D. 661\\nLi, K.-C. 480\\nLi, R. 92\\nLin, H. 331\\nLin, Y. 90, 304, 428, 455\\nLiotta, L. A. 664\\nLittle, R. 332, 647\\nLittman, M. 578\\nLiu, H. 90, 304\\nLloyd, S. 481, 578\\nLoader, C. 209, 216\\nLoda, M. 654, 658\\nLugosi, G. 384\\nLoh, M. 663\\nLoh, W. 310\\nMa, Y. 257\\nMacnaughton Smith, P. 526\\nMacKay, D. 623\\nMacQueen, J. 481, 578\\nMadigan, D. 257, 292, 661\\nMakeig, S. 564, 565\\nMannila, H. 489–491, 578\\nMardia, K. 94, 135, 441, 539, 559,\\n578, 630, 679\\nMarron, J. 695\\nMason, L. 384\\nMassart, D. 517\\nMatas, J. 624\\nMcCullagh, P. 638, 640\\nMcCulloch, C. 331\\nMcCulloch, W. 414\\nMcLachlan, G. 135, 247\\nMcNeal, J. 3, 49\\nMcNeil, B. 317\\nMcShane, L. M. 693\\nMease, D. 384, 603\\nMeinshausen, N. 91, 635, 642\\nMeir, R. 384\\nMesirov, J. 654, 658, 663\\nMills, G. B. 664Mockett, L. 526\\nMorgan, J. N. 334\\nMotwani, R. 577\\nMukherjee, S. 654, 658\\nMulier, F. 38, 239\\nMuller-Hermelink, H. K. 674\\nM¨ uller, K.-R. 547, 548\\nMunro, S. 397\\nMurray, W. 96, 421\\nMyles, J. 475\\nNadler, B. 679\\nNarasimhan, B. 693\\nNeal, R. 268, 292, 409–412, 414,\\n605, 623\\nNelder, J. 638, 640\\nNoble, W. S. 668, 669\\nNolan, G. 625\\nNowlan, S. 334\\nOja, E. 560, 562, 578, 583\\nOlesen, K. G. 629\\nOlshen, R. 251, 308, 310, 334, 367,\\n451, 453\\nOnton, J. 564, 565\\nOsborne, M. 76, 94\\nOsindero, S. 644\\nPaatero, A. 531, 532, 578\\nPace, R. K. 371\\nPage, L. 577\\nPalmer, R. 414\\nPantoni, L. 551\\nPark, M. Y. 94, 126, 661\\nParker, D. 414\\nPaul, D. 676, 679–683, 693\\nPearl, J. 629, 645\\nPe’er, D. 625\\nPerez, O. 625\\nPeterson 641\\nPetricoin, E. F. 664\\nPitts, W. 414\\nPlastria, F. 517\\nPlatt, J. 453\\nPoggio, T. 168, 174, 181, 415, 455,\\n654, 658'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 752}, page_content='734 Author Index\\nPontil, M. 168, 181, 455\\nPopescu, B. 617–619, 621, 623\\nPrentice, R. 674, 693\\nPresnell, B. 76, 94\\nPˇ reuˇ cil, S. 640\\nQu, Y. 664\\nQuinlan, R. 312, 334, 624\\nRadmacher, M. D. 693\\nRaftery, A. 257, 292\\nRamaswamy, S. 654, 658\\nRamsay, J. 181, 578\\nRao, C. R. 455\\nR¨ atsch, G. 384, 615\\nRavikumar, P. 90, 304, 642\\nRedwine, E. 3, 49\\nReich, M. 654, 658\\nRichardson, J. 375\\nRichardson, T. S. 631, 633\\nRidgeway, G. 361\\nRieger, K. 684\\nRifkin, R. 654, 658\\nRipley, B. D. 38, 131, 135, 136,\\n234, 308, 310, 400, 414,\\n415, 455, 468, 480, 481,\\n641, 645\\nRissanen, J. 257\\nRitov, Y. 89, 91\\nRobbins, H. 397\\nRocha, G. 90\\nRoosen, C. 414\\nRosenblatt, F. 102, 129, 414\\nRosenwald, A. 674\\nRosset, S. 89, 98, 348, 349, 385,\\n426, 428, 434, 610, 611,\\n615, 657, 661, 664, 666,\\n693\\nRostrup, E. 551\\nRousseauw, J. 122\\nRousseeuw, P. 517, 526, 578\\nRowe, D. 375\\nRoweis, S. T. 573\\nRubin, D. 292, 332, 449, 647\\nRumelhart, D. 414Ryberg, C. 551\\nSaarela, A. 531, 532, 578\\nSachs, K. 625\\nSaloj¨ arvi, J. 531, 532, 578\\nSaul, L. K. 573\\nSaunders, M. 68, 94, 666, 693\\nSchapire, R. 337, 380, 383, 384,\\n615\\nSchellhammer, P. F. 664\\nSchnitzler, C. 334\\nSch¨ olkopf, B. 547, 548\\nSchroeder, A. 391\\nSchwarz, G. 233, 257\\nScott, D. 216\\nSeber, G. 94\\nSegal, M. 596\\nSejnowski, T. 578, 645\\nSemmes, O. J. 664\\nSeung, H. 552, 553\\nShafer, G. 629\\nShao, J. 257\\nShenoy, P. 629\\nShort, R. 475\\nShustek, L. 480\\nShyu, M. 369\\nSiegmund, D. 689\\nSilverman, B. 181, 183, 216, 334,\\n486, 567, 578\\nSilvey, S. 292\\nSimard, P. 407, 471, 480, 481\\nSimon, R. M. 693\\nSimone, C. 664\\nSinger, Y. 384\\nSj¨ ostrand, K. 551\\nSlate, E. 331\\nSlonim, D. 631, 663\\nSmeland, E. B. 674\\nSmith, A. 292\\nSmola, A. 547, 548\\nSonquist, J. A. 334\\nSpector, P. 243, 257\\nSpeed, T. 632, 686, 693\\nSpiegelhalter, D. 292, 629\\nSpiegelman, C. 679'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 753}, page_content='Author Index 735\\nSpielman, D. A. 578\\nSrikant, R. 489–491, 578\\nStamey, T. 3, 49\\nStaudt, L. M. 674\\nSteinberg, S. M. 664\\nStern, H. 292\\nStodden, V. 554\\nStone, C. 251, 308, 310, 328, 334,\\n367, 451, 453\\nStone, M. 81, 257\\nStorey, J. 689, 692, 693, 697, 698\\nStork, D. 38, 135\\nStudholme, C. 551\\nStuetzle, W. 391, 414, 541, 578\\nSurowiecki, J. 286\\nSwayne, D. 565, 578\\nTamayo, P. 631, 654, 658, 663\\nTang, J. 684\\nTanner, M. 292\\nTao, T. 89, 613\\nTarpey, T. 578\\nTaylor, J. 88, 94, 610, 614, 689\\nTaylor, P. 375, 376, 378\\nTeh, Y.-W. 644\\nTenenbaum, J. B. 573\\nTeng, S.-H. 578\\nThomas, J. 257\\nTibshirani, R. 73, 78, 86, 88, 90,\\n92–94, 97, 98, 110, 121,\\n122, 126, 137, 216, 257,\\n292, 297, 299, 304, 334,\\n339, 345, 384, 428, 431,\\n434, 437, 441, 446, 451,\\n455, 475, 478, 480, 481,\\n519, 550, 565, 568, 609–\\n611, 614, 636, 642, 657,\\n658, 660, 661, 666, 667,\\n676, 679–684, 692, 693\\nToivonen, H. 489–491, 578\\nTraskin, M. 384\\nTrendaﬁlov, N. T. 550\\nTropp, J. 91\\nTruong, Y. 328\\nTsybakov, A. 89, 91Tukey, J. 414, 565, 578\\nTurlach, B. 76, 94\\nTurnbull, B. 331\\nTusher, V. 684, 692\\nTusn´ ady, G. 292\\nUddin, M. 550\\nValiant, L. G. 380\\nvan der Merwe, A. 84\\nVan Loan, C. 335, 535\\nVandenberghe, L. 632\\nVanichsetakul, N. 310\\nVapnik, V. 38, 102, 132, 135, 171,\\n257, 438, 455, 658\\nVayatis, N. 384\\nVazirani, U. 380\\nVerkamo, A. I. 489–491, 578\\nVidakovic, B. 181\\nvon Luxburg, U. 578\\nWahba, G. 168, 169, 181, 257, 268,\\n428, 429, 455\\nWainwright, M. 91\\nWainwright, M. J. 642\\nWaldemar, G. 551\\nWalther, G. 88, 94, 519, 610, 614\\nWang, P. 667\\nWard, M. D. 664\\nWarmuth, M. 615\\nWasserman, L. 90, 304, 626, 645,\\n693\\nWatkins, C. 658\\nWegkamp, M. 91\\nWeisberg, S. 94\\nWerbos, P. 414\\nWermuth, N. 645\\nWeston, J. 658, 668, 669\\nWhittaker, J. 632, 633, 641, 645\\nWickerhauser, M. 181\\nWidrow, B. 396, 414\\nWild, C. 300\\nWilliams, R. 414\\nWilliams, W. 526\\nWilson, R. 57'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 754}, page_content='736 Author Index\\nWinograd, T. 577\\nWold, H. 94\\nWolpert, D. 292\\nWong, M. A. 510\\nWong, W. 292\\nWright, G. 664, 674, 693\\nWright, M. 96, 421\\nWu, T. 92, 294, 583\\nWyner, A. 384, 603\\nYang, N. 3, 49\\nYang, Y. 686, 693\\nYasui, Y. 664\\nYeang, C. 654, 658\\nYee, T. 300\\nYekutieli, Y. 693\\nYu, B. 90, 91, 384\\nYuan, M. 90\\nZhang, H. 90, 304, 428, 455\\nZhang, J. 409–412, 605\\nZhang, P. 257\\nZhang, T. 384\\nZhao, P. 90, 91\\nZhao, Y. 693\\nZhu, J. 89, 98, 174, 348, 349, 385,\\n426, 428, 434, 610, 611,\\n615, 657, 661, 664, 666,\\n693\\nZidek, J. 84\\nZou, H. 72, 78, 92, 349, 385, 550,\\n662, 693'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 755}, page_content='This is page 737\\nPrinter: Opaque this\\nIndex\\nL1regularization, seeLasso\\nActivation function, 392–395\\nAdaBoost, 337–346\\nAdaptive lasso, 92\\nAdaptive methods, 429\\nAdaptive nearest neighbor meth-\\nods, 475–478\\nAdaptive wavelet ﬁltering, 181\\nAdditive model, 295–304\\nAdjusted response, 297\\nAﬃne set, 130\\nAﬃne-invariant average, 482, 540\\nAIC,seeAkaike information cri-\\nterion\\nAkaike information criterion (AIC),\\n230\\nAnalysis of deviance, 124\\nApplications\\nabstracts, 672\\naorta, 204\\nbone, 152\\nCalifornia housing, 371–372,\\n591\\ncountries, 517demographics, 379–380\\ndocument, 532\\nﬂow cytometry, 637\\ngalaxy, 201\\nheart attack, 122, 146, 207\\nlymphoma, 674\\nmarketing, 488\\nmicroarray, 5, 505, 532\\nnested spheres, 590\\nNew Zealand ﬁsh, 375–379\\nnuclear magnetic resonance,\\n176\\nozone, 201\\nprostate cancer, 3, 49, 61, 608\\nprotein mass spectrometry, 664\\nsatellite image, 470\\nskin of the orange, 429–432\\nspam, 2, 300–304, 313, 320,\\n328, 352, 593\\nvowel, 440, 464\\nwaveform, 451\\nZIP code, 4, 404, 536–539\\nArchetypal analysis, 554–557\\nAssociation rules, 492–495, 499–\\n501'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 756}, page_content='738 Index\\nAutomatic relevance determination,\\n411\\nAutomatic selection of smoothing\\nparameters , 156\\nB-Spline, 186\\nBack-propagation, 392–397, 408–\\n409\\nBackﬁtting, 297, 391\\nBackward\\nselection, 58\\nstepwise selection, 59\\nBackward pass, 396\\nBagging, 282–288, 409, 587\\nBasis expansions and regulariza-\\ntion, 139–189\\nBasis functions, 141, 186, 189, 321,\\n328\\nBatch learning, 397\\nBaum–Welch algorithm, 272\\nBayes\\nclassiﬁer, 21\\nfactor, 234\\nmethods, 233–235, 267–272\\nrate, 21\\nBayesian, 409\\nBayesian information criterion (BIC),\\n233\\nBenjamini–Hochberg method, 688\\nBest-subset selection, 57, 610\\nBetween class covariance matrix,\\n114\\nBias, 16, 24, 37, 160, 219\\nBias-variance decomposition, 24,\\n37, 219\\nBias-variance tradeoﬀ, 37, 219\\nBIC,seeBayesian Information Cri-\\nterion\\nBoltzmann machines, 638–648\\nBonferroni method, 686\\nBoosting, 337–386, 409\\nas lasso regression, 607–609\\nexponential loss and AdaBoost,\\n343\\ngradient boosting, 358implementations, 360\\nmargin maximization, 613\\nnumerical optimization, 358\\npartial-dependence plots, 369\\nregularization path, 607\\nshrinkage, 364\\nstochastic gradient boosting,\\n365\\ntree size, 361\\nvariable importance, 367\\nBootstrap, 249, 261–264, 267, 271–\\n282, 587\\nrelationship to Bayesian method,\\n271\\nrelationship to maximum like-\\nlihood method, 267\\nBottom-up clustering, 520–528\\nBump hunting, seePatient rule\\ninduction method\\nBumping, 290–292\\nC5.0, 624\\nCanonical variates, 441\\nCART, seeClassiﬁcation and re-\\ngression trees\\nCategorical predictors, 10, 310\\nCensored data, 674\\nClassical multidimensional scaling,\\n570\\nClassiﬁcation, 22, 101–137, 305–\\n317, 417–429\\nClassiﬁcation and regression trees\\n(CART), 305–317\\nClique, 628\\nClustering, 501–528\\nk-means, 509–510\\nagglomerative, 523–528\\nhierarchical, 520–528\\nCodebook, 515\\nCombinatorial algorithms, 507\\nCombining models, 288–290\\nCommittee, 289, 587, 605\\nComparison of learning methods,\\n350–352\\nComplete data, 276'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 757}, page_content='Index 739\\nComplexity parameter, 37\\nComputational shortcuts\\nquadratic penalty, 659\\nCondensing procedure, 480\\nConditional likelihood, 31\\nConfusion matrix, 301\\nConjugate gradients, 396\\nConsensus, 285–286\\nConvolutional networks, 407\\nCoordinate descent, 92, 636, 668\\nCOSSO, 304\\nCost complexity pruning, 308\\nCovariance graph, 631\\nCpstatistic, 230\\nCross-entropy, 308–310\\nCross-validation, 241–245\\nCubic smoothing spline, 151–153\\nCubic spline, 151–153\\nCurse of dimensionality, 22–26\\nDantzig selector, 89\\nData augmentation, 276\\nDaubechies symmlet-8 wavelets,\\n176\\nDe-correlation, 597\\nDecision boundary, 13–15, 21\\nDecision trees, 305–317\\nDecoder, 515, seeencoder\\nDecomposable models, 641\\nDegrees of freedom\\nin an additive model, 302\\nin ridge regression, 68\\nof a tree, 336\\nof smoother matrices, 153–154,\\n158\\nDelta rule, 397\\nDemmler-Reinsch basis for splines,\\n156\\nDensity estimation, 208–215\\nDeviance, 124, 309\\nDiagonal linear discriminant anal-\\nysis, 651–654\\nDimension reduction, 658\\nfor nearest neighbors, 479\\nDiscrete variables, 10, 310–311Discriminant\\nadaptive nearest neighbor clas-\\nsiﬁer, 475–480\\nanalysis, 106–119\\ncoordinates, 108\\nfunctions, 109–110\\nDissimilarity measure, 503–504\\nDummy variables, 10\\nEarly stopping, 398\\nEﬀective degrees of freedom, 17,\\n68, 153–154, 158, 232, 302,\\n336\\nEﬀective number of parameters,\\n15, 68, 153–154, 158, 232,\\n302, 336\\nEigenvalues of a smoother matrix,\\n154\\nElastic net, 662\\nEM algorithm, 272–279\\nas a maximization-maximization\\nprocedure, 277\\nfor two component Gaussian\\nmixture, 272\\nEncoder, 514–515\\nEnsemble, 616–623\\nEnsemble learning, 605–624\\nEntropy, 309\\nEquivalent kernel, 156\\nError rate, 219–230\\nError-correcting codes, 606\\nEstimates of in-sample prediction\\nerror, 230\\nExpectation-maximization algorithm,\\nseeEM algorithm\\nExtra-sample error, 228\\nFalse discovery rate, 687–690, 692,\\n693\\nFeature, 1\\nextraction, 150\\nselection, 409, 658, 681–683\\nFeed-forward neural networks, 392–\\n408'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 758}, page_content='740 Index\\nFisher’s linear discriminant, 106–\\n119, 438\\nFlexible discriminant analysis, 440–\\n445\\nForward\\nselection, 58\\nstagewise, 86, 608\\nstagewise additive modeling,\\n342\\nstepwise, 73\\nForward pass algorithm, 395\\nFourier transform, 168\\nFrequentist methods, 267\\nFunction approximation, 28–36\\nFused lasso, 666\\nGap statistic, 519\\nGating networks, 329\\nGauss-Markov theorem, 51–52\\nGauss-Newton method, 391\\nGaussian (normal) distribution, 16\\nGaussian graphical model, 630\\nGaussian mixtures, 273, 463, 492,\\n509\\nGaussian radial basis functions,\\n212\\nGBM, seeGradient boosting\\nGBM package, seeGradient boost-\\ning\\nGCV, seeGeneralized cross-validation\\nGEM (generalized EM), 277\\nGeneralization\\nerror, 220\\nperformance, 220\\nGeneralized additive model, 295–\\n304\\nGeneralized association rules, 497–\\n499\\nGeneralized cross-validation, 244\\nGeneralized linear discriminant anal-\\nysis, 438\\nGeneralized linear models, 125\\nGibbs sampler, 279–280, 641\\nfor mixtures, 280\\nGini index, 309Global Markov property, 628\\nGradient Boosting, 359–361\\nGradient descent, 358, 395–397\\nGraph Laplacian, 545\\nGraphical lasso, 636\\nGrouped lasso, 90\\nHaar basis function, 176\\nHammersley-Cliﬀord theorem, 629\\nHard-thresholding, 653\\nHat matrix, 46\\nHelix, 582\\nHessian matrix, 121\\nHidden nodes, 641–642\\nHidden units, 393–394\\nHierarchical clustering, 520–528\\nHierarchical mixtures of experts,\\n329–332\\nHigh-dimensional problems, 649\\nHints, 96\\nHyperplane, seeSeparating Hy-\\nperplane\\nICA,seeIndependent components\\nanalysis\\nImportance sampling, 617\\nIn-sample prediction error, 230\\nIncomplete data, 332\\nIndependent components analysis,\\n557–570\\nIndependent variables, 9\\nIndicator response matrix, 103\\nInference, 261–294\\nInformation\\nFisher, 266\\nobserved, 274\\nInformation theory, 236, 561\\nInner product, 53, 668, 670\\nInputs, 10\\nInstability of trees, 312\\nIntercept, 11\\nInvariance manifold, 471\\nInvariant metric, 471\\nInverse wavelet transform, 179'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 759}, page_content='Index 741\\nIRLS, seeIteratively reweighted\\nleast squares\\nIrreducible error, 224\\nIsing model, 638\\nISOMAP, 572\\nIsometric feature mapping, 572\\nIterative proportional scaling, 585\\nIteratively reweighted least squares\\n(IRLS), 121\\nJensen’s inequality, 293\\nJoin tree, 629\\nJunction tree, 629\\nK-means clustering, 460, 509–514\\nK-medoid clustering, 515–520\\nK-nearest neighbor classiﬁers, 463\\nKarhunen-Loeve transformation (prin-\\ncipal components), 66–\\n67, 79, 534–539\\nKarush-Kuhn-Tucker conditions,\\n133, 420\\nKernel\\nclassiﬁcation, 670\\ndensity classiﬁcation, 210\\ndensity estimation, 208–215\\nfunction, 209\\nlogistic regression, 654\\nprincipal component, 547–550\\nstring, 668–669\\ntrick, 660\\nKernel methods, 167–176, 208–215,\\n423–438, 659\\nKnot, 141, 322\\nKriging, 171\\nKruskal-Shephard scaling, 570\\nKullback-Leibler distance, 561\\nLagrange multipliers, 293\\nLandmark, 539\\nLaplacian, 545\\nLaplacian distribution, 72\\nLAR, seeLeast angle regression\\nLasso, 68–69, 86–90, 609, 635, 636,\\n661fused, 666\\nLatent\\nfactor, 674\\nvariable, 678\\nLearning, 1\\nLearning rate, 396\\nLearning vector quantization, 462\\nLeast angle regression, 73–79, 86,\\n610\\nLeast squares, 11, 32\\nLeave-one-out cross-validation, 243\\nLeNet, 406\\nLikelihood function, 265, 273\\nLinear basis expansion, 139–148\\nLinear combination splits, 312\\nLinear discriminant function, 106–\\n119\\nLinear methods\\nfor classiﬁcation, 101–137\\nfor regression, 43–99\\nLinear models and least squares,\\n11\\nLinear regression of an indicator\\nmatrix, 103\\nLinear separability, 129\\nLinear smoother, 153\\nLink function, 296\\nLLE,seeLocal linear embedding\\nLocal false discovery rate, 693\\nLocal likelihood, 205\\nLocal linear embedding, 572\\nLocal methods in high dimensions,\\n22–27\\nLocal minima, 400\\nLocal polynomial regression, 197\\nLocal regression, 194, 200\\nLocalization in time/frequency, 175\\nLoess (local regression), 194, 200\\nLog-linear model, 639\\nLog-odds ratio (logit), 119\\nLogistic (sigmoid) function, 393\\nLogistic regression, 119–128, 299\\nLogit (log-odds ratio), 119\\nLoss function, 18, 21, 219–223, 346\\nLoss matrix, 310'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 760}, page_content='742 Index\\nLossless compression, 515\\nLossy compression, 515\\nLVQ,seeLearning Vector Quan-\\ntization\\nMahalanobis distance, 441\\nMajority vote, 337\\nMajorization, 294, 553\\nMajorize-Minimize algorithm, 294,\\n584\\nMAP (maximum aposteriori) es-\\ntimate, 270\\nMargin, 134, 418\\nMarket basket analysis, 488, 499\\nMarkov chain Monte Carlo (MCMC)\\nmethods, 279\\nMarkov graph, 627\\nMarkov networks, 638–648\\nMARS, seeMultivariate adaptive\\nregression splines\\nMART, seeMultiple additive re-\\ngression trees\\nMaximum likelihood estimation,\\n31, 261, 265\\nMCMC, seeMarkov Chain Monte\\nCarlo Methods\\nMDL, seeMinimum description\\nlength\\nMean ﬁeld approximation, 641\\nMean squared error, 24, 285\\nMemory-based method, 463\\nMetropolis-Hastings algorithm, 282\\nMinimum description length (MDL),\\n235\\nMinorization, 294, 553\\nMinorize-Maximize algorithm, 294,\\n584\\nMisclassiﬁcation error, 17, 309\\nMissing data, 276, 332–333\\nMissing predictor values, 332–333\\nMixing proportions, 214\\nMixture discriminant analysis, 449–\\n455\\nMixture modeling, 214–215, 272–\\n275, 449–455, 692Mixture of experts, 329–332\\nMixtures and the EM algorithm,\\n272–275\\nMM algorithm, 294, 584\\nMode seekers, 507\\nModel averaging and stacking, 288\\nModel combination, 289\\nModel complexity, 221–222\\nModel selection, 57, 222–223, 230–\\n231\\nModiﬁed regression, 634\\nMonte Carlo method, 250, 495\\nMother wavelet, 178\\nMultidimensional scaling, 570–572\\nMultidimensional splines, 162\\nMultiedit algorithm, 480\\nMultilayer perceptron, 400, 401\\nMultinomial distribution, 120\\nMultiple additive regression trees\\n(MART), 361\\nMultiple hypothesis testing, 683–\\n693\\nMultiple minima, 291, 400\\nMultiple outcome shrinkage and\\nselection, 84\\nMultiple outputs, 56, 84, 103–106\\nMultiple regression from simple uni-\\nvariate regression, 52\\nMultiresolution analysis, 178\\nMultivariate adaptive regression\\nsplines (MARS), 321–327\\nMultivariate nonparametric regres-\\nsion, 445\\nNadaraya–Watson estimate, 193\\nNaive Bayes classiﬁer, 108, 210–\\n211, 694\\nNatural cubic splines, 144–146\\nNearest centroids, 670\\nNearest neighbor methods, 463–\\n483\\nNearest shrunken centroids, 651–\\n654, 694\\nNetwork diagram, 392\\nNeural networks, 389–416'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 761}, page_content='Index 743\\nNewton’s method (Newton-Raphson\\nprocedure), 120–122\\nNon-negative matrix factorization,\\n553–554\\nNonparametric logistic regression,\\n299–304\\nNormal (Gaussian) distribution,\\n16, 31\\nNormal equations, 12\\nNumerical optimization, 395–396\\nObject dissimilarity, 505–507\\nOnline algorithm, 397\\nOptimal scoring, 445, 450–451\\nOptimal separating hyperplane, 132–\\n135\\nOptimism of the training error rate,\\n228–230\\nOrdered categorical (ordinal) pre-\\ndictor, 10, 504\\nOrdered features, 666\\nOrthogonal predictors, 53\\nOverﬁtting, 220, 228–230, 364\\nPageRank, 576\\nPairwise distance, 668\\nPairwise Markov property, 628\\nParametric bootstrap, 264\\nPartial dependence plots, 369–370\\nPartial least squares, 80–82, 680\\nPartition function, 638\\nParzen window, 208\\nPasting, 318\\nPath algorithm, 73–79, 86–89, 432\\nPatient rule induction method(PRIM),\\n317–321, 499–501\\nPeeling, 318\\nPenalization, 607, seeregulariza-\\ntion\\nPenalized discriminant analysis, 446–\\n449\\nPenalized polynomial regression,\\n171\\nPenalized regression, 34, 61–69, 171\\nPenalty matrix, 152, 189Perceptron, 392–416\\nPiecewise polynomials and splines,\\n36, 143\\nPosterior\\ndistribution, 268\\nprobability, 233–235, 268\\nPower method, 577\\nPre-conditioning, 681–683\\nPrediction accuracy, 329\\nPrediction error, 18\\nPredictive distribution, 268\\nPRIM, seePatient rule induction\\nmethod\\nPrincipal components, 66–67, 79–\\n80, 534–539, 547\\nregression, 79–80\\nsparse, 550\\nsupervised, 674\\nPrincipal curves and surfaces, 541–\\n544\\nPrincipal points, 541\\nPrior distribution, 268–272\\nProcrustes\\naverage, 540\\ndistance, 539\\nProjection pursuit, 389–392, 565\\nregression, 389–392\\nPrototype classiﬁer, 459–463\\nPrototype methods, 459–463\\nProximity matrices, 503\\nPruning, 308\\nQR decomposition, 55\\nQuadratic approximations and in-\\nference, 124\\nQuadratic discriminant function,\\n108, 110\\nRadial basis function (RBF) net-\\nwork, 392\\nRadial basis functions, 212–214,\\n275, 393\\nRadial kernel, 548\\nRandom forest, 409, 587–604\\nalgorithm, 588'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 762}, page_content='744 Index\\nbias, 596–601\\ncomparison to boosting, 589\\nexample, 589\\nout-of-bag ( oob), 592\\noverﬁt, 596\\nproximity plot, 595\\nvariable importance, 593\\nvariance, 597–601\\nRao score test, 125\\nRayleigh quotient, 116\\nReceiver operating characteristic\\n(ROC) curve, 317\\nReduced-rank linear discriminant\\nanalysis, 113\\nRegression, 11–14, 43–99, 200–204\\nRegression spline, 144\\nRegularization, 34, 167–176\\nRegularized discriminant analysis,\\n112–113, 654\\nRelevance network, 631\\nRepresenter of evaluation, 169\\nReproducing kernel Hilbert space,\\n167–176, 428–429\\nReproducing property, 169\\nResponsibilities, 274–275\\nRidge regression, 61–68, 650, 659\\nRisk factor, 122\\nRobust ﬁtting, 346–350\\nRosenblatt’s perceptron learning\\nalgorithm, 130\\nRug plot, 303\\nRuleﬁt, 623\\nSAM, 690–693, seeSigniﬁcance Anal-\\nysis of Microarrays\\nSammon mapping, 571\\nSCAD, 92\\nScaling of the inputs, 398\\nSchwarz’s criterion, 230–235\\nScore equations, 120, 265\\nSelf-consistency property, 541–543\\nSelf-organizing map (SOM), 528–\\n534\\nSensitivity of a test, 314–317\\nSeparating hyperplane, 132–135Separating hyperplanes, 136, 417–\\n419\\nSeparator, 628\\nShape average, 482, 540\\nShrinkage methods, 61–69, 652\\nSigmoid, 393\\nSigniﬁcance Analysis of Microar-\\nrays, 690–693\\nSimilarity measure, seeDissimi-\\nlarity measure\\nSingle index model, 390\\nSingular value decomposition, 64,\\n535–536, 659\\nsingular values, 535\\nsingular vectors, 535\\nSliced inverse regression, 480\\nSmoother, 139–156, 192–199\\nmatrix, 153\\nSmoothing parameter, 37, 156–161,\\n198–199\\nSmoothing spline, 151–156\\nSoft clustering, 512\\nSoft-thresholding, 653\\nSoftmax function, 393\\nSOM, seeSelf-organizing map\\nSparse, 175, 304, 610–613, 636\\nadditive model, 91\\ngraph, 625, 635\\nSpeciﬁcity of a test, 314–317\\nSpectral clustering, 544–547\\nSpline, 186\\nadditive, 297–299\\ncubic, 151–153\\ncubic smoothing, 151–153\\ninteraction, 428\\nregression, 144\\nsmoothing, 151–156\\nthin plate, 165\\nSquared error loss, 18, 24, 37, 219\\nSRM, seeStructural risk minimiza-\\ntion\\nStacking (stacked generalization),\\n290\\nStarting values, 397\\nStatistical decision theory, 18–22'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 763}, page_content='Index 745\\nStatistical model, 28–29\\nSteepest descent, 358, 395–397\\nStepwise selection, 60\\nStochastic approximation, 397\\nStochastic search (bumping), 290–\\n292\\nStress function, 570–572\\nStructural risk minimization (SRM),\\n239–241\\nSubset selection, 57–60\\nSupervised learning, 2\\nSupervised principal components,\\n674–681\\nSupport vector classiﬁer, 417–421,\\n654\\nmulticlass, 657\\nSupport vector machine, 423–437\\nSURE shrinkage method, 179\\nSurvival analysis, 674\\nSurvival curve, 674\\nSVD, seeSingular value decom-\\nposition\\nSymmlet basis, 176\\nTangent distance, 471–475\\nTanh activation function, 424\\nTarget variables, 10\\nTensor product basis, 162\\nTest error, 220–223\\nTest set, 220\\nThin plate spline, 165\\nThinning strategy, 189\\nTrace of a matrix, 153\\nTraining epoch, 397\\nTraining error, 220–223\\nTraining set, 219–223\\nTree for regression, 307–308\\nTree-based methods, 305–317\\nTrees for classiﬁcation, 308–310\\nTrellis display, 202\\nUndirected graph, 625–648\\nUniversal approximator, 390\\nUnsupervised learning, 2, 485–585Unsupervised learning as super-\\nvised learning, 495–497\\nValidation set, 222\\nVapnik-Chervonenkis (VC) dimen-\\nsion, 237–239\\nVariable importance plot, 594\\nVariable types and terminology, 9\\nVariance, 16, 25, 37, 158–161, 219\\nbetween, 114\\nwithin, 114, 446\\nVariance reduction, 588\\nVarying coeﬃcient models, 203–\\n204\\nVC dimension, seeVapnik–Chervon-\\nenkis dimension\\nVector quantization, 514–515\\nVoronoi regions, 510\\nWald test, 125\\nWavelet\\nbasis functions, 176–179\\nsmoothing, 174\\ntransform, 176–179\\nWeak learner, 383, 605\\nWeakest link pruning, 308\\nWebpages, 576\\nWebsite for book, 8\\nWeight decay, 398\\nWeight elimination, 398\\nWeights in a neural network, 395\\nWithin class covariance matrix, 114,\\n446')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = 'mixed_data/element_of_SL.pdf'\n",
    "\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "docs = loader.load_and_split()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cb43610-0858-4be3-9d91-d0345564adda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "index = Chroma.from_documents(\n",
    "    docs,\n",
    "    embedding = OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf6051-4fee-4ee8-81ae-10a2f7af2867",
   "metadata": {},
   "source": [
    "## Stuff strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15299e31-29ca-407c-8b27-710f19a36bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "100 3. Linear Methods for Regression\n",
      "\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression function E( Y|X) is\n",
      "linear in the inputs X1,... ,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s computer era there\n",
      "are still good reasons to study and use them. They are simple and often\n",
      "provide an adequate and interpretable description of how the inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outperform fancier\n",
      "nonlinear models, especially in situations with small numbers of training\n",
      "cases, low signal-to-noise ratio or sparse data. Finally, linear methods can be\n",
      "applied to transformations of the inputs and this considerably expands their\n",
      "scope. These generalizations are sometimes called basis-function methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, while in the\n",
      "next chapter we discuss linear methods for classiﬁcation. On some topics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an understanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of the linear methods\n",
      "discussed here.\n",
      "\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "As introduced in Chapter 2, we have an input vector XT= (X1,X2,... ,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression function E( Y|X) is\n",
      "linear, or that the linear model is a reasonable approximation. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, square-root or\n",
      "square;\n",
      "•basis expansions, such as X2=X2\n",
      "1,X3=X3\n",
      "1, leading to a polynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative inputs. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,... ,5,such that Xj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1≤X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Each xi= (xi1,xi2,... ,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "method is least squares , in which we pick the coeﬃcients β= (β0,β1,... ,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonable if the tr aining\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if the yi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, for example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regression wa s\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was proposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalties were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron et al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits the piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its transparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et a l.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm similar to l east\n",
      "angle regression for generalized regression models. Partial least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage methods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1 Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2 Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wider? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3 Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estimate of a\n",
      "parameter aTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds if A−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. The linear regression model assumes that the relationship between the variables can be represented by a linear equation. This model aims to find the best-fitting straight line that predicts the dependent variable based on the independent variable(s). The parameters of the linear regression model are estimated using techniques like least squares, where the goal is to minimize the sum of the squared differences between the observed values and the values predicted by the model.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type='stuff',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    'What is linear regression?',\n",
    "    callbacks=[StdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b544f00-5da8-4d93-b7ea-69782100eb61",
   "metadata": {},
   "source": [
    "## The map-reduce chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc430596-c7a2-4570-b46b-fb90867cedb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "100 3. Linear Methods for Regression\n",
      "Human: What is linear regression?\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression function E( Y|X) is\n",
      "linear in the inputs X1,... ,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s computer era there\n",
      "are still good reasons to study and use them. They are simple and often\n",
      "provide an adequate and interpretable description of how the inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outperform fancier\n",
      "nonlinear models, especially in situations with small numbers of training\n",
      "cases, low signal-to-noise ratio or sparse data. Finally, linear methods can be\n",
      "applied to transformations of the inputs and this considerably expands their\n",
      "scope. These generalizations are sometimes called basis-function methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, while in the\n",
      "next chapter we discuss linear methods for classiﬁcation. On some topics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an understanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of the linear methods\n",
      "discussed here.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "As introduced in Chapter 2, we have an input vector XT= (X1,X2,... ,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression function E( Y|X) is\n",
      "linear, or that the linear model is a reasonable approximation. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, square-root or\n",
      "square;\n",
      "•basis expansions, such as X2=X2\n",
      "1,X3=X3\n",
      "1, leading to a polynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative inputs. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,... ,5,such that Xj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1≤X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Each xi= (xi1,xi2,... ,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "method is least squares , in which we pick the coeﬃcients β= (β0,β1,... ,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonable if the tr aining\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if the yi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "Human: What is linear regression?\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following portion of a long document to see if any of the text is relevant to answer the question. \n",
      "Return any relevant text verbatim.\n",
      "______________________\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, for example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regression wa s\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was proposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalties were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron et al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits the piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its transparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et a l.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm similar to l east\n",
      "angle regression for generalized regression models. Partial least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage methods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1 Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2 Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wider? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3 Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estimate of a\n",
      "parameter aTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds if A−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Given the following extracted parts of a long document and a question, create a final answer. \n",
      "If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
      "______________________\n",
      "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. It is a commonly used technique in regression analysis for predicting the value of the dependent variable based on the values of the independent variables.\n",
      "\n",
      "Linear regression is a statistical method that assumes the relationship between the dependent variable (Y) and one or more independent variables (X) is linear. This means that the regression function E(Y|X) is linear in the inputs X1,...,Xp. Linear regression models were developed before the computer age, but they are still widely used today because they are simple, interpretable, and can sometimes outperform more complex nonlinear models, especially in situations with small amounts of training data, low signal-to-noise ratio, or sparse data. Additionally, linear methods can be applied to transformations of the inputs, which expands their scope.\n",
      "\n",
      "Linear regression is a statistical method used to model the relationship between a dependent variable (often denoted as Y) and one or more independent variables (often denoted as X). In linear regression, the relationship between the variables is assumed to be linear, and the goal is to find the best-fitting line or plane that minimizes the sum of squared differences between the observed values and the values predicted by the model. The equation for a simple linear regression model with one independent variable is typically represented as Y = β0 + β1X + ε, where Y is the dependent variable, X is the independent variable, β0 and β1 are coefficients, and ε represents the error term. The coefficients are estimated using methods like least squares, where the sum of squared residuals is minimized to find the best-fitting line.\n",
      "\n",
      "Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. This method assumes that the relationship between the variables can be approximated by a straight line. The goal of linear regression is to find the best-fitting line that describes the relationship between the variables, allowing for predictions or explanations based on this relationship. It is a fundamental technique in statistical modeling and is extensively discussed in various statistics books and research articles.\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. It assumes that the relationship between the variables is linear and aims to find the best-fitting line that describes this relationship.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type='map_reduce',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    'What is linear regression?',\n",
    "    callbacks=[StdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad34b6a9-16a3-48b8-814e-8a93b98f9491",
   "metadata": {},
   "source": [
    "## The refine strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e075749b-9322-4556-971a-34ba7cbd6f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Context information is below.\n",
      "------------\n",
      "100 3. Linear Methods for Regression\n",
      "------------\n",
      "Given the context information and not prior knowledge, answer any questions\n",
      "Human: What is linear regression?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is linear regression?\n",
      "AI: Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. The goal of linear regression is to find the best-fitting line that represents the relationship between the variables, allowing us to make predictions or understand the impact of the independent variables on the dependent variable.\n",
      "Human: We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression function E( Y|X) is\n",
      "linear in the inputs X1,... ,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s computer era there\n",
      "are still good reasons to study and use them. They are simple and often\n",
      "provide an adequate and interpretable description of how the inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outperform fancier\n",
      "nonlinear models, especially in situations with small numbers of training\n",
      "cases, low signal-to-noise ratio or sparse data. Finally, linear methods can be\n",
      "applied to transformations of the inputs and this considerably expands their\n",
      "scope. These generalizations are sometimes called basis-function methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, while in the\n",
      "next chapter we discuss linear methods for classiﬁcation. On some topics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an understanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of the linear methods\n",
      "discussed here.\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is linear regression?\n",
      "AI: Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. It assumes that the regression function is linear in the inputs X1, X2,... ,Xp. Linear models are simple and provide an interpretable description of how the inputs affect the output. They can outperform fancier nonlinear models in situations with small training cases, low signal-to-noise ratio, or sparse data. Linear methods can be applied to transformations of the inputs, expanding their scope through basis-function methods, as discussed in Chapter 5 of linear methods for regression. Understanding linear methods is considered essential for grasping nonlinear techniques, as many nonlinear techniques are direct generalizations of linear methods.\n",
      "Human: We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "As introduced in Chapter 2, we have an input vector XT= (X1,X2,... ,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression function E( Y|X) is\n",
      "linear, or that the linear model is a reasonable approximation. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, square-root or\n",
      "square;\n",
      "•basis expansions, such as X2=X2\n",
      "1,X3=X3\n",
      "1, leading to a polynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative inputs. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,... ,5,such that Xj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1≤X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Each xi= (xi1,xi2,... ,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "method is least squares , in which we pick the coeﬃcients β= (β0,β1,... ,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonable if the tr aining\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if the yi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: What is linear regression?\n",
      "AI: Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. The linear regression model assumes that the regression function is linear in the inputs X1, X2,... ,Xp. The model has the form f(X) = β0 + ∑(j=1 to p) Xjβj. The parameters βj represent unknown coefficients, and the variables Xj can come from various sources such as quantitative inputs, transformations of quantitative inputs, basis expansions, numeric or \"dummy\" coding of qualitative inputs, and interactions between variables.\n",
      "\n",
      "To estimate the parameters β, least squares is the most popular method used in linear regression. The goal is to minimize the residual sum of squares (RSS) by choosing the coefficients that best fit the training data. This method is valid if the training observations are independent random draws from their population or if the dependent variables are conditionally independent given the inputs. Linear regression is widely used in statistical modeling to understand and predict relationships between variables.\n",
      "Human: We have the opportunity to refine the existing answer (only if needed) with some more context below.\n",
      "------------\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, for example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regression wa s\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was proposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalties were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron et al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits the piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its transparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et a l.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm similar to l east\n",
      "angle regression for generalized regression models. Partial least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage methods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1 Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2 Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wider? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3 Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estimate of a\n",
      "parameter aTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds if A−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "------------\n",
      "Given the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The additional context provided does not require any changes to the original answer.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type='refine',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    'What is linear regression?',\n",
    "    callbacks=[StdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b799546-1d48-4a78-8d08-b6f084a2d8f0",
   "metadata": {},
   "source": [
    "## The map-rerank strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e05fd3-d248-40fc-89c1-617818402f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapRerankDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "100 3. Linear Methods for Regression\n",
      "---------\n",
      "Question: What is linear regression?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression function E( Y|X) is\n",
      "linear in the inputs X1,... ,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s computer era there\n",
      "are still good reasons to study and use them. They are simple and often\n",
      "provide an adequate and interpretable description of how the inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outperform fancier\n",
      "nonlinear models, especially in situations with small numbers of training\n",
      "cases, low signal-to-noise ratio or sparse data. Finally, linear methods can be\n",
      "applied to transformations of the inputs and this considerably expands their\n",
      "scope. These generalizations are sometimes called basis-function methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, while in the\n",
      "next chapter we discuss linear methods for classiﬁcation. On some topics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an understanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of the linear methods\n",
      "discussed here.\n",
      "---------\n",
      "Question: What is linear regression?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "As introduced in Chapter 2, we have an input vector XT= (X1,X2,... ,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression function E( Y|X) is\n",
      "linear, or that the linear model is a reasonable approximation. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, square-root or\n",
      "square;\n",
      "•basis expansions, such as X2=X2\n",
      "1,X3=X3\n",
      "1, leading to a polynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative inputs. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,... ,5,such that Xj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1≤X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Each xi= (xi1,xi2,... ,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "method is least squares , in which we pick the coeﬃcients β= (β0,β1,... ,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonable if the tr aining\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if the yi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "---------\n",
      "Question: What is linear regression?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, for example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regression wa s\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was proposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalties were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron et al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits the piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its transparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et a l.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm similar to l east\n",
      "angle regression for generalized regression models. Partial least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage methods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1 Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2 Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wider? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3 Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estimate of a\n",
      "parameter aTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds if A−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "---------\n",
      "Question: What is linear regression?\n",
      "Helpful Answer:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\chains\\llm.py:367: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. This method is commonly used for predicting future values based on past data. '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=index.as_retriever(),\n",
    "    chain_type='map_rerank',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\n",
    "    'What is linear regression?',\n",
    "    callbacks=[StdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c6233-d31a-4ab4-a38d-2d301bc3edfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
