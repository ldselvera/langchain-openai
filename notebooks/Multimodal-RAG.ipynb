{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cf9cd58-813d-4f4c-b990-b5a71db848da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U unstructured unstructured-inference onnx pytesseract  chromadb\n",
    "# pip install -U python-poppler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c641b7-2c72-484f-b875-d91af671744b",
   "metadata": {},
   "source": [
    "## Multi-vector retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a30bc397-dc08-46ee-95cc-2e9fc00c9b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 0}, page_content='Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\\nnology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\\nThis major new edition features many topics not covered in the original, including graphical\\nmodels, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\\nStanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\\n›springer.comSTATISTICS\\nisbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\\nThe Elements of Statictical Learning\\nHastie • Tibshirani • Friedman\\nSecond Edition'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 1}, page_content='This is page v\\nPrinter: Opaque this\\nTo our parents:\\nValerie and Patrick Hastie\\nVera and Sami Tibshirani\\nFlorence and Harry Friedman\\nand to our families:\\nSamantha, Timothy, and Lynda\\nCharlie, Ryan, Julie, and Cheryl\\nMelanie, Dora, Monika, and Ildiko'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 2}, page_content='vi'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 3}, page_content='This is page vii\\nPrinter: Opaque this\\nPreface to the Second Edition\\nIn God we trust, all others bring data.\\n–William Edwards Deming (1900-1993)1\\nWe have been gratiﬁed by the popularity of the ﬁrst edition of The\\nElements of Statistical Learning. This, along with the fast pace of research\\nin the statistical learning ﬁeld, motivated us to update our book with a\\nsecond edition.\\nWe have added four new chapters and updated some of the existing\\nchapters. Because many readers are familiar with the layout of the ﬁrst\\nedition, we have tried to change it as little as possible. Here is a summary\\nof the main changes:\\n1On the Web, this quote has been widely attributed to both Demi ng and Robert W.\\nHayden; however Professor Hayden told us that he can claim no credit for this quote,\\nand ironically we could ﬁnd no “data” conﬁrming that Deming a ctually said this.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 4}, page_content='viii Preface to the Second Edition\\nChapter What’s new\\n1.Introduction\\n2.Overview of Supervised Learning\\n3.Linear Methods for Regression LAR algorithm and generalizations\\nof the lasso\\n4.Linear Methods for Classiﬁcation Lasso path for logistic regression\\n5.Basis Expansions and Regulariza-\\ntionAdditional illustrations of RKHS\\n6.Kernel Smoothing Methods\\n7.Model Assessment and Selection Strengths and pitfalls of cross-\\nvalidation\\n8.Model Inference and Averaging\\n9.Additive Models, Trees, and\\nRelated Methods\\n10.Boosting and Additive Trees New example from ecology; some\\nmaterial split oﬀ to Chapter 16.\\n11.Neural Networks Bayesian neural nets and the NIPS\\n2003 challenge\\n12.Support Vector Machines and\\nFlexible DiscriminantsPath algorithm for SVM classiﬁer\\n13. Prototype Methods and\\nNearest-Neighbors\\n14.Unsupervised Learning Spectral clustering, kernel PCA,\\nsparse PCA, non-negative matrix\\nfactorization archetypal analysis,\\nnonlinear dimension reduction,\\nGoogle page rank algorithm, a\\ndirect approach to ICA\\n15.Random Forests New\\n16.Ensemble Learning New\\n17.Undirected Graphical Models New\\n18.High-Dimensional Problems New\\nSome further notes:\\n•Our ﬁrst edition was unfriendly to colorblind readers; in particular,\\nwe tended to favor red/green contrasts which are particularly trou-\\nblesome. We have changed the color palette in this edition to a large\\nextent, replacing the above with an orange /bluecontrast.\\n•We have changed the name of Chapter 6 from “Kernel Methods” to\\n“Kernel Smoothing Methods”, to avoid confusion with the machine-\\nlearning kernel method that is discussed in the context of support vec-\\ntor machines (Chapter 11) and more generally in Chapters 5 and 14.\\n•In the ﬁrst edition, the discussion of error-rate estimation in Chap-\\nter 7 was sloppy, as we did not clearly diﬀerentiate the notions of\\nconditional error rates (conditional on the training set) and uncondi-\\ntional rates. We have ﬁxed this in the new edition.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "file_path = 'mixed_data/element_of_SL.pdf'\n",
    "\n",
    "loader = PyPDFLoader(file_path=file_path)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "sl_data = loader.load_and_split(text_splitter=text_splitter)\n",
    "sl_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac5b14d-cd77-4733-b13e-38b4b7df444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\Owner\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiVectorRetriever(vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000001D1F9C76AE0>, docstore=<langchain_core.stores.InMemoryStore object at 0x000001D1F6DB3E90>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"statistical_learning\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    docstore=store, \n",
    "    id_key=id_key,\n",
    ")\n",
    "\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59831d6-5167-442f-8d50-a8eb27a9f00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b6a30443-5efc-4efd-aa5a-12ecb088ebff',\n",
       " '7460ebe9-dd33-49c5-bebf-66a40732f22e',\n",
       " '31f767ce-bc85-4dcc-b16b-4c2c1468b0cf',\n",
       " 'f7faad30-f6ce-415c-8763-b8edbf60d8f1',\n",
       " 'ed015eef-34ca-4b39-8b0e-c9145a191e7c']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "doc_ids = [str(uuid.uuid4()) for _ in sl_data]\n",
    "doc_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03d8cfb7-dd4d-4836-a3d2-a0b5b2630667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 0, 'doc_id': 'b6a30443-5efc-4efd-aa5a-12ecb088ebff'}, page_content='Springer Series in Statistics\\nTrevor Hastie\\nRobert TibshiraniJerome FriedmanSpringer Series in Statistics\\nThe Elements of\\nStatistical Learning\\nData Mining, Inference, and Prediction\\nThe Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 0, 'doc_id': 'b6a30443-5efc-4efd-aa5a-12ecb088ebff'}, page_content='nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 0, 'doc_id': 'b6a30443-5efc-4efd-aa5a-12ecb088ebff'}, page_content='tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 0, 'doc_id': 'b6a30443-5efc-4efd-aa5a-12ecb088ebff'}, page_content='This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 0, 'doc_id': 'b6a30443-5efc-4efd-aa5a-12ecb088ebff'}, page_content='with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_text_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "all_sub_docs = []\n",
    "for i, doc in enumerate(sl_data):\n",
    "    doc_id = doc_ids[i]\n",
    "    sub_docs = child_text_splitter.split_documents([doc])\n",
    "    for sub_doc in sub_docs:\n",
    "        sub_doc.metadata[id_key] = doc_id\n",
    "    all_sub_docs.extend(sub_docs)\n",
    "    \n",
    "all_sub_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4e373e3-4ca9-4ed0-a2d6-642b6d92f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(all_sub_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, sl_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc77b4a-55dc-46cb-8b33-011137d8c952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '066bc2a3-2954-4f3b-8c24-0825a3ac7784', 'page': 118, 'source': 'mixed_data/element_of_SL.pdf'}, page_content='100 3. Linear Methods for Regression'),\n",
       " Document(metadata={'doc_id': 'c1257b88-c5bf-4189-9a30-e44cad19d87b', 'page': 72, 'source': 'mixed_data/element_of_SL.pdf'}, page_content='54 3. Linear Methods for Regression\\nx1x2y\\nˆ yz z z z z\\nFIGURE 3.4. Least squares regression by orthogonalization of the inputs. The\\nvector x2is regressed on the vector x1, leaving the residual vector z. The regres-\\nsion of yonzgives the multiple regression coeﬃcient of x2. Adding together the\\nprojections of yon each of x1andzgives the least squares ﬁt ˆy.'),\n",
       " Document(metadata={'doc_id': '28eb91a6-a299-4c3d-bbd9-6cac7f34540f', 'page': 222, 'source': 'mixed_data/element_of_SL.pdf'}, page_content='linear model\\nf(X) =α(Z) +β1(Z)X1+≤≤≤+βq(Z)Xq. (6.16)\\nFor given Z, this is a linear model, but each of the coeﬃcients can vary\\nwithZ. It is natural to ﬁt such a model by locally weighted least squares:\\nmin\\nα(z0),β(z0)N∑\\ni=1Kλ(z0,zi)(yi−α(z0)−x1iβ1(z0)− ≤≤≤ − xqiβq(z0))2.\\n(6.17)\\nFigure 6.10 illustrates the idea on measurements of the human aorta.'),\n",
       " Document(metadata={'doc_id': '72e56868-05f1-44e5-857c-0d805b221192', 'page': 71, 'source': 'mixed_data/element_of_SL.pdf'}, page_content='3.2 Linear Regression Models and Least Squares 53\\ntheinner product between xandy1. Then we can write\\nˆβ=⟨x,y⟩\\n⟨x,x⟩,\\nr=y−xˆβ.(3.26)\\nAs we will see, this simple univariate regression provides the building block\\nfor multiple linear regression. Suppose next that the inputs x1,x2,... ,xp\\n(the columns of the data matrix X) are orthogonal; that is ⟨xj,xk⟩= 0')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"Linear regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55be9888-4be6-48ab-8d30-9bbc76000f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 118}, page_content='100 3. Linear Methods for Regression'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 72}, page_content='54 3. Linear Methods for Regression\\nx1x2y\\nˆ yz z z z z\\nFIGURE 3.4. Least squares regression by orthogonalization of the inputs. The\\nvector x2is regressed on the vector x1, leaving the residual vector z. The regres-\\nsion of yonzgives the multiple regression coeﬃcient of x2. Adding together the\\nprojections of yon each of x1andzgives the least squares ﬁt ˆy.\\nAlgorithm 3.1 Regression by Successive Orthogonalization.\\n1. Initialize z0=x0=1.\\n2. For j= 1,2,... ,p\\nRegress xjonz0,z1,... ,,zj−1to produce coeﬃcients ˆ γℓj=\\n⟨zℓ,xj⟩/⟨zℓ,zℓ⟩,ℓ= 0,... ,j −1 and residual vector zj=\\nxj−∑j−1\\nk=0ˆγkjzk.\\n3. Regress yon the residual zpto give the estimate ˆβp.\\nThe result of this algorithm is\\nˆβp=⟨zp,y⟩\\n⟨zp,zp⟩. (3.28)\\nRe-arranging the residual in step 2, we can see that each of the xjis a linear\\ncombination of the zk, k≤j. Since the zjare all orthogonal, they form\\na basis for the column space of X, and hence the least squares projection\\nonto this subspace is ˆy. Since zpalone involves xp(with coeﬃcient 1), we\\nsee that the coeﬃcient (3.28) is indeed the multiple regression coeﬃcient of\\nyonxp. This key result exposes the eﬀect of correlated inputs in multiple\\nregression. Note also that by rearranging the xj, any one of them could\\nbe in the last position, and a similar results holds. Hence stated more\\ngenerally, we have shown that the jth multiple regression coeﬃcient is the\\nunivariate regression coeﬃcient of yonxj≤012...(j−1)(j+1)...,p, the residual\\nafter regressing xjonx0,x1,... ,xj−1,xj+1,... ,xp:'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 222}, page_content='204 6. Kernel Smoothing Methods\\n1012141618202224DepthFemale\\n20 30 40 50 60DepthFemale\\nDepthFemale\\n20 30 40 50 60DepthFemale\\nDepthFemale\\n20 30 40 50 60DepthFemaleDepthMale\\nDepthMale20 30 40 50 60\\nDepthMale\\nDepthMale20 30 40 50 60\\nDepthMale\\n1012141618202224DepthMale20 30 40 50 60\\nAgeDiameterAortic Diameter vs Age\\nFIGURE 6.10. In each panel the aorta diameter is modeled as a linear func-\\ntion ofage. The coeﬃcients of this model vary with gender anddepth down\\ntheaorta (left is near the top, right is low down). There is a clear trend in the\\ncoeﬃcients of the linear model.\\nthe variables we collect in the vector Z. We then assume the conditionally\\nlinear model\\nf(X) =α(Z) +β1(Z)X1+≤≤≤+βq(Z)Xq. (6.16)\\nFor given Z, this is a linear model, but each of the coeﬃcients can vary\\nwithZ. It is natural to ﬁt such a model by locally weighted least squares:\\nmin\\nα(z0),β(z0)N∑\\ni=1Kλ(z0,zi)(yi−α(z0)−x1iβ1(z0)− ≤≤≤ − xqiβq(z0))2.\\n(6.17)\\nFigure 6.10 illustrates the idea on measurements of the human aorta.\\nA longstanding claim has been that the aorta thickens with age. Here we\\nmodel the diameter of the aorta as a linear function of age, but allow the\\ncoeﬃcients to vary with gender anddepth down the aorta. We used a local\\nregression model separately for males and females. While the aorta clearly\\ndoes thicken with age at the higher regions of the aorta, the relationship\\nfades with distance down the aorta. Figure 6.11 shows the intercept and\\nslope as a function of depth.'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 71}, page_content='3.2 Linear Regression Models and Least Squares 53\\ntheinner product between xandy1. Then we can write\\nˆβ=⟨x,y⟩\\n⟨x,x⟩,\\nr=y−xˆβ.(3.26)\\nAs we will see, this simple univariate regression provides the building block\\nfor multiple linear regression. Suppose next that the inputs x1,x2,... ,xp\\n(the columns of the data matrix X) are orthogonal; that is ⟨xj,xk⟩= 0\\nfor all j̸=k. Then it is easy to check that the multiple least squares esti-\\nmates ˆβjare equal to ⟨xj,y⟩/⟨xj,xj⟩—the univariate estimates. In other\\nwords, when the inputs are orthogonal, they have no eﬀect on each other’s\\nparameter estimates in the model.\\nOrthogonal inputs occur most often with balanced, designed experiments\\n(where orthogonality is enforced), but almost never with observational\\ndata. Hence we will have to orthogonalize them in order to carry this idea\\nfurther. Suppose next that we have an intercept and a single input x. Then\\nthe least squares coeﬃcient of xhas the form\\nˆβ1=⟨x−¯x1,y⟩\\n⟨x−¯x1,x−¯x1⟩, (3.27)\\nwhere ¯ x=∑\\nixi/N, and1=x0, the vector of Nones. We can view the\\nestimate (3.27) as the result of two applications of the simple regression\\n(3.26). The steps are:\\n1. regress xon1to produce the residual z=x−¯x1;\\n2. regress yon the residual zto give the coeﬃcient ˆβ1.\\nIn this procedure, “regress bona” means a simple univariate regression of b\\nonawith no intercept, producing coeﬃcient ˆ γ=⟨a,b⟩/⟨a,a⟩and residual\\nvector b−ˆγa. We say that bis adjusted for a, or is “orthogonalized” with\\nrespect to a.\\nStep 1 orthogonalizes xwith respect to x0=1. Step 2 is just a simple\\nunivariate regression, using the orthogonal predictors 1andz. Figure 3.4\\nshows this process for two general inputs x1andx2. The orthogonalization\\ndoes not change the subspace spanned by x1andx2, it simply produces an\\northogonal basis for representing it.\\nThis recipe generalizes to the case of pinputs, as shown in Algorithm 3.1.\\nNote that the inputs z0,... ,zj−1in step 2 are orthogonal, hence the simple\\nregression coeﬃcients computed there are in fact also the multiple regres-\\nsion coeﬃcients.\\n1The inner-product notation is suggestive of generalizatio ns of linear regression to\\ndiﬀerent metric spaces, as well as to probability spaces.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"Linear regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf383846-45d8-4715-be5e-6cb25ed17973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\Owner\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. The goal of linear regression is to find the best-fitting line that describes the relationship between the variables. In this method, the regression function is assumed to be linear in the inputs, and the parameters of the linear equation are estimated using techniques like least squares to minimize the difference between the observed values and the predicted values.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"What is linear regression?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4db4b23-4d97-441e-9f4f-5f8a86696114",
   "metadata": {},
   "source": [
    "## Hypothetical queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a71476f-ac1e-4ceb-96d2-218f23292be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2 1. Introduction\n",
      "TABLE 1.1. Average percentage of words or characters in an email message\n",
      "equal to the indicated word or character. We have chosen the wo rds and characters\n",
      "showing the largest diﬀerence between spamandemail.\n",
      "george you your hp free hpl ! our re edu remove\n",
      "spam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28\n",
      "email 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01\n",
      "measurements for a set of objects (such as people). Using this data we build\n",
      "a prediction model, or learner , which will enable us to predict the outcome\n",
      "for new unseen objects. A good learner is one that accurately predicts such\n",
      "an outcome.\n",
      "The examples above describe what is called the supervised learning prob-\n",
      "lem. It is called “supervised” because of the presence of the outcome vari-\n",
      "able to guide the learning process. In the unsupervised learning problem ,\n",
      "we observe only the features and have no measurements of the outcome.\n",
      "Our task is rather to describe how the data are organized or clustered. We\n",
      "devote most of this book to supervised learning; the unsupervised problem\n",
      "is less developed in the literature, and is the focus of Chapter 14.\n",
      "Here are some examples of real learning problems that are discussed in\n",
      "this book.\n",
      "Example 1: Email Spam\n",
      "The data for this example consists of information from 4601 email mes-\n",
      "sages, in a study to try to predict whether the email was junk email, or\n",
      "“spam.” The objective was to design an automatic spam detector that\n",
      "could ﬁlter out spam before clogging the users’ mailboxes. For all 4601\n",
      "email messages, the true outcome (email type) email orspamis available,\n",
      "along with the relative frequencies of 57 of the most commonly occurring\n",
      "words and punctuation marks in the email message. This is a supervised\n",
      "learning problem, with the outcome the class variable email/spam. It is also\n",
      "called a classiﬁcation problem.\n",
      "Table 1.1 lists the words and characters showing the largest average\n",
      "diﬀerence between spamandemail.\n",
      "Our learning method has to decide which features to use and how: for\n",
      "example, we might use a rule such as\n",
      "if (%george <0.6) & (%you>1.5) then spam\n",
      "elseemail.\n",
      "Another form of a rule might be:\n",
      "if (0.2≤%you−0.3≤%george )>0 then spam\n",
      "elseemail.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What are the average percentages of words or characters in an email message that are equal to specific words or characters?',\n",
       " 'How can the data in Table 1.1 be used to build a prediction model for classifying email messages as spam or email?',\n",
       " 'What are some examples of real learning problems discussed in the book, and how do they relate to supervised and unsupervised learning?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import NumberedListOutputParser\n",
    "\n",
    "prompt = \"\"\"\n",
    "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
    "\n",
    "{doc}\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name='gpt-3.5-turbo-16k')\n",
    "\n",
    "chain = LLMChain.from_string(\n",
    "    llm=llm,\n",
    "    template=prompt,\n",
    ")\n",
    "\n",
    "chain.verbose = True\n",
    "chain.output_parser = NumberedListOutputParser()\n",
    "\n",
    "chain.run(sl_data[20].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a28a010-d814-4a3f-a363-1d37bd9a4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=\"hypo-questions\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore, \n",
    "    docstore=store, \n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in sl_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d695fdf-2e43-4695-a587-e4734a106890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Springer Series in Statistics\n",
      "Trevor Hastie\n",
      "Robert TibshiraniJerome FriedmanSpringer Series in Statistics\n",
      "The Elements of\n",
      "Statistical Learning\n",
      "Data Mining, Inference, and Prediction\n",
      "The Elements of Statistical LearningDuring the past decade there has been an explosion in computation and information tech-\n",
      "nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining,machine learning, and bioinformatics. Many of these tools have common underpinnings butare often expressed with different terminology. This book describes the important ideas inthese areas in a common conceptual framework. While the approach is statistical, theemphasis is on concepts rather than mathematics. Many examples are given, with a liberaluse of color graphics. It should be a valuable resource for statisticians and anyone interestedin data mining in science or industry. The book’s coverage is broad, from supervised learning(prediction) to unsupervised learning. The many topics include neural networks, supportvector machines, classification trees and boosting—the first comprehensive treatment of thistopic in any book.\n",
      "This major new edition features many topics not covered in the original, including graphical\n",
      "models, random forests, ensemble methods, least angle regression & path algorithms for thelasso, non-negative matrix factorization, and spectral clustering. There is also a chapter onmethods for “wide” data (p bigger than n), including multiple testing and false discovery rates.\n",
      "Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at\n",
      "Stanford University. They are prominent researchers in this area: Hastie and Tibshiranideveloped generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS andinvented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of thevery successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.\n",
      "›springer.comSTATISTICS\n",
      "isbn 978-0-387-84857-0Trevor Hastie • Robert Tibshirani • Jerome Friedman\n",
      "The Elements of Statictical Learning\n",
      "Hastie • Tibshirani • Friedman\n",
      "Second Edition\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page v\n",
      "Printer: Opaque this\n",
      "To our parents:\n",
      "Valerie and Patrick Hastie\n",
      "Vera and Sami Tibshirani\n",
      "Florence and Harry Friedman\n",
      "and to our families:\n",
      "Samantha, Timothy, and Lynda\n",
      "Charlie, Ryan, Julie, and Cheryl\n",
      "Melanie, Dora, Monika, and Ildiko\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "vi\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page vii\n",
      "Printer: Opaque this\n",
      "Preface to the Second Edition\n",
      "In God we trust, all others bring data.\n",
      "–William Edwards Deming (1900-1993)1\n",
      "We have been gratiﬁed by the popularity of the ﬁrst edition of The\n",
      "Elements of Statistical Learning. This, along with the fast pace of research\n",
      "in the statistical learning ﬁeld, motivated us to update our book with a\n",
      "second edition.\n",
      "We have added four new chapters and updated some of the existing\n",
      "chapters. Because many readers are familiar with the layout of the ﬁrst\n",
      "edition, we have tried to change it as little as possible. Here is a summary\n",
      "of the main changes:\n",
      "1On the Web, this quote has been widely attributed to both Demi ng and Robert W.\n",
      "Hayden; however Professor Hayden told us that he can claim no credit for this quote,\n",
      "and ironically we could ﬁnd no “data” conﬁrming that Deming a ctually said this.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "viii Preface to the Second Edition\n",
      "Chapter What’s new\n",
      "1.Introduction\n",
      "2.Overview of Supervised Learning\n",
      "3.Linear Methods for Regression LAR algorithm and generalizations\n",
      "of the lasso\n",
      "4.Linear Methods for Classiﬁcation Lasso path for logistic regression\n",
      "5.Basis Expansions and Regulariza-\n",
      "tionAdditional illustrations of RKHS\n",
      "6.Kernel Smoothing Methods\n",
      "7.Model Assessment and Selection Strengths and pitfalls of cross-\n",
      "validation\n",
      "8.Model Inference and Averaging\n",
      "9.Additive Models, Trees, and\n",
      "Related Methods\n",
      "10.Boosting and Additive Trees New example from ecology; some\n",
      "material split oﬀ to Chapter 16.\n",
      "11.Neural Networks Bayesian neural nets and the NIPS\n",
      "2003 challenge\n",
      "12.Support Vector Machines and\n",
      "Flexible DiscriminantsPath algorithm for SVM classiﬁer\n",
      "13. Prototype Methods and\n",
      "Nearest-Neighbors\n",
      "14.Unsupervised Learning Spectral clustering, kernel PCA,\n",
      "sparse PCA, non-negative matrix\n",
      "factorization archetypal analysis,\n",
      "nonlinear dimension reduction,\n",
      "Google page rank algorithm, a\n",
      "direct approach to ICA\n",
      "15.Random Forests New\n",
      "16.Ensemble Learning New\n",
      "17.Undirected Graphical Models New\n",
      "18.High-Dimensional Problems New\n",
      "Some further notes:\n",
      "•Our ﬁrst edition was unfriendly to colorblind readers; in particular,\n",
      "we tended to favor red/green contrasts which are particularly trou-\n",
      "blesome. We have changed the color palette in this edition to a large\n",
      "extent, replacing the above with an orange /bluecontrast.\n",
      "•We have changed the name of Chapter 6 from “Kernel Methods” to\n",
      "“Kernel Smoothing Methods”, to avoid confusion with the machine-\n",
      "learning kernel method that is discussed in the context of support vec-\n",
      "tor machines (Chapter 11) and more generally in Chapters 5 and 14.\n",
      "•In the ﬁrst edition, the discussion of error-rate estimation in Chap-\n",
      "ter 7 was sloppy, as we did not clearly diﬀerentiate the notions of\n",
      "conditional error rates (conditional on the training set) and uncondi-\n",
      "tional rates. We have ﬁxed this in the new edition.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Preface to the Second Edition ix\n",
      "•Chapters 15 and 16 follow naturally from Chapter 10, and the chap-\n",
      "ters are probably best read in that order.\n",
      "•In Chapter 17, we have not attempted a comprehensive treatment\n",
      "of graphical models, and discuss only undirected models and some\n",
      "new methods for their estimation. Due to a lack of space, we have\n",
      "speciﬁcally omitted coverage of directed graphical models.\n",
      "•Chapter 18 explores the “ p≫N” problem, which is learning in high-\n",
      "dimensional feature spaces. These problems arise in many areas, in-\n",
      "cluding genomic and proteomic studies, and document classiﬁcation.\n",
      "We thank the many readers who have found the (too numerous) errors in\n",
      "the ﬁrst edition. We apologize for those and have done our best to avoid er-\n",
      "rors in this new edition. We thank Mark Segal, Bala Rajaratnam, and Larry\n",
      "Wasserman for comments on some of the new chapters, and many Stanford\n",
      "graduate and post-doctoral students who oﬀered comments, in particular\n",
      "Mohammed AlQuraishi, John Boik, Holger Hoeﬂing, Arian Maleki, Donal\n",
      "McMahon, Saharon Rosset, Babak Shababa, Daniela Witten, Ji Zhu and\n",
      "Hui Zou. We thank John Kimmel for his patience in guiding us through this\n",
      "new edition. RT dedicates this edition to the memory of Anna McPhee.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "August 2008\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "x Preface to the Second Edition\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page xi\n",
      "Printer: Opaque this\n",
      "Preface to the First Edition\n",
      "We are drowning in information and starving for knowledge.\n",
      "–Rutherford D. Roger\n",
      "The ﬁeld of Statistics is constantly challenged by the problems that science\n",
      "and industry brings to its door. In the early days, these problems often came\n",
      "from agricultural and industrial experiments and were relatively small in\n",
      "scope. With the advent of computers and the information age, statistical\n",
      "problems have exploded both in size and complexity. Challenges in the\n",
      "areas of data storage, organization and searching have led to the new ﬁeld\n",
      "of “data mining”; statistical and computational problems in biology and\n",
      "medicine have created “bioinformatics.” Vast amounts of data are being\n",
      "generated in many ﬁelds, and the statistician’s job is to make sense of it\n",
      "all: to extract important patterns and trends, and understand “what the\n",
      "data says.” We call this learning from data .\n",
      "The challenges in learning from data have led to a revolution in the sta-\n",
      "tistical sciences. Since computation plays such a key role, it is not surprising\n",
      "that much of this new development has been done by researchers in other\n",
      "ﬁelds such as computer science and engineering.\n",
      "The learning problems that we consider can be roughly categorized as\n",
      "either supervised orunsupervised . In supervised learning, the goal is to pre-\n",
      "dict the value of an outcome measure based on a number of input measures;\n",
      "in unsupervised learning, there is no outcome measure, and the goal is to\n",
      "describe the associations and patterns among a set of input measures.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "xii Preface to the First Edition\n",
      "This book is our attempt to bring together many of the important new\n",
      "ideas in learning, and explain them in a statistical framework. While some\n",
      "mathematical details are needed, we emphasize the methods and their con-\n",
      "ceptual underpinnings rather than their theoretical properties. As a result,\n",
      "we hope that this book will appeal not just to statisticians but also to\n",
      "researchers and practitioners in a wide variety of ﬁelds.\n",
      "Just as we have learned a great deal from researchers outside of the ﬁeld\n",
      "of statistics, our statistical viewpoint may help others to better understa nd\n",
      "diﬀerent aspects of learning:\n",
      "There is no true interpretation of anything; interpretatio n is a\n",
      "vehicle in the service of human comprehension. The value of\n",
      "interpretation is in enabling others to fruitfully think ab out an\n",
      "idea.\n",
      "–Andreas Buja\n",
      "We would like to acknowledge the contribution of many people to the\n",
      "conception and completion of this book. David Andrews, Leo Breiman,\n",
      "Andreas Buja, John Chambers, Bradley Efron, Geoﬀrey Hinton, Werner\n",
      "Stuetzle, and John Tukey have greatly inﬂuenced our careers. Balasub-\n",
      "ramanian Narasimhan gave us advice and help on many computational\n",
      "problems, and maintained an excellent computing environment. Shin-Ho\n",
      "Bang helped in the production of a number of the ﬁgures. Lee Wilkinson\n",
      "gave valuable tips on color production. Ilana Belitskaya, Eva Cantoni, Ma ya\n",
      "Gupta, Michael Jordan, Shanti Gopatam, Radford Neal, Jorge Picazo, Bog-\n",
      "dan Popescu, Olivier Renaud, Saharon Rosset, John Storey, Ji Zhu, Mu\n",
      "Zhu, two reviewers and many students read parts of the manuscript and\n",
      "oﬀered helpful suggestions. John Kimmel was supportive, patient and help-\n",
      "ful at every phase; MaryAnn Brickner and Frank Ganz headed a superb\n",
      "production team at Springer. Trevor Hastie would like to thank the statis-\n",
      "tics department at the University of Cape Town for their hospitality during\n",
      "the ﬁnal stages of this book. We gratefully acknowledge NSF and NIH for\n",
      "their support of this work. Finally, we would like to thank our families and\n",
      "our parents for their love and support.\n",
      "Trevor Hastie\n",
      "Robert Tibshirani\n",
      "Jerome Friedman\n",
      "Stanford, California\n",
      "May 2001\n",
      "The quiet statisticians have changed our world; not by disco v-\n",
      "ering new facts or technical developments, but by changing t he\n",
      "ways that we reason, experiment and form our opinions ....\n",
      "–Ian Hacking\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page xiii\n",
      "Printer: Opaque this\n",
      "Contents\n",
      "Preface to the Second Edition vii\n",
      "Preface to the First Edition xi\n",
      "1 Introduction 1\n",
      "2 Overview of Supervised Learning 9\n",
      "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2 Variable Types and Terminology . . . . . . . . . . . . . . 9\n",
      "2.3 Two Simple Approaches to Prediction:\n",
      "Least Squares and Nearest Neighbors . . . . . . . . . . . 11\n",
      "2.3.1 Linear Models and Least Squares . . . . . . . . 11\n",
      "2.3.2 Nearest-Neighbor Methods . . . . . . . . . . . . 14\n",
      "2.3.3 From Least Squares to Nearest Neighbors . . . . 16\n",
      "2.4 Statistical Decision Theory . . . . . . . . . . . . . . . . . 18\n",
      "2.5 Local Methods in High Dimensions . . . . . . . . . . . . . 22\n",
      "2.6 Statistical Models, Supervised Learning\n",
      "and Function Approximation . . . . . . . . . . . . . . . . 28\n",
      "2.6.1 A Statistical Model\n",
      "for the Joint Distribution Pr( X,Y) . . . . . . . 28\n",
      "2.6.2 Supervised Learning . . . . . . . . . . . . . . . . 29\n",
      "2.6.3 Function Approximation . . . . . . . . . . . . . 29\n",
      "2.7 Structured Regression Models . . . . . . . . . . . . . . . 32\n",
      "2.7.1 Diﬃculty of the Problem . . . . . . . . . . . . . 32\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "xiv Contents\n",
      "2.8 Classes of Restricted Estimators . . . . . . . . . . . . . . 33\n",
      "2.8.1 Roughness Penalty and Bayesian Methods . . . 34\n",
      "2.8.2 Kernel Methods and Local Regression . . . . . . 34\n",
      "2.8.3 Basis Functions and Dictionary Methods . . . . 35\n",
      "2.9 Model Selection and the Bias–Variance Tradeoﬀ . . . . . 37\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "3 Linear Methods for Regression 43\n",
      "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 43\n",
      "3.2 Linear Regression Models and Least Squares . . . . . . . 44\n",
      "3.2.1 Example: Prostate Cancer . . . . . . . . . . . . 49\n",
      "3.2.2 The Gauss–Markov Theorem . . . . . . . . . . . 51\n",
      "3.2.3 Multiple Regression\n",
      "from Simple Univariate Regression . . . . . . . . 52\n",
      "3.2.4 Multiple Outputs . . . . . . . . . . . . . . . . . 56\n",
      "3.3 Subset Selection . . . . . . . . . . . . . . . . . . . . . . . 57\n",
      "3.3.1 Best-Subset Selection . . . . . . . . . . . . . . . 57\n",
      "3.3.2 Forward- and Backward-Stepwise Selection . . . 58\n",
      "3.3.3 Forward-Stagewise Regression . . . . . . . . . . 60\n",
      "3.3.4 Prostate Cancer Data Example (Continued) . . 61\n",
      "3.4 Shrinkage Methods . . . . . . . . . . . . . . . . . . . . . . 61\n",
      "3.4.1 Ridge Regression . . . . . . . . . . . . . . . . . 61\n",
      "3.4.2 The Lasso . . . . . . . . . . . . . . . . . . . . . 68\n",
      "3.4.3 Discussion: Subset Selection, Ridge Regression\n",
      "and the Lasso . . . . . . . . . . . . . . . . . . . 69\n",
      "3.4.4 Least Angle Regression . . . . . . . . . . . . . . 73\n",
      "3.5 Methods Using Derived Input Directions . . . . . . . . . 79\n",
      "3.5.1 Principal Components Regression . . . . . . . . 79\n",
      "3.5.2 Partial Least Squares . . . . . . . . . . . . . . . 80\n",
      "3.6 Discussion: A Comparison of the Selection\n",
      "and Shrinkage Methods . . . . . . . . . . . . . . . . . . . 82\n",
      "3.7 Multiple Outcome Shrinkage and Selection . . . . . . . . 84\n",
      "3.8 More on the Lasso and Related Path Algorithms . . . . . 86\n",
      "3.8.1 Incremental Forward Stagewise Regression . . . 86\n",
      "3.8.2 Piecewise-Linear Path Algorithms . . . . . . . . 89\n",
      "3.8.3 The Dantzig Selector . . . . . . . . . . . . . . . 89\n",
      "3.8.4 The Grouped Lasso . . . . . . . . . . . . . . . . 90\n",
      "3.8.5 Further Properties of the Lasso . . . . . . . . . . 91\n",
      "3.8.6 Pathwise Coordinate Optimization . . . . . . . . 92\n",
      "3.9 Computational Considerations . . . . . . . . . . . . . . . 93\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Contents xv\n",
      "4 Linear Methods for Classiﬁcation 101\n",
      "4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101\n",
      "4.2 Linear Regression of an Indicator Matrix . . . . . . . . . 103\n",
      "4.3 Linear Discriminant Analysis . . . . . . . . . . . . . . . . 106\n",
      "4.3.1 Regularized Discriminant Analysis . . . . . . . . 112\n",
      "4.3.2 Computations for LDA . . . . . . . . . . . . . . 113\n",
      "4.3.3 Reduced-Rank Linear Discriminant Analysis . . 113\n",
      "4.4 Logistic Regression . . . . . . . . . . . . . . . . . . . . . . 119\n",
      "4.4.1 Fitting Logistic Regression Models . . . . . . . . 120\n",
      "4.4.2 Example: South African Heart Disease . . . . . 122\n",
      "4.4.3 Quadratic Approximations and Inference . . . . 124\n",
      "4.4.4 L1Regularized Logistic Regression . . . . . . . . 125\n",
      "4.4.5 Logistic Regression or LDA? . . . . . . . . . . . 127\n",
      "4.5 Separating Hyperplanes . . . . . . . . . . . . . . . . . . . 129\n",
      "4.5.1 Rosenblatt’s Perceptron Learning Algorithm . . 130\n",
      "4.5.2 Optimal Separating Hyperplanes . . . . . . . . . 132\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "5 Basis Expansions and Regularization 139\n",
      "5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139\n",
      "5.2 Piecewise Polynomials and Splines . . . . . . . . . . . . . 141\n",
      "5.2.1 Natural Cubic Splines . . . . . . . . . . . . . . . 144\n",
      "5.2.2 Example: South African Heart Disease (Continued)146\n",
      "5.2.3 Example: Phoneme Recognition . . . . . . . . . 148\n",
      "5.3 Filtering and Feature Extraction . . . . . . . . . . . . . . 150\n",
      "5.4 Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . 151\n",
      "5.4.1 Degrees of Freedom and Smoother Matrices . . . 153\n",
      "5.5 Automatic Selection of the Smoothing Parameters . . . . 156\n",
      "5.5.1 Fixing the Degrees of Freedom . . . . . . . . . . 158\n",
      "5.5.2 The Bias–Variance Tradeoﬀ . . . . . . . . . . . . 158\n",
      "5.6 Nonparametric Logistic Regression . . . . . . . . . . . . . 161\n",
      "5.7 Multidimensional Splines . . . . . . . . . . . . . . . . . . 162\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces . 167\n",
      "5.8.1 Spaces of Functions Generated by Kernels . . . 168\n",
      "5.8.2 Examples of RKHS . . . . . . . . . . . . . . . . 170\n",
      "5.9 Wavelet Smoothing . . . . . . . . . . . . . . . . . . . . . 174\n",
      "5.9.1 Wavelet Bases and the Wavelet Transform . . . 176\n",
      "5.9.2 Adaptive Wavelet Filtering . . . . . . . . . . . . 179\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181\n",
      "Appendix: Computational Considerations for Splines . . . . . . 186\n",
      "Appendix: B-splines . . . . . . . . . . . . . . . . . . . . . 186\n",
      "Appendix: Computations for Smoothing Splines . . . . . 189\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "xvi Contents\n",
      "6 Kernel Smoothing Methods 191\n",
      "6.1 One-Dimensional Kernel Smoothers . . . . . . . . . . . . 192\n",
      "6.1.1 Local Linear Regression . . . . . . . . . . . . . . 194\n",
      "6.1.2 Local Polynomial Regression . . . . . . . . . . . 197\n",
      "6.2 Selecting the Width of the Kernel . . . . . . . . . . . . . 198\n",
      "6.3 Local Regression in IRp. . . . . . . . . . . . . . . . . . . 200\n",
      "6.4 Structured Local Regression Models in IRp. . . . . . . . 201\n",
      "6.4.1 Structured Kernels . . . . . . . . . . . . . . . . . 203\n",
      "6.4.2 Structured Regression Functions . . . . . . . . . 203\n",
      "6.5 Local Likelihood and Other Models . . . . . . . . . . . . 205\n",
      "6.6 Kernel Density Estimation and Classiﬁcation . . . . . . . 208\n",
      "6.6.1 Kernel Density Estimation . . . . . . . . . . . . 208\n",
      "6.6.2 Kernel Density Classiﬁcation . . . . . . . . . . . 210\n",
      "6.6.3 The Naive Bayes Classiﬁer . . . . . . . . . . . . 210\n",
      "6.7 Radial Basis Functions and Kernels . . . . . . . . . . . . 212\n",
      "6.8 Mixture Models for Density Estimation and Classiﬁcation 214\n",
      "6.9 Computational Considerations . . . . . . . . . . . . . . . 216\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "7 Model Assessment and Selection 219\n",
      "7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219\n",
      "7.2 Bias, Variance and Model Complexity . . . . . . . . . . . 219\n",
      "7.3 The Bias–Variance Decomposition . . . . . . . . . . . . . 223\n",
      "7.3.1 Example: Bias–Variance Tradeoﬀ . . . . . . . . 226\n",
      "7.4 Optimism of the Training Error Rate . . . . . . . . . . . 228\n",
      "7.5 Estimates of In-Sample Prediction Error . . . . . . . . . . 230\n",
      "7.6 The Eﬀective Number of Parameters . . . . . . . . . . . . 232\n",
      "7.7 The Bayesian Approach and BIC . . . . . . . . . . . . . . 233\n",
      "7.8 Minimum Description Length . . . . . . . . . . . . . . . . 235\n",
      "7.9 Vapnik–Chervonenkis Dimension . . . . . . . . . . . . . . 237\n",
      "7.9.1 Example (Continued) . . . . . . . . . . . . . . . 239\n",
      "7.10 Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . 241\n",
      "7.10.1 K-Fold Cross-Validation . . . . . . . . . . . . . 241\n",
      "7.10.2 The Wrong and Right Way\n",
      "to Do Cross-validation . . . . . . . . . . . . . . . 245\n",
      "7.10.3 Does Cross-Validation Really Work? . . . . . . . 247\n",
      "7.11 Bootstrap Methods . . . . . . . . . . . . . . . . . . . . . 249\n",
      "7.11.1 Example (Continued) . . . . . . . . . . . . . . . 252\n",
      "7.12 Conditional or Expected Test Error? . . . . . . . . . . . . 254\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "8 Model Inference and Averaging 261\n",
      "8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Contents xvii\n",
      "8.2 The Bootstrap and Maximum Likelihood Methods . . . . 261\n",
      "8.2.1 A Smoothing Example . . . . . . . . . . . . . . 261\n",
      "8.2.2 Maximum Likelihood Inference . . . . . . . . . . 265\n",
      "8.2.3 Bootstrap versus Maximum Likelihood . . . . . 267\n",
      "8.3 Bayesian Methods . . . . . . . . . . . . . . . . . . . . . . 267\n",
      "8.4 Relationship Between the Bootstrap\n",
      "and Bayesian Inference . . . . . . . . . . . . . . . . . . . 271\n",
      "8.5 The EM Algorithm . . . . . . . . . . . . . . . . . . . . . 272\n",
      "8.5.1 Two-Component Mixture Model . . . . . . . . . 272\n",
      "8.5.2 The EM Algorithm in General . . . . . . . . . . 276\n",
      "8.5.3 EM as a Maximization–Maximization Procedure 277\n",
      "8.6 MCMC for Sampling from the Posterior . . . . . . . . . . 279\n",
      "8.7 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n",
      "8.7.1 Example: Trees with Simulated Data . . . . . . 283\n",
      "8.8 Model Averaging and Stacking . . . . . . . . . . . . . . . 288\n",
      "8.9 Stochastic Search: Bumping . . . . . . . . . . . . . . . . . 290\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 292\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n",
      "9 Additive Models, Trees, and Related Methods 295\n",
      "9.1 Generalized Additive Models . . . . . . . . . . . . . . . . 295\n",
      "9.1.1 Fitting Additive Models . . . . . . . . . . . . . . 297\n",
      "9.1.2 Example: Additive Logistic Regression . . . . . 299\n",
      "9.1.3 Summary . . . . . . . . . . . . . . . . . . . . . . 304\n",
      "9.2 Tree-Based Methods . . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.1 Background . . . . . . . . . . . . . . . . . . . . 305\n",
      "9.2.2 Regression Trees . . . . . . . . . . . . . . . . . . 307\n",
      "9.2.3 Classiﬁcation Trees . . . . . . . . . . . . . . . . 308\n",
      "9.2.4 Other Issues . . . . . . . . . . . . . . . . . . . . 310\n",
      "9.2.5 Spam Example (Continued) . . . . . . . . . . . 313\n",
      "9.3 PRIM: Bump Hunting . . . . . . . . . . . . . . . . . . . . 317\n",
      "9.3.1 Spam Example (Continued) . . . . . . . . . . . 320\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines . . . . . 321\n",
      "9.4.1 Spam Example (Continued) . . . . . . . . . . . 326\n",
      "9.4.2 Example (Simulated Data) . . . . . . . . . . . . 327\n",
      "9.4.3 Other Issues . . . . . . . . . . . . . . . . . . . . 328\n",
      "9.5 Hierarchical Mixtures of Experts . . . . . . . . . . . . . . 329\n",
      "9.6 Missing Data . . . . . . . . . . . . . . . . . . . . . . . . . 332\n",
      "9.7 Computational Considerations . . . . . . . . . . . . . . . 334\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 334\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "10 Boosting and Additive Trees 337\n",
      "10.1 Boosting Methods . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "10.1.1 Outline of This Chapter . . . . . . . . . . . . . . 340\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "xviii Contents\n",
      "10.2 Boosting Fits an Additive Model . . . . . . . . . . . . . . 341\n",
      "10.3 Forward Stagewise Additive Modeling . . . . . . . . . . . 342\n",
      "10.4 Exponential Loss and AdaBoost . . . . . . . . . . . . . . 343\n",
      "10.5 Why Exponential Loss? . . . . . . . . . . . . . . . . . . . 345\n",
      "10.6 Loss Functions and Robustness . . . . . . . . . . . . . . . 346\n",
      "10.7 “Oﬀ-the-Shelf” Procedures for Data Mining . . . . . . . . 350\n",
      "10.8 Example: Spam Data . . . . . . . . . . . . . . . . . . . . 352\n",
      "10.9 Boosting Trees . . . . . . . . . . . . . . . . . . . . . . . . 353\n",
      "10.10 Numerical Optimization via Gradient Boosting . . . . . . 358\n",
      "10.10.1 Steepest Descent . . . . . . . . . . . . . . . . . . 358\n",
      "10.10.2 Gradient Boosting . . . . . . . . . . . . . . . . . 359\n",
      "10.10.3 Implementations of Gradient Boosting . . . . . . 360\n",
      "10.11 Right-Sized Trees for Boosting . . . . . . . . . . . . . . . 361\n",
      "10.12 Regularization . . . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.1 Shrinkage . . . . . . . . . . . . . . . . . . . . . . 364\n",
      "10.12.2 Subsampling . . . . . . . . . . . . . . . . . . . . 365\n",
      "10.13 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367\n",
      "10.13.1 Relative Importance of Predictor Variables . . . 367\n",
      "10.13.2 Partial Dependence Plots . . . . . . . . . . . . . 369\n",
      "10.14 Illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n",
      "10.14.1 California Housing . . . . . . . . . . . . . . . . . 371\n",
      "10.14.2 New Zealand Fish . . . . . . . . . . . . . . . . . 375\n",
      "10.14.3 Demographics Data . . . . . . . . . . . . . . . . 379\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 380\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384\n",
      "11 Neural Networks 389\n",
      "11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389\n",
      "11.2 Projection Pursuit Regression . . . . . . . . . . . . . . . 389\n",
      "11.3 Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 392\n",
      "11.4 Fitting Neural Networks . . . . . . . . . . . . . . . . . . . 395\n",
      "11.5 Some Issues in Training Neural Networks . . . . . . . . . 397\n",
      "11.5.1 Starting Values . . . . . . . . . . . . . . . . . . . 397\n",
      "11.5.2 Overﬁtting . . . . . . . . . . . . . . . . . . . . . 398\n",
      "11.5.3 Scaling of the Inputs . . . . . . . . . . . . . . . 398\n",
      "11.5.4 Number of Hidden Units and Layers . . . . . . . 400\n",
      "11.5.5 Multiple Minima . . . . . . . . . . . . . . . . . . 400\n",
      "11.6 Example: Simulated Data . . . . . . . . . . . . . . . . . . 401\n",
      "11.7 Example: ZIP Code Data . . . . . . . . . . . . . . . . . . 404\n",
      "11.8 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge . . . 409\n",
      "11.9.1 Bayes, Boosting and Bagging . . . . . . . . . . . 410\n",
      "11.9.2 Performance Comparisons . . . . . . . . . . . . 412\n",
      "11.10 Computational Considerations . . . . . . . . . . . . . . . 414\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Contents xix\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415\n",
      "12 Support Vector Machines and\n",
      "Flexible Discriminants 417\n",
      "12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417\n",
      "12.2 The Support Vector Classiﬁer . . . . . . . . . . . . . . . . 417\n",
      "12.2.1 Computing the Support Vector Classiﬁer . . . . 420\n",
      "12.2.2 Mixture Example (Continued) . . . . . . . . . . 421\n",
      "12.3 Support Vector Machines and Kernels . . . . . . . . . . . 423\n",
      "12.3.1 Computing the SVM for Classiﬁcation . . . . . . 423\n",
      "12.3.2 The SVM as a Penalization Method . . . . . . . 426\n",
      "12.3.3 Function Estimation and Reproducing Kernels . 428\n",
      "12.3.4 SVMs and the Curse of Dimensionality . . . . . 431\n",
      "12.3.5 A Path Algorithm for the SVM Classiﬁer . . . . 432\n",
      "12.3.6 Support Vector Machines for Regression . . . . . 434\n",
      "12.3.7 Regression and Kernels . . . . . . . . . . . . . . 436\n",
      "12.3.8 Discussion . . . . . . . . . . . . . . . . . . . . . 438\n",
      "12.4 Generalizing Linear Discriminant Analysis . . . . . . . . 438\n",
      "12.5 Flexible Discriminant Analysis . . . . . . . . . . . . . . . 440\n",
      "12.5.1 Computing the FDA Estimates . . . . . . . . . . 444\n",
      "12.6 Penalized Discriminant Analysis . . . . . . . . . . . . . . 446\n",
      "12.7 Mixture Discriminant Analysis . . . . . . . . . . . . . . . 449\n",
      "12.7.1 Example: Waveform Data . . . . . . . . . . . . . 451\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n",
      "13 Prototype Methods and Nearest-Neighbors 459\n",
      "13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2 Prototype Methods . . . . . . . . . . . . . . . . . . . . . 459\n",
      "13.2.1 K-means Clustering . . . . . . . . . . . . . . . . 460\n",
      "13.2.2 Learning Vector Quantization . . . . . . . . . . 462\n",
      "13.2.3 Gaussian Mixtures . . . . . . . . . . . . . . . . . 463\n",
      "13.3 k-Nearest-Neighbor Classiﬁers . . . . . . . . . . . . . . . 463\n",
      "13.3.1 Example: A Comparative Study . . . . . . . . . 468\n",
      "13.3.2 Example: k-Nearest-Neighbors\n",
      "and Image Scene Classiﬁcation . . . . . . . . . . 470\n",
      "13.3.3 Invariant Metrics and Tangent Distance . . . . . 471\n",
      "13.4 Adaptive Nearest-Neighbor Methods . . . . . . . . . . . . 475\n",
      "13.4.1 Example . . . . . . . . . . . . . . . . . . . . . . 478\n",
      "13.4.2 Global Dimension Reduction\n",
      "for Nearest-Neighbors . . . . . . . . . . . . . . . 479\n",
      "13.5 Computational Considerations . . . . . . . . . . . . . . . 480\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "xx Contents\n",
      "14 Unsupervised Learning 485\n",
      "14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485\n",
      "14.2 Association Rules . . . . . . . . . . . . . . . . . . . . . . 487\n",
      "14.2.1 Market Basket Analysis . . . . . . . . . . . . . . 488\n",
      "14.2.2 The Apriori Algorithm . . . . . . . . . . . . . . 489\n",
      "14.2.3 Example: Market Basket Analysis . . . . . . . . 492\n",
      "14.2.4 Unsupervised as Supervised Learning . . . . . . 495\n",
      "14.2.5 Generalized Association Rules . . . . . . . . . . 497\n",
      "14.2.6 Choice of Supervised Learning Method . . . . . 499\n",
      "14.2.7 Example: Market Basket Analysis (Continued) . 499\n",
      "14.3 Cluster Analysis . . . . . . . . . . . . . . . . . . . . . . . 501\n",
      "14.3.1 Proximity Matrices . . . . . . . . . . . . . . . . 503\n",
      "14.3.2 Dissimilarities Based on Attributes . . . . . . . 503\n",
      "14.3.3 Object Dissimilarity . . . . . . . . . . . . . . . . 505\n",
      "14.3.4 Clustering Algorithms . . . . . . . . . . . . . . . 507\n",
      "14.3.5 Combinatorial Algorithms . . . . . . . . . . . . 507\n",
      "14.3.6 K-means . . . . . . . . . . . . . . . . . . . . . . 509\n",
      "14.3.7 Gaussian Mixtures as Soft K-means Clustering . 510\n",
      "14.3.8 Example: Human Tumor Microarray Data . . . 512\n",
      "14.3.9 Vector Quantization . . . . . . . . . . . . . . . . 514\n",
      "14.3.10 K-medoids . . . . . . . . . . . . . . . . . . . . . 515\n",
      "14.3.11 Practical Issues . . . . . . . . . . . . . . . . . . 518\n",
      "14.3.12 Hierarchical Clustering . . . . . . . . . . . . . . 520\n",
      "14.4 Self-Organizing Maps . . . . . . . . . . . . . . . . . . . . 528\n",
      "14.5 Principal Components, Curves and Surfaces . . . . . . . . 534\n",
      "14.5.1 Principal Components . . . . . . . . . . . . . . . 534\n",
      "14.5.2 Principal Curves and Surfaces . . . . . . . . . . 541\n",
      "14.5.3 Spectral Clustering . . . . . . . . . . . . . . . . 544\n",
      "14.5.4 Kernel Principal Components . . . . . . . . . . . 547\n",
      "14.5.5 Sparse Principal Components . . . . . . . . . . . 550\n",
      "14.6 Non-negative Matrix Factorization . . . . . . . . . . . . . 553\n",
      "14.6.1 Archetypal Analysis . . . . . . . . . . . . . . . . 554\n",
      "14.7 Independent Component Analysis\n",
      "and Exploratory Projection Pursuit . . . . . . . . . . . . 557\n",
      "14.7.1 Latent Variables and Factor Analysis . . . . . . 558\n",
      "14.7.2 Independent Component Analysis . . . . . . . . 560\n",
      "14.7.3 Exploratory Projection Pursuit . . . . . . . . . . 565\n",
      "14.7.4 A Direct Approach to ICA . . . . . . . . . . . . 565\n",
      "14.8 Multidimensional Scaling . . . . . . . . . . . . . . . . . . 570\n",
      "14.9 Nonlinear Dimension Reduction\n",
      "and Local Multidimensional Scaling . . . . . . . . . . . . 572\n",
      "14.10 The Google PageRank Algorithm . . . . . . . . . . . . . 576\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 578\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Contents xxi\n",
      "15 Random Forests 587\n",
      "15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587\n",
      "15.2 Deﬁnition of Random Forests . . . . . . . . . . . . . . . . 587\n",
      "15.3 Details of Random Forests . . . . . . . . . . . . . . . . . 592\n",
      "15.3.1 Out of Bag Samples . . . . . . . . . . . . . . . . 592\n",
      "15.3.2 Variable Importance . . . . . . . . . . . . . . . . 593\n",
      "15.3.3 Proximity Plots . . . . . . . . . . . . . . . . . . 595\n",
      "15.3.4 Random Forests and Overﬁtting . . . . . . . . . 596\n",
      "15.4 Analysis of Random Forests . . . . . . . . . . . . . . . . . 597\n",
      "15.4.1 Variance and the De-Correlation Eﬀect . . . . . 597\n",
      "15.4.2 Bias . . . . . . . . . . . . . . . . . . . . . . . . . 600\n",
      "15.4.3 Adaptive Nearest Neighbors . . . . . . . . . . . 601\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 602\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603\n",
      "16 Ensemble Learning 605\n",
      "16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605\n",
      "16.2 Boosting and Regularization Paths . . . . . . . . . . . . . 607\n",
      "16.2.1 Penalized Regression . . . . . . . . . . . . . . . 607\n",
      "16.2.2 The “Bet on Sparsity” Principle . . . . . . . . . 610\n",
      "16.2.3 Regularization Paths, Over-ﬁtting and Margins . 613\n",
      "16.3 Learning Ensembles . . . . . . . . . . . . . . . . . . . . . 616\n",
      "16.3.1 Learning a Good Ensemble . . . . . . . . . . . . 617\n",
      "16.3.2 Rule Ensembles . . . . . . . . . . . . . . . . . . 622\n",
      "Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . 623\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624\n",
      "17 Undirected Graphical Models 625\n",
      "17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625\n",
      "17.2 Markov Graphs and Their Properties . . . . . . . . . . . 627\n",
      "17.3 Undirected Graphical Models for Continuous Variables . 630\n",
      "17.3.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 631\n",
      "17.3.2 Estimation of the Graph Structure . . . . . . . . 635\n",
      "17.4 Undirected Graphical Models for Discrete Variables . . . 638\n",
      "17.4.1 Estimation of the Parameters\n",
      "when the Graph Structure is Known . . . . . . . 639\n",
      "17.4.2 Hidden Nodes . . . . . . . . . . . . . . . . . . . 641\n",
      "17.4.3 Estimation of the Graph Structure . . . . . . . . 642\n",
      "17.4.4 Restricted Boltzmann Machines . . . . . . . . . 643\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645\n",
      "18 High-Dimensional Problems: p≫N 649\n",
      "18.1 When pis Much Bigger than N. . . . . . . . . . . . . . 649\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "xxii Contents\n",
      "18.2 Diagonal Linear Discriminant Analysis\n",
      "and Nearest Shrunken Centroids . . . . . . . . . . . . . . 651\n",
      "18.3 Linear Classiﬁers with Quadratic Regularization . . . . . 654\n",
      "18.3.1 Regularized Discriminant Analysis . . . . . . . . 656\n",
      "18.3.2 Logistic Regression\n",
      "with Quadratic Regularization . . . . . . . . . . 657\n",
      "18.3.3 The Support Vector Classiﬁer . . . . . . . . . . 657\n",
      "18.3.4 Feature Selection . . . . . . . . . . . . . . . . . . 658\n",
      "18.3.5 Computational Shortcuts When p≫N. . . . . 659\n",
      "18.4 Linear Classiﬁers with L1Regularization . . . . . . . . . 661\n",
      "18.4.1 Application of Lasso\n",
      "to Protein Mass Spectroscopy . . . . . . . . . . 664\n",
      "18.4.2 The Fused Lasso for Functional Data . . . . . . 666\n",
      "18.5 Classiﬁcation When Features are Unavailable . . . . . . . 668\n",
      "18.5.1 Example: String Kernels\n",
      "and Protein Classiﬁcation . . . . . . . . . . . . . 668\n",
      "18.5.2 Classiﬁcation and Other Models Using\n",
      "Inner-Product Kernels and Pairwise Distances . 670\n",
      "18.5.3 Example: Abstracts Classiﬁcation . . . . . . . . 672\n",
      "18.6 High-Dimensional Regression:\n",
      "Supervised Principal Components . . . . . . . . . . . . . 674\n",
      "18.6.1 Connection to Latent-Variable Modeling . . . . 678\n",
      "18.6.2 Relationship with Partial Least Squares . . . . . 680\n",
      "18.6.3 Pre-Conditioning for Feature Selection . . . . . 681\n",
      "18.7 Feature Assessment and the Multiple-Testing Problem . . 683\n",
      "18.7.1 The False Discovery Rate . . . . . . . . . . . . . 687\n",
      "18.7.2 Asymmetric Cutpoints and the SAM Procedure 690\n",
      "18.7.3 A Bayesian Interpretation of the FDR . . . . . . 692\n",
      "18.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . 693\n",
      "Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694\n",
      "References 699\n",
      "Author Index 729\n",
      "Index 737\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 1\n",
      "Printer: Opaque this\n",
      "1\n",
      "Introduction\n",
      "Statistical learning plays a key role in many areas of science, ﬁnance and\n",
      "industry. Here are some examples of learning problems:\n",
      "•Predict whether a patient, hospitalized due to a heart attack, will\n",
      "have a second heart attack. The prediction is to be based on demo-\n",
      "graphic, diet and clinical measurements for that patient.\n",
      "•Predict the price of a stock in 6 months from now, on the basis of\n",
      "company performance measures and economic data.\n",
      "•Identify the numbers in a handwritten ZIP code, from a digitized\n",
      "image.\n",
      "•Estimate the amount of glucose in the blood of a diabetic person,\n",
      "from the infrared absorption spectrum of that person’s blood.\n",
      "•Identify the risk factors for prostate cancer, based on clinical and\n",
      "demographic variables.\n",
      "The science of learning plays a key role in the ﬁelds of statistics, data\n",
      "mining and artiﬁcial intelligence, intersecting with areas of engineering and\n",
      "other disciplines.\n",
      "This book is about learning from data. In a typical scenario, we have\n",
      "an outcome measurement, usually quantitative (such as a stock price) or\n",
      "categorical (such as heart attack/no heart attack), that we wish to predict\n",
      "based on a set of features (such as diet and clinical measurements). We\n",
      "have a training set of data, in which we observe the outcome and feature\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2 1. Introduction\n",
      "TABLE 1.1. Average percentage of words or characters in an email message\n",
      "equal to the indicated word or character. We have chosen the wo rds and characters\n",
      "showing the largest diﬀerence between spamandemail.\n",
      "george you your hp free hpl ! our re edu remove\n",
      "spam 0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01 0.28\n",
      "email 1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29 0.01\n",
      "measurements for a set of objects (such as people). Using this data we build\n",
      "a prediction model, or learner , which will enable us to predict the outcome\n",
      "for new unseen objects. A good learner is one that accurately predicts such\n",
      "an outcome.\n",
      "The examples above describe what is called the supervised learning prob-\n",
      "lem. It is called “supervised” because of the presence of the outcome vari-\n",
      "able to guide the learning process. In the unsupervised learning problem ,\n",
      "we observe only the features and have no measurements of the outcome.\n",
      "Our task is rather to describe how the data are organized or clustered. We\n",
      "devote most of this book to supervised learning; the unsupervised problem\n",
      "is less developed in the literature, and is the focus of Chapter 14.\n",
      "Here are some examples of real learning problems that are discussed in\n",
      "this book.\n",
      "Example 1: Email Spam\n",
      "The data for this example consists of information from 4601 email mes-\n",
      "sages, in a study to try to predict whether the email was junk email, or\n",
      "“spam.” The objective was to design an automatic spam detector that\n",
      "could ﬁlter out spam before clogging the users’ mailboxes. For all 4601\n",
      "email messages, the true outcome (email type) email orspamis available,\n",
      "along with the relative frequencies of 57 of the most commonly occurring\n",
      "words and punctuation marks in the email message. This is a supervised\n",
      "learning problem, with the outcome the class variable email/spam. It is also\n",
      "called a classiﬁcation problem.\n",
      "Table 1.1 lists the words and characters showing the largest average\n",
      "diﬀerence between spamandemail.\n",
      "Our learning method has to decide which features to use and how: for\n",
      "example, we might use a rule such as\n",
      "if (%george <0.6) & (%you>1.5) then spam\n",
      "elseemail.\n",
      "Another form of a rule might be:\n",
      "if (0.2≤%you−0.3≤%george )>0 then spam\n",
      "elseemail.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "1. Introduction 3\n",
      "lpsa−1 1 2 3 4\n",
      "oooooo ooo ooo o oooo o oooo oooooooooo oooooooooooooooooooooooooooooo oo oooo ooooooooooooooooooooooooooooo\n",
      "ooo ooo ooooo oooooooooooooooooooooooooo o ooooooooo oo ooooooo oooooooooooo ooooooooooooooooooooooooooooo40 50 60 70 80\n",
      "oo oooo ooo ooo oooooo o ooooooooooooo oooooooooooooooo o o oooooo oo oooo oooooooooooooooooooo ooooooooooooooo\n",
      "oo o ooo o o o oo ooooo o oo o o o ooooo ooo ooo oo o oo oo oo o o ooo o o oo ooo o o o ooooo o o o ooo o o o o oooo oo oo ooooo o o oo o o oooooo0.0 0.4 0.8\n",
      "oo o ooo ooo oo o oooooooooooooooooooo oooooo o ooooooo o oo oooooooooooo o o o oooo oo o o oooo oo o ooo o oo o o ooo o oooooo\n",
      "oo o ooo ooo oo o ooooooooo o o o oooooo oo oooo o o oooooooo o oo ooooo ooooooo ooo ooooooo ooooooo o ooooooo oooo ooooooo6.0 7.0 8.0 9.0\n",
      "oo oooo ooo oo o oooooooooooooooooooo oooooooo o ooooo o oo ooooooooooooo ooooooooooo o ooooooooo o ooooooooooooo\n",
      "0 1 2 3 4 5oo oooo ooo oo o ooooooooooo o oo o o o ooo oooooooo o ooooo o oo o o o oooooooooo oo oo o ooo ooo o ooo o oooooooo oooo oooooo o−1 1 2 3 4ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "lcavol\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo o\n",
      "oo\n",
      "o oo\n",
      "ooooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o o\n",
      "ooo\n",
      "ooo\n",
      "o oooo\n",
      "oo oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o o\n",
      "ooo\n",
      "ooo\n",
      "o oooo\n",
      "oo o o\n",
      "oo o o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo o o\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "o oo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooo o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "o oo\n",
      "ooooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o o\n",
      "oo o\n",
      "ooo\n",
      "ooooo\n",
      "oo oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooolweight\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "o ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oooooo o\n",
      "oo o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o oooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo o\n",
      "oooo\n",
      "o ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "ooo o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o oo oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "ooo o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "ooo o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "2.5 3.5 4.5oo\n",
      "oooooo o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "ooo o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo o\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo40 50 60 70 80ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oooooo ooo oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooooo ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "age\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o o oooooooo o oo\n",
      "oo oo\n",
      "oo\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o o oooooo oo ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooooooo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "oo o ooooo\n",
      "o ooo\n",
      "o o o oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oooo oooo\n",
      "o o oo\n",
      "o oooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o ooo\n",
      "o\n",
      "o oo ooooo\n",
      "oo oo\n",
      "o o ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o ooo\n",
      "o\n",
      "o o oo oooo\n",
      "o ooo\n",
      "o oo oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o o oo\n",
      "olbph\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "o o o oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o oo\n",
      "o\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "oo ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o ooo\n",
      "o\n",
      "o o oo o ooo\n",
      "o o oo\n",
      "o o ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o oo\n",
      "o\n",
      "−1 0 1 2o o oo o ooo\n",
      "o o oo\n",
      "oo o oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "oooo\n",
      "o0.0 0.4 0.8oo o ooo o o o oo o o o o o o o o o o o o o o o o o o o o o o o o o o oo\n",
      "o o o o o o oo\n",
      "o o o o o o o o o o o o o oo\n",
      "oo\n",
      "o o o o o oo\n",
      "oo oo o\n",
      "o oo\n",
      "o o oo\n",
      "o oo\n",
      "ooo o\n",
      "oooooo o\n",
      "oooo oo o oo o oo o oooo oo o o oo oo ooo o oo o oo o o ooo\n",
      "o o oo oooo\n",
      "o oo o oo o oooo o ooo\n",
      "oo\n",
      "oo ooo oo\n",
      "oo o o o\n",
      "o oo\n",
      "oo oo\n",
      "ooo\n",
      "oo oo\n",
      "ooo oo o o\n",
      "o oo ooo o o oo o oo o oo ooo oooo ooo ooo o oooo o oo oo\n",
      "oo oo oo oo\n",
      "oo o o oo oo oo o oo oo\n",
      "oo\n",
      "oo oo o oo\n",
      "ooo oo\n",
      "o oo\n",
      "oo oo\n",
      "ooo\n",
      "oo oo\n",
      "ooo oo oo\n",
      "o o oo oo ooo ooo o oo o oo o oo o o o o ooo oo o o oo o o ooo\n",
      "o o oo o ooo\n",
      "o o o oo o oo ooo oo oo\n",
      "oo\n",
      "oo o oooo\n",
      "ooo o o\n",
      "o oo\n",
      "oooo\n",
      "ooo\n",
      "ooo o\n",
      "oo oo o o o\n",
      "o o o o o o o oo o o oo o o o oo o oo oo o ooo ooo o oo o o oooo\n",
      "o o o oo o oo\n",
      "o o oo o oo o oo oo o oo\n",
      "oo\n",
      "o o o oo oo\n",
      "oo oo o\n",
      "ooo\n",
      "o o oo\n",
      "o oo\n",
      "oo o o\n",
      "oo o o o oo\n",
      "svi\n",
      "o o o o o o o o o o o o oo oo o oo o o oo oo o oo o ooo o o oo ooo\n",
      "oo o o oooo\n",
      "oo oo o o oo o oo o o oo\n",
      "oo\n",
      "o o ooo oo\n",
      "oo o o o\n",
      "ooo\n",
      "o o oo\n",
      "ooo\n",
      "oo oo\n",
      "oo o oo o o\n",
      "o o oo o o o o o o o o o o oo oo o o o oo oo o o o oo o o o o o o ooo\n",
      "o ooo o o oo\n",
      "oo o oo o o o o oo o ooo\n",
      "oo\n",
      "o o o oo oo\n",
      "oo oo o\n",
      "o oo\n",
      "o o oo\n",
      "o oo\n",
      "oo o o\n",
      "oo o o o o o\n",
      "o o oo o o o o o o o o oo o o oo o o o oo oo o oo oo o o o o oo o oo\n",
      "o oooo o oo\n",
      "oo o oo ooo oo o o ooo\n",
      "oo\n",
      "o o ooooo\n",
      "oo oo o\n",
      "o oo\n",
      "oo oo\n",
      "ooo\n",
      "oo o o\n",
      "oo ooo oo\n",
      "oo o ooo o o o oo oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "oooo oo o oo o ooo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "ooo\n",
      "o o ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o oo ooo o o oo o oo\n",
      "oo\n",
      "ooo\n",
      "o ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o oo oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o o oo oo ooo oooo\n",
      "oo\n",
      "ooo\n",
      "o ooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "o oo oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o o o o o o o oo o o oo\n",
      "oo\n",
      "ooo\n",
      "o ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "oo o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "o o o o o o o o o o o oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o o oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "lcp\n",
      "o o oo o o o o o o o oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "o o ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo\n",
      "−1 0 1 2 3o o oo o o o o o o o oo\n",
      "oo\n",
      "ooo\n",
      "o o oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "o o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o ooo\n",
      "o oo\n",
      "o o ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o oooo\n",
      "oo6.0 7.0 8.0 9.0ooo\n",
      "ooo o o o oo oo o o\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o oo o o\n",
      "o o o o o ooo\n",
      "o o oo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "oo o o o o\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo o o\n",
      "oo o o oo\n",
      "o o o o o o o o oo\n",
      "o o\n",
      "ooo o\n",
      "oooooo o\n",
      "ooo\n",
      "o oo o oo o ooo oo\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o ooo o\n",
      "oo o oo ooo\n",
      "o ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oo o ooo\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo oo\n",
      "oo oo oo\n",
      "o oo o o oo o oo\n",
      "o o\n",
      "oo oo\n",
      "ooo oo o o\n",
      "o oo\n",
      "ooo o o oo o oo o o\n",
      "oo\n",
      "oo ooo\n",
      "oo\n",
      "ooooo\n",
      "o oooo ooo\n",
      "o ooo\n",
      "o\n",
      "ooo oo\n",
      "o\n",
      "o oo\n",
      "oo oo oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo oo\n",
      "oooo oo\n",
      "oo o oo oo o oo\n",
      "oo\n",
      "oo oo\n",
      "ooo oo oo\n",
      "o oo\n",
      "o oo ooo oooo oo\n",
      "oo\n",
      "o o ooo\n",
      "oo\n",
      "o ooo o\n",
      "o o o oo ooo\n",
      "o ooo\n",
      "o\n",
      "oo ooo\n",
      "o\n",
      "o oo\n",
      "oo oo oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo o o\n",
      "ooo ooo\n",
      "o o o ooooo oo\n",
      "o o\n",
      "ooo o\n",
      "oo oo o o o\n",
      "o oo\n",
      "o o o o oo o o oo o o\n",
      "oo\n",
      "o o ooo\n",
      "oo\n",
      "ooo oo\n",
      "o o oo o ooo\n",
      "o o oo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "ooo o oo\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo o o\n",
      "oo o ooo\n",
      "o o ooo o o ooo\n",
      "o o\n",
      "oo o o\n",
      "oo o o o oo\n",
      "o oo\n",
      "o o o o o o o o oo o o\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o oo o o\n",
      "o o o o o ooo\n",
      "o ooo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "oo o o o o\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo o o\n",
      "oo oo oo\n",
      "o o o o oo o o oo\n",
      "o o\n",
      "oo o o\n",
      "oo o o o o o\n",
      "o oo\n",
      "o o o o o o o o ooo o\n",
      "oo\n",
      "oo o oo\n",
      "oo\n",
      "o ooo o\n",
      "o oo o o ooo\n",
      "o ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo oo o o\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo oo\n",
      "oo oo oo\n",
      "o ooo oo o ooo\n",
      "o o\n",
      "oo oo\n",
      "oo o oo o ogleason\n",
      "o oo\n",
      "o o o o o o o o ooo o\n",
      "oo\n",
      "o o o oo\n",
      "oo\n",
      "o oo o o\n",
      "o o o o o ooo\n",
      "o ooo\n",
      "o\n",
      "oo o oo\n",
      "o\n",
      "o oo\n",
      "oooo oo\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "oo oo\n",
      "oo oo oo\n",
      "o o oo ooo ooo\n",
      "o o\n",
      "oo o o\n",
      "oo ooo oo\n",
      "0 1 2 3 4 5ooo\n",
      "ooo o o o oo oo\n",
      "o ooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o o o ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o oo o oo o ooo\n",
      "oooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "oo o oo ooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "2.5 3.5 4.5o oo\n",
      "ooo o o oo o oo\n",
      "o ooo\n",
      "oo ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o oooo ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "o oo ooo oooo\n",
      "oooo\n",
      "o o ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o oo ooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "−1 0 1 2o oo\n",
      "o o o o oo o o oo\n",
      "o ooo\n",
      "o o ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o o oo o ooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "o o o o o o o o oo\n",
      "o ooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o o o ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "−1 0 1 2 3o oo\n",
      "o o o o o o o o oo\n",
      "o ooo\n",
      "oo o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o oo o o ooo oo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "o o o o o o o o oo\n",
      "o ooo\n",
      "o o o oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "o o o o o ooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "0 20 60 100\n",
      "0 20 60 100pgg45\n",
      "FIGURE 1.1. Scatterplot matrix of the prostate cancer data. The ﬁrst row sho ws\n",
      "the response against each of the predictors in turn. Two of the pr edictors, sviand\n",
      "gleason , are categorical.\n",
      "For this problem not all errors are equal; we want to avoid ﬁltering out\n",
      "good email, while letting spam get through is not desirable but less serious\n",
      "in its consequences. We discuss a number of diﬀerent methods for tackling\n",
      "this learning problem in the book.\n",
      "Example 2: Prostate Cancer\n",
      "The data for this example, displayed in Figure 1.11, come from a study\n",
      "by Stamey et al. (1989) that examined the correlation between the level of\n",
      "1There was an error in these data in the ﬁrst edition of this boo k. Subject 32 had\n",
      "a value of 6.1 for lweight , which translates to a 449 gm prostate! The correct value is\n",
      "44.9 gm. We are grateful to Prof. Stephen W. Link for alerting us to this error.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4 1. Introduction\n",
      "FIGURE 1.2. Examples of handwritten digits from U.S. postal envelopes.\n",
      "prostate speciﬁc antigen (PSA) and a number of clinical measures, in 97\n",
      "men who were about to receive a radical prostatectomy.\n",
      "The goal is to predict the log of PSA ( lpsa) from a number of measure-\n",
      "ments including log cancer volume ( lcavol ), log prostate weight lweight ,\n",
      "age, log of benign prostatic hyperplasia amount lbph, seminal vesicle in-\n",
      "vasionsvi, log of capsular penetration lcp, Gleason score gleason , and\n",
      "percent of Gleason scores 4 or 5 pgg45. Figure 1.1 is a scatterplot matrix\n",
      "of the variables. Some correlations with lpsaare evident, but a good pre-\n",
      "dictive model is diﬃcult to construct by eye.\n",
      "This is a supervised learning problem, known as a regression problem ,\n",
      "because the outcome measurement is quantitative.\n",
      "Example 3: Handwritten Digit Recognition\n",
      "The data from this example come from the handwritten ZIP codes on\n",
      "envelopes from U.S. postal mail. Each image is a segment from a ﬁve digi t\n",
      "ZIP code, isolating a single digit. The images are 16 ×16 eight-bit grayscale\n",
      "maps, with each pixel ranging in intensity from 0 to 255. Some sample\n",
      "images are shown in Figure 1.2.\n",
      "The images have been normalized to have approximately the same size\n",
      "and orientation. The task is to predict, from the 16 ×16 matrix of pixel\n",
      "intensities, the identity of each image (0 ,1,... ,9) quickly and accurately. If\n",
      "it is accurate enough, the resulting algorithm would be used as part of an\n",
      "automatic sorting procedure for envelopes. This is a classiﬁcation problem\n",
      "for which the error rate needs to be kept very low to avoid misdirection of\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "1. Introduction 5\n",
      "mail. In order to achieve this low error rate, some objects can be assigned\n",
      "to a “don’t know” category, and sorted instead by hand.\n",
      "Example 4: DNA Expression Microarrays\n",
      "DNA stands for deoxyribonucleic acid, and is the basic material that makes\n",
      "up human chromosomes. DNA microarrays measure the expression of a\n",
      "gene in a cell by measuring the amount of mRNA (messenger ribonucleic\n",
      "acid) present for that gene. Microarrays are considered a breakthrough\n",
      "technology in biology, facilitating the quantitative study of thousands of\n",
      "genes simultaneously from a single sample of cells.\n",
      "Here is how a DNA microarray works. The nucleotide sequences for a few\n",
      "thousand genes are printed on a glass slide. A target sample and a reference\n",
      "sample are labeled with red and green dyes, and each are hybridized with\n",
      "the DNA on the slide. Through ﬂuoroscopy, the log (red/green) intensities\n",
      "of RNA hybridizing at each site is measured. The result is a few thousand\n",
      "numbers, typically ranging from say −6 to 6, measuring the expression level\n",
      "of each gene in the target relative to the reference sample. Positive values\n",
      "indicate higher expression in the target versus the reference, and vice versa\n",
      "for negative values.\n",
      "A gene expression dataset collects together the expression values from a\n",
      "series of DNA microarray experiments, with each column representing an\n",
      "experiment. There are therefore several thousand rows representing individ-\n",
      "ual genes, and tens of columns representing samples: in the particular ex-\n",
      "ample of Figure 1.3 there are 6830 genes (rows) and 64 samples (columns),\n",
      "although for clarity only a random sample of 100 rows are shown. The ﬁg-\n",
      "ure displays the data set as a heat map, ranging from green (negative) to\n",
      "red (positive). The samples are 64 cancer tumors from diﬀerent patients.\n",
      "The challenge here is to understand how the genes and samples are or-\n",
      "ganized. Typical questions include the following:\n",
      "(a) which samples are most similar to each other, in terms of their expres-\n",
      "sion proﬁles across genes?\n",
      "(b) which genes are most similar to each other, in terms of their expression\n",
      "proﬁles across samples?\n",
      "(c) do certain genes show very high (or low) expression for certain cancer\n",
      "samples?\n",
      "We could view this task as a regression problem, with two categorical\n",
      "predictor variables—genes and samples—with the response variable being\n",
      "the level of expression. However, it is probably more useful to view it as\n",
      "unsupervised learning problem. For example, for question (a) above, we\n",
      "think of the samples as points in 6830–dimensional space, which we want\n",
      "tocluster together in some way.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6 1. Introduction\n",
      "SID42354SID31984SID301902SIDW128368SID375990SID360097SIDW325120ESTsChr.10SIDW365099SID377133SID381508SIDW308182SID380265SIDW321925ESTsChr.15SIDW362471SIDW417270SIDW298052SID381079SIDW428642TUPLE1TUP1ERLUMENSIDW416621SID43609ESTsSID52979SIDW357197SIDW366311ESTsSMALLNUCSIDW486740ESTsSID297905SID485148SID284853ESTsChr.15SID200394SIDW322806ESTsChr.2SIDW257915SID46536SIDW488221ESTsChr.5SID280066SIDW376394ESTsChr.15SIDW321854WASWiskottHYPOTHETICALSIDW376776SIDW205716SID239012SIDW203464HLACLASSISIDW510534SIDW279664SIDW201620SID297117SID377419SID114241ESTsCh31SIDW376928SIDW310141SIDW298203PTPRCSID289414SID127504ESTsChr.3SID305167SID488017SIDW296310ESTsChr.6SID47116MITOCHONDRIAL60ChrSIDW376586HomosapiensSIDW487261SIDW470459SID167117SIDW31489SID375812DNAPOLYMERSID377451ESTsChr.1MYBPROTOSID471915ESTsSIDW469884HumanmRNASIDW377402ESTsSID207172RASGTPASESID325394H.sapiensmRNAGNALSID73161SIDW380102SIDW299104BREAST\n",
      "RENAL\n",
      "MELANOMAMELANOMA\n",
      "MCF7D-repro\n",
      "COLONCOLON\n",
      "K562B-repro\n",
      "COLON\n",
      "NSCLC\n",
      "LEUKEMIA\n",
      "RENAL\n",
      "MELANOMA\n",
      "BREAST\n",
      "CNSCNS\n",
      "RENAL\n",
      "MCF7A-repro\n",
      "NSCLC\n",
      "K562A-repro\n",
      "COLON\n",
      "CNS\n",
      "NSCLCNSCLC\n",
      "LEUKEMIA\n",
      "CNS\n",
      "OVARIAN\n",
      "BREAST\n",
      "LEUKEMIA\n",
      "MELANOMAMELANOMA\n",
      "OVARIANOVARIAN\n",
      "NSCLC\n",
      "RENAL\n",
      "BREAST\n",
      "MELANOMA\n",
      "OVARIANOVARIAN\n",
      "NSCLC\n",
      "RENAL\n",
      "BREAST\n",
      "MELANOMA\n",
      "LEUKEMIA\n",
      "COLON\n",
      "BREAST\n",
      "LEUKEMIA\n",
      "COLON\n",
      "CNS\n",
      "MELANOMA\n",
      "NSCLC\n",
      "PROSTATE\n",
      "NSCLC\n",
      "RENALRENAL\n",
      "NSCLC\n",
      "RENAL\n",
      "LEUKEMIA\n",
      "OVARIAN\n",
      "PROSTATE\n",
      "COLON\n",
      "BREAST\n",
      "RENAL\n",
      "UNKNOWNFIGURE 1.3. DNA microarray data: expression matrix of 6830genes (rows)\n",
      "and64samples (columns), for the human tumor data. Only a random sampl e\n",
      "of100rows are shown. The display is a heat map, ranging from bright gre en\n",
      "(negative, under expressed) to bright red (positive, over expre ssed). Missing values\n",
      "are gray. The rows and columns are displayed in a randomly chosen order.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "1. Introduction 7\n",
      "Who Should Read this Book\n",
      "This book is designed for researchers and students in a broad variety of\n",
      "ﬁelds: statistics, artiﬁcial intelligence, engineering, ﬁnance and others. We\n",
      "expect that the reader will have had at least one elementary course in\n",
      "statistics, covering basic topics including linear regression.\n",
      "We have not attempted to write a comprehensive catalog of learning\n",
      "methods, but rather to describe some of the most important techniques.\n",
      "Equally notable, we describe the underlying concepts and considerations\n",
      "by which a researcher can judge a learning method. We have tried to write\n",
      "this book in an intuitive fashion, emphasizing concepts rather than math-\n",
      "ematical details.\n",
      "As statisticians, our exposition will naturally reﬂect our backgrounds and\n",
      "areas of expertise. However in the past eight years we have been attending\n",
      "conferences in neural networks, data mining and machine learning, and our\n",
      "thinking has been heavily inﬂuenced by these exciting ﬁelds. This inﬂuence\n",
      "is evident in our current research, and in this book.\n",
      "How This Book is Organized\n",
      "Our view is that one must understand simple methods before trying to\n",
      "grasp more complex ones. Hence, after giving an overview of the supervis-\n",
      "ing learning problem in Chapter 2 , we discuss linear methods for regression\n",
      "and classiﬁcation in Chapters 3 and4. InChapter 5 we describe splines,\n",
      "wavelets and regularization/penalization methods for a single predictor,\n",
      "while Chapter 6 covers kernel methods and local regression. Both of these\n",
      "sets of methods are important building blocks for high-dimensional learn-\n",
      "ing techniques. Model assessment and selection is the topic of Chapter 7 ,\n",
      "covering the concepts of bias and variance, overﬁtting and methods such as\n",
      "cross-validation for choosing models. Chapter 8 discusses model inference\n",
      "and averaging, including an overview of maximum likelihood, Bayesian in-\n",
      "ference and the bootstrap, the EM algorithm, Gibbs sampling and bagging,\n",
      "A related procedure called boosting is the focus of Chapter 10 .\n",
      "InChapters 9–13 we describe a series of structured methods for su-\n",
      "pervised learning, with Chapters 9 and 11 covering regression and Chap-\n",
      "ters 12 and 13 focusing on classiﬁcation. Chapter 14 describes methods for\n",
      "unsupervised learning. Two recently proposed techniques, random forests\n",
      "and ensemble learning, are discussed in Chapters 15 and 16 . We describe\n",
      "undirected graphical models in Chapter 17 and ﬁnally we study high-\n",
      "dimensional problems in Chapter 18 .\n",
      "At the end of each chapter we discuss computational considerations im-\n",
      "portant for data mining applications, including how the computations scale\n",
      "with the number of observations and predictors. Each chapter ends with\n",
      "Bibliographic Notes giving background references for the material.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8 1. Introduction\n",
      "We recommend that Chapters 1–4 be ﬁrst read in sequence. Chapter 7\n",
      "should also be considered mandatory, as it covers central concepts that\n",
      "pertain to all learning methods. With this in mind, the rest of the book\n",
      "can be read sequentially, or sampled, depending on the reader’s interest.\n",
      "The symbol\n",
      " indicates a technically diﬃcult section, one that can\n",
      "be skipped without interrupting the ﬂow of the discussion.\n",
      "Book Website\n",
      "The website for this book is located at\n",
      "http://www-stat.stanford.edu/ElemStatLearn\n",
      "It contains a number of resources, including many of the datasets used in\n",
      "this book.\n",
      "Note for Instructors\n",
      "We have successively used the ﬁrst edition of this book as the basis for a\n",
      "two-quarter course, and with the additional materials in this second edition,\n",
      "it could even be used for a three-quarter sequence. Exercises are provided at\n",
      "the end of each chapter. It is important for students to have access to good\n",
      "software tools for these topics. We used the R and S-PLUS programming\n",
      "languages in our courses.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 9\n",
      "Printer: Opaque this\n",
      "2\n",
      "Overview of Supervised Learning\n",
      "2.1 Introduction\n",
      "The ﬁrst three examples described in Chapter 1 have several components\n",
      "in common. For each there is a set of variables that might be denoted as\n",
      "inputs , which are measured or preset. These have some inﬂuence on one or\n",
      "moreoutputs . For each example the goal is to use the inputs to predict the\n",
      "values of the outputs. This exercise is called supervised learning .\n",
      "We have used the more modern language of machine learning. In the\n",
      "statistical literature the inputs are often called the predictors , a term we\n",
      "will use interchangeably with inputs, and more classically the independent\n",
      "variables . In the pattern recognition literature the term features is preferred,\n",
      "which we use as well. The outputs are called the responses , or classically\n",
      "thedependent variables .\n",
      "2.2 Variable Types and Terminology\n",
      "The outputs vary in nature among the examples. In the glucose prediction\n",
      "example, the output is a quantitative measurement, where some measure-\n",
      "ments are bigger than others, and measurements close in value are close\n",
      "in nature. In the famous Iris discrimination example due to R. A. Fisher,\n",
      "the output is qualitative (species of Iris) and assumes values in a ﬁnite set\n",
      "G={Virginica ,Setosa andVersicolor }. In the handwritten digit example\n",
      "the output is one of 10 diﬀerent digit classes :G={0,1,... ,9}. In both of\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10 2. Overview of Supervised Learning\n",
      "these there is no explicit ordering in the classes, and in fact often descrip-\n",
      "tive labels rather than numbers are used to denote the classes. Qualitative\n",
      "variables are also referred to as categorical ordiscrete variables as well as\n",
      "factors .\n",
      "For both types of outputs it makes sense to think of using the inputs to\n",
      "predict the output. Given some speciﬁc atmospheric measurements today\n",
      "and yesterday, we want to predict the ozone level tomorrow. Given the\n",
      "grayscale values for the pixels of the digitized image of the handwritten\n",
      "digit, we want to predict its class label.\n",
      "This distinction in output type has led to a naming convention for the\n",
      "prediction tasks: regression when we predict quantitative outputs, and clas-\n",
      "siﬁcation when we predict qualitative outputs. We will see that these two\n",
      "tasks have a lot in common, and in particular both can be viewed as a task\n",
      "in function approximation.\n",
      "Inputs also vary in measurement type; we can have some of each of qual-\n",
      "itative and quantitative input variables. These have also led to distinctio ns\n",
      "in the types of methods that are used for prediction: some methods are\n",
      "deﬁned most naturally for quantitative inputs, some most naturally for\n",
      "qualitative and some for both.\n",
      "A third variable type is ordered categorical , such as small, medium and\n",
      "large, where there is an ordering between the values, but no metric notion\n",
      "is appropriate (the diﬀerence between medium and small need not be the\n",
      "same as that between large and medium). These are discussed further in\n",
      "Chapter 4.\n",
      "Qualitative variables are typically represented numerically by codes. The\n",
      "easiest case is when there are only two classes or categories, such as “suc-\n",
      "cess” or “failure,” “survived” or “died.” These are often represented by a\n",
      "single binary digit or bit as 0 or 1, or else by −1 and 1. For reasons that will\n",
      "become apparent, such numeric codes are sometimes referred to as targets.\n",
      "When there are more than two categories, several alternatives are available.\n",
      "The most useful and commonly used coding is via dummy variables . Here a\n",
      "K-level qualitative variable is represented by a vector of Kbinary variables\n",
      "or bits, only one of which is “on” at a time. Although more compact coding\n",
      "schemes are possible, dummy variables are symmetric in the levels of the\n",
      "factor.\n",
      "We will typically denote an input variable by the symbol X. IfXis\n",
      "a vector, its components can be accessed by subscripts Xj. Quantitative\n",
      "outputs will be denoted by Y, and qualitative outputs by G(for group).\n",
      "We use uppercase letters such as X,YorGwhen referring to the generic\n",
      "aspects of a variable. Observed values are written in lowercase; hence the\n",
      "ith observed value of Xis written as xi(where xiis again a scalar or\n",
      "vector). Matrices are represented by bold uppercase letters; for example, a\n",
      "set of Ninput p-vectors xi, i= 1,... ,N would be represented by the N×p\n",
      "matrix X. In general, vectors will not be bold, except when they have N\n",
      "components; this convention distinguishes a p-vector of inputs xifor the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.3 Least Squares and Nearest Neighbors 11\n",
      "ith observation from the N-vector xjconsisting of all the observations on\n",
      "variable Xj. Since all vectors are assumed to be column vectors, the ith\n",
      "row of XisxT\n",
      "i, the vector transpose of xi.\n",
      "For the moment we can loosely state the learning task as follows: given\n",
      "the value of an input vector X, make a good prediction of the output Y,\n",
      "denoted by ˆY(pronounced “y-hat”). If Ytakes values in IR then so should\n",
      "ˆY; likewise for categorical outputs, ˆGshould take values in the same set G\n",
      "associated with G.\n",
      "For a two-class G, one approach is to denote the binary coded target\n",
      "asY, and then treat it as a quantitative output. The predictions ˆYwill\n",
      "typically lie in [0 ,1], and we can assign to ˆGthe class label according to\n",
      "whether ˆ y >0.5. This approach generalizes to K-level qualitative outputs\n",
      "as well.\n",
      "We need data to construct prediction rules, often a lot of it. We thus\n",
      "suppose we have available a set of measurements ( xi,yi) or ( xi,gi), i=\n",
      "1,... ,N , known as the training data , with which to construct our prediction\n",
      "rule.\n",
      "2.3 Two Simple Approaches to Prediction: Least\n",
      "Squares and Nearest Neighbors\n",
      "In this section we develop two simple but powerful prediction methods: the\n",
      "linear model ﬁt by least squares and the k-nearest-neighbor prediction rule.\n",
      "The linear model makes huge assumptions about structure and yields stable\n",
      "but possibly inaccurate predictions. The method of k-nearest neighbors\n",
      "makes very mild structural assumptions: its predictions are often accurate\n",
      "but can be unstable.\n",
      "2.3.1 Linear Models and Least Squares\n",
      "The linear model has been a mainstay of statistics for the past 30 years\n",
      "and remains one of our most important tools. Given a vector of inputs\n",
      "XT= (X1,X2,... ,X p), we predict the output Yvia the model\n",
      "ˆY=ˆβ0+p∑\n",
      "j=1Xjˆβj. (2.1)\n",
      "The term ˆβ0is the intercept, also known as the biasin machine learning.\n",
      "Often it is convenient to include the constant variable 1 in X, include ˆβ0in\n",
      "the vector of coeﬃcients ˆβ, and then write the linear model in vector form\n",
      "as an inner product\n",
      "ˆY=XTˆβ, (2.2)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12 2. Overview of Supervised Learning\n",
      "where XTdenotes vector or matrix transpose ( Xbeing a column vector).\n",
      "Here we are modeling a single output, so ˆYis a scalar; in general ˆYcan be\n",
      "aK–vector, in which case βwould be a p×Kmatrix of coeﬃcients. In the\n",
      "(p+ 1)-dimensional input–output space, ( X,ˆY) represents a hyperplane.\n",
      "If the constant is included in X, then the hyperplane includes the origin\n",
      "and is a subspace; if not, it is an aﬃne set cutting the Y-axis at the point\n",
      "(0,ˆβ0). From now on we assume that the intercept is included in ˆβ.\n",
      "Viewed as a function over the p-dimensional input space, f(X) =XTβ\n",
      "is linear, and the gradient f′(X) =βis a vector in input space that points\n",
      "in the steepest uphill direction.\n",
      "How do we ﬁt the linear model to a set of training data? There are\n",
      "many diﬀerent methods, but by far the most popular is the method of\n",
      "least squares . In this approach, we pick the coeﬃcients βto minimize the\n",
      "residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−xT\n",
      "iβ)2. (2.3)\n",
      "RSS(β) is a quadratic function of the parameters, and hence its minimum\n",
      "always exists, but may not be unique. The solution is easiest to characterize\n",
      "in matrix notation. We can write\n",
      "RSS(β) = (y−Xβ)T(y−Xβ), (2.4)\n",
      "where Xis an N×pmatrix with each row an input vector, and yis an\n",
      "N-vector of the outputs in the training set. Diﬀerentiating w.r.t. βwe get\n",
      "thenormal equations\n",
      "XT(y−Xβ) = 0. (2.5)\n",
      "IfXTXis nonsingular, then the unique solution is given by\n",
      "ˆβ= (XTX)−1XTy, (2.6)\n",
      "and the ﬁtted value at the ith input xiis ˆyi= ˆy(xi) =xT\n",
      "iˆβ. At an arbi-\n",
      "trary input x0the prediction is ˆ y(x0) =xT\n",
      "0ˆβ. The entire ﬁtted surface is\n",
      "characterized by the pparameters ˆβ. Intuitively, it seems that we do not\n",
      "need a very large data set to ﬁt such a model.\n",
      "Let’s look at an example of the linear model in a classiﬁcation context.\n",
      "Figure 2.1 shows a scatterplot of training data on a pair of inputs X1and\n",
      "X2. The data are simulated, and for the present the simulation model is\n",
      "not important. The output class variable Ghas the values BLUEorORANGE ,\n",
      "and is represented as such in the scatterplot. There are 100 points in each\n",
      "of the two classes. The linear regression model was ﬁt to these data, with\n",
      "the response Ycoded as 0 for BLUEand 1 for ORANGE . The ﬁtted values ˆY\n",
      "are converted to a ﬁtted class variable ˆGaccording to the rule\n",
      "ˆG={\n",
      "ORANGE ifˆY >0.5,\n",
      "BLUE ifˆY≤0.5.(2.7)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.3 Least Squares and Nearest Neighbors 13\n",
      "Linear Regression of 0/1 Response\n",
      ".. . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "FIGURE 2.1. A classiﬁcation example in two dimensions. The classes are coded\n",
      "as a binary variable ( BLUE= 0,ORANGE = 1), and then ﬁt by linear regression.\n",
      "The line is the decision boundary deﬁned by xTˆβ= 0.5. The orange shaded region\n",
      "denotes that part of input space classiﬁed as ORANGE , while the blue region is\n",
      "classiﬁed as BLUE.\n",
      "The set of points in IR2classiﬁed as ORANGE corresponds to {x:xTˆβ >0.5},\n",
      "indicated in Figure 2.1, and the two predicted classes are separated by the\n",
      "decision boundary {x:xTˆβ= 0.5}, which is linear in this case. We see\n",
      "that for these data there are several misclassiﬁcations on both sides of the\n",
      "decision boundary. Perhaps our linear model is too rigid— or are such errors\n",
      "unavoidable? Remember that these are errors on the training data itself,\n",
      "and we have not said where the constructed data came from. Consider the\n",
      "two possible scenarios:\n",
      "Scenario 1: The training data in each class were generated from bivariate\n",
      "Gaussian distributions with uncorrelated components and diﬀerent\n",
      "means.\n",
      "Scenario 2: The training data in each class came from a mixture of 10 low-\n",
      "variance Gaussian distributions, with individual means themselves\n",
      "distributed as Gaussian.\n",
      "A mixture of Gaussians is best described in terms of the generative\n",
      "model. One ﬁrst generates a discrete variable that determines which of\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14 2. Overview of Supervised Learning\n",
      "the component Gaussians to use, and then generates an observation from\n",
      "the chosen density. In the case of one Gaussian per class, we will see in\n",
      "Chapter 4 that a linear decision boundary is the best one can do, and that\n",
      "our estimate is almost optimal. The region of overlap is inevitable, and\n",
      "future data to be predicted will be plagued by this overlap as well.\n",
      "In the case of mixtures of tightly clustered Gaussians the story is dif-\n",
      "ferent. A linear decision boundary is unlikely to be optimal, and in fact is\n",
      "not. The optimal decision boundary is nonlinear and disjoint, and as such\n",
      "will be much more diﬃcult to obtain.\n",
      "We now look at another classiﬁcation and regression procedure that is\n",
      "in some sense at the opposite end of the spectrum to the linear model, and\n",
      "far better suited to the second scenario.\n",
      "2.3.2 Nearest-Neighbor Methods\n",
      "Nearest-neighbor methods use those observations in the training set Tclos-\n",
      "est in input space to xto form ˆY. Speciﬁcally, the k-nearest neighbor ﬁt\n",
      "forˆYis deﬁned as follows:\n",
      "ˆY(x) =1\n",
      "k∑\n",
      "xi∈Nk(x)yi, (2.8)\n",
      "where Nk(x) is the neighborhood of xdeﬁned by the kclosest points xiin\n",
      "the training sample. Closeness implies a metric, which for the moment we\n",
      "assume is Euclidean distance. So, in words, we ﬁnd the kobservations with\n",
      "xiclosest to xin input space, and average their responses.\n",
      "In Figure 2.2 we use the same training data as in Figure 2.1, and use\n",
      "15-nearest-neighbor averaging of the binary coded response as the method\n",
      "of ﬁtting. Thus ˆYis the proportion of ORANGE ’s in the neighborhood, and\n",
      "so assigning class ORANGE toˆGifˆY >0.5 amounts to a majority vote in\n",
      "the neighborhood. The colored regions indicate all those points in input\n",
      "space classiﬁed as BLUEorORANGE by such a rule, in this case found by\n",
      "evaluating the procedure on a ﬁne grid in input space. We see that the\n",
      "decision boundaries that separate the BLUEfrom the ORANGE regions are far\n",
      "more irregular, and respond to local clusters where one class dominates.\n",
      "Figure 2.3 shows the results for 1-nearest-neighbor classiﬁcation: ˆYis\n",
      "assigned the value yℓof the closest point xℓtoxin the training data. In\n",
      "this case the regions of classiﬁcation can be computed relatively easily, and\n",
      "correspond to a Voronoi tessellation of the training data. Each point xi\n",
      "has an associated tile bounding the region for which it is the closest input\n",
      "point. For all points xin the tile, ˆG(x) =gi. The decision boundary is even\n",
      "more irregular than before.\n",
      "The method of k-nearest-neighbor averaging is deﬁned in exactly the\n",
      "same way for regression of a quantitative output Y, although k= 1 would\n",
      "be an unlikely choice.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.3 Least Squares and Nearest Neighbors 15\n",
      "15-Nearest Neighbor Classifier\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "... .. .. .. .. . .. . .. . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "FIGURE 2.2. The same classiﬁcation example in two dimensions as in Fig-\n",
      "ure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE = 1)and\n",
      "then ﬁt by 15-nearest-neighbor averaging as in (2.8). The predicted class i s hence\n",
      "chosen by majority vote amongst the 15-nearest neighbors.\n",
      "In Figure 2.2 we see that far fewer training observations are misclassiﬁed\n",
      "than in Figure 2.1. This should not give us too much comfort, though, since\n",
      "in Figure 2.3 noneof the training data are misclassiﬁed. A little thought\n",
      "suggests that for k-nearest-neighbor ﬁts, the error on the training data\n",
      "should be approximately an increasing function of k, and will always be 0\n",
      "fork= 1. An independent test set would give us a more satisfactory means\n",
      "for comparing the diﬀerent methods.\n",
      "It appears that k-nearest-neighbor ﬁts have a single parameter, the num-\n",
      "ber of neighbors k, compared to the pparameters in least-squares ﬁts. Al-\n",
      "though this is the case, we will see that the eﬀective number of parameters\n",
      "ofk-nearest neighbors is N/kand is generally bigger than p, and decreases\n",
      "with increasing k. To get an idea of why, note that if the neighborhoods\n",
      "were nonoverlapping, there would be N/kneighborhoods and we would ﬁt\n",
      "one parameter (a mean) in each neighborhood.\n",
      "It is also clear that we cannot use sum-of-squared errors on the training\n",
      "set as a criterion for picking k, since we would always pick k= 1! It would\n",
      "seem that k-nearest-neighbor methods would be more appropriate for the\n",
      "mixture Scenario 2 described above, while for Gaussian data the decision\n",
      "boundaries of k-nearest neighbors would be unnecessarily noisy.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "16 2. Overview of Supervised Learning\n",
      "1−Nearest Neighbor Classifier\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "FIGURE 2.3. The same classiﬁcation example in two dimensions as in Fig-\n",
      "ure 2.1. The classes are coded as a binary variable (BLUE= 0,ORANGE = 1), and\n",
      "then predicted by 1-nearest-neighbor classiﬁcation.\n",
      "2.3.3 From Least Squares to Nearest Neighbors\n",
      "The linear decision boundary from least squares is very smooth, and ap-\n",
      "parently stable to ﬁt. It does appear to rely heavily on the assumption\n",
      "that a linear decision boundary is appropriate. In language we will develop\n",
      "shortly, it has low variance and potentially high bias.\n",
      "On the other hand, the k-nearest-neighbor procedures do not appear to\n",
      "rely on any stringent assumptions about the underlying data, and can adapt\n",
      "to any situation. However, any particular subregion of the decision bound-\n",
      "ary depends on a handful of input points and their particular positions,\n",
      "and is thus wiggly and unstable—high variance and low bias.\n",
      "Each method has its own situations for which it works best; in particular\n",
      "linear regression is more appropriate for Scenario 1 above, while nearest\n",
      "neighbors are more suitable for Scenario 2. The time has come to expose\n",
      "the oracle! The data in fact were simulated from a model somewhere be-\n",
      "tween the two, but closer to Scenario 2. First we generated 10 means mk\n",
      "from a bivariate Gaussian distribution N((1,0)T,I) and labeled this class\n",
      "BLUE. Similarly, 10 more were drawn from N((0,1)T,I) and labeled class\n",
      "ORANGE . Then for each class we generated 100 observations as follows: for\n",
      "each observation, we picked an mkat random with probability 1 /10, and\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.3 Least Squares and Nearest Neighbors 17\n",
      "Degrees of Freedom − N/kTest Error\n",
      "0.10 0.15 0.20 0.25 0.30\n",
      "  2   3   5   8  12  18  29  67 200151 101  69  45  31  21  11   7   5   3   1\n",
      "Train\n",
      "Test\n",
      "Bayesk −  Number of Nearest Neighbors\n",
      "Linear\n",
      "FIGURE 2.4. Misclassiﬁcation curves for the simulation example used in Fi g-\n",
      "ures 2.1, 2.2 and 2.3. A single training sample of size 200was used, and a test\n",
      "sample of size 10,000. The orange curves are test and the blue are training er-\n",
      "ror for k-nearest-neighbor classiﬁcation. The results for linear regres sion are the\n",
      "bigger orange and blue squares at three degrees of freedom. The purple line is the\n",
      "optimal Bayes error rate.\n",
      "then generated a N(mk,I/5), thus leading to a mixture of Gaussian clus-\n",
      "ters for each class. Figure 2.4 shows the results of classifying 10,000 new\n",
      "observations generated from the model. We compare the results for least\n",
      "squares and those for k-nearest neighbors for a range of values of k.\n",
      "A large subset of the most popular techniques in use today are variants of\n",
      "these two simple procedures. In fact 1-nearest-neighbor, the simplest of all,\n",
      "captures a large percentage of the market for low-dimensional problems.\n",
      "The following list describes some ways in which these simple procedures\n",
      "have been enhanced:\n",
      "•Kernel methods use weights that decrease smoothly to zero with dis-\n",
      "tance from the target point, rather than the eﬀective 0 /1 weights used\n",
      "byk-nearest neighbors.\n",
      "•In high-dimensional spaces the distance kernels are modiﬁed to em-\n",
      "phasize some variable more than others.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "18 2. Overview of Supervised Learning\n",
      "•Local regression ﬁts linear models by locally weighted least squares,\n",
      "rather than ﬁtting constants locally.\n",
      "•Linear models ﬁt to a basis expansion of the original inputs allow\n",
      "arbitrarily complex models.\n",
      "•Projection pursuit and neural network models consist of sums of non-\n",
      "linearly transformed linear models.\n",
      "2.4 Statistical Decision Theory\n",
      "In this section we develop a small amount of theory that provides a frame-\n",
      "work for developing models such as those discussed informally so far. We\n",
      "ﬁrst consider the case of a quantitative output, and place ourselves in the\n",
      "world of random variables and probability spaces. Let X∈IRpdenote a\n",
      "real valued random input vector, and Y∈IR a real valued random out-\n",
      "put variable, with joint distribution Pr( X,Y). We seek a function f(X)\n",
      "for predicting Ygiven values of the input X. This theory requires a loss\n",
      "function L(Y,f(X)) for penalizing errors in prediction, and by far the most\n",
      "common and convenient is squared error loss :L(Y,f(X)) = ( Y−f(X))2.\n",
      "This leads us to a criterion for choosing f,\n",
      "EPE(f) = E( Y−f(X))2(2.9)\n",
      "=∫\n",
      "[y−f(x)]2Pr(dx,dy ), (2.10)\n",
      "the expected (squared) prediction error . By conditioning1onX, we can\n",
      "write EPE as\n",
      "EPE(f) = E XEY|X(\n",
      "[Y−f(X)]2|X)\n",
      "(2.11)\n",
      "and we see that it suﬃces to minimize EPE pointwise:\n",
      "f(x) = argmincEY|X(\n",
      "[Y−c]2|X=x)\n",
      ". (2.12)\n",
      "The solution is\n",
      "f(x) = E( Y|X=x), (2.13)\n",
      "the conditional expectation, also known as the regression function. Thus\n",
      "the best prediction of Yat any point X=xis the conditional mean, when\n",
      "best is measured by average squared error.\n",
      "The nearest-neighbor methods attempt to directly implement this recipe\n",
      "using the training data. At each point x, we might ask for the average of all\n",
      "1Conditioning here amounts to factoring the joint density Pr (X, Y) = Pr( Y|X)Pr(X)\n",
      "where Pr( Y|X) = Pr( Y, X)/Pr(X), and splitting up the bivariate integral accordingly.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.4 Statistical Decision Theory 19\n",
      "those yis with input xi=x. Since there is typically at most one observation\n",
      "at any point x, we settle for\n",
      "ˆf(x) = Ave( yi|xi∈Nk(x)), (2.14)\n",
      "where “Ave” denotes average, and Nk(x) is the neighborhood containing\n",
      "thekpoints in T closest to x. Two approximations are happening here:\n",
      "•expectation is approximated by averaging over sample data;\n",
      "•conditioning at a point is relaxed to conditioning on some region\n",
      "“close” to the target point.\n",
      "For large training sample size N, the points in the neighborhood are likely\n",
      "to be close to x, and as kgets large the average will get more stable.\n",
      "In fact, under mild regularity conditions on the joint probability distri-\n",
      "bution Pr( X,Y), one can show that as N,k→ ∞ such that k/N→0,\n",
      "ˆf(x)→E(Y|X=x). In light of this, why look further, since it seems\n",
      "we have a universal approximator? We often do not have very large sam-\n",
      "ples. If the linear or some more structured model is appropriate, then we\n",
      "can usually get a more stable estimate than k-nearest neighbors, although\n",
      "such knowledge has to be learned from the data as well. There are other\n",
      "problems though, sometimes disastrous. In Section 2.5 we see that as the\n",
      "dimension pgets large, so does the metric size of the k-nearest neighbor-\n",
      "hood. So settling for nearest neighborhood as a surrogate for conditioning\n",
      "will fail us miserably. The convergence above still holds, but the rateof\n",
      "convergence decreases as the dimension increases.\n",
      "How does linear regression ﬁt into this framework? The simplest explana-\n",
      "tion is that one assumes that the regression function f(x) is approximately\n",
      "linear in its arguments:\n",
      "f(x)≈xTβ. (2.15)\n",
      "This is a model-based approach—we specify a model for the regression func-\n",
      "tion. Plugging this linear model for f(x) into EPE (2.9) and diﬀerentiating\n",
      "we can solve for βtheoretically:\n",
      "β= [E(XXT)]−1E(XY). (2.16)\n",
      "Note we have notconditioned on X; rather we have used our knowledge\n",
      "of the functional relationship to poolover values of X. The least squares\n",
      "solution (2.6) amounts to replacing the expectation in (2.16) by averages\n",
      "over the training data.\n",
      "So both k-nearest neighbors and least squares end up approximating\n",
      "conditional expectations by averages. But they diﬀer dramatically in terms\n",
      "of model assumptions:\n",
      "•Least squares assumes f(x) is well approximated by a globally linear\n",
      "function.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "20 2. Overview of Supervised Learning\n",
      "•k-nearest neighbors assumes f(x) is well approximated by a locally\n",
      "constant function.\n",
      "Although the latter seems more palatable, we have already seen that we\n",
      "may pay a price for this ﬂexibility.\n",
      "Many of the more modern techniques described in this book are model\n",
      "based, although far more ﬂexible than the rigid linear model. For example,\n",
      "additive models assume that\n",
      "f(X) =p∑\n",
      "j=1fj(Xj). (2.17)\n",
      "This retains the additivity of the linear model, but each coordinate function\n",
      "fjis arbitrary. It turns out that the optimal estimate for the additive model\n",
      "uses techniques such as k-nearest neighbors to approximate univariate con-\n",
      "ditional expectations simultaneously for each of the coordinate functions.\n",
      "Thus the problems of estimating a conditional expectation in high dimen-\n",
      "sions are swept away in this case by imposing some (often unrealistic) model\n",
      "assumptions, in this case additivity.\n",
      "Are we happy with the criterion (2.11)? What happens if we replace the\n",
      "L2loss function with the L1:E|Y−f(X)|? The solution in this case is the\n",
      "conditional median,\n",
      "ˆf(x) = median( Y|X=x), (2.18)\n",
      "which is a diﬀerent measure of location, and its estimates are more robust\n",
      "than those for the conditional mean. L1criteria have discontinuities in\n",
      "their derivatives, which have hindered their widespread use. Other more\n",
      "resistant loss functions will be mentioned in later chapters, but squared\n",
      "error is analytically convenient and the most popular.\n",
      "What do we do when the output is a categorical variable G? The same\n",
      "paradigm works here, except we need a diﬀerent loss function for penalizing\n",
      "prediction errors. An estimate ˆGwill assume values in G, the set of possible\n",
      "classes. Our loss function can be represented by a K×Kmatrix L, where\n",
      "K= card( G).Lwill be zero on the diagonal and nonnegative elsewhere,\n",
      "where L(k,ℓ) is the price paid for classifying an observation belonging to\n",
      "classGkasGℓ. Most often we use the zero–one loss function, where all\n",
      "misclassiﬁcations are charged a single unit. The expected prediction error\n",
      "is\n",
      "EPE = E[ L(G,ˆG(X))], (2.19)\n",
      "where again the expectation is taken with respect to the joint distribution\n",
      "Pr(G,X). Again we condition, and can write EPE as\n",
      "EPE = E XK∑\n",
      "k=1L[Gk,ˆG(X)]Pr(Gk|X) (2.20)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.4 Statistical Decision Theory 21\n",
      "Bayes Optimal Classifier\n",
      "... .. . . .. . . . .. . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "FIGURE 2.5. The optimal Bayes decision boundary for the simulation exampl e\n",
      "of Figures 2.1, 2.2 and 2.3. Since the generating density is known for each class,\n",
      "this boundary can be calculated exactly (Exercise 2.2).\n",
      "and again it suﬃces to minimize EPE pointwise:\n",
      "ˆG(x) = argming∈GK∑\n",
      "k=1L(Gk,g)Pr(Gk|X=x). (2.21)\n",
      "With the 0–1 loss function this simpliﬁes to\n",
      "ˆG(x) = argming∈G[1−Pr(g|X=x)] (2.22)\n",
      "or simply\n",
      "ˆG(X) =Gkif Pr(Gk|X=x) = max\n",
      "g∈GPr(g|X=x). (2.23)\n",
      "This reasonable solution is known as the Bayes classiﬁer , and says that\n",
      "we classify to the most probable class, using the conditional (discrete) dis-\n",
      "tribution Pr( G|X). Figure 2.5 shows the Bayes-optimal decision boundary\n",
      "for our simulation example. The error rate of the Bayes classiﬁer is called\n",
      "theBayes rate .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "22 2. Overview of Supervised Learning\n",
      "Again we see that the k-nearest neighbor classiﬁer directly approximates\n",
      "this solution—a majority vote in a nearest neighborhood amounts to ex-\n",
      "actly this, except that conditional probability at a point is relaxed to con-\n",
      "ditional probability within a neighborhood of a point, and probabilities ar e\n",
      "estimated by training-sample proportions.\n",
      "Suppose for a two-class problem we had taken the dummy-variable ap-\n",
      "proach and coded Gvia a binary Y, followed by squared error loss estima-\n",
      "tion. Then ˆf(X) = E( Y|X) = Pr( G=G1|X) ifG1corresponded to Y= 1.\n",
      "Likewise for a K-class problem, E( Yk|X) = Pr( G=Gk|X). This shows\n",
      "that our dummy-variable regression procedure, followed by classiﬁcation to\n",
      "the largest ﬁtted value, is another way of representing the Bayes classiﬁer.\n",
      "Although this theory is exact, in practice problems can occur, depending\n",
      "on the regression model used. For example, when linear regression is used,\n",
      "ˆf(X) need not be positive, and we might be suspicious about using it as\n",
      "an estimate of a probability. We will discuss a variety of approaches to\n",
      "modeling Pr( G|X) in Chapter 4.\n",
      "2.5 Local Methods in High Dimensions\n",
      "We have examined two learning techniques for prediction so far: the stable\n",
      "but biased linear model and the less stable but apparently less biased class\n",
      "ofk-nearest-neighbor estimates. It would seem that with a reasonably large\n",
      "set of training data, we could always approximate the theoretically optimal\n",
      "conditional expectation by k-nearest-neighbor averaging, since we should\n",
      "be able to ﬁnd a fairly large neighborhood of observations close to any x\n",
      "and average them. This approach and our intuition breaks down in high\n",
      "dimensions, and the phenomenon is commonly referred to as the curse\n",
      "of dimensionality (Bellman, 1961). There are many manifestations of this\n",
      "problem, and we will examine a few here.\n",
      "Consider the nearest-neighbor procedure for inputs uniformly distributed\n",
      "in ap-dimensional unit hypercube, as in Figure 2.6. Suppose we send out a\n",
      "hypercubical neighborhood about a target point to capture a fraction rof\n",
      "the observations. Since this corresponds to a fraction rof the unit volume,\n",
      "the expected edge length will be ep(r) =r1/p. In ten dimensions e10(0.01) =\n",
      "0.63 and e10(0.1) = 0 .80, while the entire range for each input is only 1 .0.\n",
      "So to capture 1% or 10% of the data to form a local average, we must cover\n",
      "63% or 80% of the range of each input variable. Such neighborhoods are no\n",
      "longer “local.” Reducing rdramatically does not help much either, since\n",
      "the fewer observations we average, the higher is the variance of our ﬁt.\n",
      "Another consequence of the sparse sampling in high dimensions is that\n",
      "all sample points are close to an edge of the sample. Consider Ndata points\n",
      "uniformly distributed in a p-dimensional unit ball centered at the origin.\n",
      "Suppose we consider a nearest-neighbor estimate at the origin. The median\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.5 Local Methods in High Dimensions 23\n",
      "1\n",
      "10Unit Cube\n",
      "Fraction of VolumeDistance\n",
      "0.0 0.2 0.4 0.60.0 0.2 0.4 0.6 0.8 1.0p=1p=2p=3p=10\n",
      "Neighborhood\n",
      "FIGURE 2.6. The curse of dimensionality is well illustrated by a subcubica l\n",
      "neighborhood for uniform data in a unit cube. The ﬁgure on the righ t shows the\n",
      "side-length of the subcube needed to capture a fraction rof the volume of the data,\n",
      "for diﬀerent dimensions p. In ten dimensions we need to cover 80%of the range\n",
      "of each coordinate to capture 10%of the data.\n",
      "distance from the origin to the closest data point is given by the expression\n",
      "d(p,N) =(\n",
      "1−1\n",
      "21/N)1/p\n",
      "(2.24)\n",
      "(Exercise 2.3). A more complicated expression exists for the mean distance\n",
      "to the closest point. For N= 500, p= 10 , d(p,N)≈0.52, more than\n",
      "halfway to the boundary. Hence most data points are closer to the boundary\n",
      "of the sample space than to any other data point. The reason that this\n",
      "presents a problem is that prediction is much more diﬃcult near the edges\n",
      "of the training sample. One must extrapolate from neighboring sample\n",
      "points rather than interpolate between them.\n",
      "Another manifestation of the curse is that the sampling density is pro-\n",
      "portional to N1/p, where pis the dimension of the input space and Nis the\n",
      "sample size. Thus, if N1= 100 represents a dense sample for a single input\n",
      "problem, then N10= 10010is the sample size required for the same sam-\n",
      "pling density with 10 inputs. Thus in high dimensions all feasible training\n",
      "samples sparsely populate the input space.\n",
      "Let us construct another uniform example. Suppose we have 1000 train-\n",
      "ing examples xigenerated uniformly on [ −1,1]p. Assume that the true\n",
      "relationship between XandYis\n",
      "Y=f(X) =e−8||X||2,\n",
      "without any measurement error. We use the 1-nearest-neighbor rule to\n",
      "predict y0at the test-point x0= 0. Denote the training set by T. We can\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "24 2. Overview of Supervised Learning\n",
      "compute the expected prediction error at x0for our procedure, averaging\n",
      "over all such samples of size 1000. Since the problem is deterministic, this\n",
      "is the mean squared error (MSE) for estimating f(0):\n",
      "MSE( x0) = E T[f(x0)−ˆy0]2\n",
      "= E T[ˆy0−ET(ˆy0)]2+ [ET(ˆy0)−f(x0)]2\n",
      "= Var T(ˆy0) + Bias2(ˆy0). (2.25)\n",
      "Figure 2.7 illustrates the setup. We have broken down the MSE into two\n",
      "components that will become familiar as we proceed: variance and squared\n",
      "bias. Such a decomposition is always possible and often useful, and is known\n",
      "as the bias–variance decomposition . Unless the nearest neighbor is at 0,\n",
      "ˆy0will be smaller than f(0) in this example, and so the average estimate\n",
      "will be biased downward. The variance is due to the sampling variance of\n",
      "the 1-nearest neighbor. In low dimensions and with N= 1000, the nearest\n",
      "neighbor is very close to 0, and so both the bias and variance are small. As\n",
      "the dimension increases, the nearest neighbor tends to stray further from\n",
      "the target point, and both bias and variance are incurred. By p= 10, for\n",
      "more than 99% of the samples the nearest neighbor is a distance greater\n",
      "than 0 .5 from the origin. Thus as pincreases, the estimate tends to be 0\n",
      "more often than not, and hence the MSE levels oﬀ at 1 .0, as does the bias,\n",
      "and the variance starts dropping (an artifact of this example).\n",
      "Although this is a highly contrived example, similar phenomena occur\n",
      "more generally. The complexity of functions of many variables can grow\n",
      "exponentially with the dimension, and if we wish to be able to estimate\n",
      "such functions with the same accuracy as function in low dimensions, then\n",
      "we need the size of our training set to grow exponentially as well. In this\n",
      "example, the function is a complex interaction of all pvariables involved.\n",
      "The dependence of the bias term on distance depends on the truth, and\n",
      "it need not always dominate with 1-nearest neighbor. For example, if the\n",
      "function always involves only a few dimensions as in Figure 2.8, then the\n",
      "variance can dominate instead.\n",
      "Suppose, on the other hand, that we know that the relationship between\n",
      "YandXis linear,\n",
      "Y=XTβ+ε, (2.26)\n",
      "where ε∼N(0,σ2) and we ﬁt the model by least squares to the train-\n",
      "ing data. For an arbitrary test point x0, we have ˆ y0=xT\n",
      "0ˆβ, which can\n",
      "be written as ˆ y0=xT\n",
      "0β+∑N\n",
      "i=1ℓi(x0)εi, where ℓi(x0) is the ith element\n",
      "ofX(XTX)−1x0. Since under this model the least squares estimates are\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.5 Local Methods in High Dimensions 25\n",
      "Xf(X)\n",
      "-1.0 -0.5 0.0 0.5 1.00.0 0.2 0.4 0.6 0.8 1.0•1-NN in One Dimension\n",
      "X1X2\n",
      "-1.0 -0.5 0.0 0.5 1.0-1.0 -0.5 0.0 0.5 1.0•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••1-NN in One vs. Two Dimensions\n",
      "DimensionAverage Distance to Nearest Neighbor\n",
      "2 4 6 8 100.0 0.2 0.4 0.6 0.8••••••••••Distance to 1-NN vs. Dimension\n",
      "DimensionMse\n",
      "2 4 6 8 100.0 0.2 0.4 0.6 0.8 1.0• •••••••••\n",
      "• •••••••• • • •••••••••MSE vs. Dimension\n",
      "•MSE\n",
      "•Variance\n",
      "•Sq. Bias\n",
      "FIGURE 2.7. A simulation example, demonstrating the curse of dimensional-\n",
      "ity and its eﬀect on MSE, bias and variance. The input features are u niformly\n",
      "distributed in [−1,1]pforp= 1, . . . ,10The top left panel shows the target func-\n",
      "tion (no noise) in I R:f(X) =e−8||X||2, and demonstrates the error that 1-nearest\n",
      "neighbor makes in estimating f(0). The training point is indicated by the blue tick\n",
      "mark. The top right panel illustrates why the radius of the 1-nearest neighborhood\n",
      "increases with dimension p. The lower left panel shows the average radius of the\n",
      "1-nearest neighborhoods. The lower-right panel shows the MSE, sq uared bias and\n",
      "variance curves as a function of dimension p.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "26 2. Overview of Supervised Learning\n",
      "Xf(X)\n",
      "-1.0 -0.5 0.0 0.5 1.00 1 2 3 4•1-NN in One Dimension\n",
      "DimensionMSE\n",
      "2 4 6 8 100.0 0.05 0.10 0.15 0.20 0.25••••••••••\n",
      "••••••••••\n",
      "• •• • ••••••MSE  vs. Dimension\n",
      "•MSE\n",
      "•Variance\n",
      "•Sq. Bias\n",
      "FIGURE 2.8. A simulation example with the same setup as in Figure 2.7. Here\n",
      "the function is constant in all but one dimension: F(X) =1\n",
      "2(X1+ 1)3. The\n",
      "variance dominates.\n",
      "unbiased, we ﬁnd that\n",
      "EPE(x0) = E y0|x0ET(y0−ˆy0)2\n",
      "= Var( y0|x0) + E T[ˆy0−ETˆy0]2+ [ETˆy0−xT\n",
      "0β]2\n",
      "= Var( y0|x0) + Var T(ˆy0) + Bias2(ˆy0)\n",
      "=σ2+ ETxT\n",
      "0(XTX)−1x0σ2+ 02. (2.27)\n",
      "Here we have incurred an additional variance σ2in the prediction error,\n",
      "since our target is not deterministic. There is no bias, and the variance\n",
      "depends on x0. IfNis large and Twere selected at random, and assuming\n",
      "E(X) = 0, then XTX→NCov(X) and\n",
      "Ex0EPE(x0)∼Ex0xT\n",
      "0Cov(X)−1x0σ2/N+σ2\n",
      "= trace[Cov( X)−1Cov(x0)]σ2/N+σ2\n",
      "=σ2(p/N) +σ2. (2.28)\n",
      "Here we see that the expected EPE increases linearly as a function of p,\n",
      "with slope σ2/N. IfNis large and/or σ2is small, this growth in vari-\n",
      "ance is negligible (0 in the deterministic case). By imposing some heavy\n",
      "restrictions on the class of models being ﬁtted, we have avoided the curse\n",
      "of dimensionality. Some of the technical details in (2.27) and (2.28) are\n",
      "derived in Exercise 2.5.\n",
      "Figure 2.9 compares 1-nearest neighbor vs. least squares in two situa-\n",
      "tions, both of which have the form Y=f(X) +ε,Xuniform as before,\n",
      "andε∼N(0,1). The sample size is N= 500. For the orange curve, f(x)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.5 Local Methods in High Dimensions 27\n",
      "DimensionEPE Ratio\n",
      "2 4 6 8 101.6 1.7 1.8 1.9 2.0 2.1••••••••••••••••••••Expected Prediction Error of 1NN vs. OLS\n",
      "•Linear\n",
      "•Cubic\n",
      "FIGURE 2.9. The curves show the expected prediction error (at x0= 0) for\n",
      "1-nearest neighbor relative to least squares for the model Y=f(X) +ε. For the\n",
      "orange curve, f(x) =x1, while for the blue curve f(x) =1\n",
      "2(x1+ 1)3.\n",
      "is linear in the ﬁrst coordinate, for the blue curve, cubic as in Figure 2.8.\n",
      "Shown is the relative EPE of 1-nearest neighbor to least squares, which\n",
      "appears to start at around 2 for the linear case. Least squares is unbiased\n",
      "in this case, and as discussed above the EPE is slightly above σ2= 1.\n",
      "The EPE for 1-nearest neighbor is always above 2, since the variance of\n",
      "ˆf(x0) in this case is at least σ2, and the ratio increases with dimension as\n",
      "the nearest neighbor strays from the target point. For the cubic case, least\n",
      "squares is biased, which moderates the ratio. Clearly we could manufacture\n",
      "examples where the bias of least squares would dominate the variance, and\n",
      "the 1-nearest neighbor would come out the winner.\n",
      "By relying on rigid assumptions, the linear model has no bias at all and\n",
      "negligible variance, while the error in 1-nearest neighbor is substantially\n",
      "larger. However, if the assumptions are wrong, all bets are oﬀ and the\n",
      "1-nearest neighbor may dominate. We will see that there is a whole spec-\n",
      "trum of models between the rigid linear models and the extremely ﬂexible\n",
      "1-nearest-neighbor models, each with their own assumptions and biases,\n",
      "which have been proposed speciﬁcally to avoid the exponential growth in\n",
      "complexity of functions in high dimensions by drawing heavily on these\n",
      "assumptions.\n",
      "Before we delve more deeply, let us elaborate a bit on the concept of\n",
      "statistical models and see how they ﬁt into the prediction framework.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "28 2. Overview of Supervised Learning\n",
      "2.6 Statistical Models, Supervised Learning and\n",
      "Function Approximation\n",
      "Our goal is to ﬁnd a useful approximation ˆf(x) to the function f(x) that\n",
      "underlies the predictive relationship between the inputs and outputs. In the\n",
      "theoretical setting of Section 2.4, we saw that squared error loss lead us\n",
      "to the regression function f(x) = E( Y|X=x) for a quantitative response.\n",
      "The class of nearest-neighbor methods can be viewed as direct estimates\n",
      "of this conditional expectation, but we have seen that they can fail in at\n",
      "least two ways:\n",
      "•if the dimension of the input space is high, the nearest neighbors need\n",
      "not be close to the target point, and can result in large errors;\n",
      "•if special structure is known to exist, this can be used to reduce both\n",
      "the bias and the variance of the estimates.\n",
      "We anticipate using other classes of models for f(x), in many cases specif-\n",
      "ically designed to overcome the dimensionality problems, and here we dis-\n",
      "cuss a framework for incorporating them into the prediction problem.\n",
      "2.6.1 A Statistical Model for the Joint Distribution Pr(X,Y)\n",
      "Suppose in fact that our data arose from a statistical model\n",
      "Y=f(X) +ε, (2.29)\n",
      "where the random error εhas E( ε) = 0 and is independent of X. Note that\n",
      "for this model, f(x) = E( Y|X=x), and in fact the conditional distribution\n",
      "Pr(Y|X) depends on Xonlythrough the conditional mean f(x).\n",
      "The additive error model is a useful approximation to the truth. For\n",
      "most systems the input–output pairs ( X,Y) will not have a deterministic\n",
      "relationship Y=f(X). Generally there will be other unmeasured variables\n",
      "that also contribute to Y, including measurement error. The additive model\n",
      "assumes that we can capture all these departures from a deterministic re-\n",
      "lationship via the error ε.\n",
      "For some problems a deterministic relationship does hold. Many of the\n",
      "classiﬁcation problems studied in machine learning are of this form, where\n",
      "the response surface can be thought of as a colored map deﬁned in IRp.\n",
      "The training data consist of colored examples from the map {xi,gi}, and\n",
      "the goal is to be able to color any point. Here the function is deterministic,\n",
      "and the randomness enters through the xlocation of the training points.\n",
      "For the moment we will not pursue such problems, but will see that they\n",
      "can be handled by techniques appropriate for the error-based models.\n",
      "The assumption in (2.29) that the errors are independent and identically\n",
      "distributed is not strictly necessary, but seems to be at the back of our mind\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.6 Statistical Models, Supervised Learning and Function Approxi mation 29\n",
      "when we average squared errors uniformly in our EPE criterion. With such\n",
      "a model it becomes natural to use least squares as a data criterion for\n",
      "model estimation as in (2.1). Simple modiﬁcations can be made to avoid\n",
      "the independence assumption; for example, we can have Var( Y|X=x) =\n",
      "σ(x), and now both the mean and variance depend on X. In general the\n",
      "conditional distribution Pr( Y|X) can depend on Xin complicated ways,\n",
      "but the additive error model precludes these.\n",
      "So far we have concentrated on the quantitative response. Additive error\n",
      "models are typically not used for qualitative outputs G; in this case the tar-\n",
      "get function p(X)isthe conditional density Pr( G|X), and this is modeled\n",
      "directly. For example, for two-class data, it is often reasonable to assume\n",
      "that the data arise from independent binary trials, with the probability of\n",
      "one particular outcome being p(X), and the other 1 −p(X). Thus if Yis\n",
      "the 0–1 coded version of G, then E( Y|X=x) =p(x), but the variance\n",
      "depends on xas well: Var( Y|X=x) =p(x)[1−p(x)].\n",
      "2.6.2 Supervised Learning\n",
      "Before we launch into more statistically oriented jargon, we present the\n",
      "function-ﬁtting paradigm from a machine learning point of view. Suppose\n",
      "for simplicity that the errors are additive and that the model Y=f(X)+ε\n",
      "is a reasonable assumption. Supervised learning attempts to learn fby\n",
      "example through a teacher . One observes the system under study, both\n",
      "the inputs and outputs, and assembles a training set of observations T=\n",
      "(xi,yi), i= 1,... ,N . The observed input values to the system xiare also\n",
      "fed into an artiﬁcial system, known as a learning algorithm (usually a com-\n",
      "puter program), which also produces outputs ˆf(xi) in response to the in-\n",
      "puts. The learning algorithm has the property that it can modify its in-\n",
      "put/output relationship ˆfin response to diﬀerences yi−ˆf(xi) between the\n",
      "original and generated outputs. This process is known as learning by exam-\n",
      "ple. Upon completion of the learning process the hope is that the artiﬁcial\n",
      "and real outputs will be close enough to be useful for all sets of inputs likely\n",
      "to be encountered in practice.\n",
      "2.6.3 Function Approximation\n",
      "The learning paradigm of the previous section has been the motivation\n",
      "for research into the supervised learning problem in the ﬁelds of machine\n",
      "learning (with analogies to human reasoning) and neural networks (with\n",
      "biological analogies to the brain). The approach taken in applied mathe-\n",
      "matics and statistics has been from the perspective of function approxima-\n",
      "tion and estimation. Here the data pairs {xi,yi}are viewed as points in a\n",
      "(p+ 1)-dimensional Euclidean space. The function f(x) has domain equal\n",
      "to the p-dimensional input subspace, and is related to the data via a model\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "30 2. Overview of Supervised Learning\n",
      "such as yi=f(xi) +εi. For convenience in this chapter we will assume the\n",
      "domain is IRp, ap-dimensional Euclidean space, although in general the\n",
      "inputs can be of mixed type. The goal is to obtain a useful approximation\n",
      "tof(x) for all xin some region of IRp, given the representations in T.\n",
      "Although somewhat less glamorous than the learning paradigm, treating\n",
      "supervised learning as a problem in function approximation encourages the\n",
      "geometrical concepts of Euclidean spaces and mathematical concepts of\n",
      "probabilistic inference to be applied to the problem. This is the approach\n",
      "taken in this book.\n",
      "Many of the approximations we will encounter have associated a set of\n",
      "parameters θthat can be modiﬁed to suit the data at hand. For example,\n",
      "the linear model f(x) =xTβhasθ=β. Another class of useful approxi-\n",
      "mators can be expressed as linear basis expansions\n",
      "fθ(x) =K∑\n",
      "k=1hk(x)θk, (2.30)\n",
      "where the hkare a suitable set of functions or transformations of the input\n",
      "vector x. Traditional examples are polynomial and trigonometric expan-\n",
      "sions, where for example hkmight be x2\n",
      "1,x1x2\n",
      "2, cos(x1) and so on. We\n",
      "also encounter nonlinear expansions, such as the sigmoid transformation\n",
      "common to neural network models,\n",
      "hk(x) =1\n",
      "1 + exp( −xTβk). (2.31)\n",
      "We can use least squares to estimate the parameters θinfθas we did\n",
      "for the linear model, by minimizing the residual sum-of-squares\n",
      "RSS(θ) =N∑\n",
      "i=1(yi−fθ(xi))2(2.32)\n",
      "as a function of θ. This seems a reasonable criterion for an additive error\n",
      "model. In terms of function approximation, we imagine our parameterized\n",
      "function as a surface in p+ 1 space, and what we observe are noisy re-\n",
      "alizations from it. This is easy to visualize when p= 2 and the vertical\n",
      "coordinate is the output y, as in Figure 2.10. The noise is in the output\n",
      "coordinate, so we ﬁnd the set of parameters such that the ﬁtted surface\n",
      "gets as close to the observed points as possible, where close is measured by\n",
      "the sum of squared vertical errors in RSS( θ).\n",
      "For the linear model we get a simple closed form solution to the mini-\n",
      "mization problem. This is also true for the basis function methods, if the\n",
      "basis functions themselves do not have any hidden parameters. Otherwise\n",
      "the solution requires either iterative methods or numerical optimization.\n",
      "While least squares is generally very convenient, it is not the only crite-\n",
      "rion used and in some cases would not make much sense. A more general\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.6 Statistical Models, Supervised Learning and Function Approxi mation 31\n",
      "•••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "•• ••••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "•\n",
      "••\n",
      "FIGURE 2.10. Least squares ﬁtting of a function of two inputs. The parameters\n",
      "offθ(x)are chosen so as to minimize the sum-of-squared vertical erro rs.\n",
      "principle for estimation is maximum likelihood estimation . Suppose we have\n",
      "a random sample yi, i= 1,... ,N from a density Pr θ(y) indexed by some\n",
      "parameters θ. The log-probability of the observed sample is\n",
      "L(θ) =N∑\n",
      "i=1log Pr θ(yi). (2.33)\n",
      "The principle of maximum likelihood assumes that the most reasonable\n",
      "values for θare those for which the probability of the observed sample is\n",
      "largest. Least squares for the additive error model Y=fθ(X) +ε, with\n",
      "ε∼N(0,σ2), is equivalent to maximum likelihood using the conditional\n",
      "likelihood\n",
      "Pr(Y|X,θ) =N(fθ(X),σ2). (2.34)\n",
      "So although the additional assumption of normality seems more restrictive,\n",
      "the results are the same. The log-likelihood of the data is\n",
      "L(θ) =−N\n",
      "2log(2π)−Nlogσ−1\n",
      "2σ2N∑\n",
      "i=1(yi−fθ(xi))2, (2.35)\n",
      "and the only term involving θis the last, which is RSS( θ) up to a scalar\n",
      "negative multiplier.\n",
      "A more interesting example is the multinomial likelihood for the regres-\n",
      "sion function Pr( G|X) for a qualitative output G. Suppose we have a model\n",
      "Pr(G=Gk|X=x) =pk,θ(x), k= 1,... ,K for the conditional probabil-\n",
      "ity of each class given X, indexed by the parameter vector θ. Then the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "32 2. Overview of Supervised Learning\n",
      "log-likelihood (also referred to as the cross-entropy) is\n",
      "L(θ) =N∑\n",
      "i=1logpgi,θ(xi), (2.36)\n",
      "and when maximized it delivers values of θthat best conform with the data\n",
      "in this likelihood sense.\n",
      "2.7 Structured Regression Models\n",
      "We have seen that although nearest-neighbor and other local methods focus\n",
      "directly on estimating the function at a point, they face problems in high\n",
      "dimensions. They may also be inappropriate even in low dimensions in\n",
      "cases where more structured approaches can make more eﬃcient use of the\n",
      "data. This section introduces classes of such structured approaches. Before\n",
      "we proceed, though, we discuss further the need for such classes.\n",
      "2.7.1 Diﬃculty of the Problem\n",
      "Consider the RSS criterion for an arbitrary function f,\n",
      "RSS(f) =N∑\n",
      "i=1(yi−f(xi))2. (2.37)\n",
      "Minimizing (2.37) leads to inﬁnitely many solutions: any function ˆfpassing\n",
      "through the training points ( xi,yi) is a solution. Any particular solution\n",
      "chosen might be a poor predictor at test points diﬀerent from the training\n",
      "points. If there are multiple observation pairs xi,yiℓ, ℓ= 1,... ,N iat each\n",
      "value of xi, the risk is limited. In this case, the solutions pass through\n",
      "the average values of the yiℓat each xi; see Exercise 2.6. The situation is\n",
      "similar to the one we have already visited in Section 2.4; indeed, (2.37) is\n",
      "the ﬁnite sample version of (2.11) on page 18. If the sample size Nwere\n",
      "suﬃciently large such that repeats were guaranteed and densely arranged,\n",
      "it would seem that these solutions might all tend to the limiting conditional\n",
      "expectation.\n",
      "In order to obtain useful results for ﬁnite N, we must restrict the eligible\n",
      "solutions to (2.37) to a smaller set of functions. How to decide on the\n",
      "nature of the restrictions is based on considerations outside of the data.\n",
      "These restrictions are sometimes encoded via the parametric representation\n",
      "offθ, or may be built into the learning method itself, either implicitly or\n",
      "explicitly. These restricted classes of solutions are the major topic of this\n",
      "book. One thing should be clear, though. Any restrictions imposed on f\n",
      "that lead to a unique solution to (2.37) do not really remove the ambiguity\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.8 Classes of Restricted Estimators 33\n",
      "caused by the multiplicity of solutions. There are inﬁnitely many possible\n",
      "restrictions, each leading to a unique solution, so the ambiguity has simply\n",
      "been transferred to the choice of constraint.\n",
      "In general the constraints imposed by most learning methods can be\n",
      "described as complexity restrictions of one kind or another. This usually\n",
      "means some kind of regular behavior in small neighborhoods of the input\n",
      "space. That is, for all input points xsuﬃciently close to each other in\n",
      "some metric, ˆfexhibits some special structure such as nearly constant,\n",
      "linear or low-order polynomial behavior. The estimator is then obtained by\n",
      "averaging or polynomial ﬁtting in that neighborhood.\n",
      "The strength of the constraint is dictated by the neighborhood size. The\n",
      "larger the size of the neighborhood, the stronger the constraint, and the\n",
      "more sensitive the solution is to the particular choice of constraint. For\n",
      "example, local constant ﬁts in inﬁnitesimally small neighborhoods is no\n",
      "constraint at all; local linear ﬁts in very large neighborhoods is almost a\n",
      "globally linear model, and is very restrictive.\n",
      "The nature of the constraint depends on the metric used. Some methods,\n",
      "such as kernel and local regression and tree-based methods, directly specify\n",
      "the metric and size of the neighborhood. The nearest-neighbor methods\n",
      "discussed so far are based on the assumption that locally the function is\n",
      "constant; close to a target input x0, the function does not change much, and\n",
      "so close outputs can be averaged to produce ˆf(x0). Other methods such\n",
      "as splines, neural networks and basis-function methods implicitly deﬁne\n",
      "neighborhoods of local behavior. In Section 5.4.1 we discuss the concept\n",
      "of anequivalent kernel (see Figure 5.8 on page 157), which describes this\n",
      "local dependence for any method linear in the outputs. These equivalent\n",
      "kernels in many cases look just like the explicitly deﬁned weighting kernels\n",
      "discussed above—peaked at the target point and falling away smoothly\n",
      "away from it.\n",
      "One fact should be clear by now. Any method that attempts to pro-\n",
      "duce locally varying functions in small isotropic neighborhoods will run\n",
      "into problems in high dimensions—again the curse of dimensionality. And\n",
      "conversely, all methods that overcome the dimensionality problems have an\n",
      "associated—and often implicit or adaptive—metric for measuring neighbor-\n",
      "hoods, which basically does not allow the neighborhood to be simultane-\n",
      "ously small in all directions.\n",
      "2.8 Classes of Restricted Estimators\n",
      "The variety of nonparametric regression techniques or learning methods fall\n",
      "into a number of diﬀerent classes depending on the nature of the restrictions\n",
      "imposed. These classes are not distinct, and indeed some methods fall in\n",
      "several classes. Here we give a brief summary, since detailed descriptions\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "34 2. Overview of Supervised Learning\n",
      "are given in later chapters. Each of the classes has associated with it one\n",
      "or more parameters, sometimes appropriately called smoothing parameters,\n",
      "that control the eﬀective size of the local neighborhood. Here we describe\n",
      "three broad classes.\n",
      "2.8.1 Roughness Penalty and Bayesian Methods\n",
      "Here the class of functions is controlled by explicitly penalizing RSS( f)\n",
      "with a roughness penalty\n",
      "PRSS( f;λ) = RSS( f) +λJ(f). (2.38)\n",
      "The user-selected functional J(f) will be large for functions fthat vary too\n",
      "rapidly over small regions of input space. For example, the popular cubic\n",
      "smoothing spline for one-dimensional inputs is the solution to the penalized\n",
      "least-squares criterion\n",
      "PRSS( f;λ) =N∑\n",
      "i=1(yi−f(xi))2+λ∫\n",
      "[f′′(x)]2dx. (2.39)\n",
      "The roughness penalty here controls large values of the second derivative\n",
      "off, and the amount of penalty is dictated by λ≥0. For λ= 0 no penalty\n",
      "is imposed, and any interpolating function will do, while for λ=∞only\n",
      "functions linear in xare permitted.\n",
      "Penalty functionals Jcan be constructed for functions in any dimension,\n",
      "and special versions can be created to impose special structure. For ex-\n",
      "ample, additive penalties J(f) =∑p\n",
      "j=1J(fj) are used in conjunction with\n",
      "additive functions f(X) =∑p\n",
      "j=1fj(Xj) to create additive models with\n",
      "smooth coordinate functions. Similarly, projection pursuit regression mod-\n",
      "els have f(X) =∑M\n",
      "m=1gm(αT\n",
      "mX) for adaptively chosen directions αm, and\n",
      "the functions gmcan each have an associated roughness penalty.\n",
      "Penalty function, or regularization methods, express our prior belief that\n",
      "the type of functions we seek exhibit a certain type of smooth behavior, and\n",
      "indeed can usually be cast in a Bayesian framework. The penalty Jcorre-\n",
      "sponds to a log-prior, and PRSS( f;λ) the log-posterior distribution, and\n",
      "minimizing PRSS( f;λ) amounts to ﬁnding the posterior mode. We discuss\n",
      "roughness-penalty approaches in Chapter 5 and the Bayesian paradigm in\n",
      "Chapter 8.\n",
      "2.8.2 Kernel Methods and Local Regression\n",
      "These methods can be thought of as explicitly providing estimates of the re-\n",
      "gression function or conditional expectation by specifying the nature of the\n",
      "local neighborhood, and of the class of regular functions ﬁtted locally. The\n",
      "local neighborhood is speciﬁed by a kernel function Kλ(x0,x) which assigns\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.8 Classes of Restricted Estimators 35\n",
      "weights to points xin a region around x0(see Figure 6.1 on page 192). For\n",
      "example, the Gaussian kernel has a weight function based on the Gaussian\n",
      "density function\n",
      "Kλ(x0,x) =1\n",
      "λexp[\n",
      "−||x−x0||2\n",
      "2λ]\n",
      "(2.40)\n",
      "and assigns weights to points that die exponentially with their squared\n",
      "Euclidean distance from x0. The parameter λcorresponds to the variance\n",
      "of the Gaussian density, and controls the width of the neighborhood. The\n",
      "simplest form of kernel estimate is the Nadaraya–Watson weighted averag e\n",
      "ˆf(x0) =∑N\n",
      "i=1Kλ(x0,xi)yi∑N\n",
      "i=1Kλ(x0,xi). (2.41)\n",
      "In general we can deﬁne a local regression estimate of f(x0) asfˆθ(x0),\n",
      "where ˆθminimizes\n",
      "RSS(fθ,x0) =N∑\n",
      "i=1Kλ(x0,xi)(yi−fθ(xi))2, (2.42)\n",
      "andfθis some parameterized function, such as a low-order polynomial.\n",
      "Some examples are:\n",
      "•fθ(x) =θ0, the constant function; this results in the Nadaraya–\n",
      "Watson estimate in (2.41) above.\n",
      "•fθ(x) =θ0+θ1xgives the popular local linear regression model.\n",
      "Nearest-neighbor methods can be thought of as kernel methods having a\n",
      "more data-dependent metric. Indeed, the metric for k-nearest neighbors is\n",
      "Kk(x,x0) =I(||x−x0|| ≤ ||x(k)−x0||),\n",
      "where x(k)is the training observation ranked kth in distance from x0, and\n",
      "I(S) is the indicator of the set S.\n",
      "These methods of course need to be modiﬁed in high dimensions, to avoid\n",
      "the curse of dimensionality. Various adaptations are discussed in Chapter 6.\n",
      "2.8.3 Basis Functions and Dictionary Methods\n",
      "This class of methods includes the familiar linear and polynomial expan-\n",
      "sions, but more importantly a wide variety of more ﬂexible models. The\n",
      "model for fis a linear expansion of basis functions\n",
      "fθ(x) =M∑\n",
      "m=1θmhm(x), (2.43)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "36 2. Overview of Supervised Learning\n",
      "where each of the hmis a function of the input x, and the term linear here\n",
      "refers to the action of the parameters θ. This class covers a wide variety of\n",
      "methods. In some cases the sequence of basis functions is prescribed, such\n",
      "as a basis for polynomials in xof total degree M.\n",
      "For one-dimensional x, polynomial splines of degree Kcan be represented\n",
      "by an appropriate sequence of Mspline basis functions, determined in turn\n",
      "byM−Kknots. These produce functions that are piecewise polynomials\n",
      "of degree Kbetween the knots, and joined up with continuity of degree\n",
      "K−1 at the knots. As an example consider linear splines, or piecewise\n",
      "linear functions. One intuitively satisfying basis consists of the functions\n",
      "b1(x) = 1, b2(x) =x, and bm+2(x) = ( x−tm)+,m= 1,... ,M −2,\n",
      "where tmis the mth knot, and z+denotes positive part. Tensor products\n",
      "of spline bases can be used for inputs with dimensions larger than one\n",
      "(see Section 5.2, and the CART and MARS models in Chapter 9.) The\n",
      "parameter θcan be the total degree of the polynomial or the number of\n",
      "knots in the case of splines.\n",
      "Radial basis functions are symmetric p-dimensional kernels located at\n",
      "particular centroids,\n",
      "fθ(x) =M∑\n",
      "m=1Kλm(θm,x)θm; (2.44)\n",
      "for example, the Gaussian kernel Kλ(θ,x) =e−||x−θ||2/2λis popular.\n",
      "Radial basis functions have centroids θmand scales λmthat have to\n",
      "be determined. The spline basis functions have knots. In general we would\n",
      "like the data to dictate them as well. Including these as parameters changes\n",
      "the regression problem from a straightforward linear problem to a combi-\n",
      "natorially hard nonlinear problem. In practice, shortcuts such as greedy\n",
      "algorithms or two stage processes are used. Section 6.7 describes some such\n",
      "approaches.\n",
      "A single-layer feed-forward neural network model with linear output\n",
      "weights can be thought of as an adaptive basis function method. The model\n",
      "has the form\n",
      "fθ(x) =M∑\n",
      "m=1βmσ(αT\n",
      "mx+bm), (2.45)\n",
      "where σ(x) = 1 /(1 +e−x) is known as the activation function. Here, as\n",
      "in the projection pursuit model, the directions αmand the biasterms bm\n",
      "have to be determined, and their estimation is the meat of the computation.\n",
      "Details are give in Chapter 11.\n",
      "These adaptively chosen basis function methods are also known as dictio-\n",
      "narymethods, where one has available a possibly inﬁnite set or dictionary\n",
      "Dof candidate basis functions from which to choose, and models are built\n",
      "up by employing some kind of search mechanism.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "2.9 Model Selection and the Bias–Variance Tradeoﬀ 37\n",
      "2.9 Model Selection and the Bias–Variance\n",
      "Tradeoﬀ\n",
      "All the models described above and many others discussed in later chapters\n",
      "have a smoothing orcomplexity parameter that has to be determined:\n",
      "•the multiplier of the penalty term;\n",
      "•the width of the kernel;\n",
      "•or the number of basis functions.\n",
      "In the case of the smoothing spline, the parameter λindexes models ranging\n",
      "from a straight line ﬁt to the interpolating model. Similarly a local degr ee-\n",
      "mpolynomial model ranges between a degree- mglobal polynomial when\n",
      "the window size is inﬁnitely large, to an interpolating ﬁt when the window\n",
      "size shrinks to zero. This means that we cannot use residual sum-of-squares\n",
      "on the training data to determine these parameters as well, since we would\n",
      "always pick those that gave interpolating ﬁts and hence zero residuals. Such\n",
      "a model is unlikely to predict future data well at all.\n",
      "Thek-nearest-neighbor regression ﬁt ˆfk(x0) usefully illustrates the com-\n",
      "peting forces that eﬀect the predictive ability of such approximations. Sup-\n",
      "pose the data arise from a model Y=f(X) +ε, with E( ε) = 0 and\n",
      "Var(ε) =σ2. For simplicity here we assume that the values of xiin the\n",
      "sample are ﬁxed in advance (nonrandom). The expected prediction error\n",
      "atx0, also known as testorgeneralization error, can be decomposed:\n",
      "EPE k(x0) = E[( Y−ˆfk(x0))2|X=x0]\n",
      "=σ2+ [Bias2(ˆfk(x0)) + Var T(ˆfk(x0))] (2.46)\n",
      "=σ2+[\n",
      "f(x0)−1\n",
      "kk∑\n",
      "ℓ=1f(x(ℓ))]2\n",
      "+σ2\n",
      "k. (2.47)\n",
      "The subscripts in parentheses ( ℓ) indicate the sequence of nearest neighbors\n",
      "tox0.\n",
      "There are three terms in this expression. The ﬁrst term σ2is the ir-\n",
      "reducible error—the variance of the new test target—and is beyond our\n",
      "control, even if we know the true f(x0).\n",
      "The second and third terms are under our control, and make up the\n",
      "mean squared error ofˆfk(x0) in estimating f(x0), which is broken down\n",
      "into a bias component and a variance component. The bias term is the\n",
      "squared diﬀerence between the true mean f(x0) and the expected value of\n",
      "the estimate—[E T(ˆfk(x0))−f(x0)]2—where the expectation averages the\n",
      "randomness in the training data. This term will most likely increase with\n",
      "k, if the true function is reasonably smooth. For small kthe few closest\n",
      "neighbors will have values f(x(ℓ)) close to f(x0), so their average should\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "38 2. Overview of Supervised Learning\n",
      "High Bias\n",
      "Low VarianceLow Bias\n",
      "High VariancePrediction Error\n",
      "Model ComplexityTraining SampleTest Sample\n",
      "Low High\n",
      "FIGURE 2.11. Test and training error as a function of model complexity.\n",
      "be close to f(x0). As kgrows, the neighbors are further away, and then\n",
      "anything can happen.\n",
      "The variance term is simply the variance of an average here, and de-\n",
      "creases as the inverse of k. So as kvaries, there is a bias–variance tradeoﬀ.\n",
      "More generally, as the model complexity of our procedure is increased,\n",
      "the variance tends to increase and the squared bias tends to decreases.\n",
      "The opposite behavior occurs as the model complexity is decreased. For\n",
      "k-nearest neighbors, the model complexity is controlled by k.\n",
      "Typically we would like to choose our model complexity to trade bias\n",
      "oﬀ with variance in such a way as to minimize the test error. An obvious\n",
      "estimate of test error is the training error1\n",
      "N∑\n",
      "i(yi−ˆyi)2. Unfortunately\n",
      "training error is not a good estimate of test error, as it does not properly\n",
      "account for model complexity.\n",
      "Figure 2.11 shows the typical behavior of the test and training error, as\n",
      "model complexity is varied. The training error tends to decrease whenever\n",
      "we increase the model complexity, that is, whenever we ﬁt the data harder.\n",
      "However with too much ﬁtting, the model adapts itself too closely to the\n",
      "training data, and will not generalize well (i.e., have large test error). In\n",
      "that case the predictions ˆf(x0) will have large variance, as reﬂected in the\n",
      "last term of expression (2.46). In contrast, if the model is not complex\n",
      "enough, it will underﬁt and may have large bias, again resulting in poor\n",
      "generalization. In Chapter 7 we discuss methods for estimating the test\n",
      "error of a prediction method, and hence estimating the optimal amount of\n",
      "model complexity for a given prediction method and training set.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 39\n",
      "Bibliographic Notes\n",
      "Some good general books on the learning problem are Duda et al. (2000),\n",
      "Bishop (1995),(Bishop, 2006), Ripley (1996), Cherkassky and Mulier (2 007)\n",
      "and Vapnik (1996). Parts of this chapter are based on Friedman (1994b).\n",
      "Exercises\n",
      "Ex. 2.1 Suppose each of K-classes has an associated target tk, which is a\n",
      "vector of all zeros, except a one in the kth position. Show that classifying to\n",
      "the largest element of ˆ yamounts to choosing the closest target, min k||tk−\n",
      "ˆy||, if the elements of ˆ ysum to one.\n",
      "Ex. 2.2 Show how to compute the Bayes decision boundary for the simula-\n",
      "tion example in Figure 2.5.\n",
      "Ex. 2.3 Derive equation (2.24).\n",
      "Ex. 2.4 The edge eﬀect problem discussed on page 23 is not peculiar to\n",
      "uniform sampling from bounded domains. Consider inputs drawn from a\n",
      "spherical multinormal distribution X∼N(0,Ip). The squared distance\n",
      "from any sample point to the origin has a χ2\n",
      "pdistribution with mean p.\n",
      "Consider a prediction point x0drawn from this distribution, and let a=\n",
      "x0/||x0||be an associated unit vector. Let zi=aTxibe the projection of\n",
      "each of the training points on this direction.\n",
      "Show that the ziare distributed N(0,1) with expected squared distance\n",
      "from the origin 1, while the target point has expected squared distance p\n",
      "from the origin.\n",
      "Hence for p= 10, a randomly drawn test point is about 3 .1 standard\n",
      "deviations from the origin, while all the training points are on average\n",
      "one standard deviation along direction a. So most prediction points see\n",
      "themselves as lying on the edge of the training set.\n",
      "Ex. 2.5\n",
      "(a) Derive equation (2.27). The last line makes use of (3.8) through a\n",
      "conditioning argument.\n",
      "(b) Derive equation (2.28), making use of the cyclic property of the trace\n",
      "operator [trace( AB) = trace( BA)], and its linearity (which allows us\n",
      "to interchange the order of trace and expectation).\n",
      "Ex. 2.6 Consider a regression problem with inputs xiand outputs yi, and a\n",
      "parameterized model fθ(x) to be ﬁt by least squares. Show that if there are\n",
      "observations with tiedoridentical values of x, then the ﬁt can be obtained\n",
      "from a reduced weighted least squares problem.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "40 2. Overview of Supervised Learning\n",
      "Ex. 2.7 Suppose we have a sample of Npairs xi,yidrawn i.i.d. from the\n",
      "distribution characterized as follows:\n",
      "xi∼h(x),the design density\n",
      "yi=f(xi) +εi, fis the regression function\n",
      "εi∼(0,σ2) (mean zero, variance σ2)\n",
      "We construct an estimator for flinear in the yi,\n",
      "ˆf(x0) =N∑\n",
      "i=1ℓi(x0;X)yi,\n",
      "where the weights ℓi(x0;X) do not depend on the yi, but do depend on the\n",
      "entire training sequence of xi, denoted here by X.\n",
      "(a) Show that linear regression and k-nearest-neighbor regression are mem-\n",
      "bers of this class of estimators. Describe explicitly the weights ℓi(x0;X)\n",
      "in each of these cases.\n",
      "(b) Decompose the conditional mean-squared error\n",
      "EY|X(f(x0)−ˆf(x0))2\n",
      "into a conditional squared bias and a conditional variance component.\n",
      "LikeX,Yrepresents the entire training sequence of yi.\n",
      "(c) Decompose the (unconditional) mean-squared error\n",
      "EY,X(f(x0)−ˆf(x0))2\n",
      "into a squared bias and a variance component.\n",
      "(d) Establish a relationship between the squared biases and variances in\n",
      "the above two cases.\n",
      "Ex. 2.8 Compare the classiﬁcation performance of linear regression and k–\n",
      "nearest neighbor classiﬁcation on the zipcode data. In particular, consider\n",
      "only the 2’s and3’s, and k= 1,3,5,7 and 15. Show both the training and\n",
      "test error for each choice. The zipcode data are available from the book\n",
      "websitewww-stat.stanford.edu/ElemStatLearn .\n",
      "Ex. 2.9 Consider a linear regression model with pparameters, ﬁt by least\n",
      "squares to a set of training data ( x1,y1),... ,(xN,yN) drawn at random\n",
      "from a population. Let ˆβbe the least squares estimate. Suppose we have\n",
      "some test data (˜ x1,˜y1),... ,(˜xM,˜yM) drawn at random from the same pop-\n",
      "ulation as the training data. If Rtr(β) =1\n",
      "N∑N\n",
      "1(yi−βTxi)2andRte(β) =\n",
      "1\n",
      "M∑M\n",
      "1(˜yi−βT˜xi)2, prove that\n",
      "E[Rtr(ˆβ)]≤E[Rte(ˆβ)],\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 41\n",
      "where the expectations are over all that is random in each expression. [This\n",
      "exercise was brought to our attention by Ryan Tibshirani, from a homework\n",
      "assignment given by Andrew Ng.]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "42 2. Overview of Supervised Learning\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 43\n",
      "Printer: Opaque this\n",
      "3\n",
      "Linear Methods for Regression\n",
      "3.1 Introduction\n",
      "A linear regression model assumes that the regression function E( Y|X) is\n",
      "linear in the inputs X1,... ,X p. Linear models were largely developed in\n",
      "the precomputer age of statistics, but even in today’s computer era there\n",
      "are still good reasons to study and use them. They are simple and often\n",
      "provide an adequate and interpretable description of how the inputs aﬀect\n",
      "the output. For prediction purposes they can sometimes outperform fancier\n",
      "nonlinear models, especially in situations with small numbers of training\n",
      "cases, low signal-to-noise ratio or sparse data. Finally, linear methods can be\n",
      "applied to transformations of the inputs and this considerably expands their\n",
      "scope. These generalizations are sometimes called basis-function methods,\n",
      "and are discussed in Chapter 5.\n",
      "In this chapter we describe linear methods for regression, while in the\n",
      "next chapter we discuss linear methods for classiﬁcation. On some topics we\n",
      "go into considerable detail, as it is our ﬁrm belief that an understanding\n",
      "of linear methods is essential for understanding nonlinear ones. In fact,\n",
      "many nonlinear techniques are direct generalizations of the linear methods\n",
      "discussed here.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "44 3. Linear Methods for Regression\n",
      "3.2 Linear Regression Models and Least Squares\n",
      "As introduced in Chapter 2, we have an input vector XT= (X1,X2,... ,X p),\n",
      "and want to predict a real-valued output Y. The linear regression model\n",
      "has the form\n",
      "f(X) =β0+p∑\n",
      "j=1Xjβj. (3.1)\n",
      "The linear model either assumes that the regression function E( Y|X) is\n",
      "linear, or that the linear model is a reasonable approximation. Here the\n",
      "βj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\n",
      "from diﬀerent sources:\n",
      "•quantitative inputs;\n",
      "•transformations of quantitative inputs, such as log, square-root or\n",
      "square;\n",
      "•basis expansions, such as X2=X2\n",
      "1,X3=X3\n",
      "1, leading to a polynomial\n",
      "representation;\n",
      "•numeric or “dummy” coding of the levels of qualitative inputs. For\n",
      "example, if Gis a ﬁve-level factor input, we might create Xj, j=\n",
      "1,... ,5,such that Xj=I(G=j). Together this group of Xjrepre-\n",
      "sents the eﬀect of Gby a set of level-dependent constants, since in∑5\n",
      "j=1Xjβj, one of the Xjs is one, and the others are zero.\n",
      "•interactions between variables, for example, X3=X1≤X2.\n",
      "No matter the source of the Xj, the model is linear in the parameters.\n",
      "Typically we have a set of training data ( x1,y1)...(xN,yN) from which\n",
      "to estimate the parameters β. Each xi= (xi1,xi2,... ,x ip)Tis a vector\n",
      "of feature measurements for the ith case. The most popular estimation\n",
      "method is least squares , in which we pick the coeﬃcients β= (β0,β1,... ,β p)T\n",
      "to minimize the residual sum of squares\n",
      "RSS(β) =N∑\n",
      "i=1(yi−f(xi))2\n",
      "=N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ". (3.2)\n",
      "From a statistical point of view, this criterion is reasonable if the tr aining\n",
      "observations ( xi,yi) represent independent random draws from their popu-\n",
      "lation. Even if the xi’s were not drawn randomly, the criterion is still valid\n",
      "if the yi’s are conditionally independent given the inputs xi. Figure 3.1\n",
      "illustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.2 Linear Regression Models and Least Squares 45\n",
      "•• •\n",
      "••••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••••••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "X1X2Y\n",
      "FIGURE 3.1. Linear least squares ﬁtting with X∈I R2. We seek the linear\n",
      "function of Xthat minimizes the sum of squared residuals from Y.\n",
      "space occupied by the pairs ( X,Y). Note that (3.2) makes no assumptions\n",
      "about the validity of model (3.1); it simply ﬁnds the best linear ﬁt to the\n",
      "data. Least squares ﬁtting is intuitively satisfying no matter how the data\n",
      "arise; the criterion measures the average lack of ﬁt.\n",
      "How do we minimize (3.2)? Denote by XtheN×(p+ 1) matrix with\n",
      "each row an input vector (with a 1 in the ﬁrst position), and similarly let\n",
      "ybe the N-vector of outputs in the training set. Then we can write the\n",
      "residual sum-of-squares as\n",
      "RSS(β) = (y−Xβ)T(y−Xβ). (3.3)\n",
      "This is a quadratic function in the p+ 1 parameters. Diﬀerentiating with\n",
      "respect to βwe obtain\n",
      "∂RSS\n",
      "∂β=−2XT(y−Xβ)\n",
      "∂2RSS\n",
      "∂β∂βT= 2XTX.(3.4)\n",
      "Assuming (for the moment) that Xhas full column rank, and hence XTX\n",
      "is positive deﬁnite, we set the ﬁrst derivative to zero\n",
      "XT(y−Xβ) = 0 (3.5)\n",
      "to obtain the unique solution\n",
      "ˆβ= (XTX)−1XTy. (3.6)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "46 3. Linear Methods for Regression\n",
      "x1x2y\n",
      "ˆ y\n",
      "FIGURE 3.2. TheN-dimensional geometry of least squares regression with two\n",
      "predictors. The outcome vector yis orthogonally projected onto the hyperplane\n",
      "spanned by the input vectors x1andx2. The projection ˆyrepresents the vector\n",
      "of the least squares predictions\n",
      "The predicted values at an input vector x0are given by ˆf(x0) = (1 : x0)Tˆβ;\n",
      "the ﬁtted values at the training inputs are\n",
      "ˆy=Xˆβ=X(XTX)−1XTy, (3.7)\n",
      "where ˆ yi=ˆf(xi). The matrix H=X(XTX)−1XTappearing in equation\n",
      "(3.7) is sometimes called the “hat” matrix because it puts the hat on y.\n",
      "Figure 3.2 shows a diﬀerent geometrical representation of the least squares\n",
      "estimate, this time in IRN. We denote the column vectors of Xbyx0,x1,... ,xp,\n",
      "withx0≡1. For much of what follows, this ﬁrst column is treated like any\n",
      "other. These vectors span a subspace of IRN, also referred to as the column\n",
      "space of X. We minimize RSS( β) =∥y−Xβ∥2by choosing ˆβso that the\n",
      "residual vector y−ˆyis orthogonal to this subspace. This orthogonality is\n",
      "expressed in (3.5), and the resulting estimate ˆyis hence the orthogonal pro-\n",
      "jection ofyonto this subspace. The hat matrix Hcomputes the orthogonal\n",
      "projection, and hence it is also known as a projection matrix.\n",
      "It might happen that the columns of Xare not linearly independent, so\n",
      "thatXis not of full rank. This would occur, for example, if two of the\n",
      "inputs were perfectly correlated, (e.g., x2= 3x1). Then XTXis singular\n",
      "and the least squares coeﬃcients ˆβare not uniquely deﬁned. However,\n",
      "the ﬁtted values ˆy=Xˆβare still the projection of yonto the column\n",
      "space of X; there is just more than one way to express that projection\n",
      "in terms of the column vectors of X. The non-full-rank case occurs most\n",
      "often when one or more qualitative inputs are coded in a redundant fashion.\n",
      "There is usually a natural way to resolve the non-unique representation,\n",
      "by recoding and/or dropping redundant columns in X. Most regression\n",
      "software packages detect these redundancies and automatically implement\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.2 Linear Regression Models and Least Squares 47\n",
      "some strategy for removing them. Rank deﬁciencies can also occur in signal\n",
      "and image analysis, where the number of inputs pcan exceed the number\n",
      "of training cases N. In this case, the features are typically reduced by\n",
      "ﬁltering or else the ﬁtting is controlled by regularization (Section 5.2.3 and\n",
      "Chapter 18).\n",
      "Up to now we have made minimal assumptions about the true distribu-\n",
      "tion of the data. In order to pin down the sampling properties of ˆβ, we now\n",
      "assume that the observations yiare uncorrelated and have constant vari-\n",
      "anceσ2, and that the xiare ﬁxed (non random). The variance–covariance\n",
      "matrix of the least squares parameter estimates is easily derived from (3.6 )\n",
      "and is given by\n",
      "Var(ˆβ) = (XTX)−1σ2. (3.8)\n",
      "Typically one estimates the variance σ2by\n",
      "ˆσ2=1\n",
      "N−p−1N∑\n",
      "i=1(yi−ˆyi)2.\n",
      "TheN−p−1 rather than Nin the denominator makes ˆ σ2an unbiased\n",
      "estimate of σ2: E(ˆσ2) =σ2.\n",
      "To draw inferences about the parameters and the model, additional as-\n",
      "sumptions are needed. We now assume that (3.1) is the correct model for\n",
      "the mean; that is, the conditional expectation of Yis linear in X1,... ,X p.\n",
      "We also assume that the deviations of Yaround its expectation are additive\n",
      "and Gaussian. Hence\n",
      "Y= E( Y|X1,... ,X p) +ε\n",
      "=β0+p∑\n",
      "j=1Xjβj+ε, (3.9)\n",
      "where the error εis a Gaussian random variable with expectation zero and\n",
      "variance σ2, written ε∼N(0,σ2).\n",
      "Under (3.9), it is easy to show that\n",
      "ˆβ∼N(β,(XTX)−1σ2). (3.10)\n",
      "This is a multivariate normal distribution with mean vector and variance–\n",
      "covariance matrix as shown. Also\n",
      "(N−p−1)ˆσ2∼σ2χ2\n",
      "N−p−1, (3.11)\n",
      "a chi-squared distribution with N−p−1 degrees of freedom. In addition ˆβ\n",
      "and ˆσ2are statistically independent. We use these distributional properties\n",
      "to form tests of hypothesis and conﬁdence intervals for the parameters βj.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "48 3. Linear Methods for Regression\n",
      "ZTail Probabilities\n",
      "2.0 2.2 2.4 2.6 2.8 3.00.01 0.02 0.03 0.04 0.05 0.06t30\n",
      "t100\n",
      "normal\n",
      "FIGURE 3.3. The tail probabilities Pr(|Z|> z)for three distributions, t30,t100\n",
      "and standard normal. Shown are the appropriate quantiles for test ing signiﬁcance\n",
      "at the p= 0.05and0.01levels. The diﬀerence between tand the standard normal\n",
      "becomes negligible for Nbigger than about 100.\n",
      "To test the hypothesis that a particular coeﬃcient βj= 0, we form the\n",
      "standardized coeﬃcient or Z-score\n",
      "zj=ˆβj\n",
      "ˆσ√vj, (3.12)\n",
      "where vjis the jth diagonal element of ( XTX)−1. Under the null hypothesis\n",
      "thatβj= 0,zjis distributed as tN−p−1(atdistribution with N−p−1\n",
      "degrees of freedom), and hence a large (absolute) value of zjwill lead to\n",
      "rejection of this null hypothesis. If ˆ σis replaced by a known value σ, then\n",
      "zjwould have a standard normal distribution. The diﬀerence between the\n",
      "tail quantiles of a t-distribution and a standard normal become negligible\n",
      "as the sample size increases, and so we typically use the normal quantiles\n",
      "(see Figure 3.3).\n",
      "Often we need to test for the signiﬁcance of groups of coeﬃcients simul-\n",
      "taneously. For example, to test if a categorical variable with klevels can\n",
      "be excluded from a model, we need to test whether the coeﬃcients of the\n",
      "dummy variables used to represent the levels can all be set to zero. Here\n",
      "we use the Fstatistic,\n",
      "F=(RSS 0−RSS1)/(p1−p0)\n",
      "RSS1/(N−p1−1), (3.13)\n",
      "where RSS 1is the residual sum-of-squares for the least squares ﬁt of the big-\n",
      "ger model with p1+1 parameters, and RSS 0the same for the nested smaller\n",
      "model with p0+1 parameters, having p1−p0parameters constrained to be\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.2 Linear Regression Models and Least Squares 49\n",
      "zero. The Fstatistic measures the change in residual sum-of-squares per\n",
      "additional parameter in the bigger model, and it is normalized by an esti-\n",
      "mate of σ2. Under the Gaussian assumptions, and the null hypothesis that\n",
      "the smaller model is correct, the Fstatistic will have a Fp1−p0,N−p1−1dis-\n",
      "tribution. It can be shown (Exercise 3.1) that the zjin (3.12) are equivalent\n",
      "to the Fstatistic for dropping the single coeﬃcient βjfrom the model. For\n",
      "largeN, the quantiles of the Fp1−p0,N−p1−1approach those of the χ2\n",
      "p1−p0.\n",
      "Similarly, we can isolate βjin (3.10) to obtain a 1 −2αconﬁdence interval\n",
      "forβj:\n",
      "(ˆβj−z(1−α)v1\n",
      "2\n",
      "jˆσ,ˆβj+z(1−α)v1\n",
      "2\n",
      "jˆσ). (3.14)\n",
      "Herez(1−α)is the 1 −αpercentile of the normal distribution:\n",
      "z(1−0.025)= 1.96,\n",
      "z(1−.05)= 1.645,etc.\n",
      "Hence the standard practice of reporting ˆβ±2≤se(ˆβ) amounts to an ap-\n",
      "proximate 95% conﬁdence interval. Even if the Gaussian error assumption\n",
      "does not hold, this interval will be approximately correct, with its coverage\n",
      "approaching 1 −2αas the sample size N→ ∞.\n",
      "In a similar fashion we can obtain an approximate conﬁdence set for the\n",
      "entire parameter vector β, namely\n",
      "Cβ={β|(ˆβ−β)TXTX(ˆβ−β)≤ˆσ2χ2\n",
      "p+1(1−α)}, (3.15)\n",
      "where χ2\n",
      "ℓ(1−α)is the 1 −αpercentile of the chi-squared distribution on ℓ\n",
      "degrees of freedom: for example, χ2\n",
      "5(1−0.05)= 11.1,χ2\n",
      "5(1−0.1)= 9.2. This\n",
      "conﬁdence set for βgenerates a corresponding conﬁdence set for the true\n",
      "function f(x) =xTβ, namely {xTβ|β∈Cβ}(Exercise 3.2; see also Fig-\n",
      "ure 5.4 in Section 5.2.2 for examples of conﬁdence bands for functions).\n",
      "3.2.1 Example: Prostate Cancer\n",
      "The data for this example come from a study by Stamey et al. (1989). They\n",
      "examined the correlation between the level of prostate-speciﬁc antigen and\n",
      "a number of clinical measures in men who were about to receive a radical\n",
      "prostatectomy. The variables are log cancer volume ( lcavol ), log prostate\n",
      "weight ( lweight ),age, log of the amount of benign prostatic hyperplasia\n",
      "(lbph), seminal vesicle invasion ( svi), log of capsular penetration ( lcp),\n",
      "Gleason score ( gleason ), and percent of Gleason scores 4 or 5 ( pgg45).\n",
      "The correlation matrix of the predictors given in Table 3.1 shows many\n",
      "strong correlations. Figure 1.1 (page 3) of Chapter 1 is a scatterplot matr ix\n",
      "showing every pairwise plot between the variables. We see that sviis a\n",
      "binary variable, and gleason is an ordered categorical variable. We see, for\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "50 3. Linear Methods for Regression\n",
      "TABLE 3.1. Correlations of predictors in the prostate cancer data.\n",
      "lcavol lweight age lbph svi lcp gleason\n",
      "lweight 0.300\n",
      "age 0.286 0.317\n",
      "lbph 0.063 0.437 0.287\n",
      "svi 0.593 0.181 0.129 −0.139\n",
      "lcp 0.692 0.157 0.173 −0.089 0.671\n",
      "gleason 0.426 0.024 0.366 0.033 0.307 0.476\n",
      "pgg45 0.483 0.074 0.276 −0.030 0.481 0.663 0.757\n",
      "TABLE 3.2. Linear model ﬁt to the prostate cancer data. The Z score is the\n",
      "coeﬃcient divided by its standard error (3.12). Roughly a Zscore larger than two\n",
      "in absolute value is signiﬁcantly nonzero at the p= 0.05level.\n",
      "Term Coeﬃcient Std. Error ZScore\n",
      "Intercept 2.46 0.09 27.60\n",
      "lcavol 0.68 0.13 5.37\n",
      "lweight 0.26 0.10 2.75\n",
      "age −0.14 0.10 −1.40\n",
      "lbph 0.21 0.10 2.06\n",
      "svi 0.31 0.12 2.47\n",
      "lcp −0.29 0.15 −1.87\n",
      "gleason −0.02 0.15 −0.15\n",
      "pgg45 0.27 0.15 1.74\n",
      "example, that both lcavol andlcpshow a strong relationship with the\n",
      "response lpsa, and with each other. We need to ﬁt the eﬀects jointly to\n",
      "untangle the relationships between the predictors and the response.\n",
      "We ﬁt a linear model to the log of prostate-speciﬁc antigen, lpsa, after\n",
      "ﬁrst standardizing the predictors to have unit variance. We randomly split\n",
      "the dataset into a training set of size 67 and a test set of size 30. We ap-\n",
      "plied least squares estimation to the training set, producing the estimates,\n",
      "standard errors and Z-scores shown in Table 3.2. The Z-scores are deﬁned\n",
      "in (3.12), and measure the eﬀect of dropping that variable from the model.\n",
      "AZ-score greater than 2 in absolute value is approximately signiﬁcant at\n",
      "the 5% level. (For our example, we have nine parameters, and the 0 .025 tail\n",
      "quantiles of the t67−9distribution are ±2.002!) The predictor lcavol shows\n",
      "the strongest eﬀect, with lweight andsvialso strong. Notice that lcpis\n",
      "not signiﬁcant, once lcavol is in the model (when used in a model without\n",
      "lcavol ,lcpis strongly signiﬁcant). We can also test for the exclusion of\n",
      "a number of terms at once, using the F-statistic (3.13). For example, we\n",
      "consider dropping all the non-signiﬁcant terms in Table 3.2, namely age,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.2 Linear Regression Models and Least Squares 51\n",
      "lcp,gleason , andpgg45. We get\n",
      "F=(32.81−29.43)/(9−5)\n",
      "29.43/(67−9)= 1.67, (3.16)\n",
      "which has a p-value of 0 .17 (Pr( F4,58>1.67) = 0 .17), and hence is not\n",
      "signiﬁcant.\n",
      "The mean prediction error on the test data is 0 .521. In contrast, predic-\n",
      "tion using the mean training value of lpsahas a test error of 1 .057, which\n",
      "is called the “base error rate.” Hence the linear model reduces the base\n",
      "error rate by about 50%. We will return to this example later to compare\n",
      "various selection and shrinkage methods.\n",
      "3.2.2 The Gauss–Markov Theorem\n",
      "One of the most famous results in statistics asserts that the least squares\n",
      "estimates of the parameters βhave the smallest variance among all linear\n",
      "unbiased estimates. We will make this precise here, and also make clear\n",
      "that the restriction to unbiased estimates is not necessarily a wise one. This\n",
      "observation will lead us to consider biased estimates such as ridge regression\n",
      "later in the chapter. We focus on estimation of any linear combination of\n",
      "the parameters θ=aTβ; for example, predictions f(x0) =xT\n",
      "0βare of this\n",
      "form. The least squares estimate of aTβis\n",
      "ˆθ=aTˆβ=aT(XTX)−1XTy. (3.17)\n",
      "Considering Xto be ﬁxed, this is a linear function cT\n",
      "0yof the response\n",
      "vector y. If we assume that the linear model is correct, aTˆβis unbiased\n",
      "since\n",
      "E(aTˆβ) = E( aT(XTX)−1XTy)\n",
      "=aT(XTX)−1XTXβ\n",
      "=aTβ. (3.18)\n",
      "The Gauss–Markov theorem states that if we have any other linear estima-\n",
      "tor˜θ=cTythat is unbiased for aTβ, that is, E( cTy) =aTβ, then\n",
      "Var(aTˆβ)≤Var(cTy). (3.19)\n",
      "The proof (Exercise 3.3) uses the triangle inequality. For simplicity we hav e\n",
      "stated the result in terms of estimation of a single parameter aTβ, but with\n",
      "a few more deﬁnitions one can state it in terms of the entire parameter\n",
      "vector β(Exercise 3.3).\n",
      "Consider the mean squared error of an estimator ˜θin estimating θ:\n",
      "MSE( ˜θ) = E( ˜θ−θ)2\n",
      "= Var( ˜θ) + [E( ˜θ)−θ]2. (3.20)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "52 3. Linear Methods for Regression\n",
      "The ﬁrst term is the variance, while the second term is the squared bias.\n",
      "The Gauss-Markov theorem implies that the least squares estimator has the\n",
      "smallest mean squared error of all linear estimators with no bias. However ,\n",
      "there may well exist a biased estimator with smaller mean squared error.\n",
      "Such an estimator would trade a little bias for a larger reduction in varia nce.\n",
      "Biased estimates are commonly used. Any method that shrinks or sets to\n",
      "zero some of the least squares coeﬃcients may result in a biased estimate.\n",
      "We discuss many examples, including variable subset selection and ridge\n",
      "regression, later in this chapter. From a more pragmatic point of view, m ost\n",
      "models are distortions of the truth, and hence are biased; picking the right\n",
      "model amounts to creating the right balance between bias and variance.\n",
      "We go into these issues in more detail in Chapter 7.\n",
      "Mean squared error is intimately related to prediction accuracy, as dis-\n",
      "cussed in Chapter 2. Consider the prediction of the new response at input\n",
      "x0,\n",
      "Y0=f(x0) +ε0. (3.21)\n",
      "Then the expected prediction error of an estimate ˜f(x0) =xT\n",
      "0˜βis\n",
      "E(Y0−˜f(x0))2=σ2+ E(xT\n",
      "0˜β−f(x0))2\n",
      "=σ2+ MSE( ˜f(x0)). (3.22)\n",
      "Therefore, expected prediction error and mean squared error diﬀer only by\n",
      "the constant σ2, representing the variance of the new observation y0.\n",
      "3.2.3 Multiple Regression from Simple Univariate Regressi on\n",
      "The linear model (3.1) with p >1 inputs is called the multiple linear\n",
      "regression model . The least squares estimates (3.6) for this model are best\n",
      "understood in terms of the estimates for the univariate (p= 1) linear\n",
      "model, as we indicate in this section.\n",
      "Suppose ﬁrst that we have a univariate model with no intercept, that is,\n",
      "Y=Xβ+ε. (3.23)\n",
      "The least squares estimate and residuals are\n",
      "ˆβ=∑N\n",
      "1xiyi∑N\n",
      "1x2\n",
      "i,\n",
      "ri=yi−xiˆβ.(3.24)\n",
      "In convenient vector notation, we let y= (y1,... ,y N)T,x= (x1,... ,x N)T\n",
      "and deﬁne\n",
      "⟨x,y⟩=N∑\n",
      "i=1xiyi,\n",
      "=xTy, (3.25)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.2 Linear Regression Models and Least Squares 53\n",
      "theinner product between xandy1. Then we can write\n",
      "ˆβ=⟨x,y⟩\n",
      "⟨x,x⟩,\n",
      "r=y−xˆβ.(3.26)\n",
      "As we will see, this simple univariate regression provides the building block\n",
      "for multiple linear regression. Suppose next that the inputs x1,x2,... ,xp\n",
      "(the columns of the data matrix X) are orthogonal; that is ⟨xj,xk⟩= 0\n",
      "for all j̸=k. Then it is easy to check that the multiple least squares esti-\n",
      "mates ˆβjare equal to ⟨xj,y⟩/⟨xj,xj⟩—the univariate estimates. In other\n",
      "words, when the inputs are orthogonal, they have no eﬀect on each other’s\n",
      "parameter estimates in the model.\n",
      "Orthogonal inputs occur most often with balanced, designed experiments\n",
      "(where orthogonality is enforced), but almost never with observational\n",
      "data. Hence we will have to orthogonalize them in order to carry this idea\n",
      "further. Suppose next that we have an intercept and a single input x. Then\n",
      "the least squares coeﬃcient of xhas the form\n",
      "ˆβ1=⟨x−¯x1,y⟩\n",
      "⟨x−¯x1,x−¯x1⟩, (3.27)\n",
      "where ¯ x=∑\n",
      "ixi/N, and1=x0, the vector of Nones. We can view the\n",
      "estimate (3.27) as the result of two applications of the simple regression\n",
      "(3.26). The steps are:\n",
      "1. regress xon1to produce the residual z=x−¯x1;\n",
      "2. regress yon the residual zto give the coeﬃcient ˆβ1.\n",
      "In this procedure, “regress bona” means a simple univariate regression of b\n",
      "onawith no intercept, producing coeﬃcient ˆ γ=⟨a,b⟩/⟨a,a⟩and residual\n",
      "vector b−ˆγa. We say that bis adjusted for a, or is “orthogonalized” with\n",
      "respect to a.\n",
      "Step 1 orthogonalizes xwith respect to x0=1. Step 2 is just a simple\n",
      "univariate regression, using the orthogonal predictors 1andz. Figure 3.4\n",
      "shows this process for two general inputs x1andx2. The orthogonalization\n",
      "does not change the subspace spanned by x1andx2, it simply produces an\n",
      "orthogonal basis for representing it.\n",
      "This recipe generalizes to the case of pinputs, as shown in Algorithm 3.1.\n",
      "Note that the inputs z0,... ,zj−1in step 2 are orthogonal, hence the simple\n",
      "regression coeﬃcients computed there are in fact also the multiple regres-\n",
      "sion coeﬃcients.\n",
      "1The inner-product notation is suggestive of generalizatio ns of linear regression to\n",
      "diﬀerent metric spaces, as well as to probability spaces.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "54 3. Linear Methods for Regression\n",
      "x1x2y\n",
      "ˆ yz z z z z\n",
      "FIGURE 3.4. Least squares regression by orthogonalization of the inputs. The\n",
      "vector x2is regressed on the vector x1, leaving the residual vector z. The regres-\n",
      "sion of yonzgives the multiple regression coeﬃcient of x2. Adding together the\n",
      "projections of yon each of x1andzgives the least squares ﬁt ˆy.\n",
      "Algorithm 3.1 Regression by Successive Orthogonalization.\n",
      "1. Initialize z0=x0=1.\n",
      "2. For j= 1,2,... ,p\n",
      "Regress xjonz0,z1,... ,,zj−1to produce coeﬃcients ˆ γℓj=\n",
      "⟨zℓ,xj⟩/⟨zℓ,zℓ⟩,ℓ= 0,... ,j −1 and residual vector zj=\n",
      "xj−∑j−1\n",
      "k=0ˆγkjzk.\n",
      "3. Regress yon the residual zpto give the estimate ˆβp.\n",
      "The result of this algorithm is\n",
      "ˆβp=⟨zp,y⟩\n",
      "⟨zp,zp⟩. (3.28)\n",
      "Re-arranging the residual in step 2, we can see that each of the xjis a linear\n",
      "combination of the zk, k≤j. Since the zjare all orthogonal, they form\n",
      "a basis for the column space of X, and hence the least squares projection\n",
      "onto this subspace is ˆy. Since zpalone involves xp(with coeﬃcient 1), we\n",
      "see that the coeﬃcient (3.28) is indeed the multiple regression coeﬃcient of\n",
      "yonxp. This key result exposes the eﬀect of correlated inputs in multiple\n",
      "regression. Note also that by rearranging the xj, any one of them could\n",
      "be in the last position, and a similar results holds. Hence stated more\n",
      "generally, we have shown that the jth multiple regression coeﬃcient is the\n",
      "univariate regression coeﬃcient of yonxj≤012...(j−1)(j+1)...,p, the residual\n",
      "after regressing xjonx0,x1,... ,xj−1,xj+1,... ,xp:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.2 Linear Regression Models and Least Squares 55\n",
      "The multiple regression coeﬃcient ˆβjrepresents the additional\n",
      "contribution of xjony, afterxjhas been adjusted for x0,x1,... ,xj−1,\n",
      "xj+1,... ,xp.\n",
      "Ifxpis highly correlated with some of the other xk’s, the residual vector\n",
      "zpwill be close to zero, and from (3.28) the coeﬃcient ˆβpwill be very\n",
      "unstable. This will be true for all the variables in the correlated set. In\n",
      "such situations, we might have all the Z-scores (as in Table 3.2) be smal l—\n",
      "any one of the set can be deleted—yet we cannot delete them all. From\n",
      "(3.28) we also obtain an alternate formula for the variance estimates ( 3.8),\n",
      "Var(ˆβp) =σ2\n",
      "⟨zp,zp⟩=σ2\n",
      "∥zp∥2. (3.29)\n",
      "In other words, the precision with which we can estimate ˆβpdepends on\n",
      "the length of the residual vector zp; this represents how much of xpis\n",
      "unexplained by the other xk’s.\n",
      "Algorithm 3.1 is known as the Gram–Schmidt procedure for multiple\n",
      "regression, and is also a useful numerical strategy for computing the esti-\n",
      "mates. We can obtain from it not just ˆβp, but also the entire multiple least\n",
      "squares ﬁt, as shown in Exercise 3.4.\n",
      "We can represent step 2 of Algorithm 3.1 in matrix form:\n",
      "X=ZΓ, (3.30)\n",
      "whereZhas as columns the zj(in order), and Γis the upper triangular ma-\n",
      "trix with entries ˆ γkj. Introducing the diagonal matrix Dwithjth diagonal\n",
      "entry Djj=∥zj∥, we get\n",
      "X=ZD−1DΓ\n",
      "=QR, (3.31)\n",
      "the so-called QRdecomposition of X. Here Qis anN×(p+1) orthogonal\n",
      "matrix, QTQ=I, andRis a (p+ 1)×(p+ 1) upper triangular matrix.\n",
      "TheQRdecomposition represents a convenient orthogonal basis for the\n",
      "column space of X. It is easy to see, for example, that the least squares\n",
      "solution is given by\n",
      "ˆβ=R−1QTy, (3.32)\n",
      "ˆy=QQTy. (3.33)\n",
      "Equation (3.32) is easy to solve because Ris upper triangular\n",
      "(Exercise 3.4).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "56 3. Linear Methods for Regression\n",
      "3.2.4 Multiple Outputs\n",
      "Suppose we have multiple outputs Y1,Y2,... ,Y Kthat we wish to predict\n",
      "from our inputs X0,X1,X2,... ,X p. We assume a linear model for each\n",
      "output\n",
      "Yk=β0k+p∑\n",
      "j=1Xjβjk+εk (3.34)\n",
      "=fk(X) +εk. (3.35)\n",
      "With Ntraining cases we can write the model in matrix notation\n",
      "Y=XB+E. (3.36)\n",
      "HereYis the N×Kresponse matrix, with ikentry yik,Xis the N×(p+1)\n",
      "input matrix, Bis the ( p+ 1)×Kmatrix of parameters and Eis the\n",
      "N×Kmatrix of errors. A straightforward generalization of the univariat e\n",
      "loss function (3.2) is\n",
      "RSS(B) =K∑\n",
      "k=1N∑\n",
      "i=1(yik−fk(xi))2(3.37)\n",
      "= tr[( Y−XB)T(Y−XB)]. (3.38)\n",
      "The least squares estimates have exactly the same form as before\n",
      "ˆB= (XTX)−1XTY. (3.39)\n",
      "Hence the coeﬃcients for the kth outcome are just the least squares es-\n",
      "timates in the regression of ykonx0,x1,... ,xp. Multiple outputs do not\n",
      "aﬀect one another’s least squares estimates.\n",
      "If the errors ε= (ε1,... ,ε K) in (3.34) are correlated, then it might seem\n",
      "appropriate to modify (3.37) in favor of a multivariate version. Speciﬁca lly,\n",
      "suppose Cov( ε) =Σ, then the multivariate weighted criterion\n",
      "RSS(B;Σ) =N∑\n",
      "i=1(yi−f(xi))TΣ−1(yi−f(xi)) (3.40)\n",
      "arises naturally from multivariate Gaussian theory. Here f(x) is the vector\n",
      "function ( f1(x),... ,f K(x)), and yithe vector of Kresponses for observa-\n",
      "tioni. However, it can be shown that again the solution is given by (3.39);\n",
      "Kseparate regressions that ignore the correlations (Exercise 3.11). If the Σi\n",
      "vary among observations, then this is no longer the case, and the solution\n",
      "forBno longer decouples.\n",
      "In Section 3.7 we pursue the multiple outcome problem, and consider\n",
      "situations where it does pay to combine the regressions.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.3 Subset Selection 57\n",
      "3.3 Subset Selection\n",
      "There are two reasons why we are often not satisﬁed with the least squares\n",
      "estimates (3.6).\n",
      "•The ﬁrst is prediction accuracy : the least squares estimates often have\n",
      "low bias but large variance. Prediction accuracy can sometimes be\n",
      "improved by shrinking or setting some coeﬃcients to zero. By doing\n",
      "so we sacriﬁce a little bit of bias to reduce the variance of the predicted\n",
      "values, and hence may improve the overall prediction accuracy.\n",
      "•The second reason is interpretation . With a large number of predic-\n",
      "tors, we often would like to determine a smaller subset that exhibit\n",
      "the strongest eﬀects. In order to get the “big picture,” we are willing\n",
      "to sacriﬁce some of the small details.\n",
      "In this section we describe a number of approaches to variable subset selec-\n",
      "tion with linear regression. In later sections we discuss shrinkage and hybrid\n",
      "approaches for controlling variance, as well as other dimension-reduction\n",
      "strategies. These all fall under the general heading model selection . Model\n",
      "selection is not restricted to linear models; Chapter 7 covers this topic in\n",
      "some detail.\n",
      "With subset selection we retain only a subset of the variables, and elim-\n",
      "inate the rest from the model. Least squares regression is used to estimate\n",
      "the coeﬃcients of the inputs that are retained. There are a number of dif-\n",
      "ferent strategies for choosing the subset.\n",
      "3.3.1 Best-Subset Selection\n",
      "Best subset regression ﬁnds for each k∈ {0,1,2,... ,p }the subset of size k\n",
      "that gives smallest residual sum of squares (3.2). An eﬃcient algorithm—\n",
      "theleaps and bounds procedure (Furnival and Wilson, 1974)—makes this\n",
      "feasible for pas large as 30 or 40. Figure 3.5 shows all the subset models\n",
      "for the prostate cancer example. The lower boundary represents the models\n",
      "that are eligible for selection by the best-subsets approach. Note that the\n",
      "best subset of size 2, for example, need not include the variable that was\n",
      "in the best subset of size 1 (for this example all the subsets are nested).\n",
      "The best-subset curve (red lower boundary in Figure 3.5) is necessarily\n",
      "decreasing, so cannot be used to select the subset size k. The question of\n",
      "how to choose kinvolves the tradeoﬀ between bias and variance, along with\n",
      "the more subjective desire for parsimony. There are a number of criteria\n",
      "that one may use; typically we choose the smallest model that minimizes\n",
      "an estimate of the expected prediction error.\n",
      "Many of the other approaches that we discuss in this chapter are similar,\n",
      "in that they use the training data to produce a sequence of models varying\n",
      "in complexity and indexed by a single parameter. In the next section we use\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "58 3. Linear Methods for Regression\n",
      "Subset Size kResidual Sum−of−Squares\n",
      "0 20 40 60 80 100\n",
      "0 1 2 3 4 5 6 7 8•\n",
      "••••••••\n",
      "••••••••••••••••••••••••••\n",
      "•••••••••••••••••••••••••••••••••••••••••••••••••\n",
      "••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n",
      "•••••••••••••••••••••••••••••••••••••••••••••\n",
      "••••••••••••••••••••••••••\n",
      "••••••••\n",
      "••\n",
      "•\n",
      "•••••• •\n",
      "FIGURE 3.5. All possible subset models for the prostate cancer example. At\n",
      "each subset size is shown the residual sum-of-squares for ea ch model of that size.\n",
      "cross-validation to estimate prediction error and select k; the AIC criterion\n",
      "is a popular alternative. We defer more detailed discussion of these and\n",
      "other approaches to Chapter 7.\n",
      "3.3.2 Forward- and Backward-Stepwise Selection\n",
      "Rather than search through all possible subsets (which becomes infeasible\n",
      "forpmuch larger than 40), we can seek a good path through them. Forward-\n",
      "stepwise selection starts with the intercept, and then sequentially adds into\n",
      "the model the predictor that most improves the ﬁt. With many candidate\n",
      "predictors, this might seem like a lot of computation; however, clever up-\n",
      "dating algorithms can exploit the QR decomposition for the current ﬁt to\n",
      "rapidly establish the next candidate (Exercise 3.9). Like best-subset re-\n",
      "gression, forward stepwise produces a sequence of models indexed by k, the\n",
      "subset size, which must be determined.\n",
      "Forward-stepwise selection is a greedy algorithm , producing a nested se-\n",
      "quence of models. In this sense it might seem sub-optimal compared to\n",
      "best-subset selection. However, there are several reasons why it might be\n",
      "preferred:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.3 Subset Selection 59\n",
      "•Computational; for large pwe cannot compute the best subset se-\n",
      "quence, but we can always compute the forward stepwise sequence\n",
      "(even when p≫N).\n",
      "•Statistical; a price is paid in variance for selecting the best subset\n",
      "of each size; forward stepwise is a more constrained search, and will\n",
      "have lower variance, but perhaps more bias.\n",
      "0 5 10 15 20 25 300.65 0.70 0.75 0.80 0.85 0.90 0.95Best Subset\n",
      "Forward Stepwise\n",
      "Backward Stepwise\n",
      "Forward StagewiseE||ˆβ(k)−β||2\n",
      "Subset Size k\n",
      "FIGURE 3.6. Comparison of four subset-selection techniques on a simulat ed lin-\n",
      "ear regression problem Y=XTβ+ε. There are N= 300 observations on p= 31\n",
      "standard Gaussian variables, with pairwise correlations all equal to 0.85. For10of\n",
      "the variables, the coeﬃcients are drawn at random from a N(0,0.4)distribution;\n",
      "the rest are zero. The noise ε∼N(0,6.25), resulting in a signal-to-noise ratio of\n",
      "0.64. Results are averaged over 50simulations. Shown is the mean-squared error\n",
      "of the estimated coeﬃcient ˆβ(k)at each step from the true β.\n",
      "Backward-stepwise selection starts with the full model, and sequentially\n",
      "deletes the predictor that has the least impact on the ﬁt. The candidate for\n",
      "dropping is the variable with the smallest Z-score (Exercise 3.10). Backw ard\n",
      "selection can only be used when N > p , while forward stepwise can always\n",
      "be used.\n",
      "Figure 3.6 shows the results of a small simulation study to compare\n",
      "best-subset regression with the simpler alternatives forward and backward\n",
      "selection. Their performance is very similar, as is often the case. Included in\n",
      "the ﬁgure is forward stagewise regression (next section), which takes longer\n",
      "to reach minimum error.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "60 3. Linear Methods for Regression\n",
      "On the prostate cancer example, best-subset, forward and backward se-\n",
      "lection all gave exactly the same sequence of terms.\n",
      "Some software packages implement hybrid stepwise-selection strategies\n",
      "that consider both forward and backward moves at each step, and select\n",
      "the “best” of the two. For example in the Rpackage the stepfunction uses\n",
      "the AIC criterion for weighing the choices, which takes proper account of\n",
      "the number of parameters ﬁt; at each step an add or drop will be performed\n",
      "that minimizes the AIC score.\n",
      "Other more traditional packages base the selection on F-statistics, adding\n",
      "“signiﬁcant” terms, and dropping “non-signiﬁcant” terms. These are out\n",
      "of fashion, since they do not take proper account of the multiple testing\n",
      "issues. It is also tempting after a model search to print out a summary of\n",
      "the chosen model, such as in Table 3.2; however, the standard errors are\n",
      "not valid, since they do not account for the search process. The bootstrap\n",
      "(Section 8.2) can be useful in such settings.\n",
      "Finally, we note that often variables come in groups (such as the dummy\n",
      "variables that code a multi-level categorical predictor). Smart stepwise pro-\n",
      "cedures (such as stepinR) will add or drop whole groups at a time, taking\n",
      "proper account of their degrees-of-freedom.\n",
      "3.3.3 Forward-Stagewise Regression\n",
      "Forward-stagewise regression (FS) is even more constrained than forward-\n",
      "stepwise regression. It starts like forward-stepwise regression, wit h an in-\n",
      "tercept equal to ¯ y, and centered predictors with coeﬃcients initially all 0.\n",
      "At each step the algorithm identiﬁes the variable most correlated with the\n",
      "current residual. It then computes the simple linear regression coeﬃcient\n",
      "of the residual on this chosen variable, and then adds it to the current co-\n",
      "eﬃcient for that variable. This is continued till none of the variables have\n",
      "correlation with the residuals—i.e. the least-squares ﬁt when N > p .\n",
      "Unlike forward-stepwise regression, none of the other variables are ad-\n",
      "justed when a term is added to the model. As a consequence, forward\n",
      "stagewise can take many more than psteps to reach the least squares ﬁt,\n",
      "and historically has been dismissed as being ineﬃcient. It turns out that\n",
      "this “slow ﬁtting” can pay dividends in high-dimensional problems. We\n",
      "see in Section 3.8.1 that both forward stagewise and a variant which is\n",
      "slowed down even further are quite competitive, especially in very high-\n",
      "dimensional problems.\n",
      "Forward-stagewise regression is included in Figure 3.6. In this example it\n",
      "takes over 1000 steps to get all the correlations below 10−4. For subset size\n",
      "k, we plotted the error for the last step for which there where knonzero\n",
      "coeﬃcients. Although it catches up with the best ﬁt, it takes longer to\n",
      "do so.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.4 Shrinkage Methods 61\n",
      "3.3.4 Prostate Cancer Data Example (Continued)\n",
      "Table 3.3 shows the coeﬃcients from a number of diﬀerent selection and\n",
      "shrinkage methods. They are best-subset selection using an all-subsets search,\n",
      "ridge regression , thelasso,principal components regression andpartial least\n",
      "squares . Each method has a complexity parameter, and this was chosen to\n",
      "minimize an estimate of prediction error based on tenfold cross-validation;\n",
      "full details are given in Section 7.10. Brieﬂy, cross-validation works by divid-\n",
      "ing the training data randomly into ten equal parts. The learning method\n",
      "is ﬁt—for a range of values of the complexity parameter—to nine-tenths of\n",
      "the data, and the prediction error is computed on the remaining one-tenth.\n",
      "This is done in turn for each one-tenth of the data, and the ten prediction\n",
      "error estimates are averaged. From this we obtain an estimated prediction\n",
      "error curve as a function of the complexity parameter.\n",
      "Note that we have already divided these data into a training set of size\n",
      "67 and a test set of size 30. Cross-validation is applied to the training set,\n",
      "since selecting the shrinkage parameter is part of the training process. The\n",
      "test set is there to judge the performance of the selected model.\n",
      "The estimated prediction error curves are shown in Figure 3.7. Many of\n",
      "the curves are very ﬂat over large ranges near their minimum. Included\n",
      "are estimated standard error bands for each estimated error rate, based on\n",
      "the ten error estimates computed by cross-validation. We have used the\n",
      "“one-standard-error” rule—we pick the most parsimonious model within\n",
      "one standard error of the minimum (Section 7.10, page 244). Such a rule\n",
      "acknowledges the fact that the tradeoﬀ curve is estimated with error, and\n",
      "hence takes a conservative approach.\n",
      "Best-subset selection chose to use the two predictors lcvol andlweight .\n",
      "The last two lines of the table give the average prediction error (and its\n",
      "estimated standard error) over the test set.\n",
      "3.4 Shrinkage Methods\n",
      "By retaining a subset of the predictors and discarding the rest, subset selec-\n",
      "tion produces a model that is interpretable and has possibly lower predic-\n",
      "tion error than the full model. However, because it is a discrete process—\n",
      "variables are either retained or discarded—it often exhibits high variance,\n",
      "and so doesn’t reduce the prediction error of the full model. Shrinkage\n",
      "methods are more continuous, and don’t suﬀer as much from high\n",
      "variability.\n",
      "3.4.1 Ridge Regression\n",
      "Ridge regression shrinks the regression coeﬃcients by imposing a penalty\n",
      "on their size. The ridge coeﬃcients minimize a penalized residual sum of\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "62 3. Linear Methods for Regression\n",
      "Subset SizeCV Error\n",
      "0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\n",
      "•\n",
      "•••••••All Subsets\n",
      "Degrees of FreedomCV Error\n",
      "0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\n",
      "•\n",
      "•\n",
      "••••••Ridge Regression\n",
      "Shrinkage Factor sCV Error\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.6 0.8 1.0 1.2 1.4 1.6 1.8•\n",
      "•\n",
      "•\n",
      "••••••Lasso\n",
      "Number of DirectionsCV Error\n",
      "0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\n",
      "••••••••Principal Components Regression\n",
      "Number of  DirectionsCV Error\n",
      "0 2 4 6 80.6 0.8 1.0 1.2 1.4 1.6 1.8•\n",
      "••••••• •Partial Least Squares\n",
      "FIGURE 3.7. Estimated prediction error curves and their standard errors f or\n",
      "the various selection and shrinkage methods. Each curve is plo tted as a function\n",
      "of the corresponding complexity parameter for that method. The horizontal axis\n",
      "has been chosen so that the model complexity increases as we mov e from left to\n",
      "right. The estimates of prediction error and their standard er rors were obtained by\n",
      "tenfold cross-validation; full details are given in Section 7. 10. The least complex\n",
      "model within one standard error of the best is chosen, indicated b y the purple\n",
      "vertical broken lines.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.4 Shrinkage Methods 63\n",
      "TABLE 3.3. Estimated coeﬃcients and test error results, for diﬀerent subs et\n",
      "and shrinkage methods applied to the prostate data. The blank ent ries correspond\n",
      "to variables omitted.\n",
      "Term LS Best Subset Ridge Lasso PCR PLS\n",
      "Intercept 2.465 2.477 2.452 2.468 2.497 2.452\n",
      "lcavol 0.680 0.740 0.420 0.533 0.543 0.419\n",
      "lweight 0.263 0.316 0.238 0.169 0.289 0.344\n",
      "age −0.141 −0.046 −0.152 −0.026\n",
      "lbph 0.210 0.162 0.002 0.214 0.220\n",
      "svi 0.305 0.227 0.094 0.315 0.243\n",
      "lcp −0.288 0.000 −0.051 0.079\n",
      "gleason −0.021 0.040 0.232 0.011\n",
      "pgg45 0.267 0.133 −0.056 0.084\n",
      "Test Error 0.521 0.492 0.492 0.479 0.449 0.528\n",
      "Std Error 0.179 0.143 0.165 0.164 0.105 0.152\n",
      "squares,\n",
      "ˆβridge= argmin\n",
      "β{N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2+λp∑\n",
      "j=1β2\n",
      "j}\n",
      ". (3.41)\n",
      "Hereλ≥0 is a complexity parameter that controls the amount of shrink-\n",
      "age: the larger the value of λ, the greater the amount of shrinkage. The\n",
      "coeﬃcients are shrunk toward zero (and each other). The idea of penaliz-\n",
      "ing by the sum-of-squares of the parameters is also used in neural networks,\n",
      "where it is known as weight decay (Chapter 11).\n",
      "An equivalent way to write the ridge problem is\n",
      "ˆβridge= argmin\n",
      "βN∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      ",\n",
      "subject top∑\n",
      "j=1β2\n",
      "j≤t,(3.42)\n",
      "which makes explicit the size constraint on the parameters. There is a one-\n",
      "to-one correspondence between the parameters λin (3.41) and tin (3.42).\n",
      "When there are many correlated variables in a linear regression model,\n",
      "their coeﬃcients can become poorly determined and exhibit high variance.\n",
      "A wildly large positive coeﬃcient on one variable can be canceled by a\n",
      "similarly large negative coeﬃcient on its correlated cousin. By imposing a\n",
      "size constraint on the coeﬃcients, as in (3.42), this problem is alleviated.\n",
      "The ridge solutions are not equivariant under scaling of the inputs, and\n",
      "so one normally standardizes the inputs before solving (3.41). In addition,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "64 3. Linear Methods for Regression\n",
      "notice that the intercept β0has been left out of the penalty term. Penal-\n",
      "ization of the intercept would make the procedure depend on the origin\n",
      "chosen for Y; that is, adding a constant cto each of the targets yiwould\n",
      "not simply result in a shift of the predictions by the same amount c. It\n",
      "can be shown (Exercise 3.5) that the solution to (3.41) can be separated\n",
      "into two parts, after reparametrization using centered inputs: each xijgets\n",
      "replaced by xij−¯xj. We estimate β0by ¯y=1\n",
      "N∑N\n",
      "1yi. The remaining co-\n",
      "eﬃcients get estimated by a ridge regression without intercept, using the\n",
      "centered xij. Henceforth we assume that this centering has been done, so\n",
      "that the input matrix Xhasp(rather than p+ 1) columns.\n",
      "Writing the criterion in (3.41) in matrix form,\n",
      "RSS(λ) = (y−Xβ)T(y−Xβ) +λβTβ, (3.43)\n",
      "the ridge regression solutions are easily seen to be\n",
      "ˆβridge= (XTX+λI)−1XTy, (3.44)\n",
      "whereIis the p×pidentity matrix. Notice that with the choice of quadratic\n",
      "penalty βTβ, the ridge regression solution is again a linear function of\n",
      "y. The solution adds a positive constant to the diagonal of XTXbefore\n",
      "inversion. This makes the problem nonsingular, even if XTXis not of full\n",
      "rank, and was the main motivation for ridge regression when it was ﬁrst\n",
      "introduced in statistics (Hoerl and Kennard, 1970). Traditional descriptions\n",
      "of ridge regression start with deﬁnition (3.44). We choose to motivat e it via\n",
      "(3.41) and (3.42), as these provide insight into how it works.\n",
      "Figure 3.8 shows the ridge coeﬃcient estimates for the prostate can-\n",
      "cer example, plotted as functions of df( λ), the eﬀective degrees of freedom\n",
      "implied by the penalty λ(deﬁned in (3.50) on page 68). In the case of or-\n",
      "thonormal inputs, the ridge estimates are just a scaled version of the least\n",
      "squares estimates, that is, ˆβridge=ˆβ/(1 +λ).\n",
      "Ridge regression can also be derived as the mean or mode of a poste-\n",
      "rior distribution, with a suitably chosen prior distribution. In detail, sup-\n",
      "poseyi∼N(β0+xT\n",
      "iβ,σ2), and the parameters βjare each distributed as\n",
      "N(0,τ2), independently of one another. Then the (negative) log-posterior\n",
      "density of β, with τ2andσ2assumed known, is equal to the expression\n",
      "in curly braces in (3.41), with λ=σ2/τ2(Exercise 3.6). Thus the ridge\n",
      "estimate is the mode of the posterior distribution; since the distribution is\n",
      "Gaussian, it is also the posterior mean.\n",
      "Thesingular value decomposition (SVD) of the centered input matrix X\n",
      "gives us some additional insight into the nature of ridge regression. This de-\n",
      "composition is extremely useful in the analysis of many statistical metho ds.\n",
      "The SVD of the N×pmatrix Xhas the form\n",
      "X=UDVT. (3.45)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.4 Shrinkage Methods 65Coefficients\n",
      "0 2 4 6 8−0.2 0.0 0.2 0.4 0.6•\n",
      "•••••\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "••••••\n",
      "•lcavol\n",
      "••••••••••••••••••••••••\n",
      "•lweight\n",
      "•••••••••••••••••••••••••\n",
      "age••••••••••••••••••••••••\n",
      "•lbph••••••••••••••••••••••••\n",
      "•svi\n",
      "••••••••••••••••••••••••\n",
      "•\n",
      "lcp••••••••••••••••••••••••\n",
      "•gleason•\n",
      "•••••••••••••••••••••••\n",
      "•pgg45\n",
      "df(λ)\n",
      "FIGURE 3.8. Proﬁles of ridge coeﬃcients for the prostate cancer example, as\n",
      "the tuning parameter λis varied. Coeﬃcients are plotted versus df(λ), the eﬀective\n",
      "degrees of freedom. A vertical line is drawn at df = 5 .0, the value chosen by\n",
      "cross-validation.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "66 3. Linear Methods for Regression\n",
      "HereUandVareN×pandp×porthogonal matrices, with the columns\n",
      "ofUspanning the column space of X, and the columns of Vspanning the\n",
      "row space. Dis ap×pdiagonal matrix, with diagonal entries d1≥d2≥\n",
      "≤≤≤ ≥ dp≥0 called the singular values of X. If one or more values dj= 0,\n",
      "Xis singular.\n",
      "Using the singular value decomposition we can write the least squares\n",
      "ﬁtted vector as\n",
      "Xˆβls=X(XTX)−1XTy\n",
      "=UUTy, (3.46)\n",
      "after some simpliﬁcation. Note that UTyare the coordinates of ywith\n",
      "respect to the orthonormal basis U. Note also the similarity with (3.33);\n",
      "QandUare generally diﬀerent orthogonal bases for the column space of\n",
      "X(Exercise 3.8).\n",
      "Now the ridge solutions are\n",
      "Xˆβridge=X(XTX+λI)−1XTy\n",
      "=U D(D2+λI)−1D UTy\n",
      "=p∑\n",
      "j=1ujd2\n",
      "j\n",
      "d2\n",
      "j+λuT\n",
      "jy, (3.47)\n",
      "where the ujare the columns of U. Note that since λ≥0, we have d2\n",
      "j/(d2\n",
      "j+\n",
      "λ)≤1. Like linear regression, ridge regression computes the coordinates of\n",
      "ywith respect to the orthonormal basis U. It then shrinks these coordinates\n",
      "by the factors d2\n",
      "j/(d2\n",
      "j+λ). This means that a greater amount of shrinkage\n",
      "is applied to the coordinates of basis vectors with smaller d2\n",
      "j.\n",
      "What does a small value of d2\n",
      "jmean? The SVD of the centered matrix\n",
      "Xis another way of expressing the principal components of the variables\n",
      "inX. The sample covariance matrix is given by S=XTX/N, and from\n",
      "(3.45) we have\n",
      "XTX=VD2VT, (3.48)\n",
      "which is the eigen decomposition ofXTX(and of S, up to a factor N).\n",
      "The eigenvectors vj(columns of V) are also called the principal compo-\n",
      "nents (or Karhunen–Loeve) directions of X. The ﬁrst principal component\n",
      "direction v1has the property that z1=Xv1has the largest sample vari-\n",
      "ance amongst all normalized linear combinations of the columns of X. This\n",
      "sample variance is easily seen to be\n",
      "Var(z1) = Var( Xv1) =d2\n",
      "1\n",
      "N, (3.49)\n",
      "and in fact z1=Xv1=u1d1. The derived variable z1is called the ﬁrst\n",
      "principal component of X, and hence u1is the normalized ﬁrst principal\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.4 Shrinkage Methods 67\n",
      "-4 -2 0 2 4-4 -2 0 2 4ooo\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooLargest Principal\n",
      "Component\n",
      "Smallest Principal\n",
      "Component\n",
      "X1X2\n",
      "FIGURE 3.9. Principal components of some input data points. The largest prin-\n",
      "cipal component is the direction that maximizes the variance of t he projected data,\n",
      "and the smallest principal component minimizes that variance. Rid ge regression\n",
      "projects yonto these components, and then shrinks the coeﬃcients of the low–\n",
      "variance components more than the high-variance components.\n",
      "component. Subsequent principal components zjhave maximum variance\n",
      "d2\n",
      "j/N, subject to being orthogonal to the earlier ones. Conversely the last\n",
      "principal component has minimum variance. Hence the small singular val-\n",
      "uesdjcorrespond to directions in the column space of Xhaving small\n",
      "variance, and ridge regression shrinks these directions the most.\n",
      "Figure 3.9 illustrates the principal components of some data points in\n",
      "two dimensions. If we consider ﬁtting a linear surface over this domain\n",
      "(theY-axis is sticking out of the page), the conﬁguration of the data allow\n",
      "us to determine its gradient more accurately in the long direction than\n",
      "the short. Ridge regression protects against the potentially high variance\n",
      "of gradients estimated in the short directions. The implicit assumption is\n",
      "that the response will tend to vary most in the directions of high variance\n",
      "of the inputs. This is often a reasonable assumption, since predictors are\n",
      "often chosen for study because they vary with the response variable, but\n",
      "need not hold in general.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "68 3. Linear Methods for Regression\n",
      "In Figure 3.7 we have plotted the estimated prediction error versus the\n",
      "quantity\n",
      "df(λ) = tr[ X(XTX+λI)−1XT],\n",
      "= tr( Hλ)\n",
      "=p∑\n",
      "j=1d2\n",
      "j\n",
      "d2\n",
      "j+λ. (3.50)\n",
      "This monotone decreasing function of λis the eﬀective degrees of freedom\n",
      "of the ridge regression ﬁt. Usually in a linear-regression ﬁt with pvariables,\n",
      "the degrees-of-freedom of the ﬁt is p, the number of free parameters. The\n",
      "idea is that although all pcoeﬃcients in a ridge ﬁt will be non-zero, they\n",
      "are ﬁt in a restricted fashion controlled by λ. Note that df( λ) =pwhen\n",
      "λ= 0 (no regularization) and df( λ)→0 asλ→ ∞ . Of course there\n",
      "is always an additional one degree of freedom for the intercept, which was\n",
      "removed apriori . This deﬁnition is motivated in more detail in Section 3.4.4\n",
      "and Sections 7.4–7.6. In Figure 3.7 the minimum occurs at df( λ) = 5 .0.\n",
      "Table 3.3 shows that ridge regression reduces the test error of the full least\n",
      "squares estimates by a small amount.\n",
      "3.4.2 The Lasso\n",
      "The lasso is a shrinkage method like ridge, with subtle but important dif-\n",
      "ferences. The lasso estimate is deﬁned by\n",
      "ˆβlasso= argmin\n",
      "βN∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2\n",
      "subject top∑\n",
      "j=1|βj| ≤t. (3.51)\n",
      "Just as in ridge regression, we can re-parametrize the constant β0by stan-\n",
      "dardizing the predictors; the solution for ˆβ0is ¯y, and thereafter we ﬁt a\n",
      "model without an intercept (Exercise 3.5). In the signal processing litera-\n",
      "ture, the lasso is also known as basis pursuit (Chen et al., 1998).\n",
      "We can also write the lasso problem in the equivalent Lagrangian form\n",
      "ˆβlasso= argmin\n",
      "β{1\n",
      "2N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2+λp∑\n",
      "j=1|βj|}\n",
      ".(3.52)\n",
      "Notice the similarity to the ridge regression problem (3.42) or (3.41) : the\n",
      "L2ridge penalty∑p\n",
      "1β2\n",
      "jis replaced by the L1lasso penalty∑p\n",
      "1|βj|. This\n",
      "latter constraint makes the solutions nonlinear in the yi, and there is no\n",
      "closed form expression as in ridge regression. Computing the lasso solution\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.4 Shrinkage Methods 69\n",
      "is a quadratic programming problem, although we see in Section 3.4.4 that\n",
      "eﬃcient algorithms are available for computing the entire path of solutions\n",
      "asλis varied, with the same computational cost as for ridge regression.\n",
      "Because of the nature of the constraint, making tsuﬃciently small will\n",
      "cause some of the coeﬃcients to be exactly zero. Thus the lasso does a kind\n",
      "of continuous subset selection. If tis chosen larger than t0=∑p\n",
      "1|ˆβj|(where\n",
      "ˆβj=ˆβls\n",
      "j, the least squares estimates), then the lasso estimates are the ˆβj’s.\n",
      "On the other hand, for t=t0/2 say, then the least squares coeﬃcients are\n",
      "shrunk by about 50% on average. However, the nature of the shrinkage\n",
      "is not obvious, and we investigate it further in Section 3.4.4 below. Like\n",
      "the subset size in variable subset selection, or the penalty parameter in\n",
      "ridge regression, tshould be adaptively chosen to minimize an estimate of\n",
      "expected prediction error.\n",
      "In Figure 3.7, for ease of interpretation, we have plotted the lasso pre-\n",
      "diction error estimates versus the standardized parameter s=t/∑p\n",
      "1|ˆβj|.\n",
      "A value ˆ s≈0.36 was chosen by 10-fold cross-validation; this caused four\n",
      "coeﬃcients to be set to zero (ﬁfth column of Table 3.3). The resulting\n",
      "model has the second lowest test error, slightly lower than the full least\n",
      "squares model, but the standard errors of the test error estimates (last line\n",
      "of Table 3.3) are fairly large.\n",
      "Figure 3.10 shows the lasso coeﬃcients as the standardized tuning pa-\n",
      "rameter s=t/∑p\n",
      "1|ˆβj|is varied. At s= 1.0 these are the least squares\n",
      "estimates; they decrease to 0 as s→0. This decrease is not always strictly\n",
      "monotonic, although it is in this example. A vertical line is drawn at\n",
      "s= 0.36, the value chosen by cross-validation.\n",
      "3.4.3 Discussion: Subset Selection, Ridge Regression and t he\n",
      "Lasso\n",
      "In this section we discuss and compare the three approaches discussed so far\n",
      "for restricting the linear regression model: subset selection, ridge regression\n",
      "and the lasso.\n",
      "In the case of an orthonormal input matrix Xthe three procedures have\n",
      "explicit solutions. Each method applies a simple transformation to the leas t\n",
      "squares estimate ˆβj, as detailed in Table 3.4.\n",
      "Ridge regression does a proportional shrinkage. Lasso translates each\n",
      "coeﬃcient by a constant factor λ, truncating at zero. This is called “soft\n",
      "thresholding,” and is used in the context of wavelet-based smoothing in Sec-\n",
      "tion 5.9. Best-subset selection drops all variables with coeﬃcients smaller\n",
      "than the Mth largest; this is a form of “hard-thresholding.”\n",
      "Back to the nonorthogonal case; some pictures help understand their re-\n",
      "lationship. Figure 3.11 depicts the lasso (left) and ridge regression (righ t)\n",
      "when there are only two parameters. The residual sum of squares has ellip-\n",
      "tical contours, centered at the full least squares estimate. The constraint\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "70 3. Linear Methods for Regression\n",
      "0.0 0.2 0.4 0.6 0.8 1.0−0.2 0.0 0.2 0.4 0.6\n",
      "Shrinkage Factor sCoefficientslcavol\n",
      "lweight\n",
      "agelbphsvi\n",
      "lcpgleasonpgg45\n",
      "FIGURE 3.10. Proﬁles of lasso coeﬃcients, as the tuning parameter tis varied.\n",
      "Coeﬃcients are plotted versus s=t/Pp\n",
      "1|ˆβj|. A vertical line is drawn at s= 0.36,\n",
      "the value chosen by cross-validation. Compare Figure 3.8 on p age 65; the lasso\n",
      "proﬁles hit zero, while those for ridge do not. The proﬁles are pi ece-wise linear,\n",
      "and so are computed only at the points displayed; see Section 3.4. 4 for details.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.4 Shrinkage Methods 71\n",
      "TABLE 3.4. Estimators of βjin the case of orthonormal columns of X.Mandλ\n",
      "are constants chosen by the corresponding techniques; signdenotes the sign of its\n",
      "argument ( ±1), and x+denotes “positive part” of x. Below the table, estimators\n",
      "are shown by broken red lines. The 45◦line in gray shows the unrestricted estimate\n",
      "for reference.\n",
      "Estimator Formula\n",
      "Best subset (size M)ˆβj≤I(|ˆβj| ≥ |ˆβ(M)|)\n",
      "Ridge ˆβj/(1 +λ)\n",
      "Lasso sign( ˆβj)(|ˆβj| −λ)+\n",
      "(0,0) (0,0) (0,0)|ˆβ(M)|λBest Subset Ridge Lasso\n",
      "β^β^ 2. . β\n",
      "1β2\n",
      "β1β\n",
      "FIGURE 3.11. Estimation picture for the lasso (left) and ridge regression\n",
      "(right). Shown are contours of the error and constraint functions. T he solid blue\n",
      "areas are the constraint regions |β1|+|β2| ≤tandβ2\n",
      "1+β2\n",
      "2≤t2, respectively,\n",
      "while the red ellipses are the contours of the least squares er ror function.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "72 3. Linear Methods for Regression\n",
      "region for ridge regression is the disk β2\n",
      "1+β2\n",
      "2≤t, while that for lasso is\n",
      "the diamond |β1|+|β2| ≤t. Both methods ﬁnd the ﬁrst point where the\n",
      "elliptical contours hit the constraint region. Unlike the disk, the diamond\n",
      "has corners; if the solution occurs at a corner, then it has one parameter\n",
      "βjequal to zero. When p >2, the diamond becomes a rhomboid, and has\n",
      "many corners, ﬂat edges and faces; there are many more opportunities for\n",
      "the estimated parameters to be zero.\n",
      "We can generalize ridge regression and the lasso, and view them as Bayes\n",
      "estimates. Consider the criterion\n",
      "˜β= argmin\n",
      "β{N∑\n",
      "i=1(\n",
      "yi−β0−p∑\n",
      "j=1xijβj)2+λp∑\n",
      "j=1|βj|q}\n",
      "(3.53)\n",
      "forq≥0. The contours of constant value of∑\n",
      "j|βj|qare shown in Fig-\n",
      "ure 3.12, for the case of two inputs.\n",
      "Thinking of |βj|qas the log-prior density for βj, these are also the equi-\n",
      "contours of the prior distribution of the parameters. The value q= 0 corre-\n",
      "sponds to variable subset selection, as the penalty simply counts the number\n",
      "of nonzero parameters; q= 1 corresponds to the lasso, while q= 2 to ridge\n",
      "regression. Notice that for q≤1, the prior is not uniform in direction, but\n",
      "concentrates more mass in the coordinate directions. The prior correspond-\n",
      "ing to the q= 1 case is an independent double exponential (or Laplace)\n",
      "distribution for each input, with density (1 /2τ)exp(−|β|/τ) and τ= 1/λ.\n",
      "The case q= 1 (lasso) is the smallest qsuch that the constraint region\n",
      "is convex; non-convex constraint regions make the optimization problem\n",
      "more diﬃcult.\n",
      "In this view, the lasso, ridge regression and best subset selection are\n",
      "Bayes estimates with diﬀerent priors. Note, however, that they are derived\n",
      "as posterior modes, that is, maximizers of the posterior. It is more com mon\n",
      "to use the mean of the posterior as the Bayes estimate. Ridge regression is\n",
      "also the posterior mean, but the lasso and best subset selection are not.\n",
      "Looking again at the criterion (3.53), we might try using other values\n",
      "ofqbesides 0, 1, or 2. Although one might consider estimating qfrom\n",
      "the data, our experience is that it is not worth the eﬀort for the extra\n",
      "variance incurred. Values of q∈(1,2) suggest a compromise between the\n",
      "lasso and ridge regression. Although this is the case, with q >1,|βj|qis\n",
      "diﬀerentiable at 0, and so does not share the ability of lasso ( q= 1) for\n",
      "q= 4 q= 2 q= 1 q= 0.5 q= 0.1\n",
      "FIGURE 3.12. Contours of constant value ofP\n",
      "j|βj|qfor given values of q.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.4 Shrinkage Methods 73\n",
      "q= 1.2 α= 0.2\n",
      "Lq Elastic Net\n",
      "FIGURE 3.13. Contours of constant value ofP\n",
      "j|βj|qforq= 1.2(left plot),\n",
      "and the elastic-net penaltyP\n",
      "j(αβ2\n",
      "j+(1−α)|βj|)forα= 0.2(right plot). Although\n",
      "visually very similar, the elastic-net has sharp (non-diﬀerent iable) corners, while\n",
      "theq= 1.2penalty does not.\n",
      "setting coeﬃcients exactly to zero. Partly for this reason as well as for\n",
      "computational tractability, Zou and Hastie (2005) introduced the elastic-\n",
      "netpenalty\n",
      "λp∑\n",
      "j=1(\n",
      "αβ2\n",
      "j+ (1−α)|βj|)\n",
      ", (3.54)\n",
      "a diﬀerent compromise between ridge and lasso. Figure 3.13 compares the\n",
      "Lqpenalty with q= 1.2 and the elastic-net penalty with α= 0.2; it is\n",
      "hard to detect the diﬀerence by eye. The elastic-net selects variables like\n",
      "the lasso, and shrinks together the coeﬃcients of correlated predictors like\n",
      "ridge. It also has considerable computational advantages over the Lqpenal-\n",
      "ties. We discuss the elastic-net further in Section 18.4.\n",
      "3.4.4 Least Angle Regression\n",
      "Least angle regression (LAR) is a relative newcomer (Efron et al., 2004) ,\n",
      "and can be viewed as a kind of “democratic” version of forward stepwise\n",
      "regression (Section 3.3.2). As we will see, LAR is intimately connected\n",
      "with the lasso, and in fact provides an extremely eﬃcient algorithm for\n",
      "computing the entire lasso path as in Figure 3.10.\n",
      "Forward stepwise regression builds a model sequentially, adding one vari-\n",
      "able at a time. At each step, it identiﬁes the best variable to include in the\n",
      "active set , and then updates the least squares ﬁt to include all the active\n",
      "variables.\n",
      "Least angle regression uses a similar strategy, but only enters “as much”\n",
      "of a predictor as it deserves. At the ﬁrst step it identiﬁes the variable\n",
      "most correlated with the response. Rather than ﬁt this variable completely,\n",
      "LAR moves the coeﬃcient of this variable continuously toward its least-\n",
      "squares value (causing its correlation with the evolving residual to decrease\n",
      "in absolute value). As soon as another variable “catches up” in terms of\n",
      "correlation with the residual, the process is paused. The second variable\n",
      "then joins the active set, and their coeﬃcients are moved together in a way\n",
      "that keeps their correlations tied and decreasing. This process is continued\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "74 3. Linear Methods for Regression\n",
      "until all the variables are in the model, and ends at the full least-squares\n",
      "ﬁt. Algorithm 3.2 provides the details. The termination condition in step 5\n",
      "requires some explanation. If p > N −1, the LAR algorithm reaches a zero\n",
      "residual solution after N−1 steps (the −1 is because we have centered the\n",
      "data).\n",
      "Algorithm 3.2 Least Angle Regression.\n",
      "1. Standardize the predictors to have mean zero and unit norm. Start\n",
      "with the residual r=y−¯y,β1,β2,... ,β p= 0.\n",
      "2. Find the predictor xjmost correlated with r.\n",
      "3. Move βjfrom 0 towards its least-squares coeﬃcient ⟨xj,r⟩, until some\n",
      "other competitor xkhas as much correlation with the current residual\n",
      "as does xj.\n",
      "4. Move βjandβkin the direction deﬁned by their joint least squares\n",
      "coeﬃcient of the current residual on ( xj,xk), until some other com-\n",
      "petitor xlhas as much correlation with the current residual.\n",
      "5. Continue in this way until all ppredictors have been entered. After\n",
      "min(N−1,p) steps, we arrive at the full least-squares solution.\n",
      "Suppose Akis the active set of variables at the beginning of the kth\n",
      "step, and let βAkbe the coeﬃcient vector for these variables at this step;\n",
      "there will be k−1 nonzero values, and the one just entered will be zero. If\n",
      "rk=y−XAkβAkis the current residual, then the direction for this step is\n",
      "δk= (XT\n",
      "AkXAk)−1XT\n",
      "Akrk. (3.55)\n",
      "The coeﬃcient proﬁle then evolves as βAk(α) =βAk+α≤δk. Exercise 3.23\n",
      "veriﬁes that the directions chosen in this fashion do what is claimed: keep\n",
      "the correlations tied and decreasing. If the ﬁt vector at the beginning of\n",
      "this step is ˆfk, then it evolves as ˆfk(α) =ˆfk+α≤uk, where uk=XAkδk\n",
      "is the new ﬁt direction. The name “least angle” arises from a geometrical\n",
      "interpretation of this process; ukmakes the smallest (and equal) angle\n",
      "with each of the predictors in Ak(Exercise 3.24). Figure 3.14 shows the\n",
      "absolute correlations decreasing and joining ranks with each step of the\n",
      "LAR algorithm, using simulated data.\n",
      "By construction the coeﬃcients in LAR change in a piecewise linear fash-\n",
      "ion. Figure 3.15 [left panel] shows the LAR coeﬃcient proﬁle evolving as a\n",
      "function of their L1arc length2. Note that we do not need to take small\n",
      "2TheL1arc-length of a diﬀerentiable curve β(s) fors∈[0, S] is given by TV( β, S) =RS\n",
      "0||˙β(s)||1ds,where ˙β(s) =∂β(s)/∂s. For the piecewise-linear LAR coeﬃcient proﬁle,\n",
      "this amounts to summing the L1norms of the changes in coeﬃcients from step to step.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.4 Shrinkage Methods 75\n",
      "0 5 10 150.0 0.1 0.2 0.3 0.4v2 v6 v4 v5 v3 v1\n",
      "L1Arc LengthAbsolute Correlations\n",
      "FIGURE 3.14. Progression of the absolute correlations during each step of t he\n",
      "LAR procedure, using a simulated data set with six predictors . The labels at the\n",
      "top of the plot indicate which variables enter the active set at each step. The step\n",
      "length are measured in units of L1arc length.\n",
      "0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Least Angle Regression\n",
      "0 5 10 15−1.5 −1.0 −0.5 0.0 0.5Lasso\n",
      "L1Arc Length L1Arc Length\n",
      "CoeﬃcientsCoeﬃcients\n",
      "FIGURE 3.15. Left panel shows the LAR coeﬃcient proﬁles on the simulated\n",
      "data, as a function of the L1arc length. The right panel shows the Lasso proﬁle.\n",
      "They are identical until the dark-blue coeﬃcient crosses zero a t an arc length of\n",
      "about 18.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "76 3. Linear Methods for Regression\n",
      "steps and recheck the correlations in step 3; using knowledge of the covari-\n",
      "ance of the predictors and the piecewise linearity of the algorithm, we can\n",
      "work out the exact step length at the beginning of each step (Exercise 3.25).\n",
      "The right panel of Figure 3.15 shows the lasso coeﬃcient proﬁles on the\n",
      "same data. They are almost identical to those in the left panel, and diﬀer\n",
      "for the ﬁrst time when the blue coeﬃcient passes back through zero. For the\n",
      "prostate data, the LAR coeﬃcient proﬁle turns out to be identical to the\n",
      "lasso proﬁle in Figure 3.10, which never crosses zero. These observations\n",
      "lead to a simple modiﬁcation of the LAR algorithm that gives the entire\n",
      "lasso path, which is also piecewise-linear.\n",
      "Algorithm 3.2a Least Angle Regression: Lasso Modiﬁcation .\n",
      "4a. If a non-zero coeﬃcient hits zero, drop its variable from the active set\n",
      "of variables and recompute the current joint least squares direction.\n",
      "The LAR(lasso) algorithm is extremely eﬃcient, requiring the same order\n",
      "of computation as that of a single least squares ﬁt using the ppredictors.\n",
      "Least angle regression always takes psteps to get to the full least squares\n",
      "estimates. The lasso path can have more than psteps, although the two\n",
      "are often quite similar. Algorithm 3.2 with the lasso modiﬁcation 3. 2a is\n",
      "an eﬃcient way of computing the solution to any lasso problem, especially\n",
      "when p≫N. Osborne et al. (2000a) also discovered a piecewise-linear path\n",
      "for computing the lasso, which they called a homotopy algorithm.\n",
      "We now give a heuristic argument for why these procedures are so similar.\n",
      "Although the LAR algorithm is stated in terms of correlations, if the input\n",
      "features are standardized, it is equivalent and easier to work with inner-\n",
      "products. Suppose Ais the active set of variables at some stage in the\n",
      "algorithm, tied in their absolute inner-product with the current residuals\n",
      "y−Xβ. We can express this as\n",
      "xT\n",
      "j(y−Xβ) =γ≤sj,∀j∈ A (3.56)\n",
      "where sj∈ {−1,1}indicates the sign of the inner-product, and γis the\n",
      "common value. Also |xT\n",
      "k(y−Xβ)| ≤γ∀k̸∈ A. Now consider the lasso\n",
      "criterion (3.52), which we write in vector form\n",
      "R(β) =1\n",
      "2||y−Xβ||2\n",
      "2+λ||β||1. (3.57)\n",
      "LetBbe the active set of variables in the solution for a given value of λ.\n",
      "For these variables R(β) is diﬀerentiable, and the stationarity conditions\n",
      "give\n",
      "xT\n",
      "j(y−Xβ) =λ≤sign(βj),∀j∈ B (3.58)\n",
      "Comparing (3.58) with (3.56), we see that they are identical only if the\n",
      "sign of βjmatches the sign of the inner product. That is why the LAR\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.4 Shrinkage Methods 77\n",
      "algorithm and lasso start to diﬀer when an active coeﬃcient passes through\n",
      "zero; condition (3.58) is violated for that variable, and it is kicked out o f the\n",
      "active set B. Exercise 3.23 shows that these equations imply a piecewise-\n",
      "linear coeﬃcient proﬁle as λdecreases. The stationarity conditions for the\n",
      "non-active variables require that\n",
      "|xT\n",
      "k(y−Xβ)| ≤λ,∀k̸∈ B, (3.59)\n",
      "which again agrees with the LAR algorithm.\n",
      "Figure 3.16 compares LAR and lasso to forward stepwise and stagewise\n",
      "regression. The setup is the same as in Figure 3.6 on page 59, except here\n",
      "N= 100 here rather than 300, so the problem is more diﬃcult. We see\n",
      "that the more aggressive forward stepwise starts to overﬁt quite earl y (well\n",
      "before the 10 true variables can enter the model), and ultimately performs\n",
      "worse than the slower forward stagewise regression. The behavior of LAR\n",
      "and lasso is similar to that of forward stagewise regression. Increment al\n",
      "forward stagewise is similar to LAR and lasso, and is described in Sec-\n",
      "tion 3.8.1.\n",
      "Degrees-of-Freedom Formula for LAR and Lasso\n",
      "Suppose that we ﬁt a linear model via the least angle regression procedure,\n",
      "stopping at some number of steps k < p, or equivalently using a lasso bound\n",
      "tthat produces a constrained version of the full least squares ﬁt. How many\n",
      "parameters, or “degrees of freedom” have we used?\n",
      "Consider ﬁrst a linear regression using a subset of kfeatures. If this subset\n",
      "is prespeciﬁed in advance without reference to the training data, then the\n",
      "degrees of freedom used in the ﬁtted model is deﬁned to be k. Indeed, in\n",
      "classical statistics, the number of linearly independent parameters is what\n",
      "is meant by “degrees of freedom.” Alternatively, suppose that we carry out\n",
      "a best subset selection to determine the “optimal” set of kpredictors. Then\n",
      "the resulting model has kparameters, but in some sense we have used up\n",
      "more than kdegrees of freedom.\n",
      "We need a more general deﬁnition for the eﬀective degrees of freedom of\n",
      "an adaptively ﬁtted model. We deﬁne the degrees of freedom of the ﬁtted\n",
      "vector ˆy= (ˆy1,ˆy2,... ,ˆyN) as\n",
      "df(ˆy) =1\n",
      "σ2N∑\n",
      "i=1Cov(ˆyi,yi). (3.60)\n",
      "Here Cov(ˆ yi,yi) refers to the sampling covariance between the predicted\n",
      "value ˆ yiand its corresponding outcome value yi. This makes intuitive sense:\n",
      "the harder that we ﬁt to the data, the larger this covariance and hence\n",
      "df(ˆy). Expression (3.60) is a useful notion of degrees of freedom, one that\n",
      "can be applied to any model prediction ˆy. This includes models that are\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "78 3. Linear Methods for Regression\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.55 0.60 0.65Forward Stepwise\n",
      "LAR\n",
      "Lasso\n",
      "Forward Stagewise\n",
      "Incremental Forward StagewiseE||ˆβ(k)−β||2\n",
      "Fraction of L1arc-length\n",
      "FIGURE 3.16. Comparison of LAR and lasso with forward stepwise, forward\n",
      "stagewise (FS) and incremental forward stagewise (FS 0) regression. The setup\n",
      "is the same as in Figure 3.6, except N= 100 here rather than 300. Here the\n",
      "slower FS regression ultimately outperforms forward stepw ise. LAR and lasso\n",
      "show similar behavior to FS and FS 0. Since the procedures take diﬀerent numbers\n",
      "of steps (across simulation replicates and methods), we plot the MSE as a function\n",
      "of the fraction of total L1arc-length toward the least-squares ﬁt.\n",
      "adaptively ﬁtted to the training data. This deﬁnition is motivated and\n",
      "discussed further in Sections 7.4–7.6.\n",
      "Now for a linear regression with kﬁxed predictors, it is easy to show\n",
      "that df( ˆy) =k. Likewise for ridge regression, this deﬁnition leads to the\n",
      "closed-form expression (3.50) on page 68: df( ˆy) = tr( Sλ). In both these\n",
      "cases, (3.60) is simple to evaluate because the ﬁt ˆy=Hλyis linear in y.\n",
      "If we think about deﬁnition (3.60) in the context of a best subset selection\n",
      "of size k, it seems clear that df( ˆy) will be larger than k, and this can be\n",
      "veriﬁed by estimating Cov(ˆ yi,yi)/σ2directly by simulation. However there\n",
      "is no closed form method for estimating df( ˆy) for best subset selection.\n",
      "For LAR and lasso, something magical happens. These techniques are\n",
      "adaptive in a smoother way than best subset selection, and hence estimation\n",
      "of degrees of freedom is more tractable. Speciﬁcally it can be shown that\n",
      "after the kth step of the LAR procedure, the eﬀective degrees of freedom of\n",
      "the ﬁt vector is exactly k. Now for the lasso, the (modiﬁed) LAR procedure\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.5 Methods Using Derived Input Directions 79\n",
      "often takes more than psteps, since predictors can drop out. Hence the\n",
      "deﬁnition is a little diﬀerent; for the lasso, at any stage df( ˆy) approximately\n",
      "equals the number of predictors in the model. While this approximation\n",
      "works reasonably well anywhere in the lasso path, for each kit works best\n",
      "at the lastmodel in the sequence that contains kpredictors. A detailed\n",
      "study of the degrees of freedom for the lasso may be found in Zou et al.\n",
      "(2007).\n",
      "3.5 Methods Using Derived Input Directions\n",
      "In many situations we have a large number of inputs, often very correlated.\n",
      "The methods in this section produce a small number of linear combinations\n",
      "Zm, m= 1,... ,M of the original inputs Xj, and the Zmare then used in\n",
      "place of the Xjas inputs in the regression. The methods diﬀer in how the\n",
      "linear combinations are constructed.\n",
      "3.5.1 Principal Components Regression\n",
      "In this approach the linear combinations Zmused are the principal com-\n",
      "ponents as deﬁned in Section 3.4.1 above.\n",
      "Principal component regression forms the derived input columns zm=\n",
      "Xvm, and then regresses yonz1,z2,... ,zMfor some M≤p. Since the zm\n",
      "are orthogonal, this regression is just a sum of univariate regressions:\n",
      "ˆypcr\n",
      "(M)= ¯y1+M∑\n",
      "m=1ˆθmzm, (3.61)\n",
      "where ˆθm=⟨zm,y⟩/⟨zm,zm⟩. Since the zmare each linear combinations\n",
      "of the original xj, we can express the solution (3.61) in terms of coeﬃcients\n",
      "of thexj(Exercise 3.13):\n",
      "ˆβpcr(M) =M∑\n",
      "m=1ˆθmvm. (3.62)\n",
      "As with ridge regression, principal components depend on the scaling of\n",
      "the inputs, so typically we ﬁrst standardize them. Note that if M=p, we\n",
      "would just get back the usual least squares estimates, since the columns of\n",
      "Z=UDspan the column space of X. ForM < p we get a reduced regres-\n",
      "sion. We see that principal components regression is very similar to ridge\n",
      "regression: both operate via the principal components of the input ma-\n",
      "trix. Ridge regression shrinks the coeﬃcients of the principal components\n",
      "(Figure 3.17), shrinking more depending on the size of the corresponding\n",
      "eigenvalue; principal components regression discards the p−Msmallest\n",
      "eigenvalue components. Figure 3.17 illustrates this.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "80 3. Linear Methods for Regression\n",
      "IndexShrinkage Factor\n",
      "2 4 6 80.0 0.2 0.4 0.6 0.8 1.0•\n",
      "••\n",
      "••••\n",
      "•• • • • • • •\n",
      "• •ridge\n",
      "pcr\n",
      "FIGURE 3.17. Ridge regression shrinks the regression coeﬃcients of the pri n-\n",
      "cipal components, using shrinkage factors d2\n",
      "j/(d2\n",
      "j+λ)as in (3.47). Principal\n",
      "component regression truncates them. Shown are the shrinkage and t runcation\n",
      "patterns corresponding to Figure 3.7, as a function of the princip al component\n",
      "index.\n",
      "In Figure 3.7 we see that cross-validation suggests seven terms; the re-\n",
      "sulting model has the lowest test error in Table 3.3.\n",
      "3.5.2 Partial Least Squares\n",
      "This technique also constructs a set of linear combinations of the inputs\n",
      "for regression, but unlike principal components regression it uses y(in ad-\n",
      "dition to X) for this construction. Like principal component regression,\n",
      "partial least squares (PLS) is not scale invariant, so we assume that eac h\n",
      "xjis standardized to have mean 0 and variance 1. PLS begins by com-\n",
      "puting ˆ ϕ1j=⟨xj,y⟩for each j. From this we construct the derived input\n",
      "z1=∑\n",
      "jˆϕ1jxj, which is the ﬁrst partial least squares direction. Hence\n",
      "in the construction of each zm, the inputs are weighted by the strength\n",
      "of their univariate eﬀect on y3. The outcome yis regressed on z1giving\n",
      "coeﬃcient ˆθ1, and then we orthogonalize x1,... ,xpwith respect to z1. We\n",
      "continue this process, until M≤pdirections have been obtained. In this\n",
      "manner, partial least squares produces a sequence of derived, orthogonal\n",
      "inputs or directions z1,z2,... ,zM. As with principal-component regres-\n",
      "sion, if we were to construct all M=pdirections, we would get back a\n",
      "solution equivalent to the usual least squares estimates; using M < p di-\n",
      "rections produces a reduced regression. The procedure is described fully in\n",
      "Algorithm 3.3.\n",
      "3Since the xjare standardized, the ﬁrst directions ˆ ϕ1jare the univariate regression\n",
      "coeﬃcients (up to an irrelevant constant); this is not the ca se for subsequent directions.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.5 Methods Using Derived Input Directions 81\n",
      "Algorithm 3.3 Partial Least Squares.\n",
      "1. Standardize each xjto have mean zero and variance one. Set ˆy(0)=\n",
      "¯y1, andx(0)\n",
      "j=xj, j= 1,... ,p .\n",
      "2. For m= 1,2,... ,p\n",
      "(a)zm=∑p\n",
      "j=1ˆϕmjx(m−1)\n",
      "j, where ˆ ϕmj=⟨x(m−1)\n",
      "j,y⟩.\n",
      "(b)ˆθm=⟨zm,y⟩/⟨zm,zm⟩.\n",
      "(c)ˆy(m)=ˆy(m−1)+ˆθmzm.\n",
      "(d) Orthogonalize each x(m−1)\n",
      "j with respect to zm:x(m)\n",
      "j=x(m−1)\n",
      "j−\n",
      "[⟨zm,x(m−1)\n",
      "j⟩/⟨zm,zm⟩]zm,j= 1,2,... ,p .\n",
      "3. Output the sequence of ﬁtted vectors {ˆy(m)}p\n",
      "1. Since the {zℓ}m\n",
      "1are\n",
      "linear in the original xj, so is ˆy(m)=Xˆβpls(m). These linear coeﬃ-\n",
      "cients can be recovered from the sequence of PLS transformations.\n",
      "In the prostate cancer example, cross-validation chose M= 2 PLS direc-\n",
      "tions in Figure 3.7. This produced the model given in the rightmost column\n",
      "of Table 3.3.\n",
      "What optimization problem is partial least squares solving? Since it uses\n",
      "the response yto construct its directions, its solution path is a nonlinear\n",
      "function of y. It can be shown (Exercise 3.15) that partial least squares\n",
      "seeks directions that have high variance andhave high correlation with the\n",
      "response, in contrast to principal components regression which keys only\n",
      "on high variance (Stone and Brooks, 1990; Frank and Friedman, 1993). In\n",
      "particular, the mth principal component direction vmsolves:\n",
      "max αVar(Xα) (3.63)\n",
      "subject to ||α||= 1, αTSvℓ= 0, ℓ= 1,... ,m −1,\n",
      "whereSis the sample covariance matrix of the xj. The conditions αTSvℓ=\n",
      "0 ensures that zm=Xαis uncorrelated with all the previous linear com-\n",
      "binations zℓ=Xvℓ. The mth PLS direction ˆ ϕmsolves:\n",
      "max αCorr2(y,Xα)Var(Xα) (3.64)\n",
      "subject to ||α||= 1, αTSˆϕℓ= 0, ℓ= 1,... ,m −1.\n",
      "Further analysis reveals that the variance aspect tends to dominate, and\n",
      "so partial least squares behaves much like ridge regression and principal\n",
      "components regression. We discuss this further in the next section.\n",
      "If the input matrix Xis orthogonal, then partial least squares ﬁnds the\n",
      "least squares estimates after m= 1 steps. Subsequent steps have no eﬀect\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "82 3. Linear Methods for Regression\n",
      "since the ˆ ϕmjare zero for m >1 (Exercise 3.14). It can also be shown that\n",
      "the sequence of PLS coeﬃcients for m= 1,2,... ,p represents the conjugate\n",
      "gradient sequence for computing the least squares solutions (Exercise 3.18).\n",
      "3.6 Discussion: A Comparison of the Selection and\n",
      "Shrinkage Methods\n",
      "There are some simple settings where we can understand better the rela-\n",
      "tionship between the diﬀerent methods described above. Consider an exam-\n",
      "ple with two correlated inputs X1andX2, with correlation ρ. We assume\n",
      "that the true regression coeﬃcients are β1= 4 and β2= 2. Figure 3.18\n",
      "shows the coeﬃcient proﬁles for the diﬀerent methods, as their tuning pa-\n",
      "rameters are varied. The top panel has ρ= 0.5, the bottom panel ρ=−0.5.\n",
      "The tuning parameters for ridge and lasso vary over a continuous range,\n",
      "while best subset, PLS and PCR take just two discrete steps to the least\n",
      "squares solution. In the top panel, starting at the origin, ridge regression\n",
      "shrinks the coeﬃcients together until it ﬁnally converges to least squares.\n",
      "PLS and PCR show similar behavior to ridge, although are discrete and\n",
      "more extreme. Best subset overshoots the solution and then backtracks.\n",
      "The behavior of the lasso is intermediate to the other methods. When the\n",
      "correlation is negative (lower panel), again PLS and PCR roughly track\n",
      "the ridge path, while all of the methods are more similar to one another.\n",
      "It is interesting to compare the shrinkage behavior of these diﬀerent\n",
      "methods. Recall that ridge regression shrinks all directions, but shrinks\n",
      "low-variance directions more. Principal components regression leaves M\n",
      "high-variance directions alone, and discards the rest. Interestingly, it can\n",
      "be shown that partial least squares also tends to shrink the low-variance\n",
      "directions, but can actually inﬂate some of the higher variance directions.\n",
      "This can make PLS a little unstable, and cause it to have slightly higher\n",
      "prediction error compared to ridge regression. A full study is given in Frank\n",
      "and Friedman (1993). These authors conclude that for minimizing predic-\n",
      "tion error, ridge regression is generally preferable to variable subset selec-\n",
      "tion, principal components regression and partial least squares. However\n",
      "the improvement over the latter two methods was only slight.\n",
      "To summarize, PLS, PCR and ridge regression tend to behave similarly.\n",
      "Ridge regression may be preferred because it shrinks smoothly, rather than\n",
      "in discrete steps. Lasso falls somewhere between ridge regression and best\n",
      "subset regression, and enjoys some of the properties of each.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.6 Discussion: A Comparison of the Selection and Shrinkage Methods 83\n",
      "0 1 2 3 4 5 6-1 0 1 2 3Least Squares\n",
      "0Ridge\n",
      "Lasso\n",
      "Best SubsetPLS PCR\n",
      "•\n",
      "0 1 2 3 4 5 6-1 0 1 2 3Least Squares\n",
      "Ridge\n",
      "Best Subset\n",
      "PLS\n",
      "PCRLasso•\n",
      "0ρ= 0.5\n",
      "ρ=−0.5\n",
      "β1β1β2 β2\n",
      "FIGURE 3.18. Coeﬃcient proﬁles from diﬀerent methods for a simple problem:\n",
      "two inputs with correlation ±0.5, and the true regression coeﬃcients β= (4,2).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "84 3. Linear Methods for Regression\n",
      "3.7 Multiple Outcome Shrinkage and Selection\n",
      "As noted in Section 3.2.4, the least squares estimates in a multiple-output\n",
      "linear model are simply the individual least squares estimates for each of\n",
      "the outputs.\n",
      "To apply selection and shrinkage methods in the multiple output case,\n",
      "one could apply a univariate technique individually to each outcome or si-\n",
      "multaneously to all outcomes. With ridge regression, for example, we could\n",
      "apply formula (3.44) to each of the Kcolumns of the outcome matrix Y,\n",
      "using possibly diﬀerent parameters λ, or apply it to all columns using the\n",
      "same value of λ. The former strategy would allow diﬀerent amounts of\n",
      "regularization to be applied to diﬀerent outcomes but require estimation\n",
      "ofkseparate regularization parameters λ1,... ,λ k, while the latter would\n",
      "permit all koutputs to be used in estimating the sole regularization pa-\n",
      "rameter λ.\n",
      "Other more sophisticated shrinkage and selection strategies that exploit\n",
      "correlations in the diﬀerent responses can be helpful in the multiple output\n",
      "case. Suppose for example that among the outputs we have\n",
      "Yk=f(X) +εk (3.65)\n",
      "Yℓ=f(X) +εℓ; (3.66)\n",
      "i.e., (3.65) and (3.66) share the same structural part f(X) in their models.\n",
      "It is clear in this case that we should pool our observations on YkandYl\n",
      "to estimate the common f.\n",
      "Combining responses is at the heart of canonical correlation analysis\n",
      "(CCA), a data reduction technique developed for the multiple output case.\n",
      "Similar to PCA, CCA ﬁnds a sequence of uncorrelated linear combina-\n",
      "tionsXvm, m= 1,... ,M of the xj, and a corresponding sequence of\n",
      "uncorrelated linear combinations Yumof the responses yk, such that the\n",
      "correlations\n",
      "Corr2(Yum,Xvm) (3.67)\n",
      "are successively maximized. Note that at most M= min( K,p) directions\n",
      "can be found. The leading canonical response variates are those linear com-\n",
      "binations (derived responses) best predicted by the xj; in contrast, the\n",
      "trailing canonical variates can be poorly predicted by the xj, and are can-\n",
      "didates for being dropped. The CCA solution is computed using a general-\n",
      "ized SVD of the sample cross-covariance matrix YTX/N(assuming Yand\n",
      "Xare centered; Exercise 3.20).\n",
      "Reduced-rank regression (Izenman, 1975; van der Merwe and Zidek, 1980)\n",
      "formalizes this approach in terms of a regression model that explicitly pool s\n",
      "information. Given an error covariance Cov( ε) =Σ, we solve the following\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.7 Multiple Outcome Shrinkage and Selection 85\n",
      "restricted multivariate regression problem:\n",
      "ˆBrr(m) = argmin\n",
      "rank(B)=mN∑\n",
      "i=1(yi−BTxi)TΣ−1(yi−BTxi). (3.68)\n",
      "WithΣreplaced by the estimate YTY/N, one can show (Exercise 3.21)\n",
      "that the solution is given by a CCA of YandX:\n",
      "ˆBrr(m) =ˆBUmU−\n",
      "m, (3.69)\n",
      "whereUmis the K×msub-matrix of Uconsisting of the ﬁrst mcolumns,\n",
      "andUis the K×Mmatrix of leftcanonical vectors u1,u2,... ,u M.U−\n",
      "m\n",
      "is its generalized inverse. Writing the solution as\n",
      "ˆBrr(M) = (XTX)−1XT(YU m)U−\n",
      "m, (3.70)\n",
      "we see that reduced-rank regression performs a linear regression on the\n",
      "pooled response matrix YU m, and then maps the coeﬃcients (and hence\n",
      "the ﬁts as well) back to the original response space. The reduced-rank ﬁts\n",
      "are given by\n",
      "ˆYrr(m) =X(XTX)−1XTYU mU−\n",
      "m\n",
      "=HYP m,(3.71)\n",
      "where His the usual linear regression projection operator, and Pmis the\n",
      "rank-mCCA response projection operator. Although a better estimate of\n",
      "Σwould be ( Y−XˆB)T(Y−XˆB)/(N−pK), one can show that the solution\n",
      "remains the same (Exercise 3.22).\n",
      "Reduced-rank regression borrows strength among responses by truncat-\n",
      "ing the CCA. Breiman and Friedman (1997) explored with some success\n",
      "shrinkage of the canonical variates between XandY, a smooth version of\n",
      "reduced rank regression. Their proposal has the form (compare (3.69))\n",
      "ˆBc+w=ˆBUΛU−1, (3.72)\n",
      "where Λis a diagonal shrinkage matrix (the “c+w” stands for “Curds\n",
      "and Whey,” the name they gave to their procedure). Based on optimal\n",
      "prediction in the population setting, they show that Λhas diagonal entries\n",
      "λm=c2\n",
      "m\n",
      "c2m+p\n",
      "N(1−c2m), m= 1,... ,M, (3.73)\n",
      "where cmis the mth canonical correlation coeﬃcient. Note that as the ratio\n",
      "of the number of input variables to sample size p/Ngets small, the shrink-\n",
      "age factors approach 1. Breiman and Friedman (1997) proposed modiﬁed\n",
      "versions of Λbased on training data and cross-validation, but the general\n",
      "form is the same. Here the ﬁtted response has the form\n",
      "ˆYc+w=HYSc+w, (3.74)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "86 3. Linear Methods for Regression\n",
      "where Sc+w=UΛU−1is the response shrinkage operator.\n",
      "Breiman and Friedman (1997) also suggested shrinking in both the Y\n",
      "space and Xspace. This leads to hybrid shrinkage models of the form\n",
      "ˆYridge,c+w=AλYSc+w, (3.75)\n",
      "whereAλ=X(XTX+λI)−1XTis the ridge regression shrinkage operator,\n",
      "as in (3.46) on page 66. Their paper and the discussions thereof contain\n",
      "many more details.\n",
      "3.8 More on the Lasso and Related Path\n",
      "Algorithms\n",
      "Since the publication of the LAR algorithm (Efron et al., 2004) there has\n",
      "been a lot of activity in developing algorithms for ﬁtting regularization\n",
      "paths for a variety of diﬀerent problems. In addition, L1regularization has\n",
      "taken on a life of its own, leading to the development of the ﬁeld compressed\n",
      "sensing in the signal-processing literature. (Donoho, 2006a; Candes, 2006).\n",
      "In this section we discuss some related proposals and other path algorithms,\n",
      "starting oﬀ with a precursor to the LAR algorithm.\n",
      "3.8.1 Incremental Forward Stagewise Regression\n",
      "Here we present another LAR-like algorithm, this time focused on forward\n",
      "stagewise regression. Interestingly, eﬀorts to understand a ﬂexible nonlinear\n",
      "regression procedure (boosting) led to a new algorithm for linear models\n",
      "(LAR). In reading the ﬁrst edition of this book and the forward stagewise\n",
      "Algorithm 3.4 Incremental Forward Stagewise Regression—FS ǫ.\n",
      "1. Start with the residual requal to yandβ1,β2,... ,β p= 0. All the\n",
      "predictors are standardized to have mean zero and unit norm.\n",
      "2. Find the predictor xjmost correlated with r\n",
      "3. Update βj←βj+δj, where δj=ǫ≤sign[⟨xj,r⟩] and ǫ >0 is a small\n",
      "step size, and set r←r−δjxj.\n",
      "4. Repeat steps 2 and 3 many times, until the residuals are uncorrelated\n",
      "with all the predictors.\n",
      "Algorithm 16.1 of Chapter 164, our colleague Brad Efron realized that with\n",
      "4In the ﬁrst edition, this was Algorithm 10.4 in Chapter 10.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.8 More on the Lasso and Related Path Algorithms 87−0.2 0.0 0.2 0.4 0.6lcavol\n",
      "lweight\n",
      "agelbphsvi\n",
      "lcpgleasonpgg45\n",
      "0 50 100 150 200\n",
      "−0.2 0.0 0.2 0.4 0.6lcavol\n",
      "lweight\n",
      "agelbphsvi\n",
      "lcpgleasonpgg45\n",
      "0.0 0.5 1.0 1.5 2.0FSǫ FS0\n",
      "Iteration\n",
      "CoeﬃcientsCoeﬃcients\n",
      "L1Arc-length of Coeﬃcients\n",
      "FIGURE 3.19. Coeﬃcient proﬁles for the prostate data. The left panel shows\n",
      "incremental forward stagewise regression with step size ǫ= 0.01. The right panel\n",
      "shows the inﬁnitesimal version FS 0obtained letting ǫ→0. This proﬁle was ﬁt by\n",
      "the modiﬁcation 3.2b to the LAR Algorithm 3.2. In this example t he FS 0proﬁles\n",
      "are monotone, and hence identical to those of lasso and LAR.\n",
      "linear models, one could explicitly construct the piecewise-linear lasso paths\n",
      "of Figure 3.10. This led him to propose the LAR procedure of Section 3.4.4,\n",
      "as well as the incremental version of forward-stagewise regression presented\n",
      "here.\n",
      "Consider the linear-regression version of the forward-stagewise boosting\n",
      "algorithm 16.1 proposed in Section 16.1 (page 608). It generates a coeﬃcient\n",
      "proﬁle by repeatedly updating (by a small amount ǫ) the coeﬃcient of the\n",
      "variable most correlated with the current residuals. Algorithm 3.4 gives\n",
      "the details. Figure 3.19 (left panel) shows the progress of the algorithm on\n",
      "the prostate data with step size ǫ= 0.01. If δj=⟨xj,r⟩(the least-squares\n",
      "coeﬃcient of the residual on jth predictor), then this is exactly the usual\n",
      "forward stagewise procedure (FS) outlined in Section 3.3.3.\n",
      "Here we are mainly interested in small values of ǫ. Letting ǫ→0 gives\n",
      "the right panel of Figure 3.19, which in this case is identical to the lasso\n",
      "path in Figure 3.10. We call this limiting procedure inﬁnitesimal forward\n",
      "stagewise regression or FS 0. This procedure plays an important role in\n",
      "non-linear, adaptive methods like boosting (Chapters 10 and 16) and is the\n",
      "version of incremental forward stagewise regression that is most amenabl e\n",
      "to theoretical analysis. B¨ uhlmann and Hothorn (2007) refer to the same\n",
      "procedure as “L2boost”, because of its connections to boosting.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "88 3. Linear Methods for Regression\n",
      "Efron originally thought that the LAR Algorithm 3.2 was an implemen-\n",
      "tation of FS 0, allowing each tied predictor a chance to update their coeﬃ-\n",
      "cients in a balanced way, while remaining tied in correlation. However, he\n",
      "then realized that the LAR least-squares ﬁt amongst the tied predictors\n",
      "can result in coeﬃcients moving in the opposite direction to their correla-\n",
      "tion, which cannot happen in Algorithm 3.4. The following modiﬁcation of\n",
      "the LAR algorithm implements FS 0:\n",
      "Algorithm 3.2b Least Angle Regression: FS 0Modiﬁcation .\n",
      "4. Find the new direction by solving the constrained least squares prob-\n",
      "lem\n",
      "min\n",
      "b||r−XAb||2\n",
      "2subject to bjsj≥0, j∈ A,\n",
      "where sjis the sign of ⟨xj,r⟩.\n",
      "The modiﬁcation amounts to a non-negative least squares ﬁt, keeping the\n",
      "signs of the coeﬃcients the same as those of the correlations. One can show\n",
      "that this achieves the optimal balancing of inﬁnitesimal “update turns”\n",
      "for the variables tied for maximal correlation (Hastie et al., 2007) . Like\n",
      "lasso, the entire FS 0path can be computed very eﬃciently via the LAR\n",
      "algorithm.\n",
      "As a consequence of these results, if the LAR proﬁles are monotone non-\n",
      "increasing or non-decreasing, as they are in Figure 3.19, then all three\n",
      "methods—LAR, lasso, and FS 0—give identical proﬁles. If the proﬁles are\n",
      "not monotone but do not cross the zero axis, then LAR and lasso are\n",
      "identical.\n",
      "Since FS 0is diﬀerent from the lasso, it is natural to ask if it optimizes\n",
      "a criterion. The answer is more complex than for lasso; the FS 0coeﬃcient\n",
      "proﬁle is the solution to a diﬀerential equation. While the lasso makes op-\n",
      "timal progress in terms of reducing the residual sum-of-squares per unit\n",
      "increase in L1-norm of the coeﬃcient vector β, FS0is optimal per unit\n",
      "increase in L1arc-length traveled along the coeﬃcient path. Hence its co-\n",
      "eﬃcient path is discouraged from changing directions too often.\n",
      "FS0is more constrained than lasso, and in fact can be viewed as a mono-\n",
      "tone version of the lasso; see Figure 16.3 on page 614 for a dramatic exa m-\n",
      "ple. FS 0may be useful in p≫Nsituations, where its coeﬃcient proﬁles\n",
      "are much smoother and hence have less variance than those of lasso. More\n",
      "details on FS 0are given in Section 16.2.3 and Hastie et al. (2007). Fig-\n",
      "ure 3.16 includes FS 0where its performance is very similar to that of the\n",
      "lasso.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.8 More on the Lasso and Related Path Algorithms 89\n",
      "3.8.2 Piecewise-Linear Path Algorithms\n",
      "The least angle regression procedure exploits the piecewise linear nature of\n",
      "the lasso solution paths. It has led to similar “path algorithms” for o ther\n",
      "regularized problems. Suppose we solve\n",
      "ˆβ(λ) = argminβ[R(β) +λJ(β)], (3.76)\n",
      "with\n",
      "R(β) =N∑\n",
      "i=1L(yi,β0+p∑\n",
      "j=1xijβj), (3.77)\n",
      "where both the loss function Land the penalty function Jare convex.\n",
      "Then the following are suﬃcient conditions for the solution path ˆβ(λ) to\n",
      "be piecewise linear (Rosset and Zhu, 2007):\n",
      "1.Ris quadratic or piecewise-quadratic as a function of β, and\n",
      "2.Jis piecewise linear in β.\n",
      "This also implies (in principle) that the solution path can be eﬃciently\n",
      "computed. Examples include squared- and absolute-error loss, “Huberized”\n",
      "losses, and the L1,L∞penalties on β. Another example is the “hinge loss”\n",
      "function used in the support vector machine. There the loss is piecewise\n",
      "linear, and the penalty is quadratic. Interestingly, this leads to a piecewise-\n",
      "linear path algorithm in the dual space ; more details are given in Sec-\n",
      "tion 12.3.5.\n",
      "3.8.3 The Dantzig Selector\n",
      "Candes and Tao (2007) proposed the following criterion:\n",
      "minβ||β||1subject to ||XT(y−Xβ)||∞≤s. (3.78)\n",
      "They call the solution the Dantzig selector (DS). It can be written equiva-\n",
      "lently as\n",
      "minβ||XT(y−Xβ)||∞subject to ||β||1≤t. (3.79)\n",
      "Here|| ≤ ||∞denotes the L∞norm, the maximum absolute value of the\n",
      "components of the vector. In this form it resembles the lasso, replacing\n",
      "squared error loss by the maximum absolute value of its gradient. Note\n",
      "that as tgets large, both procedures yield the least squares solution if\n",
      "N < p . Ifp≥N, they both yield the least squares solution with minimum\n",
      "L1norm. However for smaller values of t, the DS procedure produces a\n",
      "diﬀerent path of solutions than the lasso.\n",
      "Candes and Tao (2007) show that the solution to DS is a linear pro-\n",
      "gramming problem; hence the name Dantzig selector, in honor of the late\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "90 3. Linear Methods for Regression\n",
      "George Dantzig, the inventor of the simplex method for linear program-\n",
      "ming. They also prove a number of interesting mathematical properties for\n",
      "the method, related to its ability to recover an underlying sparse coeﬃ-\n",
      "cient vector. These same properties also hold for the lasso, as shown later\n",
      "by Bickel et al. (2008).\n",
      "Unfortunately the operating properties of the DS method are somewhat\n",
      "unsatisfactory. The method seems similar in spirit to the lasso, especiall y\n",
      "when we look at the lasso’s stationary conditions (3.58). Like the LAR a l-\n",
      "gorithm, the lasso maintains the same inner product (and correlation) with\n",
      "the current residual for all variables in the active set, and moves their co-\n",
      "eﬃcients to optimally decrease the residual sum of squares. In the process,\n",
      "this common correlation is decreased monotonically (Exercise 3.23), and at\n",
      "all times this correlation is larger than that for non-active variables. The\n",
      "Dantzig selector instead tries to minimize the maximum inner product of\n",
      "the current residual with all the predictors. Hence it can achieve a smaller\n",
      "maximum than the lasso, but in the process a curious phenomenon can\n",
      "occur. If the size of the active set is m, there will be mvariables tied with\n",
      "maximum correlation. However, these need not coincide with the active set!\n",
      "Hence it can include a variable in the model that has smaller correlation\n",
      "with the current residual than some of the excluded variables (Efron et\n",
      "al., 2007). This seems unreasonable and may be responsible for its some-\n",
      "times inferior prediction accuracy. Efron et al. (2007) also show that DS\n",
      "can yield extremely erratic coeﬃcient paths as the regularization parameter\n",
      "sis varied.\n",
      "3.8.4 The Grouped Lasso\n",
      "In some problems, the predictors belong to pre-deﬁned groups; for example\n",
      "genes that belong to the same biological pathway, or collections of indicator\n",
      "(dummy) variables for representing the levels of a categorical predictor. In\n",
      "this situation it may be desirable to shrink and select the members of a\n",
      "group together. The grouped lasso is one way to achieve this. Suppose that\n",
      "theppredictors are divided into Lgroups, with pℓthe number in group\n",
      "ℓ. For ease of notation, we use a matrix Xℓto represent the predictors\n",
      "corresponding to the ℓth group, with corresponding coeﬃcient vector βℓ.\n",
      "The grouped-lasso minimizes the convex criterion\n",
      "min\n",
      "β∈I Rp(\n",
      "||y−β01−L∑\n",
      "ℓ=1Xℓβℓ||2\n",
      "2+λL∑\n",
      "ℓ=1√pℓ||βℓ||2)\n",
      ", (3.80)\n",
      "where the√pℓterms accounts for the varying group sizes, and || ≤ ||2is\n",
      "the Euclidean norm (not squared). Since the Euclidean norm of a vector\n",
      "βℓis zero only if all of its components are zero, this procedure encourages\n",
      "sparsity at both the group and individual levels. That is, for some values of\n",
      "λ, an entire group of predictors may drop out of the model. This procedure\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.8 More on the Lasso and Related Path Algorithms 91\n",
      "was proposed by Bakin (1999) and Lin and Zhang (2006), and studied and\n",
      "generalized by Yuan and Lin (2007). Generalizations include more general\n",
      "L2norms ||η||K= (ηTKη)1/2, as well as allowing overlapping groups of\n",
      "predictors (Zhao et al., 2008). There are also connections to methods for\n",
      "ﬁtting sparse additive models (Lin and Zhang, 2006; Ravikumar et al.,\n",
      "2008).\n",
      "3.8.5 Further Properties of the Lasso\n",
      "A number of authors have studied the ability of the lasso and related pro-\n",
      "cedures to recover the correct model, as Nandpgrow. Examples of this\n",
      "work include Knight and Fu (2000), Greenshtein and Ritov (2004), Tropp\n",
      "(2004), Donoho (2006b), Meinshausen (2007), Meinshausen and B¨ uhlmann\n",
      "(2006), Tropp (2006), Zhao and Yu (2006), Wainwright (2006), and Bunea\n",
      "et al. (2007). For example Donoho (2006b) focuses on the p > N case and\n",
      "considers the lasso solution as the bound tgets large. In the limit this gives\n",
      "the solution with minimum L1norm among all models with zero training\n",
      "error. He shows that under certain assumptions on the model matrix X, if\n",
      "the true model is sparse, this solution identiﬁes the correct predictors with\n",
      "high probability.\n",
      "Many of the results in this area assume a condition on the model matrix\n",
      "of the form\n",
      "||(XSTXS)−1XSTXSc||∞≤(1−ǫ) for some ǫ∈(0,1]. (3.81)\n",
      "HereSindexes the subset of features with non-zero coeﬃcients in the true\n",
      "underlying model, and XSare the columns of Xcorresponding to those\n",
      "features. Similarly Scare the features with true coeﬃcients equal to zero,\n",
      "andXScthe corresponding columns. This says that the least squares coef-\n",
      "ﬁcients for the columns of XSconXSare not too large, that is, the “good”\n",
      "variables Sare not too highly correlated with the nuisance variables Sc.\n",
      "Regarding the coeﬃcients themselves, the lasso shrinkage causes the esti-\n",
      "mates of the non-zero coeﬃcients to be biased towards zero, and in general\n",
      "they are not consistent5. One approach for reducing this bias is to run\n",
      "the lasso to identify the set of non-zero coeﬃcients, and then ﬁt an un-\n",
      "restricted linear model to the selected set of features. This is not always\n",
      "feasible, if the selected set is large. Alternatively, one can use the lasso to\n",
      "select the set of non-zero predictors, and then apply the lasso again, but\n",
      "using only the selected predictors from the ﬁrst step. This is known as the\n",
      "relaxed lasso (Meinshausen, 2007). The idea is to use cross-validation to\n",
      "estimate the initial penalty parameter for the lasso, and then again for a\n",
      "second penalty parameter applied to the selected set of predictors. Since\n",
      "5Statistical consistency means as the sample size grows, the estimates converge to\n",
      "the true values.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "92 3. Linear Methods for Regression\n",
      "the variables in the second step have less “competition” from noise vari-\n",
      "ables, cross-validation will tend to pick a smaller value for λ, and hence\n",
      "their coeﬃcients will be shrunken less than those in the initial estimate.\n",
      "Alternatively, one can modify the lasso penalty function so that larger co-\n",
      "eﬃcients are shrunken less severely; the smoothly clipped absolute deviation\n",
      "(SCAD) penalty of Fan and Li (2005) replaces λ|β|byJa(β,λ), where\n",
      "dJa(β,λ)\n",
      "dβ=λ≤sign(β)[\n",
      "I(|β| ≤λ) +(aλ− |β|)+\n",
      "(a−1)λI(|β|> λ)]\n",
      "(3.82)\n",
      "for some a≥2. The second term in square-braces reduces the amount of\n",
      "shrinkage in the lasso for larger values of β, with ultimately no shrinkage\n",
      "asa→ ∞. Figure 3.20 shows the SCAD penalty, along with the lasso and\n",
      "−4 −2 0 2 40 1 2 3 4 5\n",
      "−4 −2 0 2 40.0 0.5 1.0 1.5 2.0 2.5\n",
      "−4 −2 0 2 40.5 1.0 1.5 2.0|β| SCAD |β|1−ν\n",
      "β β β\n",
      "FIGURE 3.20. The lasso and two alternative non-convex penalties designed to\n",
      "penalize large coeﬃcients less. For SCAD we use λ= 1anda= 4, and ν=1\n",
      "2in\n",
      "the last panel.\n",
      "|β|1−ν. However this criterion is non-convex, which is a drawback since it\n",
      "makes the computation much more diﬃcult. The adaptive lasso (Zou, 2006)\n",
      "uses a weighted penalty of the form∑p\n",
      "j=1wj|βj|where wj= 1/|ˆβj|ν,ˆβjis\n",
      "the ordinary least squares estimate and ν >0. This is a practical approxi-\n",
      "mation to the |β|qpenalties ( q= 1−νhere) discussed in Section 3.4.3. The\n",
      "adaptive lasso yields consistent estimates of the parameters while retaining\n",
      "the attractive convexity property of the lasso.\n",
      "3.8.6 Pathwise Coordinate Optimization\n",
      "An alternate approach to the LARS algorithm for computing the lasso\n",
      "solution is simple coordinate descent. This idea was proposed by Fu (1998)\n",
      "and Daubechies et al. (2004), and later studied and generalized by Friedman\n",
      "et al. (2007), Wu and Lange (2008) and others. The idea is to ﬁx the penalty\n",
      "parameter λin the Lagrangian form (3.52) and optimize successively over\n",
      "each parameter, holding the other parameters ﬁxed at their current values.\n",
      "Suppose the predictors are all standardized to have mean zero and unit\n",
      "norm. Denote by ˜βk(λ) the current estimate for βkat penalty parameter\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "3.9 Computational Considerations 93\n",
      "λ. We can rearrange (3.52) to isolate βj,\n",
      "R(˜β(λ),βj) =1\n",
      "2N∑\n",
      "i=1(\n",
      "yi−∑\n",
      "k̸=jxik˜βk(λ)−xijβj)2\n",
      "+λ∑\n",
      "k̸=j|˜βk(λ)|+λ|βj|,\n",
      "(3.83)\n",
      "where we have suppressed the intercept and introduced a factor1\n",
      "2for con-\n",
      "venience. This can be viewed as a univariate lasso problem with response\n",
      "variable the partial residual yi−˜y(j)\n",
      "i=yi−∑\n",
      "k̸=jxik˜βk(λ). This has an\n",
      "explicit solution, resulting in the update\n",
      "˜βj(λ)←S(N∑\n",
      "i=1xij(yi−˜y(j)\n",
      "i),λ)\n",
      ". (3.84)\n",
      "HereS(t,λ) = sign( t)(|t|−λ)+is the soft-thresholding operator in Table 3.4\n",
      "on page 71. The ﬁrst argument to S(≤) is the simple least-squares coeﬃcient\n",
      "of the partial residual on the standardized variable xij. Repeated iteration\n",
      "of (3.84)—cycling through each variable in turn until convergence—yields\n",
      "the lasso estimate ˆβ(λ).\n",
      "We can also use this simple algorithm to eﬃciently compute the lasso\n",
      "solutions at a grid of values of λ. We start with the smallest value λmax\n",
      "for which ˆβ(λmax) = 0, decrease it a little and cycle through the variables\n",
      "until convergence. Then λis decreased again and the process is repeated,\n",
      "using the previous solution as a “warm start” for the new value of λ. This\n",
      "can be faster than the LARS algorithm, especially in large problems. A\n",
      "key to its speed is the fact that the quantities in (3.84) can be updated\n",
      "quickly as jvaries, and often the update is to leave ˜βj= 0. On the other\n",
      "hand, it delivers solutions over a grid of λvalues, rather than the entire\n",
      "solution path. The same kind of algorithm can be applied to the elastic\n",
      "net, the grouped lasso and many other models in which the penalty is a\n",
      "sum of functions of the individual parameters (Friedman et al., 2010). It\n",
      "can also be applied, with some substantial modiﬁcations, to the fused lasso\n",
      "(Section 18.4.2); details are in Friedman et al. (2007).\n",
      "3.9 Computational Considerations\n",
      "Least squares ﬁtting is usually done via the Cholesky decomposition of\n",
      "the matrix XTXor a QR decomposition of X. With Nobservations and p\n",
      "features, the Cholesky decomposition requires p3+Np2/2 operations, while\n",
      "the QR decomposition requires Np2operations. Depending on the relative\n",
      "size of Nandp, the Cholesky can sometimes be faster; on the other hand,\n",
      "it can be less numerically stable (Lawson and Hansen, 1974). Computation\n",
      "of the lasso via the LAR algorithm has the same order of computation as\n",
      "a least squares ﬁt.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "94 3. Linear Methods for Regression\n",
      "Bibliographic Notes\n",
      "Linear regression is discussed in many statistics books, for example, Seber\n",
      "(1984), Weisberg (1980) and Mardia et al. (1979). Ridge regression wa s\n",
      "introduced by Hoerl and Kennard (1970), while the lasso was proposed by\n",
      "Tibshirani (1996). Around the same time, lasso-type penalties were pro-\n",
      "posed in the basis pursuit method for signal processing (Chen et al., 1998).\n",
      "The least angle regression procedure was proposed in Efron et al. (2004);\n",
      "related to this is the earlier homotopy procedure of Osborne et al. (2000a)\n",
      "and Osborne et al. (2000b). Their algorithm also exploits the piecewise\n",
      "linearity used in the LAR/lasso algorithm, but lacks its transparency. The\n",
      "criterion for the forward stagewise criterion is discussed in Hastie et a l.\n",
      "(2007). Park and Hastie (2007) develop a path algorithm similar to l east\n",
      "angle regression for generalized regression models. Partial least squares\n",
      "was introduced by Wold (1975). Comparisons of shrinkage methods may\n",
      "be found in Copas (1983) and Frank and Friedman (1993).\n",
      "Exercises\n",
      "Ex. 3.1 Show that the Fstatistic (3.13) for dropping a single coeﬃcient\n",
      "from a model is equal to the square of the corresponding z-score (3.12).\n",
      "Ex. 3.2 Given data on two variables XandY, consider ﬁtting a cubic\n",
      "polynomial regression model f(X) =∑3\n",
      "j=0βjXj. In addition to plotting\n",
      "the ﬁtted curve, you would like a 95% conﬁdence band about the curve.\n",
      "Consider the following two approaches:\n",
      "1. At each point x0, form a 95% conﬁdence interval for the linear func-\n",
      "tionaTβ=∑3\n",
      "j=0βjxj\n",
      "0.\n",
      "2. Form a 95% conﬁdence set for βas in (3.15), which in turn generates\n",
      "conﬁdence intervals for f(x0).\n",
      "How do these approaches diﬀer? Which band is likely to be wider? Conduct\n",
      "a small simulation experiment to compare the two methods.\n",
      "Ex. 3.3 Gauss–Markov theorem:\n",
      "(a) Prove the Gauss–Markov theorem: the least squares estimate of a\n",
      "parameter aTβhas variance no bigger than that of any other linear\n",
      "unbiased estimate of aTβ(Section 3.2.2).\n",
      "(b) The matrix inequality B⪯Aholds if A−Bis positive semideﬁnite.\n",
      "Show that if ˆVis the variance-covariance matrix of the least squares\n",
      "estimate of βand˜Vis the variance-covariance matrix of any other\n",
      "linear unbiased estimate, then ˆV⪯˜V.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 95\n",
      "Ex. 3.4 Show how the vector of least squares coeﬃcients can be obtained\n",
      "from a single pass of the Gram–Schmidt procedure (Algorithm 3.1). Rep-\n",
      "resent your solution in terms of the QR decomposition of X.\n",
      "Ex. 3.5 Consider the ridge regression problem (3.41). Show that this prob-\n",
      "lem is equivalent to the problem\n",
      "ˆβc= argmin\n",
      "βc{N∑\n",
      "i=1[\n",
      "yi−βc\n",
      "0−p∑\n",
      "j=1(xij−¯xj)βc\n",
      "j]2+λp∑\n",
      "j=1βc\n",
      "j2}\n",
      ".(3.85)\n",
      "Give the correspondence between βcand the original βin (3.41). Char-\n",
      "acterize the solution to this modiﬁed criterion. Show that a similar result\n",
      "holds for the lasso.\n",
      "Ex. 3.6 Show that the ridge regression estimate is the mean (and mode)\n",
      "of the posterior distribution, under a Gaussian prior β∼N(0,τI), and\n",
      "Gaussian sampling model y∼N(Xβ,σ2I). Find the relationship between\n",
      "the regularization parameter λin the ridge formula, and the variances τ\n",
      "andσ2.\n",
      "Ex. 3.7 Assume yi∼N(β0+xT\n",
      "iβ,σ2),i= 1,2,... ,N , and the parameters\n",
      "βjare each distributed as N(0,τ2), independently of one another. Assuming\n",
      "σ2andτ2are known, show that the (minus) log-posterior density of βis\n",
      "proportional to∑N\n",
      "i=1(yi−β0−∑\n",
      "jxijβj)2+λ∑p\n",
      "j=1β2\n",
      "jwhere λ=σ2/τ2.\n",
      "Ex. 3.8 Consider the QR decomposition of the uncentered N×(p+ 1)\n",
      "matrix X(whose ﬁrst column is all ones), and the SVD of the N×p\n",
      "centered matrix ˜X. Show that Q2andUspan the same subspace, where\n",
      "Q2is the sub-matrix of Qwith the ﬁrst column removed. Under what\n",
      "circumstances will they be the same, up to sign ﬂips?\n",
      "Ex. 3.9 Forward stepwise regression. Suppose we have the QR decomposi-\n",
      "tion for the N×qmatrix X1in a multiple regression problem with response\n",
      "y, and we have an additional p−qpredictors in the matrix X2. Denote the\n",
      "current residual by r. We wish to establish which one of these additional\n",
      "variables will reduce the residual-sum-of squares the most when included\n",
      "with those in X1. Describe an eﬃcient procedure for doing this.\n",
      "Ex. 3.10 Backward stepwise regression. Suppose we have the multiple re-\n",
      "gression ﬁt of yonX, along with the standard errors and Z-scores as in\n",
      "Table 3.2. We wish to establish which variable, when dropped, will increase\n",
      "the residual sum-of-squares the least. How would you do this?\n",
      "Ex. 3.11 Show that the solution to the multivariate linear regression prob-\n",
      "lem (3.40) is given by (3.39). What happens if the covariance matrices Σi\n",
      "are diﬀerent for each observation?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "96 3. Linear Methods for Regression\n",
      "Ex. 3.12 Show that the ridge regression estimates can be obtained by\n",
      "ordinary least squares regression on an augmented data set. We augment\n",
      "the centered matrix Xwithpadditional rows√\n",
      "λI, and augment ywithp\n",
      "zeros. By introducing artiﬁcial data having response value zero, the ﬁtting\n",
      "procedure is forced to shrink the coeﬃcients toward zero. This is related to\n",
      "the idea of hintsdue to Abu-Mostafa (1995), where model constraints are\n",
      "implemented by adding artiﬁcial data examples that satisfy them.\n",
      "Ex. 3.13 Derive the expression (3.62), and show that ˆβpcr(p) =ˆβls.\n",
      "Ex. 3.14 Show that in the orthogonal case, PLS stops after m= 1 steps,\n",
      "because subsequent ˆ ϕmjin step 2 in Algorithm 3.3 are zero.\n",
      "Ex. 3.15 Verify expression (3.64), and hence show that the partial least\n",
      "squares directions are a compromise between the ordinary regression coef-\n",
      "ﬁcient and the principal component directions.\n",
      "Ex. 3.16 Derive the entries in Table 3.4, the explicit forms for estimators\n",
      "in the orthogonal case.\n",
      "Ex. 3.17 Repeat the analysis of Table 3.3 on the spam data discussed in\n",
      "Chapter 1.\n",
      "Ex. 3.18 Read about conjugate gradient algorithms (Murray et al., 1981, for\n",
      "example), and establish a connection between these algorithms and partial\n",
      "least squares.\n",
      "Ex. 3.19 Show that ∥ˆβridge∥increases as its tuning parameter λ→0. Does\n",
      "the same property hold for the lasso and partial least squares estimates?\n",
      "For the latter, consider the “tuning parameter” to be the successive steps\n",
      "in the algorithm.\n",
      "Ex. 3.20 Consider the canonical-correlation problem (3.67). Show that the\n",
      "leading pair of canonical variates u1andv1solve the problem\n",
      "max\n",
      "uT(YTY)u=1\n",
      "vT(XTX)v=1uT(YTX)v, (3.86)\n",
      "a generalized SVD problem. Show that the solution is given by u1=\n",
      "(YTY)−1\n",
      "2u∗\n",
      "1, and v1= (XTX)−1\n",
      "2v∗\n",
      "1, where u∗\n",
      "1andv∗\n",
      "1are the leading left\n",
      "and right singular vectors in\n",
      "(YTY)−1\n",
      "2(YTX)(XTX)−1\n",
      "2=U∗D∗V∗T. (3.87)\n",
      "Show that the entire sequence um, vm, m= 1,... ,min(K,p) is also given\n",
      "by (3.87).\n",
      "Ex. 3.21 Show that the solution to the reduced-rank regression problem\n",
      "(3.68), with Σestimated by YTY/N, is given by (3.69). Hint: Transform\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 97\n",
      "YtoY∗=YΣ−1\n",
      "2, and solved in terms of the canonical vectors u∗\n",
      "m. Show\n",
      "thatUm=Σ−1\n",
      "2U∗\n",
      "m, and a generalized inverse is U−\n",
      "m=U∗\n",
      "mTΣ1\n",
      "2.\n",
      "Ex. 3.22 Show that the solution in Exercise 3.21 does not change if Σis\n",
      "estimated by the more natural quantity ( Y−XˆB)T(Y−XˆB)/(N−pK).\n",
      "Ex. 3.23 Consider a regression problem with all variables and response hav-\n",
      "ing mean zero and standard deviation one. Suppose also that each variable\n",
      "has identical absolute correlation with the response:\n",
      "1\n",
      "N|⟨xj,y⟩|=λ, j= 1,... ,p.\n",
      "Letˆβbe the least-squares coeﬃcient of yonX, and let u(α) =αXˆβfor\n",
      "α∈[0,1] be the vector that moves a fraction αtoward the least squares ﬁt\n",
      "u. LetRSSbe the residual sum-of-squares from the full least squares ﬁt.\n",
      "(a) Show that\n",
      "1\n",
      "N|⟨xj,y−u(α)⟩|= (1−α)λ, j= 1,... ,p,\n",
      "and hence the correlations of each xjwith the residuals remain equal\n",
      "in magnitude as we progress toward u.\n",
      "(b) Show that these correlations are all equal to\n",
      "λ(α) =(1−α)√\n",
      "(1−α)2+α(2−α)\n",
      "N≤RSS≤λ,\n",
      "and hence they decrease monotonically to zero.\n",
      "(c) Use these results to show that the LAR algorithm in Section 3.4.4\n",
      "keeps the correlations tied and monotonically decreasing, as claimed\n",
      "in (3.55).\n",
      "Ex. 3.24 LAR directions. Using the notation around equation (3.55) on\n",
      "page 74, show that the LAR direction makes an equal angle with each of\n",
      "the predictors in Ak.\n",
      "Ex. 3.25 LAR look-ahead (Efron et al., 2004, Sec. 2). Starting at the be-\n",
      "ginning of the kth step of the LAR algorithm, derive expressions to identify\n",
      "the next variable to enter the active set at step k+1, and the value of αat\n",
      "which this occurs (using the notation around equation (3.55) on page 74).\n",
      "Ex. 3.26 Forward stepwise regression enters the variable at each step that\n",
      "most reduces the residual sum-of-squares. LAR adjusts variables that have\n",
      "the most (absolute) correlation with the current residuals. Show that these\n",
      "two entry criteria are not necessarily the same. [Hint: let xj.Abe the jth\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "98 3. Linear Methods for Regression\n",
      "variable, linearly adjusted for all the variables currently in the model. Show\n",
      "that the ﬁrst criterion amounts to identifying the jfor which Cor( xj.A,r)\n",
      "is largest in magnitude.\n",
      "Ex. 3.27 Lasso and LAR : Consider the lasso problem in Lagrange multiplier\n",
      "form: with L(β) =1\n",
      "2∑\n",
      "i(yi−∑\n",
      "jxijβj)2, we minimize\n",
      "L(β) +λ∑\n",
      "j|βj| (3.88)\n",
      "for ﬁxed λ >0.\n",
      "(a) Setting βj=β+\n",
      "j−β−\n",
      "jwithβ+\n",
      "j,β−\n",
      "j≥0, expression (3.88) becomes\n",
      "L(β) +λ∑\n",
      "j(β+\n",
      "j+β−\n",
      "j). Show that the Lagrange dual function is\n",
      "L(β) +λ∑\n",
      "j(β+\n",
      "j+β−\n",
      "j)−∑\n",
      "jλ+\n",
      "jβ+\n",
      "j−∑\n",
      "jλ−\n",
      "jβ−\n",
      "j (3.89)\n",
      "and the Karush–Kuhn–Tucker optimality conditions are\n",
      "∇L(β)j+λ−λ+\n",
      "j= 0\n",
      "−∇L(β)j+λ−λ−\n",
      "j= 0\n",
      "λ+\n",
      "jβ+\n",
      "j= 0\n",
      "λ−\n",
      "jβ−\n",
      "j= 0,\n",
      "along with the non-negativity constraints on the parameters and all\n",
      "the Lagrange multipliers.\n",
      "(b) Show that |∇L(β)j| ≤λ∀j,and that the KKT conditions imply one\n",
      "of the following three scenarios:\n",
      "λ= 0⇒ ∇ L(β)j= 0∀j\n",
      "β+\n",
      "j>0, λ > 0⇒λ+\n",
      "j= 0,∇L(β)j=−λ <0, β−\n",
      "j= 0\n",
      "β−\n",
      "j>0, λ > 0⇒λ−\n",
      "j= 0,∇L(β)j=λ >0, β+\n",
      "j= 0.\n",
      "Hence show that for any “active” predictor having βj̸= 0, we must\n",
      "have∇L(β)j=−λifβj>0, and ∇L(β)j=λifβj<0. Assuming\n",
      "the predictors are standardized, relate λto the correlation between\n",
      "thejth predictor and the current residuals.\n",
      "(c) Suppose that the set of active predictors is unchanged for λ0≥λ≥λ1.\n",
      "Show that there is a vector γ0such that\n",
      "ˆβ(λ) =ˆβ(λ0)−(λ−λ0)γ0 (3.90)\n",
      "Thus the lasso solution path is linear as λranges from λ0toλ1(Efron\n",
      "et al., 2004; Rosset and Zhu, 2007).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 99\n",
      "Ex. 3.28 Suppose for a given tin (3.51), the ﬁtted lasso coeﬃcient for\n",
      "variable Xjisˆβj=a. Suppose we augment our set of variables with an\n",
      "identical copy X∗\n",
      "j=Xj. Characterize the eﬀect of this exact collinearity\n",
      "by describing the set of solutions for ˆβjandˆβ∗\n",
      "j, using the same value of t.\n",
      "Ex. 3.29 Suppose we run a ridge regression with parameter λon a single\n",
      "variable X, and get coeﬃcient a. We now include an exact copy X∗=X,\n",
      "and reﬁt our ridge regression. Show that both coeﬃcients are identical, and\n",
      "derive their value. Show in general that if mcopies of a variable Xjare\n",
      "included in a ridge regression, their coeﬃcients are all the same.\n",
      "Ex. 3.30 Consider the elastic-net optimization problem:\n",
      "min\n",
      "β||y−Xβ||2+λ[\n",
      "α||β||2\n",
      "2+ (1−α)||β||1]\n",
      ". (3.91)\n",
      "Show how one can turn this into a lasso problem, using an augmented\n",
      "version of Xandy.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "100 3. Linear Methods for Regression\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 101\n",
      "Printer: Opaque this\n",
      "4\n",
      "Linear Methods for Classiﬁcation\n",
      "4.1 Introduction\n",
      "In this chapter we revisit the classiﬁcation problem and focus on linear\n",
      "methods for classiﬁcation. Since our predictor G(x) takes values in a dis-\n",
      "crete set G, we can always divide the input space into a collection of regions\n",
      "labeled according to the classiﬁcation. We saw in Chapter 2 that the bound-\n",
      "aries of these regions can be rough or smooth, depending on the prediction\n",
      "function. For an important class of procedures, these decision boundaries\n",
      "are linear; this is what we will mean by linear methods for classiﬁcation.\n",
      "There are several diﬀerent ways in which linear decision boundaries can\n",
      "be found. In Chapter 2 we ﬁt linear regression models to the class indicator\n",
      "variables, and classify to the largest ﬁt. Suppose there are Kclasses, for\n",
      "convenience labeled 1 ,2,... ,K , and the ﬁtted linear model for the kth\n",
      "indicator response variable is ˆfk(x) =ˆβk0+ˆβT\n",
      "kx. The decision boundary\n",
      "between class kandℓis that set of points for which ˆfk(x) =ˆfℓ(x), that is,\n",
      "the set {x: (ˆβk0−ˆβℓ0) + (ˆβk−ˆβℓ)Tx= 0}, an aﬃne set or hyperplane1\n",
      "Since the same is true for any pair of classes, the input space is divided\n",
      "into regions of constant classiﬁcation, with piecewise hyperplanar decision\n",
      "boundaries. This regression approach is a member of a class of methods\n",
      "that model discriminant functions δk(x) for each class, and then classify x\n",
      "to the class with the largest value for its discriminant function. Methods\n",
      "1Strictly speaking, a hyperplane passes through the origin, while an aﬃne set need\n",
      "not. We sometimes ignore the distinction and refer in genera l to hyperplanes.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "102 4. Linear Methods for Classiﬁcation\n",
      "that model the posterior probabilities Pr( G=k|X=x) are also in this\n",
      "class. Clearly, if either the δk(x) or Pr( G=k|X=x) are linear in x, then\n",
      "the decision boundaries will be linear.\n",
      "Actually, all we require is that some monotone transformation of δkor\n",
      "Pr(G=k|X=x) be linear for the decision boundaries to be linear. For\n",
      "example, if there are two classes, a popular model for the posterior proba-\n",
      "bilities is\n",
      "Pr(G= 1|X=x) =exp(β0+βTx)\n",
      "1 + exp( β0+βTx),\n",
      "Pr(G= 2|X=x) =1\n",
      "1 + exp( β0+βTx).(4.1)\n",
      "Here the monotone transformation is the logittransformation: log[ p/(1−p)],\n",
      "and in fact we see that\n",
      "logPr(G= 1|X=x)\n",
      "Pr(G= 2|X=x)=β0+βTx. (4.2)\n",
      "The decision boundary is the set of points for which the log-odds are zero,\n",
      "and this is a hyperplane deﬁned by{\n",
      "x|β0+βTx= 0}\n",
      ". We discuss two very\n",
      "popular but diﬀerent methods that result in linear log-odds or logits: linear\n",
      "discriminant analysis and linear logistic regression. Although they diﬀer in\n",
      "their derivation, the essential diﬀerence between them is in the way the\n",
      "linear function is ﬁt to the training data.\n",
      "A more direct approach is to explicitly model the boundaries between\n",
      "the classes as linear. For a two-class problem in a p-dimensional input\n",
      "space, this amounts to modeling the decision boundary as a hyperplane—in\n",
      "other words, a normal vector and a cut-point. We will look at two methods\n",
      "that explicitly look for “separating hyperplanes.” The ﬁrst is the well-\n",
      "known perceptron model of Rosenblatt (1958), with an algorithm that ﬁnds\n",
      "a separating hyperplane in the training data, if one exists. The second\n",
      "method, due to Vapnik (1996), ﬁnds an optimally separating hyperplane if\n",
      "one exists, else ﬁnds a hyperplane that minimizes some measure of overlap\n",
      "in the training data. We treat the separable case here, and defer treatment\n",
      "of the nonseparable case to Chapter 12.\n",
      "While this entire chapter is devoted to linear decision boundaries, there is\n",
      "considerable scope for generalization. For example, we can expand our vari-\n",
      "able set X1,... ,X pby including their squares and cross-products X2\n",
      "1,X2\n",
      "2,... ,\n",
      "X1X2,..., thereby adding p(p+1)/2 additional variables. Linear functions\n",
      "in the augmented space map down to quadratic functions in the original\n",
      "space—hence linear decision boundaries to quadratic decision boundaries.\n",
      "Figure 4.1 illustrates the idea. The data are the same: the left plot uses\n",
      "linear decision boundaries in the two-dimensional space shown, while the\n",
      "right plot uses linear decision boundaries in the augmented ﬁve-dimensional\n",
      "space described above. This approach can be used with any basis transfor-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.2 Linear Regression of an Indicator Matrix 103\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1111 1\n",
      "111\n",
      "1\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "1 1\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "1 1\n",
      "11\n",
      "1111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "11 111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11 1\n",
      "1\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "111\n",
      "1111\n",
      "11\n",
      "1 11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "12\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "2 2\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "2 2\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2222\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2222 2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "33333\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1111 1\n",
      "111\n",
      "1\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "1 1\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "1 1\n",
      "11\n",
      "1111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "11 111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11 1\n",
      "1\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "111\n",
      "1111\n",
      "11\n",
      "1 11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "12\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "2 2\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "2 2\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2222\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2222 2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "33333\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "FIGURE 4.1. The left plot shows some data from three classes, with linear\n",
      "decision boundaries found by linear discriminant analysis. The ri ght plot shows\n",
      "quadratic decision boundaries. These were obtained by ﬁnding line ar boundaries\n",
      "in the ﬁve-dimensional space X1, X2, X1X2, X2\n",
      "1, X2\n",
      "2. Linear inequalities in this\n",
      "space are quadratic inequalities in the original space.\n",
      "mation h(X) where h: IRp↦→IRqwithq > p, and will be explored in later\n",
      "chapters.\n",
      "4.2 Linear Regression of an Indicator Matrix\n",
      "Here each of the response categories are coded via an indicator variable.\n",
      "Thus if GhasKclasses, there will be Ksuch indicators Yk, k= 1,... ,K ,\n",
      "withYk= 1 if G=kelse 0. These are collected together in a vector\n",
      "Y= (Y1,... ,Y K), and the Ntraining instances of these form an N×K\n",
      "indicator response matrix Y.Yis a matrix of 0’s and 1’s, with each row\n",
      "having a single 1. We ﬁt a linear regression model to each of the columns\n",
      "ofYsimultaneously, and the ﬁt is given by\n",
      "ˆY=X(XTX)−1XTY. (4.3)\n",
      "Chapter 3 has more details on linear regression. Note that we have a coeﬃ-\n",
      "cient vector for each response column yk, and hence a ( p+1)×Kcoeﬃcient\n",
      "matrix ˆB= (XTX)−1XTY. HereXis the model matrix with p+1 columns\n",
      "corresponding to the pinputs, and a leading column of 1’s for the intercept.\n",
      "A new observation with input xis classiﬁed as follows:\n",
      "•compute the ﬁtted output ˆf(x)T= (1,xT)ˆB, aKvector;\n",
      "•identify the largest component and classify accordingly:\n",
      "ˆG(x) = argmaxk∈Gˆfk(x). (4.4)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "104 4. Linear Methods for Classiﬁcation\n",
      "What is the rationale for this approach? One rather formal justiﬁcation\n",
      "is to view the regression as an estimate of conditional expectation. For the\n",
      "random variable Yk,E(Yk|X=x) = Pr( G=k|X=x), so conditional\n",
      "expectation of each of the Ykseems a sensible goal. The real issue is: how\n",
      "good an approximation to conditional expectation is the rather rigid linear\n",
      "regression model? Alternatively, are the ˆfk(x) reasonable estimates of the\n",
      "posterior probabilities Pr( G=k|X=x), and more importantly, does this\n",
      "matter?\n",
      "It is quite straightforward to verify that∑\n",
      "k∈Gˆfk(x) = 1 for any x, as\n",
      "long as there is an intercept in the model (column of 1’s in X). However,\n",
      "theˆfk(x) can be negative or greater than 1, and typically some are. This\n",
      "is a consequence of the rigid nature of linear regression, especially if we\n",
      "make predictions outside the hull of the training data. These violations in\n",
      "themselves do not guarantee that this approach will not work, and in fact\n",
      "on many problems it gives similar results to more standard linear meth-\n",
      "ods for classiﬁcation. If we allow linear regression onto basis expansions\n",
      "h(X) of the inputs, this approach can lead to consistent estimates of the\n",
      "probabilities. As the size of the training set Ngrows bigger, we adaptively\n",
      "include more basis elements so that linear regression onto these basis func-\n",
      "tions approaches conditional expectation. We discuss such approaches in\n",
      "Chapter 5.\n",
      "A more simplistic viewpoint is to construct targets tkfor each class,\n",
      "where tkis the kth column of the K×Kidentity matrix. Our prediction\n",
      "problem is to try and reproduce the appropriate target for an observation.\n",
      "With the same coding as before, the response vector yi(ith row of Y) for\n",
      "observation ihas the value yi=tkifgi=k. We might then ﬁt the linear\n",
      "model by least squares:\n",
      "min\n",
      "BN∑\n",
      "i=1||yi−[(1,xT\n",
      "i)B]T||2. (4.5)\n",
      "The criterion is a sum-of-squared Euclidean distances of the ﬁtted vectors\n",
      "from their targets. A new observation is classiﬁed by computing its ﬁtted\n",
      "vector ˆf(x) and classifying to the closest target:\n",
      "ˆG(x) = argmin\n",
      "k||ˆf(x)−tk||2. (4.6)\n",
      "This is exactly the same as the previous approach:\n",
      "•The sum-of-squared-norm criterion is exactly the criterion for multi-\n",
      "ple response linear regression, just viewed slightly diﬀerently. Since\n",
      "a squared norm is itself a sum of squares, the components decouple\n",
      "and can be rearranged as a separate linear model for each element.\n",
      "Note that this is only possible because there is nothing in the model\n",
      "that binds the diﬀerent responses together.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.2 Linear Regression of an Indicator Matrix 105\n",
      "Linear Regression\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      "11111\n",
      "11\n",
      "1\n",
      "11111\n",
      "111\n",
      "11 111\n",
      "111\n",
      "1111\n",
      "111\n",
      "1\n",
      "11111\n",
      "111\n",
      "11\n",
      "1111\n",
      "11\n",
      "11\n",
      "11111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111111\n",
      "11\n",
      "111\n",
      "11 1\n",
      "111\n",
      "1\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "111\n",
      "11111\n",
      "1\n",
      "1111\n",
      "11\n",
      "111\n",
      "111\n",
      "111\n",
      "1\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "111 11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "111 111\n",
      "11\n",
      "11\n",
      "11\n",
      "111111\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "111\n",
      "1\n",
      "11 1\n",
      "111\n",
      "1111\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "11 1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "1\n",
      "11\n",
      "111 1\n",
      "11\n",
      "1\n",
      "11 11\n",
      "1\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "1\n",
      "1111\n",
      "1111\n",
      "11\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "1 1\n",
      "111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "1\n",
      "1111111\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "11\n",
      "11\n",
      "1 1\n",
      "11\n",
      "11111\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1 12\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "222\n",
      "2222\n",
      "2\n",
      "22222\n",
      "2\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22222\n",
      "2222\n",
      "2\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2222222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2 2\n",
      "22\n",
      "2222\n",
      "2\n",
      "2\n",
      "222\n",
      "2222\n",
      "22\n",
      "222\n",
      "222\n",
      "222\n",
      "222\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "2222\n",
      "222\n",
      "2222\n",
      "22\n",
      "222\n",
      "2\n",
      "2222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "2222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22222\n",
      "2222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "22222\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "2\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2222\n",
      "2222\n",
      "222\n",
      "222\n",
      "222\n",
      "2222\n",
      "2\n",
      "222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "222 22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22 222\n",
      "222\n",
      "2\n",
      "22\n",
      "22 2\n",
      "2\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "222 22\n",
      "2\n",
      "222\n",
      "2222\n",
      "222\n",
      "2\n",
      "2 2222 2\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "2222\n",
      "2222\n",
      "2\n",
      "222\n",
      "22 2\n",
      "22\n",
      "2\n",
      "22\n",
      "22222\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "2 22 222\n",
      "22\n",
      "22\n",
      "222\n",
      "22 22\n",
      "222\n",
      "22\n",
      "22\n",
      "22 22\n",
      "22\n",
      "223\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "3333\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33333\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3333\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33333\n",
      "33\n",
      "333\n",
      "333\n",
      "3333\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "3333\n",
      "333\n",
      "333\n",
      "3\n",
      "333\n",
      "3\n",
      "3\n",
      "33333333333\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33333\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "3 33\n",
      "333\n",
      "3\n",
      "33 33\n",
      "3\n",
      "33\n",
      "333\n",
      "333333\n",
      "3\n",
      "3333\n",
      "33\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "33333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "33 3\n",
      "3\n",
      "3333\n",
      "333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "3333333\n",
      "3333\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "33 3\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "3\n",
      "33Linear Discriminant Analysis\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      "11111\n",
      "11\n",
      "1\n",
      "11111\n",
      "111\n",
      "11 111\n",
      "111\n",
      "1111\n",
      "111\n",
      "1\n",
      "11111\n",
      "111\n",
      "11\n",
      "1111\n",
      "11\n",
      "11\n",
      "11111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111111\n",
      "11\n",
      "111\n",
      "11 1\n",
      "111\n",
      "1\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "111\n",
      "11111\n",
      "1\n",
      "1111\n",
      "11\n",
      "111\n",
      "111\n",
      "111\n",
      "1\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "111 11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "111 111\n",
      "11\n",
      "11\n",
      "11\n",
      "111111\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "111\n",
      "1\n",
      "11 1\n",
      "111\n",
      "1111\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "11 1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "1\n",
      "11\n",
      "111 1\n",
      "11\n",
      "1\n",
      "11 11\n",
      "1\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "1\n",
      "1111\n",
      "1111\n",
      "11\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "1 1\n",
      "111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "1\n",
      "1111111\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "11\n",
      "11\n",
      "1 1\n",
      "11\n",
      "11111\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1 12\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "222\n",
      "2222\n",
      "2\n",
      "22222\n",
      "2\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22222\n",
      "2222\n",
      "2\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2222222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2 2\n",
      "22\n",
      "2222\n",
      "2\n",
      "2\n",
      "222\n",
      "2222\n",
      "22\n",
      "222\n",
      "222\n",
      "222\n",
      "222\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "2222\n",
      "222\n",
      "2222\n",
      "22\n",
      "222\n",
      "2\n",
      "2222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "2222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22222\n",
      "2222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "22222\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "2\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2222\n",
      "2222\n",
      "222\n",
      "222\n",
      "222\n",
      "2222\n",
      "2\n",
      "222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "222 22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22 222\n",
      "222\n",
      "2\n",
      "22\n",
      "22 2\n",
      "2\n",
      "2222\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "222 22\n",
      "2\n",
      "222\n",
      "2222\n",
      "222\n",
      "2\n",
      "2 2222 2\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "2222\n",
      "2222\n",
      "2\n",
      "222\n",
      "22 2\n",
      "22\n",
      "2\n",
      "22\n",
      "22222\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "2 22 222\n",
      "22\n",
      "22\n",
      "222\n",
      "22 22\n",
      "222\n",
      "22\n",
      "22\n",
      "22 22\n",
      "22\n",
      "223\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "3333\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33333\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3333\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33333\n",
      "33\n",
      "333\n",
      "333\n",
      "3333\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "3333\n",
      "333\n",
      "333\n",
      "3\n",
      "333\n",
      "3\n",
      "3\n",
      "33333333333\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33333\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "3 33\n",
      "333\n",
      "3\n",
      "33 33\n",
      "3\n",
      "33\n",
      "333\n",
      "333333\n",
      "3\n",
      "3333\n",
      "33\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "33333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "33 3\n",
      "3\n",
      "3333\n",
      "333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "3\n",
      "3333333\n",
      "3333\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "33 3\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "X1 X1\n",
      "X2X2\n",
      "FIGURE 4.2. The data come from three classes in I R2and are easily separated\n",
      "by linear decision boundaries. The right plot shows the boundar ies found by linear\n",
      "discriminant analysis. The left plot shows the boundaries found b y linear regres-\n",
      "sion of the indicator response variables. The middle class is c ompletely masked\n",
      "(never dominates).\n",
      "•The closest target classiﬁcation rule (4.6) is easily seen to be exactly\n",
      "the same as the maximum ﬁtted component criterion (4.4), but does\n",
      "require that the ﬁtted values sum to 1.\n",
      "There is a serious problem with the regression approach when the number\n",
      "of classes K≥3, especially prevalent when Kis large. Because of the rigid\n",
      "nature of the regression model, classes can be masked by others. Figure 4.2\n",
      "illustrates an extreme situation when K= 3. The three classes are perfectly\n",
      "separated by linear decision boundaries, yet linear regression misses the\n",
      "middle class completely.\n",
      "In Figure 4.3 we have projected the data onto the line joining the three\n",
      "centroids (there is no information in the orthogonal direction in this case),\n",
      "and we have included and coded the three response variables Y1,Y2and\n",
      "Y3. The three regression lines (left panel) are included, and we see that\n",
      "the line corresponding to the middle class is horizontal and its ﬁtted values\n",
      "are never dominant! Thus, observations from class 2 are classiﬁed either\n",
      "as class 1 or class 3. The right panel uses quadratic regression rather than\n",
      "linear regression. For this simple example a quadratic rather than linear\n",
      "ﬁt (for the middle class at least) would solve the problem. However, it\n",
      "can be seen that if there were four rather than three classes lined up like\n",
      "this, a quadratic would not come down fast enough, and a cubic would\n",
      "be needed as well. A loose but general rule is that if K≥3 classes are\n",
      "lined up, polynomial terms up to degree K−1 might be needed to resolve\n",
      "them. Note also that these are polynomials along the derived direction\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "106 4. Linear Methods for Classiﬁcation\n",
      "111\n",
      "111\n",
      "1\n",
      "1\n",
      "1111\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11111\n",
      "1\n",
      "11111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "11111\n",
      "111\n",
      "111\n",
      "1\n",
      "1111\n",
      "111\n",
      "111\n",
      "111\n",
      "111111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "1111\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "1111\n",
      "111\n",
      "111\n",
      "12222 2222222 2 2 222 2 2 222222 222222 2 2 2222 22 22222 2222222222222 22 22222 222 2222222 2 22 222 2 22222 22 22 222 222222 22222222 22 2 222222222 22222222 222 222 2 222222222222222 2\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "33333\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "333333\n",
      "333333\n",
      "3\n",
      "3333\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "33333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "33\n",
      "3333\n",
      "33333\n",
      "3333\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "3\n",
      "3333\n",
      "333\n",
      "3\n",
      "33333\n",
      "0.00.51.0\n",
      "0.0 0.2 0.4 0.6 0.8 1.0111\n",
      "111\n",
      "1\n",
      "1\n",
      "1111\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "11111\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "1111\n",
      "111\n",
      "111\n",
      "111\n",
      "111111\n",
      "111\n",
      "11111\n",
      "111111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "111\n",
      "1\n",
      "1111111 111\n",
      "111111111111\n",
      "1111 1111111 11111111111111112\n",
      "2\n",
      "22\n",
      "2\n",
      "2222\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "2\n",
      "22222\n",
      "2\n",
      "22\n",
      "222\n",
      "22\n",
      "222222\n",
      "222222\n",
      "2\n",
      "22222\n",
      "2\n",
      "222\n",
      "222222222222222\n",
      "22\n",
      "22222222222222222 22 22\n",
      "22 22\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "222\n",
      "222\n",
      "2333333\n",
      "333333\n",
      "3333\n",
      "33333333\n",
      "333333\n",
      "33\n",
      "33333\n",
      "3 333333333333 3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333 333\n",
      "33333\n",
      "33\n",
      "3\n",
      "333333333\n",
      "33\n",
      "3333\n",
      "33333\n",
      "3333\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "3\n",
      "333\n",
      "3\n",
      "33\n",
      "3\n",
      "33333\n",
      "0.00.51.0\n",
      "0.0 0.2 0.4 0.6 0.8 1.0Degree = 1; Error = 0.33 Degree = 2; Error = 0.04\n",
      "FIGURE 4.3. The eﬀects of masking on linear regression in I Rfor a three-class\n",
      "problem. The rug plot at the base indicates the positions and class membership of\n",
      "each observation. The three curves in each panel are the ﬁtted re gressions to the\n",
      "three-class indicator variables; for example, for the blue cl ass,yblueis1for the\n",
      "blue observations, and 0for the green and orange. The ﬁts are linear and quadratic\n",
      "polynomials. Above each plot is the training error rate. The Bay es error rate is\n",
      "0.025for this problem, as is the LDA error rate.\n",
      "passing through the centroids, which can have arbitrary orientation. So in\n",
      "p-dimensional input space, one would need general polynomial terms and\n",
      "cross-products of total degree K−1,O(pK−1) terms in all, to resolve such\n",
      "worst-case scenarios.\n",
      "The example is extreme, but for large Kand small psuch maskings\n",
      "naturally occur. As a more realistic illustration, Figure 4.4 is a project ion\n",
      "of the training data for a vowel recognition problem onto an informative\n",
      "two-dimensional subspace. There are K= 11 classes in p= 10 dimensions.\n",
      "This is a diﬃcult classiﬁcation problem, and the best methods achieve\n",
      "around 40% errors on the test data. The main point here is summarized in\n",
      "Table 4.1; linear regression has an error rate of 67%, while a close relat ive,\n",
      "linear discriminant analysis, has an error rate of 56%. It seems that mask ing\n",
      "has hurt in this case. While all the other methods in this chapter are based\n",
      "on linear functions of xas well, they use them in such a way that avoids\n",
      "this masking problem.\n",
      "4.3 Linear Discriminant Analysis\n",
      "Decision theory for classiﬁcation (Section 2.4) tells us that we need to know\n",
      "the class posteriors Pr( G|X) for optimal classiﬁcation. Suppose fk(x) is\n",
      "the class-conditional density of Xin class G=k, and let πkbe the prior\n",
      "probability of class k, with∑K\n",
      "k=1πk= 1. A simple application of Bayes\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.3 Linear Discriminant Analysis 107\n",
      "Coordinate 1 for Training DataCoordinate 2 for Training Data\n",
      "-4 -2 0 2 4-6 -4 -2 0 2 4oooo oo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oooooooooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o ooooooo\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooooooooooooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "oooooo o\n",
      "o\n",
      "o\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooooooooooo\n",
      "oooooooooooooooooo\n",
      "oooooooooooo\n",
      "o\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "oooooooooooooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oooooooo\n",
      "oooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "ooooooooo oooo\n",
      "o\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooooooooooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooooooooooooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooooooooooooo\n",
      "o ooooooooooooooooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooooooooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo oo\n",
      "oooooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "o\n",
      "o\n",
      "ooooooo\n",
      "oooooooooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "••••••••••••••\n",
      "••\n",
      "••\n",
      "••••Linear Discriminant Analysis\n",
      "FIGURE 4.4. A two-dimensional plot of the vowel training data. There are\n",
      "eleven classes with X∈I R10, and this is the best view in terms of a LDA model\n",
      "(Section 4.3.3). The heavy circles are the projected mean vec tors for each class.\n",
      "The class overlap is considerable.\n",
      "TABLE 4.1. Training and test error rates using a variety of linear techniques\n",
      "on the vowel data. There are eleven classes in ten dimensions, o f which three\n",
      "account for 90%of the variance (via a principal components analysis). We see\n",
      "that linear regression is hurt by masking, increasing the test and training error\n",
      "by over 10%.\n",
      "Technique Error Rates\n",
      "Training Test\n",
      "Linear regression 0.48 0.67\n",
      "Linear discriminant analysis 0.32 0.56\n",
      "Quadratic discriminant analysis 0.01 0.53\n",
      "Logistic regression 0.22 0.51\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "108 4. Linear Methods for Classiﬁcation\n",
      "theorem gives us\n",
      "Pr(G=k|X=x) =fk(x)πk∑K\n",
      "ℓ=1fℓ(x)πℓ. (4.7)\n",
      "We see that in terms of ability to classify, having the fk(x) is almost equiv-\n",
      "alent to having the quantity Pr( G=k|X=x).\n",
      "Many techniques are based on models for the class densities:\n",
      "•linear and quadratic discriminant analysis use Gaussian densities;\n",
      "•more ﬂexible mixtures of Gaussians allow for nonlinear decision bound-\n",
      "aries (Section 6.8);\n",
      "•general nonparametric density estimates for each class density allow\n",
      "the most ﬂexibility (Section 6.6.2);\n",
      "•Naive Bayes models are a variant of the previous case, and assume\n",
      "that each of the class densities are products of marginal densities;\n",
      "that is, they assume that the inputs are conditionally independent in\n",
      "each class (Section 6.6.3).\n",
      "Suppose that we model each class density as multivariate Gaussian\n",
      "fk(x) =1\n",
      "(2π)p/2|Σk|1/2e−1\n",
      "2(x−θk)TΣ−1\n",
      "k(x−θk). (4.8)\n",
      "Linear discriminant analysis (LDA) arises in the special case when we\n",
      "assume that the classes have a common covariance matrix Σk=Σ∀k. In\n",
      "comparing two classes kandℓ, it is suﬃcient to look at the log-ratio, and\n",
      "we see that\n",
      "logPr(G=k|X=x)\n",
      "Pr(G=ℓ|X=x)= logfk(x)\n",
      "fℓ(x)+ logπk\n",
      "πℓ\n",
      "= logπk\n",
      "πℓ−1\n",
      "2(θk+θℓ)TΣ−1(θk−θℓ)\n",
      "+xTΣ−1(θk−θℓ),(4.9)\n",
      "an equation linear in x. The equal covariance matrices cause the normal-\n",
      "ization factors to cancel, as well as the quadratic part in the exponents.\n",
      "This linear log-odds function implies that the decision boundary between\n",
      "classes kandℓ—the set where Pr( G=k|X=x) = Pr( G=ℓ|X=x)—is\n",
      "linear in x; inpdimensions a hyperplane. This is of course true for any pair\n",
      "of classes, so all the decision boundaries are linear. If we divide IRpinto\n",
      "regions that are classiﬁed as class 1, class 2, etc., these regions will be sep-\n",
      "arated by hyperplanes. Figure 4.5 (left panel) shows an idealized example\n",
      "with three classes and p= 2. Here the data do arise from three Gaus-\n",
      "sian distributions with a common covariance matrix. We have included in\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.3 Linear Discriminant Analysis 109\n",
      "+++\n",
      "3\n",
      "21\n",
      "11\n",
      "233\n",
      "3\n",
      "123\n",
      "32\n",
      "11211\n",
      "33\n",
      "12 1\n",
      "23\n",
      "23\n",
      "3\n",
      "12\n",
      "211\n",
      "1\n",
      "13\n",
      "222\n",
      "21 3\n",
      "2 23\n",
      "13\n",
      "13\n",
      "32\n",
      "13\n",
      "3\n",
      "23\n",
      "133\n",
      "2133\n",
      "22\n",
      "3\n",
      "22\n",
      "21\n",
      "11\n",
      "11\n",
      "2\n",
      "133\n",
      "1\n",
      "13\n",
      "32\n",
      "222 3\n",
      "12\n",
      "FIGURE 4.5. The left panel shows three Gaussian distributions, with the sa me\n",
      "covariance and diﬀerent means. Included are the contours of constant density\n",
      "enclosing 95% of the probability in each case. The Bayes decision boundari es\n",
      "between each pair of classes are shown (broken straight lines ), and the Bayes\n",
      "decision boundaries separating all three classes are the thic ker solid lines (a subset\n",
      "of the former). On the right we see a sample of 30drawn from each Gaussian\n",
      "distribution, and the ﬁtted LDA decision boundaries.\n",
      "the ﬁgure the contours corresponding to 95% highest probability density,\n",
      "as well as the class centroids. Notice that the decision boundaries are not\n",
      "the perpendicular bisectors of the line segments joining the centroids. This\n",
      "would be the case if the covariance Σwere spherical σ2I, and the class\n",
      "priors were equal. From (4.9) we see that the linear discriminant functions\n",
      "δk(x) =xTΣ−1θk−1\n",
      "2θT\n",
      "kΣ−1θk+ logπk (4.10)\n",
      "are an equivalent description of the decision rule, with G(x) = argmaxkδk(x).\n",
      "In practice we do not know the parameters of the Gaussian distributions,\n",
      "and will need to estimate them using our training data:\n",
      "•ˆπk=Nk/N, where Nkis the number of class- kobservations;\n",
      "•ˆθk=∑\n",
      "gi=kxi/Nk;\n",
      "•ˆΣ=∑K\n",
      "k=1∑\n",
      "gi=k(xi−ˆθk)(xi−ˆθk)T/(N−K).\n",
      "Figure 4.5 (right panel) shows the estimated decision boundaries based on\n",
      "a sample of size 30 each from three Gaussian distributions. Figure 4.1 on\n",
      "page 103 is another example, but here the classes are not Gaussian.\n",
      "With two classes there is a simple correspondence between linear dis-\n",
      "criminant analysis and classiﬁcation by linear least squares, as in (4.5) .\n",
      "The LDA rule classiﬁes to class 2 if\n",
      "xTˆΣ−1(ˆθ2−ˆθ1)>1\n",
      "2ˆθT\n",
      "2ˆΣ−1ˆθ2−1\n",
      "2ˆθT\n",
      "1ˆΣ−1ˆθ1+ log( N1/N)−log(N2/N)\n",
      "(4.11)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "110 4. Linear Methods for Classiﬁcation\n",
      "and class 1 otherwise. Suppose we code the targets in the two classes as +1\n",
      "and−1, respectively. It is easy to show that the coeﬃcient vector from least\n",
      "squares is proportional to the LDA direction given in (4.11) (Exercise 4. 2).\n",
      "[In fact, this correspondence occurs for any (distinct) coding of the targets;\n",
      "see Exercise 4.2]. However unless N1=N2the intercepts are diﬀerent and\n",
      "hence the resulting decision rules are diﬀerent.\n",
      "Since this derivation of the LDA direction via least squares does not use a\n",
      "Gaussian assumption for the features, its applicability extends beyond the\n",
      "realm of Gaussian data. However the derivation of the particular intercept\n",
      "or cut-point given in (4.11) doesrequire Gaussian data. Thus it makes\n",
      "sense to instead choose the cut-point that empirically minimizes training\n",
      "error for a given dataset. This is something we have found to work well in\n",
      "practice, but have not seen it mentioned in the literature.\n",
      "With more than two classes, LDA is not the same as linear regression of\n",
      "the class indicator matrix, and it avoids the masking problems associated\n",
      "with that approach (Hastie et al., 1994). A correspondence between regres-\n",
      "sion and LDA can be established through the notion of optimal scoring ,\n",
      "discussed in Section 12.5.\n",
      "Getting back to the general discriminant problem (4.8), if the Σkare\n",
      "not assumed to be equal, then the convenient cancellations in (4.9) do not\n",
      "occur; in particular the pieces quadratic in xremain. We then get quadratic\n",
      "discriminant functions (QDA),\n",
      "δk(x) =−1\n",
      "2log|Σk| −1\n",
      "2(x−θk)TΣ−1\n",
      "k(x−θk) + log πk. (4.12)\n",
      "The decision boundary between each pair of classes kandℓis described by\n",
      "a quadratic equation {x:δk(x) =δℓ(x)}.\n",
      "Figure 4.6 shows an example (from Figure 4.1 on page 103) where the\n",
      "three classes are Gaussian mixtures (Section 6.8) and the decision bound-\n",
      "aries are approximated by quadratic equations in x. Here we illustrate\n",
      "two popular ways of ﬁtting these quadratic boundaries. The right plot\n",
      "uses QDA as described here, while the left plot uses LDA in the enlarged\n",
      "ﬁve-dimensional quadratic polynomial space. The diﬀerences are generally\n",
      "small; QDA is the preferred approach, with the LDA method a convenient\n",
      "substitute2.\n",
      "The estimates for QDA are similar to those for LDA, except that separate\n",
      "covariance matrices must be estimated for each class. When pis large this\n",
      "can mean a dramatic increase in parameters. Since the decision boundaries\n",
      "are functions of the parameters of the densities, counting the number of\n",
      "parameters must be done with care. For LDA, it seems there are ( K−\n",
      "1)×(p+ 1) parameters, since we only need the diﬀerences δk(x)−δK(x)\n",
      "2For this ﬁgure and many similar ﬁgures in the book we compute t he decision bound-\n",
      "aries by an exhaustive contouring method. We compute the dec ision rule on a ﬁne lattice\n",
      "of points, and then use contouring algorithms to compute the boundaries.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.3 Linear Discriminant Analysis 111\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1111 1\n",
      "111\n",
      "1\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "1 1\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "1 1\n",
      "11\n",
      "1111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "11 111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11 1\n",
      "1\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "111\n",
      "1111\n",
      "11\n",
      "1 11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "12\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "2 2\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "2 2\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2222\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2222 2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "33333\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1111 1\n",
      "111\n",
      "1\n",
      "11\n",
      "111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "1 1\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "1 1\n",
      "11\n",
      "1111\n",
      "11111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11 1\n",
      "11\n",
      "11 111\n",
      "1 11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11 1\n",
      "1\n",
      "11\n",
      "11\n",
      "1111\n",
      "1111\n",
      "1\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "111\n",
      "1111\n",
      "11\n",
      "1 11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "12\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "2 2\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "2 2\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "2222\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2222 2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "22\n",
      "222\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "22222\n",
      "22\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "333\n",
      "3333\n",
      "333\n",
      "3333\n",
      "3\n",
      "33333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "333\n",
      "3\n",
      "333\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "333\n",
      "33333\n",
      "3\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3333\n",
      "33\n",
      "333\n",
      "3\n",
      "333\n",
      "33333\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "FIGURE 4.6. Two methods for ﬁtting quadratic boundaries. The left plot show s\n",
      "the quadratic decision boundaries for the data in Figure 4.1 ( obtained using LDA\n",
      "in the ﬁve-dimensional space X1, X2, X1X2, X2\n",
      "1, X2\n",
      "2). The right plot shows the\n",
      "quadratic decision boundaries found by QDA. The diﬀerences are small, as is\n",
      "usually the case.\n",
      "between the discriminant functions where Kis some pre-chosen class (here\n",
      "we have chosen the last), and each diﬀerence requires p+ 1 parameters3.\n",
      "Likewise for QDA there will be ( K−1)× {p(p+ 3)/2 + 1}parameters.\n",
      "Both LDA and QDA perform well on an amazingly large and diverse set\n",
      "of classiﬁcation tasks. For example, in the STATLOG project (Michie et\n",
      "al., 1994) LDA was among the top three classiﬁers for 7 of the 22 datasets,\n",
      "QDA among the top three for four datasets, and one of the pair were in the\n",
      "top three for 10 datasets. Both techniques are widely used, and entire books\n",
      "are devoted to LDA. It seems that whatever exotic tools are the rage of the\n",
      "day, we should always have available these two simple tools. The question\n",
      "arises why LDA and QDA have such a good track record. The reason is not\n",
      "likely to be that the data are approximately Gaussian, and in addition for\n",
      "LDA that the covariances are approximately equal. More likely a reason is\n",
      "that the data can only support simple decision boundaries such as linear or\n",
      "quadratic, and the estimates provided via the Gaussian models are stable.\n",
      "This is a bias variance tradeoﬀ—we can put up with the bias of a linear\n",
      "decision boundary because it can be estimated with much lower variance\n",
      "than more exotic alternatives. This argument is less believable for QDA,\n",
      "since it can have many parameters itself, although perhaps fewer than the\n",
      "non-parametric alternatives.\n",
      "3Although we ﬁt the covariance matrix ˆΣto compute the LDA discriminant functions,\n",
      "a much reduced function of it is all that is required to estima te the O(p) parameters\n",
      "needed to compute the decision boundaries.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "112 4. Linear Methods for Classiﬁcation\n",
      "Misclassification Rate\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Regularized Discriminant Analysis on the Vowel Data\n",
      "Test Data\n",
      "Train Data\n",
      "α\n",
      "FIGURE 4.7. Test and training errors for the vowel data, using regularized\n",
      "discriminant analysis with a series of values of α∈[0,1]. The optimum for the\n",
      "test data occurs around α= 0.9, close to quadratic discriminant analysis.\n",
      "4.3.1 Regularized Discriminant Analysis\n",
      "Friedman (1989) proposed a compromise between LDA and QDA, which\n",
      "allows one to shrink the separate covariances of QDA toward a common\n",
      "covariance as in LDA. These methods are very similar in ﬂavor to ridge\n",
      "regression. The regularized covariance matrices have the form\n",
      "ˆΣk(α) =αˆΣk+ (1−α)ˆΣ, (4.13)\n",
      "where ˆΣis the pooled covariance matrix as used in LDA. Here α∈[0,1]\n",
      "allows a continuum of models between LDA and QDA, and needs to be\n",
      "speciﬁed. In practice αcan be chosen based on the performance of the\n",
      "model on validation data, or by cross-validation.\n",
      "Figure 4.7 shows the results of RDA applied to the vowel data. Both\n",
      "the training and test error improve with increasing α, although the test\n",
      "error increases sharply after α= 0.9. The large discrepancy between the\n",
      "training and test error is partly due to the fact that there are many repeat\n",
      "measurements on a small number of individuals, diﬀerent in the training\n",
      "and test set.\n",
      "Similar modiﬁcations allow ˆΣitself to be shrunk toward the scalar\n",
      "covariance,\n",
      "ˆΣ(γ) =γˆΣ+ (1−γ)ˆσ2I (4.14)\n",
      "forγ∈[0,1]. Replacing ˆΣin (4.13) by ˆΣ(γ) leads to a more general family\n",
      "of covariances ˆΣ(α,γ) indexed by a pair of parameters.\n",
      "In Chapter 12, we discuss other regularized versions of LDA, which are\n",
      "more suitable when the data arise from digitized analog signals and images.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.3 Linear Discriminant Analysis 113\n",
      "In these situations the features are high-dimensional and correlated, and the\n",
      "LDA coeﬃcients can be regularized to be smooth or sparse in the original\n",
      "domain of the signal. This leads to better generalization and allows for\n",
      "easier interpretation of the coeﬃcients. In Chapter 18 we also deal with\n",
      "very high-dimensional problems, where for example the features are gene-\n",
      "expression measurements in microarray studies. There the methods focus\n",
      "on the case γ= 0 in (4.14), and other severely regularized versions of LDA.\n",
      "4.3.2 Computations for LDA\n",
      "As a lead-in to the next topic, we brieﬂy digress on the computations\n",
      "required for LDA and especially QDA. Their computations are simpliﬁed\n",
      "by diagonalizing ˆΣorˆΣk. For the latter, suppose we compute the eigen-\n",
      "decomposition for each ˆΣk=UkDkUT\n",
      "k, where Ukisp×porthonormal,\n",
      "andDka diagonal matrix of positive eigenvalues dkℓ. Then the ingredients\n",
      "forδk(x) (4.12) are\n",
      "•(x−ˆθk)TˆΣ−1\n",
      "k(x−ˆθk) = [UT\n",
      "k(x−ˆθk)]TD−1\n",
      "k[UT\n",
      "k(x−ˆθk)];\n",
      "•log|ˆΣk|=∑\n",
      "ℓlogdkℓ.\n",
      "In light of the computational steps outlined above, the LDA classiﬁer\n",
      "can be implemented by the following pair of steps:\n",
      "•Sphere the data with respect to the common covariance estimate ˆΣ:\n",
      "X∗←D−1\n",
      "2UTX, where ˆΣ=UDUT. The common covariance esti-\n",
      "mate of X∗will now be the identity.\n",
      "•Classify to the closest class centroid in the transformed space, modulo\n",
      "the eﬀect of the class prior probabilities πk.\n",
      "4.3.3 Reduced-Rank Linear Discriminant Analysis\n",
      "So far we have discussed LDA as a restricted Gaussian classiﬁer. Part of\n",
      "its popularity is due to an additional restriction that allows us to view\n",
      "informative low-dimensional projections of the data.\n",
      "TheKcentroids in p-dimensional input space lie in an aﬃne subspace\n",
      "of dimension ≤K−1, and if pis much larger than K, this will be a con-\n",
      "siderable drop in dimension. Moreover, in locating the closest centroid, we\n",
      "can ignore distances orthogonal to this subspace, since they will contribute\n",
      "equally to each class. Thus we might just as well project the X∗onto this\n",
      "centroid-spanning subspace HK−1, and make distance comparisons there.\n",
      "Thus there is a fundamental dimension reduction in LDA, namely, that we\n",
      "need only consider the data in a subspace of dimension at most K−1.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "114 4. Linear Methods for Classiﬁcation\n",
      "IfK= 3, for instance, this could allow us to view the data in a two-\n",
      "dimensional plot, color-coding the classes. In doing so we would not have\n",
      "relinquished any of the information needed for LDA classiﬁcation.\n",
      "What if K >3? We might then ask for a L < K −1 dimensional subspace\n",
      "HL⊆HK−1optimal for LDA in some sense. Fisher deﬁned optimal to\n",
      "mean that the projected centroids were spread out as much as possible in\n",
      "terms of variance. This amounts to ﬁnding principal component subspaces\n",
      "of the centroids themselves (principal components are described brieﬂy in\n",
      "Section 3.5.1, and in more detail in Section 14.5.1). Figure 4.4 shows such an\n",
      "optimal two-dimensional subspace for the vowel data. Here there are eleven\n",
      "classes, each a diﬀerent vowel sound, in a ten-dimensional input space. The\n",
      "centroids require the full space in this case, since K−1 =p, but we have\n",
      "shown an optimal two-dimensional subspace. The dimensions are ordered,\n",
      "so we can compute additional dimensions in sequence. Figure 4.8 shows four\n",
      "additional pairs of coordinates, also known as canonical ordiscriminant\n",
      "variables. In summary then, ﬁnding the sequences of optimal subspaces\n",
      "for LDA involves the following steps:\n",
      "•compute the K×pmatrix of class centroids Mand the common\n",
      "covariance matrix W(forwithin-class covariance);\n",
      "•compute M∗=MW−1\n",
      "2using the eigen-decomposition of W;\n",
      "•compute B∗, the covariance matrix of M∗(Bforbetween-class covari-\n",
      "ance), and its eigen-decomposition B∗=V∗DBV∗T. The columns\n",
      "v∗\n",
      "ℓofV∗in sequence from ﬁrst to last deﬁne the coordinates of the\n",
      "optimal subspaces.\n",
      "Combining all these operations the ℓthdiscriminant variable is given by\n",
      "Zℓ=vT\n",
      "ℓXwithvℓ=W−1\n",
      "2v∗\n",
      "ℓ.\n",
      "Fisher arrived at this decomposition via a diﬀerent route, without refer-\n",
      "ring to Gaussian distributions at all. He posed the problem:\n",
      "Find the linear combination Z=aTXsuch that the between-\n",
      "class variance is maximized relative to the within-class var iance.\n",
      "Again, the between class variance is the variance of the class means of\n",
      "Z, and the within class variance is the pooled variance about the means.\n",
      "Figure 4.9 shows why this criterion makes sense. Although the direction\n",
      "joining the centroids separates the means as much as possible (i.e., max-\n",
      "imizes the between-class variance), there is considerable overlap between\n",
      "the projected classes due to the nature of the covariances. By taking the\n",
      "covariance into account as well, a direction with minimum overlap can be\n",
      "found.\n",
      "The between-class variance of ZisaTBaand the within-class variance\n",
      "aTWa, where Wis deﬁned earlier, and Bis the covariance matrix of the\n",
      "class centroid matrix M. Note that B+W=T, where Tis the total\n",
      "covariance matrix of X, ignoring class information.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.3 Linear Discriminant Analysis 115\n",
      "Coordinate 1 Coordinate 3 \n",
      "-4 -2 0 2 4-2 0 2o\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooooo ooooooo\n",
      "o\n",
      "o\n",
      "ooo oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooooooo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "oooooo\n",
      "oooooooooooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "oo ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooooooooooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooooooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooooooooooo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "oooooooo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooooooooo oooo ooooooo\n",
      "ooooooo\n",
      "••••••••\n",
      "••••••••••••••\n",
      "Coordinate 2 Coordinate 3 \n",
      "-6 -4 -2 0 2 4-2 0 2o\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "o\n",
      "o\n",
      "ooo oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooooooo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "oooooo\n",
      "oooooooooooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "oo ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooooooooooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooooooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooooooooooo\n",
      "oo oo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "oooooooo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooooooooo ooooooooooo\n",
      "ooooooo\n",
      "••••••••\n",
      "••••••••••••••\n",
      "Coordinate 1 Coordinate 7 \n",
      "-4 -2 0 2 4-3 -2 -1 0 1 2 3ooo ooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooooooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooooooooooooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooooooooooo\n",
      "oo oooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooooooo\n",
      "o\n",
      "ooooooooo\n",
      "o\n",
      "ooooooooooo\n",
      "o\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooooooo\n",
      "ooo o\n",
      "ooo\n",
      "ooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "ooooooooooo\n",
      "o\n",
      "oooooo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oooooooooooo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooooooooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "ooooooooooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "••••••••••••••••••••••\n",
      "Coordinate 9 Coordinate 10 \n",
      "-2 -1 0 1 2 3-2 -1 0 1 2oo\n",
      "o\n",
      "o\n",
      "oooooooo\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooooooooo\n",
      "o\n",
      "o\n",
      "oooooooooooo o\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooooo o\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooooooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooooooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooooooooooo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "oooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooooo\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooooooooo\n",
      "oooo\n",
      "o\n",
      "oooooooooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooooooooo\n",
      "oooo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooooooooooooooo\n",
      "ooo\n",
      "o\n",
      "o ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "o••••••••••••••••••••••Linear Discriminant Analysis\n",
      "FIGURE 4.8. Four projections onto pairs of canonical variates. Notice that a s\n",
      "the rank of the canonical variates increases, the centroids becom e less spread out.\n",
      "In the lower right panel they appear to be superimposed, and the classes most\n",
      "confused.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "116 4. Linear Methods for Classiﬁcation\n",
      "++\n",
      "++\n",
      "FIGURE 4.9. Although the line joining the centroids deﬁnes the direction of\n",
      "greatest centroid spread, the projected data overlap becaus e of the covariance\n",
      "(left panel). The discriminant direction minimizes this overla p for Gaussian data\n",
      "(right panel).\n",
      "Fisher’s problem therefore amounts to maximizing the Rayleigh quotient ,\n",
      "max\n",
      "aaTBa\n",
      "aTWa, (4.15)\n",
      "or equivalently\n",
      "max\n",
      "aaTBasubject to aTWa= 1. (4.16)\n",
      "This is a generalized eigenvalue problem, with agiven by the largest\n",
      "eigenvalue of W−1B. It is not hard to show (Exercise 4.1) that the optimal\n",
      "a1is identical to v1deﬁned above. Similarly one can ﬁnd the next direction\n",
      "a2, orthogonal in Wtoa1, such that aT\n",
      "2Ba2/aT\n",
      "2Wa2is maximized; the\n",
      "solution is a2=v2, and so on. The aℓare referred to as discriminant\n",
      "coordinates , not to be confused with discriminant functions. They are also\n",
      "referred to as canonical variates , since an alternative derivation of these\n",
      "results is through a canonical correlation analysis of the indicator response\n",
      "matrix Yon the predictor matrix X. This line is pursued in Section 12.5.\n",
      "To summarize the developments so far:\n",
      "•Gaussian classiﬁcation with common covariances leads to linear deci-\n",
      "sion boundaries. Classiﬁcation can be achieved by sphering the data\n",
      "with respect to W, and classifying to the closest centroid (modulo\n",
      "logπk) in the sphered space.\n",
      "•Since only the relative distances to the centroids count, one can con-\n",
      "ﬁne the data to the subspace spanned by the centroids in the sphered\n",
      "space.\n",
      "•This subspace can be further decomposed into successively optimal\n",
      "subspaces in term of centroid separation. This decomposition is iden-\n",
      "tical to the decomposition due to Fisher.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.3 Linear Discriminant Analysis 117\n",
      "DimensionMisclassification Rate\n",
      "2 4 6 8 100.3 0.4 0.5 0.6 0.7LDA and Dimension Reduction on the Vowel Data\n",
      "•\n",
      "••••• • ••••\n",
      "•\n",
      "• •\n",
      "•\n",
      "•••••Test Data\n",
      "Train Data\n",
      "FIGURE 4.10. Training and test error rates for the vowel data, as a function\n",
      "of the dimension of the discriminant subspace. In this case the b est error rate is\n",
      "for dimension 2. Figure 4.11 shows the decision boundaries in this space.\n",
      "The reduced subspaces have been motivated as a data reduction (for\n",
      "viewing) tool. Can they also be used for classiﬁcation, and what is the\n",
      "rationale? Clearly they can, as in our original derivation; we simply limit\n",
      "the distance-to-centroid calculations to the chosen subspace. One can show\n",
      "that this is a Gaussian classiﬁcation rule with the additional restriction\n",
      "that the centroids of the Gaussians lie in a L-dimensional subspace of IRp.\n",
      "Fitting such a model by maximum likelihood, and then constructing the\n",
      "posterior probabilities using Bayes’ theorem amounts to the classiﬁcation\n",
      "rule described above (Exercise 4.8).\n",
      "Gaussian classiﬁcation dictates the log πkcorrection factor in the dis-\n",
      "tance calculation. The reason for this correction can be seen in Figure 4.9.\n",
      "The misclassiﬁcation rate is based on the area of overlap between the two\n",
      "densities. If the πkare equal (implicit in that ﬁgure), then the optimal\n",
      "cut-point is midway between the projected means. If the πkare not equal,\n",
      "moving the cut-point toward the smaller class will improve the error rate.\n",
      "As mentioned earlier for two classes, one can derive the linear rule using\n",
      "LDA (or any other method), and then choose the cut-point to minimize\n",
      "misclassiﬁcation error over the training data.\n",
      "As an example of the beneﬁt of the reduced-rank restriction, we return\n",
      "to the vowel data. There are 11 classes and 10 variables, and hence 10\n",
      "possible dimensions for the classiﬁer. We can compute the training and\n",
      "test error in each of these hierarchical subspaces; Figure 4.10 shows the\n",
      "results. Figure 4.11 shows the decision boundaries for the classiﬁer based\n",
      "on the two-dimensional LDA solution.\n",
      "There is a close connection between Fisher’s reduced rank discriminant\n",
      "analysis and regression of an indicator response matrix. It turns out that\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "118 4. Linear Methods for Classiﬁcation\n",
      "oooo\n",
      "oo o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "o o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "o o\n",
      "oo\n",
      "o o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "Canonical Coordinate 1Canonical Coordinate 2Classification in Reduced Subspace\n",
      "••••••••••••••\n",
      "••\n",
      "••\n",
      "••••\n",
      "FIGURE 4.11. Decision boundaries for the vowel training data, in the two-di-\n",
      "mensional subspace spanned by the ﬁrst two canonical variates. Note that in\n",
      "any higher-dimensional subspace, the decision boundaries are h igher-dimensional\n",
      "aﬃne planes, and could not be represented as lines.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.4 Logistic Regression 119\n",
      "LDA amounts to the regression followed by an eigen-decomposition of\n",
      "ˆYTY. In the case of two classes, there is a single discriminant variable\n",
      "that is identical up to a scalar multiplication to either of the columns of ˆY.\n",
      "These connections are developed in Chapter 12. A related fact is that if one\n",
      "transforms the original predictors XtoˆY, then LDA using ˆYis identical\n",
      "to LDA in the original space (Exercise 4.3).\n",
      "4.4 Logistic Regression\n",
      "The logistic regression model arises from the desire to model the posterior\n",
      "probabilities of the Kclasses via linear functions in x, while at the same\n",
      "time ensuring that they sum to one and remain in [0 ,1]. The model has\n",
      "the form\n",
      "logPr(G= 1|X=x)\n",
      "Pr(G=K|X=x)=β10+βT\n",
      "1x\n",
      "logPr(G= 2|X=x)\n",
      "Pr(G=K|X=x)=β20+βT\n",
      "2x\n",
      "...\n",
      "logPr(G=K−1|X=x)\n",
      "Pr(G=K|X=x)=β(K−1)0+βT\n",
      "K−1x.(4.17)\n",
      "The model is speciﬁed in terms of K−1 log-odds or logit transformations\n",
      "(reﬂecting the constraint that the probabilities sum to one). Although the\n",
      "model uses the last class as the denominator in the odds-ratios, the choice\n",
      "of denominator is arbitrary in that the estimates are equivariant under this\n",
      "choice. A simple calculation shows that\n",
      "Pr(G=k|X=x) =exp(βk0+βT\n",
      "kx)\n",
      "1 +∑K−1\n",
      "ℓ=1exp(βℓ0+βT\n",
      "ℓx), k= 1,... ,K −1,\n",
      "Pr(G=K|X=x) =1\n",
      "1 +∑K−1\n",
      "ℓ=1exp(βℓ0+βT\n",
      "ℓx), (4.18)\n",
      "and they clearly sum to one. To emphasize the dependence on the entire pa-\n",
      "rameter set θ={β10,βT\n",
      "1,... ,β (K−1)0,βT\n",
      "K−1}, we denote the probabilities\n",
      "Pr(G=k|X=x) =pk(x;θ).\n",
      "When K= 2, this model is especially simple, since there is only a single\n",
      "linear function. It is widely used in biostatistical applications where binary\n",
      "responses (two classes) occur quite frequently. For example, patients survive\n",
      "or die, have heart disease or not, or a condition is present or absent.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "120 4. Linear Methods for Classiﬁcation\n",
      "4.4.1 Fitting Logistic Regression Models\n",
      "Logistic regression models are usually ﬁt by maximum likelihood, using the\n",
      "conditional likelihood of Ggiven X. Since Pr( G|X) completely speciﬁes the\n",
      "conditional distribution, the multinomial distribution is appropriate. The\n",
      "log-likelihood for Nobservations is\n",
      "ℓ(θ) =N∑\n",
      "i=1logpgi(xi;θ), (4.19)\n",
      "where pk(xi;θ) = Pr( G=k|X=xi;θ).\n",
      "We discuss in detail the two-class case, since the algorithms simplify\n",
      "considerably. It is convenient to code the two-class givia a 0 /1 response yi,\n",
      "where yi= 1 when gi= 1, and yi= 0 when gi= 2. Let p1(x;θ) =p(x;θ),\n",
      "andp2(x;θ) = 1−p(x;θ). The log-likelihood can be written\n",
      "ℓ(β) =N∑\n",
      "i=1{\n",
      "yilogp(xi;β) + (1 −yi)log(1 −p(xi;β))}\n",
      "=N∑\n",
      "i=1{\n",
      "yiβTxi−log(1 + eβTxi)}\n",
      ". (4.20)\n",
      "Hereβ={β10,β1}, and we assume that the vector of inputs xiincludes\n",
      "the constant term 1 to accommodate the intercept.\n",
      "To maximize the log-likelihood, we set its derivatives to zero. These score\n",
      "equations are\n",
      "∂ℓ(β)\n",
      "∂β=N∑\n",
      "i=1xi(yi−p(xi;β)) = 0 , (4.21)\n",
      "which are p+1 equations nonlinear inβ. Notice that since the ﬁrst compo-\n",
      "nent of xiis 1, the ﬁrst score equation speciﬁes that∑N\n",
      "i=1yi=∑N\n",
      "i=1p(xi;β);\n",
      "theexpected number of class ones matches the observed number (and hence\n",
      "also class twos.)\n",
      "To solve the score equations (4.21), we use the Newton–Raphson algo-\n",
      "rithm, which requires the second-derivative or Hessian matrix\n",
      "∂2ℓ(β)\n",
      "∂β∂βT=−N∑\n",
      "i=1xixiTp(xi;β)(1−p(xi;β)). (4.22)\n",
      "Starting with βold, a single Newton update is\n",
      "βnew=βold−(∂2ℓ(β)\n",
      "∂β∂βT)−1∂ℓ(β)\n",
      "∂β, (4.23)\n",
      "where the derivatives are evaluated at βold.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.4 Logistic Regression 121\n",
      "It is convenient to write the score and Hessian in matrix notation. Let\n",
      "ydenote the vector of yivalues, XtheN×(p+ 1) matrix of xivalues,\n",
      "pthe vector of ﬁtted probabilities with ith element p(xi;βold) andWa\n",
      "N×Ndiagonal matrix of weights with ith diagonal element p(xi;βold)(1−\n",
      "p(xi;βold)). Then we have\n",
      "∂ℓ(β)\n",
      "∂β=XT(y−p) (4.24)\n",
      "∂2ℓ(β)\n",
      "∂β∂βT=−XTWX (4.25)\n",
      "The Newton step is thus\n",
      "βnew=βold+ (XTWX)−1XT(y−p)\n",
      "= (XTWX)−1XTW(\n",
      "Xβold+W−1(y−p))\n",
      "= (XTWX)−1XTWz. (4.26)\n",
      "In the second and third line we have re-expressed the Newton step as a\n",
      "weighted least squares step, with the response\n",
      "z=Xβold+W−1(y−p), (4.27)\n",
      "sometimes known as the adjusted response . These equations get solved re-\n",
      "peatedly, since at each iteration pchanges, and hence so does Wandz.\n",
      "This algorithm is referred to as iteratively reweighted least squares or IRLS,\n",
      "since each iteration solves the weighted least squares problem:\n",
      "βnew←arg min\n",
      "β(z−Xβ)TW(z−Xβ). (4.28)\n",
      "It seems that β= 0 is a good starting value for the iterative procedure,\n",
      "although convergence is never guaranteed. Typically the algorithm does\n",
      "converge, since the log-likelihood is concave, but overshooting can occur.\n",
      "In the rare cases that the log-likelihood decreases, step size halving will\n",
      "guarantee convergence.\n",
      "For the multiclass case ( K≥3) the Newton algorithm can also be ex-\n",
      "pressed as an iteratively reweighted least squares algorithm, but with a\n",
      "vector ofK−1 responses and a nondiagonal weight matrix per observation.\n",
      "The latter precludes any simpliﬁed algorithms, and in this case it is numer-\n",
      "ically more convenient to work with the expanded vector θdirectly (Ex-\n",
      "ercise 4.4). Alternatively coordinate-descent methods (Section 3.8.6) can\n",
      "be used to maximize the log-likelihood eﬃciently. The Rpackageglmnet\n",
      "(Friedman et al., 2010) can ﬁt very large logistic regression problems ef-\n",
      "ﬁciently, both in Nandp. Although designed to ﬁt regularized models,\n",
      "options allow for unregularized ﬁts.\n",
      "Logistic regression models are used mostly as a data analysis and infer-\n",
      "ence tool, where the goal is to understand the role of the input variables\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "122 4. Linear Methods for Classiﬁcation\n",
      "TABLE 4.2. Results from a logistic regression ﬁt to the South African hear t\n",
      "disease data.\n",
      "Coeﬃcient Std. Error ZScore\n",
      "(Intercept) −4.130 0 .964 −4.285\n",
      "sbp 0.006 0 .006 1 .023\n",
      "tobacco 0.080 0 .026 3 .034\n",
      "ldl 0.185 0 .057 3 .219\n",
      "famhist 0.939 0 .225 4 .178\n",
      "obesity -0.035 0 .029 −1.187\n",
      "alcohol 0.001 0 .004 0 .136\n",
      "age 0.043 0 .010 4 .184\n",
      "inexplaining the outcome. Typically many models are ﬁt in a search for a\n",
      "parsimonious model involving a subset of the variables, possibly with some\n",
      "interactions terms. The following example illustrates some of the issues\n",
      "involved.\n",
      "4.4.2 Example: South African Heart Disease\n",
      "Here we present an analysis of binary data to illustrate the traditional\n",
      "statistical use of the logistic regression model. The data in Figure 4.1 2 are a\n",
      "subset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried\n",
      "out in three rural areas of the Western Cape, South Africa (Rousseauw et\n",
      "al., 1983). The aim of the study was to establish the intensity of ischemic\n",
      "heart disease risk factors in that high-incidence region. The data represent\n",
      "white males between 15 and 64, and the response variable is the presence or\n",
      "absence of myocardial infarction (MI) at the time of the survey (the overall\n",
      "prevalence of MI was 5.1% in this region). There are 160 cases in our data\n",
      "set, and a sample of 302 controls. These data are described in more detail\n",
      "in Hastie and Tibshirani (1987).\n",
      "We ﬁt a logistic-regression model by maximum likelihood, giving the\n",
      "results shown in Table 4.2. This summary includes Zscores for each of the\n",
      "coeﬃcients in the model (coeﬃcients divided by their standard errors); a\n",
      "nonsigniﬁcant Zscore suggests a coeﬃcient can be dropped from the model.\n",
      "Each of these correspond formally to a test of the null hypothesis that the\n",
      "coeﬃcient in question is zero, while all the others are not (also known as\n",
      "the Wald test). A Zscore greater than approximately 2 in absolute value\n",
      "is signiﬁcant at the 5% level.\n",
      "There are some surprises in this table of coeﬃcients, which must be in-\n",
      "terpreted with caution. Systolic blood pressure ( sbp) is not signiﬁcant! Nor\n",
      "isobesity , and its sign is negative. This confusion is a result of the corre-\n",
      "lation between the set of predictors. On their own, both sbpandobesity\n",
      "are signiﬁcant, and with positive sign. However, in the presence of many\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.4 Logistic Regression 123\n",
      "sbp0 10 20 30\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oooooooooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooooooo\n",
      "ooooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "o oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo ooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooooooo\n",
      "ooooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooo oo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooo oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo0.0 0.4 0.8\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo o ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo ooo\n",
      "ooo\n",
      "oo\n",
      "ooo oo\n",
      "oooo\n",
      "oooooo o ooo oo\n",
      "oooo o\n",
      "o ooo\n",
      "o oooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "o ooo\n",
      "oooooooo oo\n",
      "o oo\n",
      "oo\n",
      "oo ooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo o\n",
      "oo oo\n",
      "oo ooooo\n",
      "oo o ooooooo\n",
      "ooo\n",
      "oo\n",
      "oo oo\n",
      "oo oo\n",
      "oooooooo\n",
      "ooooo\n",
      "o\n",
      "o ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo oo\n",
      "oooooooo\n",
      "oooo oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oooooo\n",
      "o oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oo oo\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "ooo\n",
      "ooo oo oo\n",
      "o oooo\n",
      "oo\n",
      "ooo o\n",
      "o o oo\n",
      "o\n",
      "o o oo\n",
      "ooo\n",
      "ooo\n",
      "o ooo\n",
      "oooo\n",
      "oooo\n",
      "o oo ooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "o ooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooo o ooo\n",
      "ooo\n",
      "oo o ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo ooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooooooooooooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooooooo\n",
      "ooooooo\n",
      "oooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo0 50 100\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooo ooo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooooooo\n",
      "ooooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooo\n",
      "oo\n",
      "oooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "100 160 220o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooooo o\n",
      "oooooooooo ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "o ooooo\n",
      "oooooo\n",
      "oooo\n",
      "oooo oo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "ooooooo\n",
      "o\n",
      "ooo oooo\n",
      "oo ooooo ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oo ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "oooo oo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o o\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oo oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo0 10 20 30o\n",
      "oooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooooo oo\n",
      "ooooooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooooo\n",
      "oo\n",
      "o oooooooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "ooo oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "ooo oooooooo\n",
      "ooo\n",
      "oooooooooooooooo oo\n",
      "otobaccoo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "ooooo\n",
      "oooooooo oo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooooo\n",
      "oo\n",
      "ooooooooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "oo ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "ooooooooo\n",
      "oo\n",
      "oooo oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oooooooooo\n",
      "oooooooo\n",
      "oo\n",
      "o ooo\n",
      "oo o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "ooooooooo\n",
      "oo\n",
      "o o ooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o o o oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o ooo oo\n",
      "ooooo\n",
      "o oo\n",
      "oo ooooooo oooo\n",
      "o\n",
      "o o ooooo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo o\n",
      "o oooo\n",
      "ooooooo o oo\n",
      "o oooo oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooooo o oooooo\n",
      "o oooo\n",
      "oo\n",
      "oo\n",
      "o o oo oo o oo oooo ooo ooooo\n",
      "oooooo\n",
      "oooo\n",
      "o ooo oo oo\n",
      "oooo\n",
      "oooo\n",
      "o o\n",
      "ooo oooo\n",
      "o ooooo\n",
      "ooooo\n",
      "ooo\n",
      "oo oooo\n",
      "ooo\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "oooooooooooooo\n",
      "o\n",
      "ooo o o oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo oo\n",
      "oo o oo\n",
      "ooooo\n",
      "ooo o\n",
      "ooooo\n",
      "oo oo o\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "oooo oo oo\n",
      "ooo\n",
      "oo\n",
      "o ooo oooo\n",
      "o oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo oo\n",
      "ooooo o o o ooo\n",
      "o oo\n",
      "oo oo oooooo\n",
      "o ooo oooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooooooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooooooo\n",
      "ooooooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooooo\n",
      "oooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "ooooooo oooo\n",
      "ooo\n",
      "oooooooooo\n",
      "oooooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooooooo\n",
      "oo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooooooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o oooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooo\n",
      "oooo\n",
      "oooooo\n",
      "oooo\n",
      "oooooooo\n",
      "oooo\n",
      "oo oo\n",
      "oo\n",
      "ooooooo\n",
      "ooooo o\n",
      "ooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooooooooooo\n",
      "oooooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooooooo\n",
      "oo\n",
      "o ooo oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o ooooooo ooo\n",
      "ooo\n",
      "oooooooooooooooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "o o ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo oooo\n",
      "ooooo\n",
      "o oo\n",
      "ooo oooooo oooo\n",
      "o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oooo ooooo\n",
      "ooooo\n",
      "o ooooooo oo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooooooooooooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooooooo\n",
      "oooo oo\n",
      "oooo\n",
      "o ooooooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "ooooooooo\n",
      "ooooo\n",
      "ooooo ooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "ooo oo\n",
      "o oooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooooooo\n",
      "oo\n",
      "oooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "oooooooooooooo\n",
      "oooooooooo\n",
      "ooo o oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooooooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oooo\n",
      "ooooo ooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "o oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooooooooo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "o oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooo oooo\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oldl\n",
      "oo oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo o oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o oo\n",
      "oo ooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo oooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oooo\n",
      "ooo o\n",
      "o oo\n",
      "oo\n",
      "oooooo\n",
      "oo oo oo\n",
      "oo\n",
      "oooo oo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo oo\n",
      "ooo o\n",
      "oo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o o ooooooo\n",
      "ooo ooo\n",
      "ooooo ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "o oo ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "oo ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o oooo\n",
      "ooooo\n",
      "o ooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oo\n",
      "oo\n",
      "ooooo o oo\n",
      "oo\n",
      "o oo\n",
      "oo oo\n",
      "o ooo\n",
      "ooooo ooooooo\n",
      "ooo\n",
      "o oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "o o o\n",
      "ooo ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "ooo ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo oooooooo\n",
      "oo oo\n",
      "oooo\n",
      "o ooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo o ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo ooooooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo oo\n",
      "o\n",
      "ooooooooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooooo\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "oooooo\n",
      "oooo\n",
      "o oo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo ooooo\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "oo\n",
      "ooooooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "o oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooooo\n",
      "ooooo oo\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo oooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "2 6 10 14oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo o ooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "o oooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "oooooo\n",
      "oooooo\n",
      "o ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo oooooo\n",
      "oooooo\n",
      "o oooo oo\n",
      "ooo\n",
      "o ooo ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "ooooooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooo\n",
      "o oooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "ooo ooooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooooooooo\n",
      "oo o\n",
      "oooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o0.0 0.4 0.8o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "o oo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooooo ooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooooooo\n",
      "ooooo o ooooooooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooooooo o oo\n",
      "oo\n",
      "ooo\n",
      "ooo o\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooo\n",
      "ooooo o\n",
      "oo\n",
      "oooo\n",
      "oooooo oo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooooo oo\n",
      "ooo\n",
      "o oooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo ooo o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooo o\n",
      "ooo\n",
      "o oo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "oo\n",
      "o oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo ooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooooooo\n",
      "ooooo o ooooooooo\n",
      "oo oooo\n",
      "ooooooooo\n",
      "o ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooo ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "o ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oo oo\n",
      "oo o ooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooooooo\n",
      "ooooooooooooooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo ooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo oooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo oo\n",
      "famhisto\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "o ooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "oo o\n",
      "oooo\n",
      "oo\n",
      "oo ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo oooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooooooo\n",
      "ooooooooooooooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo o\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo oooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "oo\n",
      "oo\n",
      "oo o oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooooooo\n",
      "o ooooooo ooooooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "oo\n",
      "ooo\n",
      "ooo o\n",
      "ooo o ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o ooooooo\n",
      "oo\n",
      "ooooo\n",
      "o ooooo\n",
      "oo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oooo\n",
      "oo\n",
      "ooo o\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo o\n",
      "oo o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "o ooooooo\n",
      "ooo\n",
      "oo ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooo o\n",
      "oooo\n",
      "ooo\n",
      "ooooo o\n",
      "ooooo\n",
      "oo oo\n",
      "oo\n",
      "o ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo oo\n",
      "ooo\n",
      "o ooo\n",
      "o oo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oo ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o o o ooooooo\n",
      "oo\n",
      "oo\n",
      "oo ooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o o ooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooooooo\n",
      "o oooo o ooo oooooo\n",
      "oo oooo\n",
      "oo o oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o oo\n",
      "oo o\n",
      "oo\n",
      "oo\n",
      "o oo oooo o oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oo ooooo\n",
      "oo oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o o oooooo\n",
      "oo\n",
      "oo ooo\n",
      "oo oooo\n",
      "oo\n",
      "oooo\n",
      "oooooooo\n",
      "ooooooo oo\n",
      "oooo ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "o ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo o\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "oo o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo oooooo\n",
      "ooo\n",
      "oo ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "ooooooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "o\n",
      "ooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooooooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oo oo\n",
      "oooo\n",
      "oooooo\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "o\n",
      "oooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooooooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooooooo oo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooo oooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooooooo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "oooooo\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "oooooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooooooooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo oo\n",
      "oo\n",
      "ooooo\n",
      "ooo oo oooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo ooo\n",
      "oooooo o ooo o\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "o o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooooo ooo\n",
      "oo\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo o ooo\n",
      "ooo\n",
      "oo oo o\n",
      "ooooooo oo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo oooo\n",
      "oo\n",
      "o ooo\n",
      "ooo oo\n",
      "o oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo o ooooo\n",
      "ooooo\n",
      "oooo oo\n",
      "o\n",
      "o oo oo o\n",
      "oooooo\n",
      "o oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o o\n",
      "oooo\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "o oooo oo oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo ooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "o oooooo oo\n",
      "o\n",
      "oo\n",
      "oooooooooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "oo ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "ooo oooooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oobesity\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oooooooooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo oo\n",
      "ooooo\n",
      "oo\n",
      "ooooooooo oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "15 25 35 45oooo\n",
      "oo\n",
      "ooooo\n",
      "ooo oooooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooooo\n",
      "oooooo ooooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooooooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "ooo oo\n",
      "oooo\n",
      "ooo ooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "ooooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooooo\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooooooooo\n",
      "o\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o0 50 100o\n",
      "oooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooooooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooooooooo\n",
      "ooooooooo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooooooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooo oooooooooooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooooooooooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo oooooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooo\n",
      "oo\n",
      "ooooo\n",
      "o ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oooooo ooooooooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oo oooo\n",
      "oo\n",
      "ooo\n",
      "o ooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooooooo\n",
      "ooooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooooooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooooooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooooooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooooooo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "oo ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo ooooo oooooooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooo oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooooooooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooooooooo\n",
      "oooooooooooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooooooooooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooooooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oooooooooo ooooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo o\n",
      "oo\n",
      "o ooo\n",
      "oo oooo\n",
      "o oo oo o\n",
      "oo\n",
      "ooo\n",
      "o o o o oo\n",
      "o oooooooo\n",
      "ooo\n",
      "o ooooo ooo\n",
      "o oo\n",
      "oooo\n",
      "ooo o o ooooo\n",
      "ooooooo\n",
      "o ooo ooo\n",
      "o o oooo\n",
      "o oo oooooooo\n",
      "oooo o oooo\n",
      "oo ooo o o oo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "o ooo\n",
      "ooo\n",
      "ooo\n",
      "o oo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooo ooo ooooooooooo\n",
      "o oooo o ooooooo ooo\n",
      "o\n",
      "ooo\n",
      "o oo\n",
      "oo\n",
      "ooooooooooo ooo oo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "o oo oo ooo\n",
      "oo oooo\n",
      "ooo\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "oooo\n",
      "o ooooo\n",
      "o o ooo ooooooo\n",
      "ooo\n",
      "oo ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo oo o oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o oo o oooooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo o ooooo\n",
      "ooo\n",
      "oo\n",
      "oo ooo\n",
      "oo\n",
      "ooo o oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o oo\n",
      "oo ooo\n",
      "oooo\n",
      "oo oo\n",
      "o\n",
      "ooooo o ooo\n",
      "o oooooooo\n",
      "oooo\n",
      "oo\n",
      "o ooo\n",
      "o\n",
      "o ooooo o oooooo oo\n",
      "oo\n",
      "o oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "oooooo\n",
      "oooooo\n",
      "oo\n",
      "ooo\n",
      "ooooooooooooooo\n",
      "ooo\n",
      "ooooo oooo\n",
      "ooo\n",
      "oooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "ooooooooooo\n",
      "ooooooooo\n",
      "o\n",
      "oooooooo\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "oo oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooooooooo\n",
      "oooooo oooooooooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "ooooooooooo ooooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oo\n",
      "oooooooo\n",
      "oooooo\n",
      "ooo\n",
      "oooo\n",
      "oooooo\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooooooooooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "ooooooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooooooooooooooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oalcoholo\n",
      "oooo\n",
      "oooooo\n",
      "ooo ooo\n",
      "oo\n",
      "ooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooo oooo\n",
      "ooo\n",
      "oooo\n",
      "ooo o ooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "oooooo\n",
      "oooo o oooooo\n",
      "ooooooooo\n",
      "ooooo oooo\n",
      "oo\n",
      "oo\n",
      "oo oooo\n",
      "o ooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooooo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooooooooooooooo o ooooo\n",
      "ooooooooo\n",
      "ooooooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo o oooooo ooooo\n",
      "oo\n",
      "ooo\n",
      "oo oo\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "oooooooo\n",
      "o oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o oooo o\n",
      "oooooooooooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "ooooo oooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo ooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oooooooo\n",
      "ooo\n",
      "oooo ooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooooooo\n",
      "oo ooo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o ooooo ooooooooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o oooo\n",
      "o\n",
      "100 160 220oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "oooooooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oo o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "oooooooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "2 6 10 14oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "oooo o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oooooooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o oo o\n",
      "ooo\n",
      "oooo o o\n",
      "oooo o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oo o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooo oo\n",
      "o\n",
      "o\n",
      "oo o o o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "oooo oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo o\n",
      "oooo o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo o\n",
      "oo o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o oo\n",
      "o o\n",
      "ooooo ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo o\n",
      "oo\n",
      "oo\n",
      "15 25 35 45oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooo oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "oooooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "ooo\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oooooo\n",
      "ooooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oooooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooooo\n",
      "ooooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o oooo\n",
      "o\n",
      "ooo\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "ooooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooooo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "20 40 60\n",
      "20 40 60age\n",
      "FIGURE 4.12. A scatterplot matrix of the South African heart disease data.\n",
      "Each plot shows a pair of risk factors, and the cases and controls are color coded\n",
      "(red is a case). The variable family history of heart disease ( famhist )is binary\n",
      "(yes or no).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "124 4. Linear Methods for Classiﬁcation\n",
      "TABLE 4.3. Results from stepwise logistic regression ﬁt to South African heart\n",
      "disease data.\n",
      "Coeﬃcient Std. Error Zscore\n",
      "(Intercept) −4.204 0 .498 −8.45\n",
      "tobacco 0.081 0 .026 3 .16\n",
      "ldl 0.168 0 .054 3 .09\n",
      "famhist 0.924 0 .223 4 .14\n",
      "age 0.044 0 .010 4 .52\n",
      "other correlated variables, they are no longer needed (and can even get a\n",
      "negative sign).\n",
      "At this stage the analyst might do some model selection; ﬁnd a subset\n",
      "of the variables that are suﬃcient for explaining their joint eﬀect on the\n",
      "prevalence of chd. One way to proceed by is to drop the least signiﬁcant co-\n",
      "eﬃcient, and reﬁt the model. This is done repeatedly until no further terms\n",
      "can be dropped from the model. This gave the model shown in Table 4.3.\n",
      "A better but more time-consuming strategy is to reﬁt each of the models\n",
      "with one variable removed, and then perform an analysis of deviance to\n",
      "decide which variable to exclude. The residual deviance of a ﬁtted model\n",
      "is minus twice its log-likelihood, and the deviance between two models is\n",
      "the diﬀerence of their individual residual deviances (in analogy to sums-of-\n",
      "squares). This strategy gave the same ﬁnal model as above.\n",
      "How does one interpret a coeﬃcient of 0 .081 (Std. Error = 0 .026) for\n",
      "tobacco , for example? Tobacco is measured in total lifetime usage in kilo-\n",
      "grams, with a median of 1 .0kg for the controls and 4 .1kg for the cases. Thus\n",
      "an increase of 1kg in lifetime tobacco usage accounts for an increase in the\n",
      "odds of coronary heart disease of exp(0 .081) = 1 .084 or 8 .4%. Incorporat-\n",
      "ing the standard error we get an approximate 95% conﬁdence interval of\n",
      "exp(0.081±2×0.026) = (1 .03,1.14).\n",
      "We return to these data in Chapter 5, where we see that some of the\n",
      "variables have nonlinear eﬀects, and when modeled appropriately, are not\n",
      "excluded from the model.\n",
      "4.4.3 Quadratic Approximations and Inference\n",
      "The maximum-likelihood parameter estimates ˆβsatisfy a self-consistency\n",
      "relationship: they are the coeﬃcients of a weighted least squares ﬁt, where\n",
      "the responses are\n",
      "zi=xT\n",
      "iˆβ+(yi−ˆpi)\n",
      "ˆpi(1−ˆpi), (4.29)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.4 Logistic Regression 125\n",
      "and the weights are wi= ˆpi(1−ˆpi), both depending on ˆβitself. Apart from\n",
      "providing a convenient algorithm, this connection with least squares has\n",
      "more to oﬀer:\n",
      "•The weighted residual sum-of-squares is the familiar Pearson chi-\n",
      "square statistic\n",
      "N∑\n",
      "i=1(yi−ˆpi)2\n",
      "ˆpi(1−ˆpi), (4.30)\n",
      "a quadratic approximation to the deviance.\n",
      "•Asymptotic likelihood theory says that if the model is correct, then\n",
      "ˆβis consistent (i.e., converges to the trueβ).\n",
      "•A central limit theorem then shows that the distribution of ˆβcon-\n",
      "verges to N(β,(XTWX)−1). This and other asymptotics can be de-\n",
      "rived directly from the weighted least squares ﬁt by mimicking normal\n",
      "theory inference.\n",
      "•Model building can be costly for logistic regression models, because\n",
      "each model ﬁtted requires iteration. Popular shortcuts are the Rao\n",
      "score test which tests for inclusion of a term, and the Wald test which\n",
      "can be used to test for exclusion of a term. Neither of these require\n",
      "iterative ﬁtting, and are based on the maximum-likelihood ﬁt of the\n",
      "current model. It turns out that both of these amount to adding\n",
      "or dropping a term from the weighted least squares ﬁt, using the\n",
      "same weights. Such computations can be done eﬃciently, without\n",
      "recomputing the entire weighted least squares ﬁt.\n",
      "Software implementations can take advantage of these connections. For\n",
      "example, the generalized linear modeling software in R (which includes lo-\n",
      "gistic regression as part of the binomial family of models) exploits them\n",
      "fully. GLM (generalized linear model) objects can be treated as linear model\n",
      "objects, and all the tools available for linear models can be applied auto-\n",
      "matically.\n",
      "4.4.4 L1Regularized Logistic Regression\n",
      "TheL1penalty used in the lasso (Section 3.4.2) can be used for variable\n",
      "selection and shrinkage with any linear regression model. For logistic re-\n",
      "gression, we would maximize a penalized version of (4.20):\n",
      "max\n",
      "β0,β\n",
      "\n",
      "N∑\n",
      "i=1[\n",
      "yi(β0+βTxi)−log(1 + eβ0+βTxi)]\n",
      "−λp∑\n",
      "j=1|βj|\n",
      "\n",
      ".(4.31)\n",
      "As with the lasso, we typically do not penalize the intercept term, and stan-\n",
      "dardize the predictors for the penalty to be meaningful. Criterion (4.31) is\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "126 4. Linear Methods for Classiﬁcation\n",
      "concave, and a solution can be found using nonlinear programming meth-\n",
      "ods (Koh et al., 2007, for example). Alternatively, using the same quadratic\n",
      "approximations that were used in the Newton algorithm in Section 4.4.1,\n",
      "we can solve (4.31) by repeated application of a weighted lasso algorit hm.\n",
      "Interestingly, the score equations [see (4.24)] for the variables with non-zer o\n",
      "coeﬃcients have the form\n",
      "xT\n",
      "j(y−p) =λ≤sign(βj), (4.32)\n",
      "which generalizes (3.58) in Section 3.4.4; the active variables are tied in\n",
      "theirgeneralized correlation with the residuals.\n",
      "Path algorithms such as LAR for lasso are more diﬃcult, because the\n",
      "coeﬃcient proﬁles are piecewise smooth rather than linear. Nevertheless,\n",
      "progress can be made using quadratic approximations.\n",
      "******************************* ************** ****** *************************************************** ****************************************** **************************************************************************** *****************\n",
      "0.0 0.5 1.0 1.5 2.00.0 0.2 0.4 0.6******************************* * ******************************************************************************************************************************************************************************************** *****************\n",
      "******************************* ************** * ****************************************************************************************************************************************************************************** *****************\n",
      "****************************** ********************************************************************************************************************************************************************************************** *****************\n",
      "******************************* ************** ****** *************************************************** *************************************************** ************************ ******************************************* ************************************************ ************** ****** *************************************************** *************************************************** ************************ *************************** *************** * ********************************************************************************************************************************************************************************************************************************************* *****************\n",
      "obesityalcoholsbptobaccoldlfamhistage1 2 4 5 6 7Coeﬃcients βj(λ)\n",
      "||β(λ)||1\n",
      "FIGURE 4.13. L1regularized logistic regression coeﬃcients for the South\n",
      "African heart disease data, plotted as a function of the L1norm. The variables\n",
      "were all standardized to have unit variance. The proﬁles are comp uted exactly at\n",
      "each of the plotted points.\n",
      "Figure 4.13 shows the L1regularization path for the South African\n",
      "heart disease data of Section 4.4.2. This was produced using the Rpackage\n",
      "glmpath (Park and Hastie, 2007), which uses predictor–corrector methods\n",
      "of convex optimization to identify the exact values of λat which the active\n",
      "set of non-zero coeﬃcients changes (vertical lines in the ﬁgure). Here the\n",
      "proﬁles look almost linear; in other examples the curvature will be more\n",
      "visible.\n",
      "Coordinate descent methods (Section 3.8.6) are very eﬃcient for comput-\n",
      "ing the coeﬃcient proﬁles on a grid of values for λ. TheRpackageglmnet\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.4 Logistic Regression 127\n",
      "(Friedman et al., 2010) can ﬁt coeﬃcient paths for very large logistic re-\n",
      "gression problems eﬃciently (large in Norp). Their algorithms can exploit\n",
      "sparsity in the predictor matrix X, which allows for even larger problems.\n",
      "See Section 18.4 for more details, and a discussion of L1-regularized multi-\n",
      "nomial models.\n",
      "4.4.5 Logistic Regression or LDA?\n",
      "In Section 4.3 we ﬁnd that the log-posterior odds between class kandK\n",
      "are linear functions of x(4.9):\n",
      "logPr(G=k|X=x)\n",
      "Pr(G=K|X=x)= logπk\n",
      "πK−1\n",
      "2(θk+θK)TΣ−1(θk−θK)\n",
      "+xTΣ−1(θk−θK)\n",
      "=αk0+αT\n",
      "kx. (4.33)\n",
      "This linearity is a consequence of the Gaussian assumption for the class\n",
      "densities, as well as the assumption of a common covariance matrix. The\n",
      "linear logistic model (4.17) by construction has linear logits:\n",
      "logPr(G=k|X=x)\n",
      "Pr(G=K|X=x)=βk0+βT\n",
      "kx. (4.34)\n",
      "It seems that the models are the same. Although they have exactly the same\n",
      "form, the diﬀerence lies in the way the linear coeﬃcients are estimated. The\n",
      "logistic regression model is more general, in that it makes less assumptio ns.\n",
      "We can write the joint density ofXandGas\n",
      "Pr(X,G=k) = Pr( X)Pr(G=k|X), (4.35)\n",
      "where Pr( X) denotes the marginal density of the inputs X. For both LDA\n",
      "and logistic regression, the second term on the right has the logit-linear\n",
      "form\n",
      "Pr(G=k|X=x) =eβk0+βT\n",
      "kx\n",
      "1 +∑K−1\n",
      "ℓ=1eβℓ0+βT\n",
      "ℓx, (4.36)\n",
      "where we have again arbitrarily chosen the last class as the reference.\n",
      "The logistic regression model leaves the marginal density of Xas an arbi-\n",
      "trary density function Pr( X), and ﬁts the parameters of Pr( G|X) by max-\n",
      "imizing the conditional likelihood —the multinomial likelihood with proba-\n",
      "bilities the Pr( G=k|X). Although Pr( X) is totally ignored, we can think\n",
      "of this marginal density as being estimated in a fully nonparametric and\n",
      "unrestricted fashion, using the empirical distribution function which places\n",
      "mass 1 /Nat each observation.\n",
      "With LDA we ﬁt the parameters by maximizing the full log-likelihood,\n",
      "based on the joint density\n",
      "Pr(X,G=k) =φ(X;θk,Σ)πk, (4.37)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "128 4. Linear Methods for Classiﬁcation\n",
      "where φis the Gaussian density function. Standard normal theory leads\n",
      "easily to the estimates ˆ θk,ˆΣ, and ˆ πkgiven in Section 4.3. Since the linear\n",
      "parameters of the logistic form (4.33) are functions of the Gaussian para m-\n",
      "eters, we get their maximum-likelihood estimates by plugging in the corre-\n",
      "sponding estimates. However, unlike in the conditional case, the marginal\n",
      "density Pr( X) does play a role here. It is a mixture density\n",
      "Pr(X) =K∑\n",
      "k=1πkφ(X;θk,Σ), (4.38)\n",
      "which also involves the parameters.\n",
      "What role can this additional component/restriction play? By relying\n",
      "on the additional model assumptions, we have more information about the\n",
      "parameters, and hence can estimate them more eﬃciently (lower variance).\n",
      "If in fact the true fk(x) are Gaussian, then in the worst case ignoring this\n",
      "marginal part of the likelihood constitutes a loss of eﬃciency of about 30%\n",
      "asymptotically in the error rate (Efron, 1975). Paraphrasing: with 3 0%\n",
      "more data, the conditional likelihood will do as well.\n",
      "For example, observations far from the decision boundary (which are\n",
      "down-weighted by logistic regression) play a role in estimating the common\n",
      "covariance matrix. This is not all good news, because it also means that\n",
      "LDA is not robust to gross outliers.\n",
      "From the mixture formulation, it is clear that even observations without\n",
      "class labels have information about the parameters. Often it is expensive\n",
      "to generate class labels, but unclassiﬁed observations come cheaply. By\n",
      "relying on strong model assumptions, such as here, we can use both types\n",
      "of information.\n",
      "The marginal likelihood can be thought of as a regularizer, requiring\n",
      "in some sense that class densities be visible from this marginal view. For\n",
      "example, if the data in a two-class logistic regression model can be per-\n",
      "fectly separated by a hyperplane, the maximum likelihood estimates of the\n",
      "parameters are undeﬁned (i.e., inﬁnite; see Exercise 4.5). The LDA coeﬃ-\n",
      "cients for the same data will be well deﬁned, since the marginal likelihood\n",
      "will not permit these degeneracies.\n",
      "In practice these assumptions are never correct, and often some of the\n",
      "components of Xare qualitative variables. It is generally felt that logistic\n",
      "regression is a safer, more robust bet than the LDA model, relying on fewer\n",
      "assumptions. It is our experience that the models give very similar results,\n",
      "even when LDA is used inappropriately, such as with qualitative predictors.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.5 Separating Hyperplanes 129\n",
      "FIGURE 4.14. A toy example with two classes separable by a hyperplane. The\n",
      "orange line is the least squares solution, which misclassiﬁes one of the training\n",
      "points. Also shown are two blue separating hyperplanes found by t heperceptron\n",
      "learning algorithm with diﬀerent random starts.\n",
      "4.5 Separating Hyperplanes\n",
      "We have seen that linear discriminant analysis and logistic regression bot h\n",
      "estimate linear decision boundaries in similar but slightly diﬀerent ways.\n",
      "For the rest of this chapter we describe separating hyperplane classiﬁers.\n",
      "These procedures construct linear decision boundaries that explicitly try\n",
      "to separate the data into diﬀerent classes as well as possible. They provide\n",
      "the basis for support vector classiﬁers, discussed in Chapter 12. The math-\n",
      "ematical level of this section is somewhat higher than that of the previous\n",
      "sections.\n",
      "Figure 4.14 shows 20 data points in two classes in IR2. These data can be\n",
      "separated by a linear boundary. Included in the ﬁgure (blue lines) are two\n",
      "of the inﬁnitely many possible separating hyperplanes . The orange line is\n",
      "the least squares solution to the problem, obtained by regressing the −1/1\n",
      "response YonX(with intercept); the line is given by\n",
      "{x:ˆβ0+ˆβ1x1+ˆβ2x2= 0}. (4.39)\n",
      "This least squares solution does not do a perfect job in separating the\n",
      "points, and makes one error. This is the same boundary found by LDA,\n",
      "in light of its equivalence with linear regression in the two-class case (Sec-\n",
      "tion 4.3 and Exercise 4.2).\n",
      "Classiﬁers such as (4.39), that compute a linear combination of the input\n",
      "features and return the sign, were called perceptrons in the engineering liter-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "130 4. Linear Methods for Classiﬁcation\n",
      "x0x\n",
      "β∗β0+βTx= 0\n",
      "FIGURE 4.15. The linear algebra of a hyperplane (aﬃne set).\n",
      "ature in the late 1950s (Rosenblatt, 1958). Perceptrons set the foundations\n",
      "for the neural network models of the 1980s and 1990s.\n",
      "Before we continue, let us digress slightly and review some vector algebra.\n",
      "Figure 4.15 depicts a hyperplane or aﬃne set Ldeﬁned by the equation\n",
      "f(x) =β0+βTx= 0; since we are in IR2this is a line.\n",
      "Here we list some properties:\n",
      "1. For any two points x1andx2lying in L,βT(x1−x2) = 0, and hence\n",
      "β∗=β/||β||is the vector normal to the surface of L.\n",
      "2. For any point x0inL,βTx0=−β0.\n",
      "3. The signed distance of any point xtoLis given by\n",
      "β∗T(x−x0) =1\n",
      "∥β∥(βTx+β0)\n",
      "=1\n",
      "||f′(x)||f(x). (4.40)\n",
      "Hence f(x) is proportional to the signed distance from xto the hyperplane\n",
      "deﬁned by f(x) = 0.\n",
      "4.5.1 Rosenblatt’s Perceptron Learning Algorithm\n",
      "Theperceptron learning algorithm tries to ﬁnd a separating hyperplane by\n",
      "minimizing the distance of misclassiﬁed points to the decision boundary. If\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.5 Separating Hyperplanes 131\n",
      "a response yi= 1 is misclassiﬁed, then xT\n",
      "iβ+β0<0, and the opposite for\n",
      "a misclassiﬁed response with yi=−1. The goal is to minimize\n",
      "D(β,β0) =−∑\n",
      "i∈Myi(xT\n",
      "iβ+β0), (4.41)\n",
      "where Mindexes the set of misclassiﬁed points. The quantity is non-\n",
      "negative and proportional to the distance of the misclassiﬁed points to\n",
      "the decision boundary deﬁned by βTx+β0= 0. The gradient (assuming\n",
      "Mis ﬁxed) is given by\n",
      "∂D(β,β0)\n",
      "∂β=−∑\n",
      "i∈Myixi, (4.42)\n",
      "∂D(β,β0)\n",
      "∂β0=−∑\n",
      "i∈Myi. (4.43)\n",
      "The algorithm in fact uses stochastic gradient descent to minimize this\n",
      "piecewise linear criterion. This means that rather than computing the sum\n",
      "of the gradient contributions of each observation followed by a step in the\n",
      "negative gradient direction, a step is taken after each observation is visit ed.\n",
      "Hence the misclassiﬁed observations are visited in some sequence, and the\n",
      "parameters βare updated via\n",
      "(\n",
      "β\n",
      "β0)\n",
      "←(\n",
      "β\n",
      "β0)\n",
      "+ρ(\n",
      "yixi\n",
      "yi)\n",
      ". (4.44)\n",
      "Hereρis the learning rate, which in this case can be taken to be 1 without\n",
      "loss in generality. If the classes are linearly separable, it can be shown that\n",
      "the algorithm converges to a separating hyperplane in a ﬁnite number of\n",
      "steps (Exercise 4.6). Figure 4.14 shows two solutions to a toy problem, eac h\n",
      "started at a diﬀerent random guess.\n",
      "There are a number of problems with this algorithm, summarized in\n",
      "Ripley (1996):\n",
      "•When the data are separable, there are many solutions, and which\n",
      "one is found depends on the starting values.\n",
      "•The “ﬁnite” number of steps can be very large. The smaller the gap,\n",
      "the longer the time to ﬁnd it.\n",
      "•When the data are not separable, the algorithm will not converge,\n",
      "and cycles develop. The cycles can be long and therefore hard to\n",
      "detect.\n",
      "The second problem can often be eliminated by seeking a hyperplane not\n",
      "in the original space, but in a much enlarged space obtained by creating\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "132 4. Linear Methods for Classiﬁcation\n",
      "many basis-function transformations of the original variables. This is a nal-\n",
      "ogous to driving the residuals in a polynomial regression problem down\n",
      "to zero by making the degree suﬃciently large. Perfect separation cannot\n",
      "always be achieved: for example, if observations from two diﬀerent classes\n",
      "share the same input. It may not be desirable either, since the resulting\n",
      "model is likely to be overﬁt and will not generalize well. We return to this\n",
      "point at the end of the next section.\n",
      "A rather elegant solution to the ﬁrst problem is to add additional con-\n",
      "straints to the separating hyperplane.\n",
      "4.5.2 Optimal Separating Hyperplanes\n",
      "Theoptimal separating hyperplane separates the two classes and maximizes\n",
      "the distance to the closest point from either class (Vapnik, 1996). Not only\n",
      "does this provide a unique solution to the separating hyperplane problem,\n",
      "but by maximizing the margin between the two classes on the training data,\n",
      "this leads to better classiﬁcation performance on test data.\n",
      "We need to generalize criterion (4.41). Consider the optimization problem\n",
      "max\n",
      "β,β0,||β||=1M\n",
      "subject to yi(xT\n",
      "iβ+β0)≥M, i= 1,... ,N.(4.45)\n",
      "The set of conditions ensure that all the points are at least a signed\n",
      "distance Mfrom the decision boundary deﬁned by βandβ0, and we seek\n",
      "the largest such Mand associated parameters. We can get rid of the ||β||=\n",
      "1 constraint by replacing the conditions with\n",
      "1\n",
      "||β||yi(xT\n",
      "iβ+β0)≥M, (4.46)\n",
      "(which redeﬁnes β0) or equivalently\n",
      "yi(xT\n",
      "iβ+β0)≥M||β||. (4.47)\n",
      "Since for any βandβ0satisfying these inequalities, any positively scaled\n",
      "multiple satisﬁes them too, we can arbitrarily set ||β||= 1/M. Thus (4.45)\n",
      "is equivalent to\n",
      "min\n",
      "β,β01\n",
      "2||β||2\n",
      "subject to yi(xT\n",
      "iβ+β0)≥1, i= 1,... ,N.(4.48)\n",
      "In light of (4.40), the constraints deﬁne an empty slab or margin around the\n",
      "linear decision boundary of thickness 1 /||β||. Hence we choose βandβ0to\n",
      "maximize its thickness. This is a convex optimization problem (quadratic\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "4.5 Separating Hyperplanes 133\n",
      "criterion with linear inequality constraints). The Lagrange (primal) func-\n",
      "tion, to be minimized w.r.t. βandβ0, is\n",
      "LP=1\n",
      "2||β||2−N∑\n",
      "i=1αi[yi(xT\n",
      "iβ+β0)−1]. (4.49)\n",
      "Setting the derivatives to zero, we obtain:\n",
      "β=N∑\n",
      "i=1αiyixi, (4.50)\n",
      "0 =N∑\n",
      "i=1αiyi, (4.51)\n",
      "and substituting these in (4.49) we obtain the so-called Wolfe dual\n",
      "LD=N∑\n",
      "i=1αi−1\n",
      "2N∑\n",
      "i=1N∑\n",
      "k=1αiαkyiykxT\n",
      "ixk\n",
      "subject to αi≥0. (4.52)\n",
      "The solution is obtained by maximizing LDin the positive orthant, a sim-\n",
      "pler convex optimization problem, for which standard software can be used.\n",
      "In addition the solution must satisfy the Karush–Kuhn–Tucker conditions,\n",
      "which include (4.50), (4.51), (4.52) and\n",
      "αi[yi(xT\n",
      "iβ+β0)−1] = 0 ∀i. (4.53)\n",
      "From these we can see that\n",
      "•ifαi>0, then yi(xT\n",
      "iβ+β0) = 1, or in other words, xiis on the\n",
      "boundary of the slab;\n",
      "•ifyi(xT\n",
      "iβ+β0)>1,xiis not on the boundary of the slab, and αi= 0.\n",
      "From (4.50) we see that the solution vector βis deﬁned in terms of a linear\n",
      "combination of the support points xi—those points deﬁned to be on the\n",
      "boundary of the slab via αi>0. Figure 4.16 shows the optimal separating\n",
      "hyperplane for our toy example; there are three support points. Likewise,\n",
      "β0is obtained by solving (4.53) for any of the support points.\n",
      "The optimal separating hyperplane produces a function ˆf(x) =xTˆβ+ˆβ0\n",
      "for classifying new observations:\n",
      "ˆG(x) = sign ˆf(x). (4.54)\n",
      "Although none of the training observations fall in the margin (by con-\n",
      "struction), this will not necessarily be the case for test observations. The\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "134 4. Linear Methods for Classiﬁcation\n",
      "FIGURE 4.16. The same data as in Figure 4.14. The shaded region delineates\n",
      "the maximum margin separating the two classes. There are three support points\n",
      "indicated, which lie on the boundary of the margin, and the optima l separating\n",
      "hyperplane (blue line) bisects the slab. Included in the ﬁgure is the boundary found\n",
      "using logistic regression (red line), which is very close to th e optimal separating\n",
      "hyperplane (see Section 12.3.3).\n",
      "intuition is that a large margin on the training data will lead to good\n",
      "separation on the test data.\n",
      "The description of the solution in terms of support points seems to sug-\n",
      "gest that the optimal hyperplane focuses more on the points that count,\n",
      "and is more robust to model misspeciﬁcation. The LDA solution, on the\n",
      "other hand, depends on all of the data, even points far away from the de-\n",
      "cision boundary. Note, however, that the identiﬁcation of these support\n",
      "points required the use of all the data. Of course, if the classes are really\n",
      "Gaussian, then LDA is optimal, and separating hyperplanes will pay a price\n",
      "for focusing on the (noisier) data at the boundaries of the classes.\n",
      "Included in Figure 4.16 is the logistic regression solution to this prob-\n",
      "lem, ﬁt by maximum likelihood. Both solutions are similar in this case.\n",
      "When a separating hyperplane exists, logistic regression will always ﬁnd\n",
      "it, since the log-likelihood can be driven to 0 in this case (Exercise 4.5).\n",
      "The logistic regression solution shares some other qualitative features with\n",
      "the separating hyperplane solution. The coeﬃcient vector is deﬁned by a\n",
      "weighted least squares ﬁt of a zero-mean linearized response on the input\n",
      "features, and the weights are larger for points near the decision boundary\n",
      "than for those further away.\n",
      "When the data are not separable, there will be no feasible solution to\n",
      "this problem, and an alternative formulation is needed. Again one can en-\n",
      "large the space using basis transformations, but this can lead to artiﬁcial\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 135\n",
      "separation through over-ﬁtting. In Chapter 12 we discuss a more attractive\n",
      "alternative known as the support vector machine , which allows for overlap,\n",
      "but minimizes a measure of the extent of this overlap.\n",
      "Bibliographic Notes\n",
      "Good general texts on classiﬁcation include Duda et al. (2000), Hand\n",
      "(1981), McLachlan (1992) and Ripley (1996). Mardia et al. (1979) have\n",
      "a concise discussion of linear discriminant analysis. Michie et al. (1994)\n",
      "compare a large number of popular classiﬁers on benchmark datasets. Lin-\n",
      "ear separating hyperplanes are discussed in Vapnik (1996). Our account of\n",
      "the perceptron learning algorithm follows Ripley (1996).\n",
      "Exercises\n",
      "Ex. 4.1 Show how to solve the generalized eigenvalue problem max aTBa\n",
      "subject to aTWa= 1 by transforming to a standard eigenvalue problem.\n",
      "Ex. 4.2 Suppose we have features x∈IRp, a two-class response, with class\n",
      "sizesN1,N2, and the target coded as −N/N 1,N/N 2.\n",
      "(a) Show that the LDA rule classiﬁes to class 2 if\n",
      "xTˆΣ−1(ˆθ2−ˆθ1)>1\n",
      "2ˆθT\n",
      "2ˆΣ−1ˆθ2−1\n",
      "2ˆθT\n",
      "1ˆΣ−1ˆθ1+ log(N1\n",
      "N)\n",
      "−log(N2\n",
      "N)\n",
      ",\n",
      "and class 1 otherwise.\n",
      "(b) Consider minimization of the least squares criterion\n",
      "N∑\n",
      "i=1(yi−β0−βTxi)2. (4.55)\n",
      "Show that the solution ˆβsatisﬁes\n",
      "[\n",
      "(N−2)ˆΣ+N1N2\n",
      "NˆΣB]\n",
      "β=N(ˆθ2−ˆθ1) (4.56)\n",
      "(after simpliﬁcation),where ˆΣB= (ˆθ2−ˆθ1)(ˆθ2−ˆθ1)T.\n",
      "(c) Hence show that ˆΣBβis in the direction (ˆ θ2−ˆθ1) and thus\n",
      "ˆβ∝ˆΣ−1(ˆθ2−ˆθ1). (4.57)\n",
      "Therefore the least squares regression coeﬃcient is identical to the\n",
      "LDA coeﬃcient, up to a scalar multiple.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "136 4. Linear Methods for Classiﬁcation\n",
      "(d) Show that this result holds for any (distinct) coding of the two classes.\n",
      "(e) Find the solution ˆβ0, and hence the predicted values ˆf=ˆβ0+ˆβTx.\n",
      "Consider the following rule: classify to class 2 if ˆ yi>0 and class\n",
      "1 otherwise. Show this is not the same as the LDA rule unless the\n",
      "classes have equal numbers of observations.\n",
      "(Fisher, 1936; Ripley, 1996)\n",
      "Ex. 4.3 Suppose we transform the original predictors XtoˆYvia linear\n",
      "regression. In detail, let ˆY=X(XTX)−1XTY=XˆB, where Yis the\n",
      "indicator response matrix. Similarly for any input x∈IRp, we get a trans-\n",
      "formed vector ˆ y=ˆBTx∈IRK. Show that LDA using ˆYis identical to\n",
      "LDA in the original space.\n",
      "Ex. 4.4 Consider the multilogit model with Kclasses (4.17). Let βbe the\n",
      "(p+ 1)(K−1)-vector consisting of all the coeﬃcients. Deﬁne a suitably\n",
      "enlarged version of the input vector xto accommodate this vectorized co-\n",
      "eﬃcient matrix. Derive the Newton-Raphson algorithm for maximizing the\n",
      "multinomial log-likelihood, and describe how you would implement this\n",
      "algorithm.\n",
      "Ex. 4.5 Consider a two-class logistic regression problem with x∈IR. Char-\n",
      "acterize the maximum-likelihood estimates of the slope and intercept pa-\n",
      "rameter if the sample xifor the two classes are separated by a point x0∈IR.\n",
      "Generalize this result to (a) x∈IRp(see Figure 4.16), and (b) more than\n",
      "two classes.\n",
      "Ex. 4.6 Suppose we have Npoints xiin IRpin general position, with class\n",
      "labels yi∈ {−1,1}. Prove that the perceptron learning algorithm converges\n",
      "to a separating hyperplane in a ﬁnite number of steps:\n",
      "(a) Denote a hyperplane by f(x) =βT\n",
      "1x+β0= 0, or in more compact\n",
      "notation βTx∗= 0, where x∗= (x,1) and β= (β1,β0). Let zi=\n",
      "x∗\n",
      "i/||x∗\n",
      "i||. Show that separability implies the existence of a βsepsuch\n",
      "thatyiβT\n",
      "sepzi≥1∀i\n",
      "(b) Given a current βold, the perceptron algorithm identiﬁes a point zithat\n",
      "is misclassiﬁed, and produces the update βnew←βold+yizi. Show\n",
      "that||βnew−βsep||2≤ ||βold−βsep||2−1, and hence that the algorithm\n",
      "converges to a separating hyperplane in no more than ||βstart−βsep||2\n",
      "steps (Ripley, 1996).\n",
      "Ex. 4.7 Consider the criterion\n",
      "D∗(β,β0) =−N∑\n",
      "i=1yi(xT\n",
      "iβ+β0), (4.58)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 137\n",
      "a generalization of (4.41) where we sum over all the observations. Consider\n",
      "minimizing D∗subject to ||β||= 1. Describe this criterion in words. Does\n",
      "it solve the optimal separating hyperplane problem?\n",
      "Ex. 4.8 Consider the multivariate Gaussian model X|G=k∼N(θk,Σ),\n",
      "with the additional restriction that rank {θk}K\n",
      "1=L < max(K−1,p).\n",
      "Derive the constrained MLEs for the θkandΣ. Show that the Bayes clas-\n",
      "siﬁcation rule is equivalent to classifying in the reduced subspace computed\n",
      "by LDA (Hastie and Tibshirani, 1996b).\n",
      "Ex. 4.9 Write a computer program to perform a quadratic discriminant\n",
      "analysis by ﬁtting a separate Gaussian model per class. Try it out on the\n",
      "vowel data, and compute the misclassiﬁcation error for the test data. The\n",
      "data can be found in the book website www-stat.stanford.edu/ElemStatLearn .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "138 4. Linear Methods for Classiﬁcation\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 139\n",
      "Printer: Opaque this\n",
      "5\n",
      "Basis Expansions and Regularization\n",
      "5.1 Introduction\n",
      "We have already made use of models linear in the input features, both for\n",
      "regression and classiﬁcation. Linear regression, linear discriminant analysi s,\n",
      "logistic regression and separating hyperplanes all rely on a linear model.\n",
      "It is extremely unlikely that the true function f(X) is actually linear in\n",
      "X. In regression problems, f(X) = E( Y|X) will typically be nonlinear and\n",
      "nonadditive in X, and representing f(X) by a linear model is usually a con-\n",
      "venient, and sometimes a necessary, approximation. Convenient because a\n",
      "linear model is easy to interpret, and is the ﬁrst-order Taylor approxima-\n",
      "tion to f(X). Sometimes necessary, because with Nsmall and/or plarge,\n",
      "a linear model might be all we are able to ﬁt to the data without overﬁt-\n",
      "ting. Likewise in classiﬁcation, a linear, Bayes-optimal decision boundary\n",
      "implies that some monotone transformation of Pr( Y= 1|X) is linear in X.\n",
      "This is inevitably an approximation.\n",
      "In this chapter and the next we discuss popular methods for moving\n",
      "beyond linearity. The core idea in this chapter is to augment/replace the\n",
      "vector of inputs Xwith additional variables, which are transformations of\n",
      "X, and then use linear models in this new space of derived input features.\n",
      "Denote by hm(X) : IRp↦→IR the mth transformation of X,m=\n",
      "1,... ,M . We then model\n",
      "f(X) =M∑\n",
      "m=1βmhm(X), (5.1)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "140 5. Basis Expansions and Regularization\n",
      "alinear basis expansion inX. The beauty of this approach is that once the\n",
      "basis functions hmhave been determined, the models are linear in these\n",
      "new variables, and the ﬁtting proceeds as before.\n",
      "Some simple and widely used examples of the hmare the following:\n",
      "•hm(X) =Xm, m= 1,... ,p recovers the original linear model.\n",
      "•hm(X) =X2\n",
      "jorhm(X) =XjXkallows us to augment the inputs with\n",
      "polynomial terms to achieve higher-order Taylor expansions. Note,\n",
      "however, that the number of variables grows exponentially in the de-\n",
      "gree of the polynomial. A full quadratic model in pvariables requires\n",
      "O(p2) square and cross-product terms, or more generally O(pd) for a\n",
      "degree- dpolynomial.\n",
      "•hm(X) = log( Xj),√\n",
      "Xj,...permits other nonlinear transformations\n",
      "of single inputs. More generally one can use similar functions involv-\n",
      "ing several inputs, such as hm(X) =||X||.\n",
      "•hm(X) =I(Lm≤Xk< Um), an indicator for a region of Xk. By\n",
      "breaking the range of Xkup into Mksuch nonoverlapping regions\n",
      "results in a model with a piecewise constant contribution for Xk.\n",
      "Sometimes the problem at hand will call for particular basis functions hm,\n",
      "such as logarithms or power functions. More often, however, we use the basis\n",
      "expansions as a device to achieve more ﬂexible representations for f(X).\n",
      "Polynomials are an example of the latter, although they are limited by\n",
      "their global nature—tweaking the coeﬃcients to achieve a functional form\n",
      "in one region can cause the function to ﬂap about madly in remote regions.\n",
      "In this chapter we consider more useful families of piecewise-polynomials\n",
      "andsplines that allow for local polynomial representations. We also discuss\n",
      "thewavelet bases, especially useful for modeling signals and images. These\n",
      "methods produce a dictionary Dconsisting of typically a very large number\n",
      "|D|of basis functions, far more than we can aﬀord to ﬁt to our data. Along\n",
      "with the dictionary we require a method for controlling the complexity\n",
      "of our model, using basis functions from the dictionary. There are three\n",
      "common approaches:\n",
      "•Restriction methods, where we decide before-hand to limit the class\n",
      "of functions. Additivity is an example, where we assume that our\n",
      "model has the form\n",
      "f(X) =p∑\n",
      "j=1fj(Xj)\n",
      "=p∑\n",
      "j=1Mj∑\n",
      "m=1βjmhjm(Xj). (5.2)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.2 Piecewise Polynomials and Splines 141\n",
      "The size of the model is limited by the number of basis functions Mj\n",
      "used for each component function fj.\n",
      "•Selection methods, which adaptively scan the dictionary and include\n",
      "only those basis functions hmthat contribute signiﬁcantly to the ﬁt of\n",
      "the model. Here the variable selection techniques discussed in Chap-\n",
      "ter 3 are useful. The stagewise greedy approaches such as CART,\n",
      "MARS and boosting fall into this category as well.\n",
      "•Regularization methods where we use the entire dictionary but re-\n",
      "strict the coeﬃcients. Ridge regression is a simple example of a regu-\n",
      "larization approach, while the lasso is both a regularization and selec-\n",
      "tion method. Here we discuss these and more sophisticated methods\n",
      "for regularization.\n",
      "5.2 Piecewise Polynomials and Splines\n",
      "We assume until Section 5.7 that Xis one-dimensional. A piecewise poly-\n",
      "nomial function f(X) is obtained by dividing the domain of Xinto contigu-\n",
      "ous intervals, and representing fby a separate polynomial in each interval.\n",
      "Figure 5.1 shows two simple piecewise polynomials. The ﬁrst is piecewise\n",
      "constant, with three basis functions:\n",
      "h1(X) =I(X < ξ 1), h 2(X) =I(ξ1≤X < ξ 2), h 3(X) =I(ξ2≤X).\n",
      "Since these are positive over disjoint regions, the least squares estimate o f\n",
      "the model f(X) =∑3\n",
      "m=1βmhm(X) amounts to ˆβm=¯Ym, the mean of Y\n",
      "in the mth region.\n",
      "The top right panel shows a piecewise linear ﬁt. Three additional basis\n",
      "functions are needed: hm+3=hm(X)X, m = 1,... ,3. Except in special\n",
      "cases, we would typically prefer the third panel, which is also piecewise\n",
      "linear, but restricted to be continuous at the two knots. These continu-\n",
      "ity restrictions lead to linear constraints on the parameters; for example,\n",
      "f(ξ−\n",
      "1) =f(ξ+\n",
      "1) implies that β1+ξ1β4=β2+ξ1β5. In this case, since there\n",
      "are two restrictions, we expect to get back two parameters, leaving four free\n",
      "parameters.\n",
      "A more direct way to proceed in this case is to use a basis that incorpo-\n",
      "rates the constraints:\n",
      "h1(X) = 1, h 2(X) =X, h 3(X) = (X−ξ1)+, h 4(X) = (X−ξ2)+,\n",
      "where t+denotes the positive part. The function h3is shown in the lower\n",
      "right panel of Figure 5.1. We often prefer smoother functions, and these\n",
      "can be achieved by increasing the order of the local polynomial. Figure 5.2\n",
      "shows a series of piecewise-cubic polynomials ﬁt to the same data, with\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "142 5. Basis Expansions and Regularization\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOPiecewise Constant\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOPiecewise Linear\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOContinuous Piecewise Linear Piecewise-linear Basis Function\n",
      "•\n",
      "•••\n",
      "••••\n",
      "••\n",
      "•• • ••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "ξ1 ξ1ξ1 ξ1\n",
      "ξ2 ξ2ξ2 ξ2\n",
      "(X−ξ1)+\n",
      "FIGURE 5.1. The top left panel shows a piecewise constant function ﬁt to some\n",
      "artiﬁcial data. The broken vertical lines indicate the positio ns of the two knots\n",
      "ξ1andξ2. The blue curve represents the true function, from which the dat a were\n",
      "generated with Gaussian noise. The remaining two panels show piec ewise lin-\n",
      "ear functions ﬁt to the same data—the top right unrestricted, and t he lower left\n",
      "restricted to be continuous at the knots. The lower right panel sh ows a piecewise–\n",
      "linear basis function, h3(X) = ( X−ξ1)+, continuous at ξ1. The black points\n",
      "indicate the sample evaluations h3(xi), i= 1, . . . , N .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.2 Piecewise Polynomials and Splines 143\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOOO\n",
      "O\n",
      "OO\n",
      "OOODiscontinuous\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOOO\n",
      "O\n",
      "OO\n",
      "OOOContinuous\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOOO\n",
      "O\n",
      "OO\n",
      "OOOContinuous First Derivative\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOOOO\n",
      "O\n",
      "OO\n",
      "OOOContinuous Second DerivativePiecewise Cubic Polynomials\n",
      "ξ1 ξ1ξ1 ξ1\n",
      "ξ2 ξ2ξ2 ξ2\n",
      "FIGURE 5.2. A series of piecewise-cubic polynomials, with increasing orde rs of\n",
      "continuity.\n",
      "increasing orders of continuity at the knots. The function in the lower\n",
      "right panel is continuous, and has continuous ﬁrst and second derivatives\n",
      "at the knots. It is known as a cubic spline . Enforcing one more order of\n",
      "continuity would lead to a global cubic polynomial. It is not hard to show\n",
      "(Exercise 5.1) that the following basis represents a cubic spline with knots\n",
      "atξ1andξ2:\n",
      "h1(X) = 1, h 3(X) =X2, h5(X) = (X−ξ1)3\n",
      "+,\n",
      "h2(X) =X, h 4(X) =X3, h6(X) = (X−ξ2)3\n",
      "+.(5.3)\n",
      "There are six basis functions corresponding to a six-dimensional linear space\n",
      "of functions. A quick check conﬁrms the parameter count: (3 regions) ×(4\n",
      "parameters per region) −(2 knots) ×(3 constraints per knot)= 6.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "144 5. Basis Expansions and Regularization\n",
      "More generally, an order- Mspline with knots ξj, j= 1,... ,K is a\n",
      "piecewise-polynomial of order M, and has continuous derivatives up to\n",
      "order M−2. A cubic spline has M= 4. In fact the piecewise-constant\n",
      "function in Figure 5.1 is an order-1 spline, while the continuous piece-\n",
      "wise linear function is an order-2 spline. Likewise the general form for the\n",
      "truncated-power basis set would be\n",
      "hj(X) = Xj−1, j= 1,... ,M,\n",
      "hM+ℓ(X) = ( X−ξℓ)M−1\n",
      "+, ℓ= 1,... ,K.\n",
      "It is claimed that cubic splines are the lowest-order spline for which the\n",
      "knot-discontinuity is not visible to the human eye. There is seldom any\n",
      "good reason to go beyond cubic-splines, unless one is interested in smooth\n",
      "derivatives. In practice the most widely used orders are M= 1,2 and 4.\n",
      "These ﬁxed-knot splines are also known as regression splines . One needs\n",
      "to select the order of the spline, the number of knots and their placement.\n",
      "One simple approach is to parameterize a family of splines by the number\n",
      "of basis functions or degrees of freedom, and have the observations xide-\n",
      "termine the positions of the knots. For example, the expression bs(x,df=7)\n",
      "inRgenerates a basis matrix of cubic-spline functions evaluated at the N\n",
      "observations in x, with the 7 −3 = 41interior knots at the appropriate per-\n",
      "centiles of x(20,40,60 and 80th.) One can be more explicit, however; bs(x,\n",
      "degree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,\n",
      "with three interior knots, and returns an N×4 matrix.\n",
      "Since the space of spline functions of a particular order and knot sequence\n",
      "is a vector space, there are many equivalent bases for representing them\n",
      "(just as there are for ordinary polynomials.) While the truncated power\n",
      "basis is conceptually simple, it is not too attractive numerically: powers of\n",
      "large numbers can lead to severe rounding problems. The B-spline basis,\n",
      "described in the Appendix to this chapter, allows for eﬃcient computations\n",
      "even when the number of knots Kis large.\n",
      "5.2.1 Natural Cubic Splines\n",
      "We know that the behavior of polynomials ﬁt to data tends to be erratic\n",
      "near the boundaries, and extrapolation can be dangerous. These problems\n",
      "are exacerbated with splines. The polynomials ﬁt beyond the boundary\n",
      "knots behave even more wildly than the corresponding global polynomials\n",
      "in that region. This can be conveniently summarized in terms of the point-\n",
      "wise variance of spline functions ﬁt by least squares (see the example in the\n",
      "next section for details on these variance calculations). Figure 5.3 compares\n",
      "1A cubic spline with four knots is eight-dimensional. The bs() function omits by\n",
      "default the constant term in the basis, since terms like this are typically included with\n",
      "other terms in the model.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.2 Piecewise Polynomials and Splines 145\n",
      "XPointwise Variances\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5 0.6•\n",
      "••\n",
      "•\n",
      "••\n",
      "•••••••••••••• •••• ••••••••••••••••••••••••••\n",
      "•••••\n",
      "•••••••••••••••••••••••••••••••••••••••••••\n",
      "•\n",
      "••\n",
      "•\n",
      "••••••••••••••••••••••••••\n",
      "•\n",
      "••••••••••••••••••\n",
      "•••••••••••••••••••• ••••••••••••••••••••••••• ••••Global Linear\n",
      "Global Cubic Polynomial\n",
      "Cubic Spline - 2 knots\n",
      "Natural Cubic Spline - 6 knots\n",
      "FIGURE 5.3. Pointwise variance curves for four diﬀerent models, with Xcon-\n",
      "sisting of 50points drawn at random from U[0,1], and an assumed error model\n",
      "with constant variance. The linear and cubic polynomial ﬁts have two and four\n",
      "degrees of freedom, respectively, while the cubic spline and na tural cubic spline\n",
      "each have six degrees of freedom. The cubic spline has two knots at0.33and0.66,\n",
      "while the natural spline has boundary knots at 0.1and0.9, and four interior knots\n",
      "uniformly spaced between them.\n",
      "the pointwise variances for a variety of diﬀerent models. The explosion of\n",
      "the variance near the boundaries is clear, and inevitably is worst for cubic\n",
      "splines.\n",
      "Anatural cubic spline adds additional constraints, namely that the func-\n",
      "tion is linear beyond the boundary knots. This frees up four degrees of\n",
      "freedom (two constraints each in both boundary regions), which can be\n",
      "spent more proﬁtably by sprinkling more knots in the interior region. This\n",
      "tradeoﬀ is illustrated in terms of variance in Figure 5.3. There will be a\n",
      "price paid in bias near the boundaries, but assuming the function is lin-\n",
      "ear near the boundaries (where we have less information anyway) is often\n",
      "considered reasonable.\n",
      "A natural cubic spline with Kknots is represented by Kbasis functions.\n",
      "One can start from a basis for cubic splines, and derive the reduced ba-\n",
      "sis by imposing the boundary constraints. For example, starting from the\n",
      "truncated power series basis described in Section 5.2, we arrive at (Exer-\n",
      "cise 5.4):\n",
      "N1(X) = 1, N 2(X) =X, N k+2(X) =dk(X)−dK−1(X),(5.4)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "146 5. Basis Expansions and Regularization\n",
      "where\n",
      "dk(X) =(X−ξk)3\n",
      "+−(X−ξK)3\n",
      "+\n",
      "ξK−ξk. (5.5)\n",
      "Each of these basis functions can be seen to have zero second and third\n",
      "derivative for X≥ξK.\n",
      "5.2.2 Example: South African Heart Disease (Continued)\n",
      "In Section 4.4.2 we ﬁt linear logistic regression models to the South African\n",
      "heart disease data. Here we explore nonlinearities in the functions using\n",
      "natural splines. The functional form of the model is\n",
      "logit[Pr(chd|X)] =θ0+h1(X1)Tθ1+h2(X2)Tθ2+≤≤≤+hp(Xp)Tθp,(5.6)\n",
      "where each of the θjare vectors of coeﬃcients multiplying their associated\n",
      "vector of natural spline basis functions hj.\n",
      "We use four natural spline bases for each term in the model. For example,\n",
      "withX1representing sbp,h1(X1) is a basis consisting of four basis func-\n",
      "tions. This actually implies three rather than two interior knots (chosen at\n",
      "uniform quantiles of sbp), plus two boundary knots at the extremes of the\n",
      "data, since we exclude the constant term from each of the hj.\n",
      "Sincefamhist is a two-level factor, it is coded by a simple binary or\n",
      "dummy variable, and is associated with a single coeﬃcient in the ﬁt of the\n",
      "model.\n",
      "More compactly we can combine all pvectors of basis functions (and\n",
      "the constant term) into one big vector h(X), and then the model is simply\n",
      "h(X)Tθ, with total number of parameters df = 1 +∑p\n",
      "j=1dfj, the sum of\n",
      "the parameters in each component term. Each basis function is evaluated\n",
      "at each of the Nsamples, resulting in a N×df basis matrix H. At this\n",
      "point the model is like any other linear logistic model, and the algorithms\n",
      "described in Section 4.4.1 apply.\n",
      "We carried out a backward stepwise deletion process, dropping terms\n",
      "from this model while preserving the group structure of each term, rather\n",
      "than dropping one coeﬃcient at a time. The AIC statistic (Section 7.5) was\n",
      "used to drop terms, and all the terms remaining in the ﬁnal model would\n",
      "cause AIC to increase if deleted from the model (see Table 5.1). Figure 5.4\n",
      "shows a plot of the ﬁnal model selected by the stepwise regression. The\n",
      "functions displayed are ˆfj(Xj) =hj(Xj)Tˆθjfor each variable Xj. The\n",
      "covariance matrix Cov( ˆθ) =Σis estimated by ˆΣ= (HTWH)−1, where W\n",
      "is the diagonal weight matrix from the logistic regression. Hence vj(Xj) =\n",
      "Var[ˆfj(Xj)] =hj(Xj)TˆΣjjhj(Xj) is the pointwise variance function of ˆfj,\n",
      "where Cov( ˆθj) =ˆΣjjis the appropriate sub-matrix of ˆΣ. The shaded region\n",
      "in each panel is deﬁned by ˆfj(Xj)±2√\n",
      "vj(Xj).\n",
      "The AIC statistic is slightly more generous than the likelihood-ratio tes t\n",
      "(deviance test). Both sbpandobesity are included in this model, while\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.2 Piecewise Polynomials and Splines 147\n",
      "100 120 140 160 180 200 220-2 0 2 4\n",
      "0 5 10 15 20 25 300 2 4 6 8\n",
      "2 4 6 8 10 12 14-4 -2 0 2 4\n",
      "-4 -2 0 2 4\n",
      "Absent Present\n",
      "15 20 25 30 35 40 45-2 0 2 4 6\n",
      "20 30 40 50 60-6 -4 -2 0 2ˆf(sbp)\n",
      "sbp\n",
      "ˆf(tobacco )\n",
      "tobaccoˆf(ldl)\n",
      "ldlˆf(obesity )\n",
      "obesity\n",
      "ˆf(age)\n",
      "ageˆf(famhist )\n",
      "famhist\n",
      "FIGURE 5.4. Fitted natural-spline functions for each of the terms in the ﬁnal\n",
      "model selected by the stepwise procedure. Included are pointw ise standard-error\n",
      "bands. The rug plot at the base of each ﬁgure indicates the location of each of the\n",
      "sample values for that variable (jittered to break ties).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "148 5. Basis Expansions and Regularization\n",
      "TABLE 5.1. Final logistic regression model, after stepwise deletion of natural\n",
      "splines terms. The column labeled “LRT” is the likelihood-ra tio test statistic when\n",
      "that term is deleted from the model, and is the change in deviance from the full\n",
      "model (labeled “none”).\n",
      "Terms Df Deviance AIC LRT P-value\n",
      "none 458.09 502.09\n",
      "sbp 4 467.16 503.16 9.076 0.059\n",
      "tobacco 4 470.48 506.48 12.387 0.015\n",
      "ldl 4 472.39 508.39 14.307 0.006\n",
      "famhist 1 479.44 521.44 21.356 0.000\n",
      "obesity 4 466.24 502.24 8.147 0.086\n",
      "age 4 481.86 517.86 23.768 0.000\n",
      "they were not in the linear model. The ﬁgure explains why, since their\n",
      "contributions are inherently nonlinear. These eﬀects at ﬁrst may come as\n",
      "a surprise, but an explanation lies in the nature of the retrospective data.\n",
      "These measurements were made sometime after the patients suﬀered a\n",
      "heart attack, and in many cases they had already beneﬁted from a healthier\n",
      "diet and lifestyle, hence the apparent increase in risk at low values for\n",
      "obesity andsbp. Table 5.1 shows a summary of the selected model.\n",
      "5.2.3 Example: Phoneme Recognition\n",
      "In this example we use splines to reduce ﬂexibility rather than increase it;\n",
      "the application comes under the general heading of functional modeling. In\n",
      "the top panel of Figure 5.5 are displayed a sample of 15 log-periodograms\n",
      "for each of the two phonemes “aa” and “ao” measured at 256 frequencies.\n",
      "The goal is to use such data to classify a spoken phoneme. These two\n",
      "phonemes were chosen because they are diﬃcult to separate.\n",
      "The input feature is a vector xof length 256, which we can think of as\n",
      "a vector of evaluations of a function X(f) over a grid of frequencies f. In\n",
      "reality there is a continuous analog signal which is a function of frequency,\n",
      "and we have a sampled version of it.\n",
      "The gray lines in the lower panel of Figure 5.5 show the coeﬃcients of\n",
      "a linear logistic regression model ﬁt by maximum likelihood to a training\n",
      "sample of 1000 drawn from the total of 695 “aa”s and 1022 “ao”s. The\n",
      "coeﬃcients are also plotted as a function of frequency, and in fact we can\n",
      "think of the model in terms of its continuous counterpart\n",
      "logPr(aa|X)\n",
      "Pr(ao|X)=∫\n",
      "X(f)β(f)df, (5.7)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.2 Piecewise Polynomials and Splines 149\n",
      "FrequencyLog-periodogram\n",
      "0 50 100 150 200 2500 5 10 15 20 25Phoneme Examples\n",
      "aa\n",
      "ao\n",
      "FrequencyLogistic Regression Coefficients\n",
      "0 50 100 150 200 250-0.4 -0.2 0.0 0.2 0.4Phoneme Classification: Raw and Restricted Logistic Regression\n",
      "FIGURE 5.5. The top panel displays the log-periodogram as a function of fre -\n",
      "quency for 15examples each of the phonemes “aa” and “ao” sampled from a total\n",
      "of695“aa”s and 1022“ao”s. Each log-periodogram is measured at 256uniformly\n",
      "spaced frequencies. The lower panel shows the coeﬃcients (as a f unction of fre-\n",
      "quency) of a logistic regression ﬁt to the data by maximum likeli hood, using the\n",
      "256log-periodogram values as inputs. The coeﬃcients are restric ted to be smooth\n",
      "in the red curve, and are unrestricted in the jagged gray curve.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "150 5. Basis Expansions and Regularization\n",
      "which we approximate by\n",
      "256∑\n",
      "j=1X(fj)β(fj) =256∑\n",
      "j=1xjβj. (5.8)\n",
      "The coeﬃcients compute a contrast functional, and will have appreciable\n",
      "values in regions of frequency where the log-periodograms diﬀer between\n",
      "the two classes.\n",
      "The gray curves are very rough. Since the input signals have fairly strong\n",
      "positive autocorrelation, this results in negative autocorrelation in t he co-\n",
      "eﬃcients. In addition the sample size eﬀectively provides only four obser-\n",
      "vations per coeﬃcient.\n",
      "Applications such as this permit a natural regularization. We force the\n",
      "coeﬃcients to vary smoothly as a function of frequency. The red curve in the\n",
      "lower panel of Figure 5.5 shows such a smooth coeﬃcient curve ﬁt to these\n",
      "data. We see that the lower frequencies oﬀer the most discriminatory power.\n",
      "Not only does the smoothing allow easier interpretation of the contrast, it\n",
      "also produces a more accurate classiﬁer:\n",
      "Raw Regularized\n",
      "Training error 0.080 0.185\n",
      "Test error 0.255 0.158\n",
      "The smooth red curve was obtained through a very simple use of natural\n",
      "cubic splines. We can represent the coeﬃcient function as an expansion of\n",
      "splines β(f) =∑M\n",
      "m=1hm(f)θm. In practice this means that β=Hθwhere,\n",
      "His ap×Mbasis matrix of natural cubic splines, deﬁned on the set of\n",
      "frequencies. Here we used M= 12 basis functions, with knots uniformly\n",
      "placed over the integers 1 ,2,... ,256 representing the frequencies. Since\n",
      "xTβ=xTHθ, we can simply replace the input features xby their ﬁltered\n",
      "versions x∗=HTx, and ﬁt θby linear logistic regression on the x∗. The\n",
      "red curve is thus ˆβ(f) =h(f)Tˆθ.\n",
      "5.3 Filtering and Feature Extraction\n",
      "In the previous example, we constructed a p×Mbasis matrix H, and then\n",
      "transformed our features xinto new features x∗=HTx. These ﬁltered\n",
      "versions of the features were then used as inputs into a learning procedure:\n",
      "in the previous example, this was linear logistic regression.\n",
      "Preprocessing of high-dimensional features is a very general and pow-\n",
      "erful method for improving the performance of a learning algorithm. The\n",
      "preprocessing need not be linear as it was above, but can be a general\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.4 Smoothing Splines 151\n",
      "(nonlinear) function of the form x∗=g(x). The derived features x∗can\n",
      "then be used as inputs into any (linear or nonlinear) learning procedure.\n",
      "For example, for signal or image recognition a popular approach is to ﬁrst\n",
      "transform the raw features via a wavelet transform x∗=HTx(Section 5.9)\n",
      "and then use the features x∗as inputs into a neural network (Chapter 11).\n",
      "Wavelets are eﬀective in capturing discrete jumps or edges, and the neural\n",
      "network is a powerful tool for constructing nonlinear functions of these\n",
      "features for predicting the target variable. By using domain knowledge\n",
      "to construct appropriate features, one can often improve upon a learning\n",
      "method that has only the raw features xat its disposal.\n",
      "5.4 Smoothing Splines\n",
      "Here we discuss a spline basis method that avoids the knot selection prob-\n",
      "lem completely by using a maximal set of knots. The complexity of the ﬁt\n",
      "is controlled by regularization. Consider the following problem: among all\n",
      "functions f(x) with two continuous derivatives, ﬁnd one that minimizes the\n",
      "penalized residual sum of squares\n",
      "RSS(f,λ) =N∑\n",
      "i=1{yi−f(xi)}2+λ∫\n",
      "{f′′(t)}2dt, (5.9)\n",
      "where λis a ﬁxed smoothing parameter . The ﬁrst term measures closeness\n",
      "to the data, while the second term penalizes curvature in the function, and\n",
      "λestablishes a tradeoﬀ between the two. Two special cases are:\n",
      "λ= 0 : fcan be any function that interpolates the data.\n",
      "λ=∞:the simple least squares line ﬁt, since no second derivative can\n",
      "be tolerated.\n",
      "These vary from very rough to very smooth, and the hope is that λ∈(0,∞)\n",
      "indexes an interesting class of functions in between.\n",
      "The criterion (5.9) is deﬁned on an inﬁnite-dimensional function space—\n",
      "in fact, a Sobolev space of functions for which the second term is deﬁned.\n",
      "Remarkably, it can be shown that (5.9) has an explicit, ﬁnite-dimensional,\n",
      "unique minimizer which is a natural cubic spline with knots at the unique\n",
      "values of the xi, i= 1,... ,N (Exercise 5.7). At face value it seems that\n",
      "the family is still over-parametrized, since there are as many as Nknots,\n",
      "which implies Ndegrees of freedom. However, the penalty term translates\n",
      "to a penalty on the spline coeﬃcients, which are shrunk some of the way\n",
      "toward the linear ﬁt.\n",
      "Since the solution is a natural spline, we can write it as\n",
      "f(x) =N∑\n",
      "j=1Nj(x)θj, (5.10)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "152 5. Basis Expansions and Regularization\n",
      "AgeRelative Change in Spinal BMD\n",
      "10 15 20 25-0.05 0.0 0.05 0.10 0.15 0.20••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "•\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "••••\n",
      "•••••\n",
      "••\n",
      "•••••\n",
      "•\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "••••\n",
      "• ••• ••••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••••\n",
      "•\n",
      "•••••\n",
      "••\n",
      "•••\n",
      "•\n",
      "• •••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "••••\n",
      "• •\n",
      "••••\n",
      "••• •\n",
      "••••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "• •••\n",
      "•\n",
      "•\n",
      "••••••\n",
      "•\n",
      "••\n",
      "••\n",
      "••••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "••••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "• •••\n",
      "• ••\n",
      "•••••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••••\n",
      "••\n",
      "•\n",
      "••••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "• ••\n",
      "•••••••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•• ••\n",
      "•\n",
      "••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "• •\n",
      "•••\n",
      "•\n",
      "•••\n",
      "• •\n",
      "••\n",
      "•\n",
      "•••\n",
      "••••\n",
      "•\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "••••\n",
      "••••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••••\n",
      "•\n",
      "••\n",
      "•\n",
      "••••\n",
      "• ••\n",
      "•\n",
      "••••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "• ••••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•Male\n",
      "Female\n",
      "FIGURE 5.6. The response is the relative change in bone mineral density mea-\n",
      "sured at the spine in adolescents, as a function of age. A separat e smoothing spline\n",
      "was ﬁt to the males and females, with λ≈0.00022. This choice corresponds to\n",
      "about 12degrees of freedom.\n",
      "where the Nj(x) are an N-dimensional set of basis functions for repre-\n",
      "senting this family of natural splines (Section 5.2.1 and Exercise 5.4). The\n",
      "criterion thus reduces to\n",
      "RSS(θ,λ) = (y−Nθ)T(y−Nθ) +λθTΩNθ, (5.11)\n",
      "where {N}ij=Nj(xi) and {ΩN}jk=∫\n",
      "N′′\n",
      "j(t)N′′\n",
      "k(t)dt. The solution is\n",
      "easily seen to be\n",
      "ˆθ= (NTN+λΩN)−1NTy, (5.12)\n",
      "a generalized ridge regression. The ﬁtted smoothing spline is given by\n",
      "ˆf(x) =N∑\n",
      "j=1Nj(x)ˆθj. (5.13)\n",
      "Eﬃcient computational techniques for smoothing splines are discussed in\n",
      "the Appendix to this chapter.\n",
      "Figure 5.6 shows a smoothing spline ﬁt to some data on bone mineral\n",
      "density (BMD) in adolescents. The response is relative change in spinal\n",
      "BMD over two consecutive visits, typically about one year apart. The data\n",
      "are color coded by gender, and two separate curves were ﬁt. This simple\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.4 Smoothing Splines 153\n",
      "summary reinforces the evidence in the data that the growth spurt for\n",
      "females precedes that for males by about two years. In both cases the\n",
      "smoothing parameter λwas approximately 0 .00022; this choice is discussed\n",
      "in the next section.\n",
      "5.4.1 Degrees of Freedom and Smoother Matrices\n",
      "We have not yet indicated how λis chosen for the smoothing spline. Later\n",
      "in this chapter we describe automatic methods using techniques such as\n",
      "cross-validation. In this section we discuss intuitive ways of prespecifying\n",
      "the amount of smoothing.\n",
      "A smoothing spline with prechosen λis an example of a linear smoother\n",
      "(as in linear operator). This is because the estimated parameters in (5.12)\n",
      "are a linear combination of the yi. Denote by ˆftheN-vector of ﬁtted values\n",
      "ˆf(xi) at the training predictors xi. Then\n",
      "ˆf=N(NTN+λΩN)−1NTy\n",
      "=Sλy. (5.14)\n",
      "Again the ﬁt is linear in y, and the ﬁnite linear operator Sλis known as\n",
      "thesmoother matrix . One consequence of this linearity is that the recipe\n",
      "for producing ˆffromydoes not depend on yitself;Sλdepends only on\n",
      "thexiandλ.\n",
      "Linear operators are familiar in more traditional least squares ﬁtting as\n",
      "well. Suppose Bξis aN×Mmatrix of Mcubic-spline basis functions\n",
      "evaluated at the Ntraining points xi, with knot sequence ξ, and M≪N.\n",
      "Then the vector of ﬁtted spline values is given by\n",
      "ˆf=Bξ(BT\n",
      "ξBξ)−1BT\n",
      "ξy\n",
      "=Hξy. (5.15)\n",
      "Here the linear operator Hξis a projection operator, also known as the hat\n",
      "matrix in statistics. There are some important similarities and di ﬀerences\n",
      "between HξandSλ:\n",
      "•Both are symmetric, positive semideﬁnite matrices.\n",
      "•HξHξ=Hξ(idempotent), while SλSλ⪯Sλ, meaning that the right-\n",
      "hand side exceeds the left-hand side by a positive semideﬁnite matrix.\n",
      "This is a consequence of the shrinking nature of Sλ, which we discuss\n",
      "further below.\n",
      "•Hξhas rank M, while Sλhas rank N.\n",
      "The expression M= trace( Hξ) gives the dimension of the projection space,\n",
      "which is also the number of basis functions, and hence the number of pa-\n",
      "rameters involved in the ﬁt. By analogy we deﬁne the eﬀective degrees of\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "154 5. Basis Expansions and Regularization\n",
      "freedom of a smoothing spline to be\n",
      "dfλ= trace( Sλ), (5.16)\n",
      "the sum of the diagonal elements of Sλ. This very useful deﬁnition allows\n",
      "us a more intuitive way to parameterize the smoothing spline, and indeed\n",
      "many other smoothers as well, in a consistent fashion. For example, in Fig-\n",
      "ure 5.6 we speciﬁed df λ= 12 for each of the curves, and the corresponding\n",
      "λ≈0.00022 was derived numerically by solving trace( Sλ) = 12. There are\n",
      "many arguments supporting this deﬁnition of degrees of freedom, and we\n",
      "cover some of them here.\n",
      "SinceSλis symmetric (and positive semideﬁnite), it has a real eigen-\n",
      "decomposition. Before we proceed, it is convenient to rewrite Sλin the\n",
      "Reinsch form\n",
      "Sλ= (I+λK)−1, (5.17)\n",
      "where Kdoes not depend on λ(Exercise 5.9). Since ˆf=Sλysolves\n",
      "min\n",
      "f(y−f)T(y−f) +λfTKf, (5.18)\n",
      "Kis known as the penalty matrix , and indeed a quadratic form in Khas\n",
      "a representation in terms of a weighted sum of squared (divided) second\n",
      "diﬀerences. The eigen-decomposition of Sλis\n",
      "Sλ=N∑\n",
      "k=1ρk(λ)ukuT\n",
      "k (5.19)\n",
      "with\n",
      "ρk(λ) =1\n",
      "1 +λdk, (5.20)\n",
      "anddkthe corresponding eigenvalue of K. Figure 5.7 (top) shows the re-\n",
      "sults of applying a cubic smoothing spline to some air pollution data (128\n",
      "observations). Two ﬁts are given: a smoother ﬁt corresponding to a larger\n",
      "penalty λand a rougher ﬁt for a smaller penalty. The lower panels repre-\n",
      "sent the eigenvalues (lower left) and some eigenvectors (lower right) of the\n",
      "corresponding smoother matrices. Some of the highlights of the eigenrep-\n",
      "resentation are the following:\n",
      "•The eigenvectors are not aﬀected by changes in λ, and hence the whole\n",
      "family of smoothing splines (for a particular sequence x) indexed by\n",
      "λhave the same eigenvectors.\n",
      "•Sλy=∑N\n",
      "k=1ukρk(λ)⟨uk,y⟩, and hence the smoothing spline oper-\n",
      "ates by decomposing yw.r.t. the (complete) basis {uk}, and diﬀer-\n",
      "entially shrinking the contributions using ρk(λ). This is to be con-\n",
      "trasted with a basis-regression method, where the components are\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.4 Smoothing Splines 155\n",
      "Daggot Pressure GradientOzone Concentration\n",
      "-50 0 50 1000 10 20 30•••\n",
      "••••••\n",
      "••\n",
      "••••••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••••\n",
      "•••••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "OrderEigenvalues\n",
      "5 10 15 20 25-0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2•••\n",
      "•\n",
      "•\n",
      "•\n",
      "••••••••••• •• • • •• • ••••••••\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•••••••••••••df=5\n",
      "df=11\n",
      "-50 0 50 100 -50 0 50 100\n",
      "FIGURE 5.7. (Top:) Smoothing spline ﬁt of ozone concentration versus Daggot\n",
      "pressure gradient. The two ﬁts correspond to diﬀerent values of t he smoothing\n",
      "parameter, chosen to achieve ﬁve and eleven eﬀective degrees o f freedom, deﬁned\n",
      "by dfλ=trace(Sλ). (Lower left:) First 25eigenvalues for the two smoothing-spline\n",
      "matrices. The ﬁrst two are exactly 1, and all are ≥0. (Lower right:) Third to\n",
      "sixth eigenvectors of the spline smoother matrices. In each cas e,ukis plotted\n",
      "against x, and as such is viewed as a function of x. The rugat the base of the\n",
      "plots indicate the occurrence of data points. The damped functio ns represent the\n",
      "smoothed versions of these functions (using the 5df smoother).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "156 5. Basis Expansions and Regularization\n",
      "either left alone, or shrunk to zero—that is, a projection matrix such\n",
      "asHξabove has Meigenvalues equal to 1, and the rest are 0. For\n",
      "this reason smoothing splines are referred to as shrinking smoothers,\n",
      "while regression splines are projection smoothers (see Figure 3.17 on\n",
      "page 80).\n",
      "•The sequence of uk, ordered by decreasing ρk(λ), appear to increase\n",
      "in complexity. Indeed, they have the zero-crossing behavior of polyno-\n",
      "mials of increasing degree. Since Sλuk=ρk(λ)uk, we see how each of\n",
      "the eigenvectors themselves are shrunk by the smoothing spline: the\n",
      "higher the complexity, the more they are shrunk. If the domain of X\n",
      "is periodic, then the ukare sines and cosines at diﬀerent frequencies.\n",
      "•The ﬁrst two eigenvalues are always one, and they correspond to the\n",
      "two-dimensional eigenspace of functions linear in x(Exercise 5.11),\n",
      "which are never shrunk.\n",
      "•The eigenvalues ρk(λ) = 1/(1 +λdk) are an inverse function of the\n",
      "eigenvalues dkof the penalty matrix K, moderated by λ;λcontrols\n",
      "the rate at which the ρk(λ) decrease to zero. d1=d2= 0 and again\n",
      "linear functions are not penalized.\n",
      "•One can reparametrize the smoothing spline using the basis vectors\n",
      "uk(theDemmler–Reinsch basis). In this case the smoothing spline\n",
      "solves\n",
      "min\n",
      "θ∥y−Uθ∥2+λθTDθ, (5.21)\n",
      "where Uhas columns ukandDis a diagonal matrix with elements\n",
      "dk.\n",
      "•dfλ= trace( Sλ) =∑N\n",
      "k=1ρk(λ). For projection smoothers, all the\n",
      "eigenvalues are 1, each one corresponding to a dimension of the pro-\n",
      "jection subspace.\n",
      "Figure 5.8 depicts a smoothing spline matrix, with the rows ordered with\n",
      "x. The banded nature of this representation suggests that a smoothing\n",
      "spline is a local ﬁtting method, much like the locally weighted regression\n",
      "procedures in Chapter 6. The right panel shows in detail selected rows of\n",
      "S, which we call the equivalent kernels . Asλ→0, df λ→N, andSλ→I,\n",
      "theN-dimensional identity matrix. As λ→ ∞, dfλ→2, and Sλ→H, the\n",
      "hat matrix for linear regression on x.\n",
      "5.5 Automatic Selection of the Smoothing\n",
      "Parameters\n",
      "The smoothing parameters for regression splines encompass the degree of\n",
      "the splines, and the number and placement of the knots. For smoothing\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.5 Automatic Selection of the Smoothing Parameters 157\n",
      "11510075502512Smoother Matrix\n",
      "••••• • •••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n",
      "•Row 115••••• ••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n",
      "•Row 100••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Row 75•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 50•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 25•••••• •••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••• • •Row 12Equivalent Kernels\n",
      "FIGURE 5.8. The smoother matrix for a smoothing spline is nearly banded,\n",
      "indicating an equivalent kernel with local support. The left pane l represents the\n",
      "elements of Sas an image. The right panel shows the equivalent kernel or weigh t-\n",
      "ing function in detail for the indicated rows.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "158 5. Basis Expansions and Regularization\n",
      "splines, we have only the penalty parameter λto select, since the knots are\n",
      "at all the unique training X’s, and cubic degree is almost always used in\n",
      "practice.\n",
      "Selecting the placement and number of knots for regression splines can be\n",
      "a combinatorially complex task, unless some simpliﬁcations are enforced.\n",
      "The MARS procedure in Chapter 9 uses a greedy algorithm with some\n",
      "additional approximations to achieve a practical compromise. We will not\n",
      "discuss this further here.\n",
      "5.5.1 Fixing the Degrees of Freedom\n",
      "Since df λ= trace( Sλ) is monotone in λfor smoothing splines, we can in-\n",
      "vert the relationship and specify λby ﬁxing df. In practice this can be\n",
      "achieved by simple numerical methods. So, for example, in Rone can use\n",
      "smooth.spline(x,y,df=6) to specify the amount of smoothing. This encour-\n",
      "ages a more traditional mode of model selection, where we might try a cou-\n",
      "ple of diﬀerent values of df, and select one based on approximate F-tests,\n",
      "residual plots and other more subjective criteria. Using df in this way pro-\n",
      "vides a uniform approach to compare many diﬀerent smoothing methods.\n",
      "It is particularly useful in generalized additive models (Chapter 9), where\n",
      "several smoothing methods can be simultaneously used in one model.\n",
      "5.5.2 The Bias–Variance Tradeoﬀ\n",
      "Figure 5.9 shows the eﬀect of the choice of df λwhen using a smoothing\n",
      "spline on a simple example:\n",
      "Y=f(X) +ε,\n",
      "f(X) =sin(12( X+ 0.2))\n",
      "X+ 0.2,(5.22)\n",
      "withX∼U[0,1] and ε∼N(0,1). Our training sample consists of N= 100\n",
      "pairsxi,yidrawn independently from this model.\n",
      "The ﬁtted splines for three diﬀerent values of df λare shown. The yellow\n",
      "shaded region in the ﬁgure represents the pointwise standard error of ˆfλ,\n",
      "that is, we have shaded the region between ˆfλ(x)±2≤se(ˆfλ(x)). Since\n",
      "ˆf=Sλy,\n",
      "Cov(ˆf) = SλCov(y)ST\n",
      "λ\n",
      "=SλST\n",
      "λ. (5.23)\n",
      "The diagonal contains the pointwise variances at the training xi. The bias\n",
      "is given by\n",
      "Bias(ˆf) = f−E(ˆf)\n",
      "=f−Sλf, (5.24)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.5 Automatic Selection of the Smoothing Parameters 159\n",
      "6 8 10 12 140.9 1.0 1.1 1.2•••••••• ••••••••••\n",
      "••••••••••••••••••\n",
      "y\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "O\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "OOOOOOOO\n",
      "OOO\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "O\n",
      "OOOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OOOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "Oy\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "O\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "OOOOOOOO\n",
      "OOO\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "O\n",
      "OOOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OOOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "O\n",
      "y\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2O\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "O\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "OOOOOOOO\n",
      "OOO\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "O\n",
      "OOOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OOOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OEPECV\n",
      "X XXdfλ= 5\n",
      "dfλ= 9 dfλ= 15dfλCross-ValidationEPE( λ) and CV( λ)\n",
      "FIGURE 5.9. The top left panel shows the EPE( λ)andCV(λ)curves for a\n",
      "realization from a nonlinear additive error model (5.22). The r emaining panels\n",
      "show the data, the true functions (in purple), and the ﬁtted curve s (in green) with\n",
      "yellow shaded ±2×standard error bands, for three diﬀerent values of dfλ.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "160 5. Basis Expansions and Regularization\n",
      "wherefis the (unknown) vector of evaluations of the true fat the training\n",
      "X’s. The expectations and variances are with respect to repeated draws\n",
      "of samples of size N= 100 from the model (5.22). In a similar fashion\n",
      "Var(ˆfλ(x0)) and Bias( ˆfλ(x0)) can be computed at any point x0(Exer-\n",
      "cise 5.10). The three ﬁts displayed in the ﬁgure give a visual demonstration\n",
      "of the bias-variance tradeoﬀ associated with selecting the smoothing\n",
      "parameter.\n",
      "dfλ= 5:The spline under ﬁts, and clearly trims down the hills and ﬁlls in\n",
      "the valleys . This leads to a bias that is most dramatic in regions of\n",
      "high curvature. The standard error band is very narrow, so we esti-\n",
      "mate a badly biased version of the true function with great reliability!\n",
      "dfλ= 9:Here the ﬁtted function is close to the true function, although a\n",
      "slight amount of bias seems evident. The variance has not increased\n",
      "appreciably.\n",
      "dfλ= 15: The ﬁtted function is somewhat wiggly, but close to the true\n",
      "function. The wiggliness also accounts for the increased width of the\n",
      "standard error bands—the curve is starting to follow some individual\n",
      "points too closely.\n",
      "Note that in these ﬁgures we are seeing a single realization of data and\n",
      "hence ﬁtted spline ˆfin each case, while the bias involves an expectation\n",
      "E(ˆf). We leave it as an exercise (5.10) to compute similar ﬁgures where the\n",
      "bias is shown as well. The middle curve seems “just right,” in that it has\n",
      "achieved a good compromise between bias and variance.\n",
      "The integrated squared prediction error (EPE) combines both bias and\n",
      "variance in a single summary:\n",
      "EPE( ˆfλ) = E( Y−ˆfλ(X))2\n",
      "= Var( Y) + E[\n",
      "Bias2(ˆfλ(X)) + Var( ˆfλ(X))]\n",
      "=σ2+ MSE( ˆfλ). (5.25)\n",
      "Note that this is averaged both over the training sample (giving rise to ˆfλ),\n",
      "and the values of the (independently chosen) prediction points ( X,Y). EPE\n",
      "is a natural quantity of interest, and does create a tradeoﬀ between bias\n",
      "and variance. The blue points in the top left panel of Figure 5.9 suggest\n",
      "that df λ= 9 is spot on!\n",
      "Since we don’t know the true function, we do not have access to EPE, and\n",
      "need an estimate. This topic is discussed in some detail in Chapter 7, and\n",
      "techniques such as K-fold cross-validation, GCV and Cpare all in common\n",
      "use. In Figure 5.9 we include the N-fold (leave-one-out) cross-validation\n",
      "curve:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.6 Nonparametric Logistic Regression 161\n",
      "CV(ˆfλ) =1\n",
      "NN∑\n",
      "i=1(yi−ˆf(−i)\n",
      "λ(xi))2(5.26)\n",
      "=1\n",
      "NN∑\n",
      "i=1(\n",
      "yi−ˆfλ(xi)\n",
      "1−Sλ(i,i))2\n",
      ", (5.27)\n",
      "which can (remarkably) be computed for each value of λfrom the original\n",
      "ﬁtted values and the diagonal elements Sλ(i,i) ofSλ(Exercise 5.13).\n",
      "The EPE and CV curves have a similar shape, but the entire CV curve\n",
      "is above the EPE curve. For some realizations this is reversed, and overall\n",
      "the CV curve is approximately unbiased as an estimate of the EPE curve.\n",
      "5.6 Nonparametric Logistic Regression\n",
      "The smoothing spline problem (5.9) in Section 5.4 is posed in a regression\n",
      "setting. It is typically straightforward to transfer this technology t o other\n",
      "domains. Here we consider logistic regression with a single quantitativ e\n",
      "input X. The model is\n",
      "logPr(Y= 1|X=x)\n",
      "Pr(Y= 0|X=x)=f(x), (5.28)\n",
      "which implies\n",
      "Pr(Y= 1|X=x) =ef(x)\n",
      "1 +ef(x). (5.29)\n",
      "Fitting f(x) in a smooth fashion leads to a smooth estimate of the condi-\n",
      "tional probability Pr( Y= 1|x), which can be used for classiﬁcation or risk\n",
      "scoring.\n",
      "We construct the penalized log-likelihood criterion\n",
      "ℓ(f;λ) =N∑\n",
      "i=1[yilogp(xi) + (1 −yi)log(1 −p(xi))]−1\n",
      "2λ∫\n",
      "{f′′(t)}2dt\n",
      "=N∑\n",
      "i=1[\n",
      "yif(xi)−log(1 + ef(xi))]\n",
      "−1\n",
      "2λ∫\n",
      "{f′′(t)}2dt, (5.30)\n",
      "where we have abbreviated p(x) = Pr( Y= 1|x). The ﬁrst term in this ex-\n",
      "pression is the log-likelihood based on the binomial distribution (c.f. Chap-\n",
      "ter 4, page 120). Arguments similar to those used in Section 5.4 show that\n",
      "the optimal fis a ﬁnite-dimensional natural spline with knots at the unique\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "162 5. Basis Expansions and Regularization\n",
      "values of x. This means that we can represent f(x) =∑N\n",
      "j=1Nj(x)θj. We\n",
      "compute the ﬁrst and second derivatives\n",
      "∂ℓ(θ)\n",
      "∂θ=NT(y−p)−λΩθ, (5.31)\n",
      "∂2ℓ(θ)\n",
      "∂θ∂θT=−NTWN−λΩ, (5.32)\n",
      "where pis the N-vector with elements p(xi), and Wis a diagonal matrix\n",
      "of weights p(xi)(1−p(xi)). The ﬁrst derivative (5.31) is nonlinear in θ, so\n",
      "we need to use an iterative algorithm as in Section 4.4.1. Using Newton–\n",
      "Raphson as in (4.23) and (4.26) for linear logistic regression, the updat e\n",
      "equation can be written\n",
      "θnew= (NTWN+λΩ)−1NTW(\n",
      "Nθold+W−1(y−p))\n",
      "= (NTWN+λΩ)−1NTWz. (5.33)\n",
      "We can also express this update in terms of the ﬁtted values\n",
      "fnew=N(NTWN+λΩ)−1NTW(\n",
      "fold+W−1(y−p))\n",
      "=Sλ,wz. (5.34)\n",
      "Referring back to (5.12) and (5.14), we see that the update ﬁts a weighted\n",
      "smoothing spline to the working response z(Exercise 5.12).\n",
      "The form of (5.34) is suggestive. It is tempting to replace Sλ,wby any\n",
      "nonparametric (weighted) regression operator, and obtain general fami-\n",
      "lies of nonparametric logistic regression models. Although here xis one-\n",
      "dimensional, this procedure generalizes naturally to higher-dimensional x.\n",
      "These extensions are at the heart of generalized additive models , which we\n",
      "pursue in Chapter 9.\n",
      "5.7 Multidimensional Splines\n",
      "So far we have focused on one-dimensional spline models. Each of the ap-\n",
      "proaches have multidimensional analogs. Suppose X∈IR2, and we have\n",
      "a basis of functions h1k(X1), k= 1,... ,M 1for representing functions of\n",
      "coordinate X1, and likewise a set of M2functions h2k(X2) for coordinate\n",
      "X2. Then the M1×M2dimensional tensor product basis deﬁned by\n",
      "gjk(X) =h1j(X1)h2k(X2), j= 1,... ,M 1, k= 1,... ,M 2 (5.35)\n",
      "can be used for representing a two-dimensional function:\n",
      "g(X) =M1∑\n",
      "j=1M2∑\n",
      "k=1θjkgjk(X). (5.36)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.7 Multidimensional Splines 163\n",
      "FIGURE 5.10. A tensor product basis of B-splines, showing some selected pair s.\n",
      "Each two-dimensional function is the tensor product of the corre sponding one\n",
      "dimensional marginals.\n",
      "Figure 5.10 illustrates a tensor product basis using B-splines. The coeﬃ-\n",
      "cients can be ﬁt by least squares, as before. This can be generalized to d\n",
      "dimensions, but note that the dimension of the basis grows exponentially\n",
      "fast—yet another manifestation of the curse of dimensionality. The MARS\n",
      "procedure discussed in Chapter 9 is a greedy forward algorithm for includ-\n",
      "ing only those tensor products that are deemed necessary by least squares.\n",
      "Figure 5.11 illustrates the diﬀerence between additive and tensor product\n",
      "(natural) splines on the simulated classiﬁcation example from Chapter 2.\n",
      "A logistic regression model logit[Pr( T|x)] =h(x)Tθis ﬁt to the binary re-\n",
      "sponse, and the estimated decision boundary is the contour h(x)Tˆθ= 0.\n",
      "The tensor product basis can achieve more ﬂexibility at the decision bound-\n",
      "ary, but introduces some spurious structure along the way.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "164 5. Basis Expansions and Regularization\n",
      "Additive Natural Cubic Splines - 4 df each\n",
      ".. .. . .. . . .. . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . . .. . . .. . . .. . . .. . .. . .. . .. .. .. ...\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "Training Error: 0.23\n",
      "Test Error:       0.28\n",
      "Bayes Error:    0.21\n",
      "Natural Cubic Splines - Tensor Product - 4 df each\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . ... . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "Training Error: 0.230\n",
      "Test Error:       0.282\n",
      "Bayes Error:    0.210\n",
      "FIGURE 5.11. The simulation example of Figure 2.1. The upper panel shows the\n",
      "decision boundary of an additive logistic regression model, using natural splines\n",
      "in each of the two coordinates (total df = 1 + (4 −1) + (4 −1) = 7 ). The lower\n",
      "panel shows the results of using a tensor product of natural spline bases in each\n",
      "coordinate (total df = 4×4 = 16) . The broken purple boundary is the Bayes\n",
      "decision boundary for this problem.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.7 Multidimensional Splines 165\n",
      "One-dimensional smoothing splines (via regularization) generalize to high-\n",
      "er dimensions as well. Suppose we have pairs yi,xiwithxi∈IRd, and we\n",
      "seek a d-dimensional regression function f(x). The idea is to set up the\n",
      "problem\n",
      "min\n",
      "fN∑\n",
      "i=1{yi−f(xi)}2+λJ[f], (5.37)\n",
      "where Jis an appropriate penalty functional for stabilizing a function fin\n",
      "IRd. For example, a natural generalization of the one-dimensional roughness\n",
      "penalty (5.9) for functions on IR2is\n",
      "J[f] =∫ ∫\n",
      "I R2[(∂2f(x)\n",
      "∂x2\n",
      "1)2\n",
      "+2(∂2f(x)\n",
      "∂x1∂x2)2\n",
      "+(∂2f(x)\n",
      "∂x2\n",
      "2)2]\n",
      "dx1dx2.(5.38)\n",
      "Optimizing (5.37) with this penalty leads to a smooth two-dimensional\n",
      "surface, known as a thin-plate spline. It shares many properties with the\n",
      "one-dimensional cubic smoothing spline:\n",
      "•asλ→0, the solution approaches an interpolating function [the one\n",
      "with smallest penalty (5.38)];\n",
      "•asλ→ ∞, the solution approaches the least squares plane;\n",
      "•for intermediate values of λ, the solution can be represented as a\n",
      "linear expansion of basis functions, whose coeﬃcients are obtained\n",
      "by a form of generalized ridge regression.\n",
      "The solution has the form\n",
      "f(x) =β0+βTx+N∑\n",
      "j=1αjhj(x), (5.39)\n",
      "where hj(x) =||x−xj||2log||x−xj||. These hjare examples of radial\n",
      "basis functions , which are discussed in more detail in the next section. The\n",
      "coeﬃcients are found by plugging (5.39) into (5.37), which reduces to a\n",
      "ﬁnite-dimensional penalized least squares problem. For the penalty to be\n",
      "ﬁnite, the coeﬃcients αjhave to satisfy a set of linear constraints; see\n",
      "Exercise 5.14.\n",
      "Thin-plate splines are deﬁned more generally for arbitrary dimension d,\n",
      "for which an appropriately more general Jis used.\n",
      "There are a number of hybrid approaches that are popular in practice,\n",
      "both for computational and conceptual simplicity. Unlike one-dimensional\n",
      "smoothing splines, the computational complexity for thin-plate splines is\n",
      "O(N3), since there is not in general any sparse structure that can be ex-\n",
      "ploited. However, as with univariate smoothing splines, we can get away\n",
      "with substantially less than the Nknots prescribed by the solution (5.39).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "166 5. Basis Expansions and Regularization\n",
      "125\n",
      "130135140145150155\n",
      "15202530354045\n",
      "20 30 40 50 60\n",
      "AgeObesitySystolic Blood Pressure\n",
      "120125130135140145150155160\n",
      "•• ••\n",
      "••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•• ••••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "•••••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••••••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "•••••\n",
      "•••\n",
      "•\n",
      "• •••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "•\n",
      "••••\n",
      "• ••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "•••••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••••\n",
      "•\n",
      "••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "•\n",
      "•••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "•••\n",
      "••••\n",
      "• •\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••••\n",
      "•\n",
      "••••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "••••\n",
      "••\n",
      "• ••\n",
      "•••\n",
      "•••\n",
      "•••\n",
      "••• •••\n",
      "•\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•••••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "••• •\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•\n",
      "••\n",
      "••••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•• • • • • •• • • • • • •• • • • • • •• • • • • •• • • •• •\n",
      "• • • • • • • •• •••• •• • • •• • • • • •• • • • • • • •\n",
      "FIGURE 5.12. A thin-plate spline ﬁt to the heart disease data, displayed as a\n",
      "contour plot. The response is systolic blood pressure , modeled as a function\n",
      "ofageandobesity . The data points are indicated, as well as the lattice of points\n",
      "used as knots. Care should be taken to use knots from the lattice inside the convex\n",
      "hull of the data (red), and ignore those outside (green).\n",
      "In practice, it is usually suﬃcient to work with a lattice of knots covering\n",
      "the domain. The penalty is computed for the reduced expansion just as\n",
      "before. Using Kknots reduces the computations to O(NK2+K3). Fig-\n",
      "ure 5.12 shows the result of ﬁtting a thin-plate spline to some heart disease\n",
      "risk factors, representing the surface as a contour plot. Indicated are the\n",
      "location of the input features, as well as the knots used in the ﬁt. Note that\n",
      "λwas speciﬁed via df λ= trace( Sλ) = 15.\n",
      "More generally one can represent f∈IRdas an expansion in any arbi-\n",
      "trarily large collection of basis functions, and control the complexity by a p-\n",
      "plying a regularizer such as (5.38). For example, we could construct a basis\n",
      "by forming the tensor products of all pairs of univariate smoothing-spline\n",
      "basis functions as in (5.35), using, for example, the univariate B-splines\n",
      "recommended in Section 5.9.2 as ingredients. This leads to an exponential\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces 167\n",
      "growth in basis functions as the dimension increases, and typically we have\n",
      "to reduce the number of functions per coordinate accordingly.\n",
      "The additive spline models discussed in Chapter 9 are a restricted class\n",
      "of multidimensional splines. They can be represented in this general formu-\n",
      "lation as well; that is, there exists a penalty J[f] that guarantees that the\n",
      "solution has the form f(X) =α+f1(X1) +≤≤≤+fd(Xd) and that each of\n",
      "the functions fjare univariate splines. In this case the penalty is somewhat\n",
      "degenerate, and it is more natural to assume thatfis additive, and then\n",
      "simply impose an additional penalty on each of the component functions:\n",
      "J[f] = J(f1+f2+≤≤≤+fd)\n",
      "=d∑\n",
      "j=1∫\n",
      "f′′\n",
      "j(tj)2dtj. (5.40)\n",
      "These are naturally extended to ANOVA spline decompositions,\n",
      "f(X) =α+∑\n",
      "jfj(Xj) +∑\n",
      "j<kfjk(Xj,Xk) +≤≤≤, (5.41)\n",
      "where each of the components are splines of the required dimension. There\n",
      "are many choices to be made:\n",
      "•The maximum order of interaction—we have shown up to order 2\n",
      "above.\n",
      "•Which terms to include—not all main eﬀects and interactions are\n",
      "necessarily needed.\n",
      "•What representation to use—some choices are:\n",
      "–regression splines with a relatively small number of basis func-\n",
      "tions per coordinate, and their tensor products for interactions;\n",
      "–a complete basis as in smoothing splines, and include appropri-\n",
      "ate regularizers for each term in the expansion.\n",
      "In many cases when the number of potential dimensions (features) is large,\n",
      "automatic methods are more desirable. The MARS and MART procedures\n",
      "(Chapters 9 and 10, respectively), both fall into this category.\n",
      "5.8 Regularization and Reproducing Kernel\n",
      "Hilbert Spaces\n",
      "In this section we cast splines into the larger context of regularization meth-\n",
      "ods and reproducing kernel Hilbert spaces. This section is quite technical\n",
      "and can be skipped by the disinterested or intimidated reader.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "168 5. Basis Expansions and Regularization\n",
      "A general class of regularization problems has the form\n",
      "min\n",
      "f∈H[N∑\n",
      "i=1L(yi,f(xi)) +λJ(f)]\n",
      "(5.42)\n",
      "where L(y,f(x)) is a loss function, J(f) is a penalty functional, and His\n",
      "a space of functions on which J(f) is deﬁned. Girosi et al. (1995) describe\n",
      "quite general penalty functionals of the form\n",
      "J(f) =∫\n",
      "I Rd|˜f(s)|2\n",
      "˜G(s)ds, (5.43)\n",
      "where ˜fdenotes the Fourier transform of f, and ˜Gis some positive function\n",
      "that falls oﬀ to zero as ||s|| → ∞ . The idea is that 1 /˜Gincreases the penalty\n",
      "for high-frequency components of f. Under some additional assumptions\n",
      "they show that the solutions have the form\n",
      "f(X) =K∑\n",
      "k=1αkφk(X) +N∑\n",
      "i=1θiG(X−xi), (5.44)\n",
      "where the φkspan the null space of the penalty functional J, and Gis the\n",
      "inverse Fourier transform of ˜G. Smoothing splines and thin-plate splines\n",
      "fall into this framework. The remarkable feature of this solution is tha t\n",
      "while the criterion (5.42) is deﬁned over an inﬁnite-dimensional space, the\n",
      "solution is ﬁnite-dimensional. In the next sections we look at some speciﬁc\n",
      "examples.\n",
      "5.8.1 Spaces of Functions Generated by Kernels\n",
      "An important subclass of problems of the form (5.42) are generated by\n",
      "a positive deﬁnite kernel K(x,y), and the corresponding space of func-\n",
      "tionsHKis called a reproducing kernel Hilbert space (RKHS). The penalty\n",
      "functional Jis deﬁned in terms of the kernel as well. We give a brief and\n",
      "simpliﬁed introduction to this class of models, adapted from Wahba (1990)\n",
      "and Girosi et al. (1995), and nicely summarized in Evgeniou et al. (2000).\n",
      "Letx,y∈IRp. We consider the space of functions generated by the linear\n",
      "span of {K(≤,y), y∈IRp)}; i.e arbitrary linear combinations of the form\n",
      "f(x) =∑\n",
      "mαmK(x,ym), where each kernel term is viewed as a function\n",
      "of the ﬁrst argument, and indexed by the second. Suppose that Khas an\n",
      "eigen-expansion\n",
      "K(x,y) =∞∑\n",
      "i=1γiφi(x)φi(y) (5.45)\n",
      "withγi≥0,∑∞\n",
      "i=1γ2\n",
      "i<∞. Elements of HKhave an expansion in terms of\n",
      "these eigen-functions,\n",
      "f(x) =∞∑\n",
      "i=1ciφi(x), (5.46)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces 169\n",
      "with the constraint that\n",
      "||f||2\n",
      "HKdef=∞∑\n",
      "i=1c2\n",
      "i/γi<∞, (5.47)\n",
      "where ||f||HKis the norm induced by K. The penalty functional in (5.42)\n",
      "for the space HKis deﬁned to be the squared norm J(f) =||f||2\n",
      "HK. The\n",
      "quantity J(f) can be interpreted as a generalized ridge penalty, where\n",
      "functions with large eigenvalues in the expansion (5.45) get penalized less,\n",
      "and vice versa.\n",
      "Rewriting (5.42) we have\n",
      "min\n",
      "f∈HK[N∑\n",
      "i=1L(yi,f(xi)) +λ||f||2\n",
      "HK]\n",
      "(5.48)\n",
      "or equivalently\n",
      "min\n",
      "{cj}∞\n",
      "1\n",
      "N∑\n",
      "i=1L(yi,∞∑\n",
      "j=1cjφj(xi)) +λ∞∑\n",
      "j=1c2\n",
      "j/γj\n",
      ". (5.49)\n",
      "It can be shown (Wahba, 1990, see also Exercise 5.15) that the solution\n",
      "to (5.48) is ﬁnite-dimensional, and has the form\n",
      "f(x) =N∑\n",
      "i=1αiK(x,xi). (5.50)\n",
      "The basis function hi(x) =K(x,xi) (as a function of the ﬁrst argument) is\n",
      "known as the representer of evaluation atxiinHK, since for f∈ H K, it is\n",
      "easily seen that ⟨K(≤,xi),f⟩HK=f(xi). Similarly ⟨K(≤,xi),K(≤,xj)⟩HK=\n",
      "K(xi,xj) (the reproducing property of HK), and hence\n",
      "J(f) =N∑\n",
      "i=1N∑\n",
      "j=1K(xi,xj)αiαj (5.51)\n",
      "forf(x) =∑N\n",
      "i=1αiK(x,xi).\n",
      "In light of (5.50) and (5.51), (5.48) reduces to a ﬁnite-dimensional crite-\n",
      "rion\n",
      "min\n",
      "αL(y,Kα) +λαTKα. (5.52)\n",
      "We are using a vector notation, in which Kis the N×Nmatrix with ijth\n",
      "entry K(xi,xj) and so on. Simple numerical algorithms can be used to\n",
      "optimize (5.52). This phenomenon, whereby the inﬁnite-dimensional prob-\n",
      "lem (5.48) or (5.49) reduces to a ﬁnite dimensional optimization problem,\n",
      "has been dubbed the kernel property in the literature on support-vector\n",
      "machines (see Chapter 12).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "170 5. Basis Expansions and Regularization\n",
      "There is a Bayesian interpretation of this class of models, in which f\n",
      "is interpreted as a realization of a zero-mean stationary Gaussian process,\n",
      "with prior covariance function K. The eigen-decomposition produces a se-\n",
      "ries of orthogonal eigen-functions φj(x) with associated variances γj. The\n",
      "typical scenario is that “smooth” functions φjhave large prior variance,\n",
      "while “rough” φjhave small prior variances. The penalty in (5.48) is the\n",
      "contribution of the prior to the joint likelihood, and penalizes more those\n",
      "components with smaller prior variance (compare with (5.43)).\n",
      "For simplicity we have dealt with the case here where all members of H\n",
      "are penalized, as in (5.48). More generally, there may be some components\n",
      "inHthat we wish to leave alone, such as the linear functions for cubic\n",
      "smoothing splines in Section 5.4. The multidimensional thin-plate splines\n",
      "of Section 5.7 and tensor product splines fall into this category as well.\n",
      "In these cases there is a more convenient representation H=H0⊕ H1,\n",
      "with the null space H0consisting of, for example, low degree polynomi-\n",
      "als in xthat do not get penalized. The penalty becomes J(f) =∥P1f∥,\n",
      "where P1is the orthogonal projection of fontoH1. The solution has the\n",
      "formf(x) =∑M\n",
      "j=1βjhj(x) +∑N\n",
      "i=1αiK(x,xi), where the ﬁrst term repre-\n",
      "sents an expansion in H0. From a Bayesian perspective, the coeﬃcients of\n",
      "components in H0have improper priors, with inﬁnite variance.\n",
      "5.8.2 Examples of RKHS\n",
      "The machinery above is driven by the choice of the kernel Kand the loss\n",
      "function L. We consider ﬁrst regression using squared-error loss. In this\n",
      "case (5.48) specializes to penalized least squares, and the solution can be\n",
      "characterized in two equivalent ways corresponding to (5.49) or (5.52):\n",
      "min\n",
      "{cj}∞\n",
      "1N∑\n",
      "i=1\n",
      "yi−∞∑\n",
      "j=1cjφj(xi)\n",
      "2\n",
      "+λ∞∑\n",
      "j=1c2\n",
      "j\n",
      "γj(5.53)\n",
      "an inﬁnite-dimensional, generalized ridge regression problem, or\n",
      "min\n",
      "α(y−Kα)T(y−Kα) +λαTKα. (5.54)\n",
      "The solution for αis obtained simply as\n",
      "ˆα= (K+λI)−1y, (5.55)\n",
      "and\n",
      "ˆf(x) =N∑\n",
      "j=1ˆαjK(x,xj). (5.56)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces 171\n",
      "The vector of Nﬁtted values is given by\n",
      "ˆf=Kˆα\n",
      "=K(K+λI)−1y (5.57)\n",
      "= (I+λK−1)−1y. (5.58)\n",
      "The estimate (5.57) also arises as the kriging estimate of a Gaussian ran-\n",
      "dom ﬁeld in spatial statistics (Cressie, 1993). Compare also (5.58) w ith the\n",
      "smoothing spline ﬁt (5.17) on page 154.\n",
      "Penalized Polynomial Regression\n",
      "The kernel K(x,y) = (⟨x,y⟩+ 1)d(Vapnik, 1996), for x,y∈IRp, has\n",
      "M=(p+d\n",
      "d)\n",
      "eigen-functions that span the space of polynomials in IRpof\n",
      "total degree d. For example, with p= 2 and d= 2,M= 6 and\n",
      "K(x,y) = 1 + 2 x1y1+ 2x2y2+x2\n",
      "1y2\n",
      "1+x2\n",
      "2y2\n",
      "2+ 2x1x2y1y2(5.59)\n",
      "=M∑\n",
      "m=1hm(x)hm(y) (5.60)\n",
      "with\n",
      "h(x)T= (1,√\n",
      "2x1,√\n",
      "2x2,x2\n",
      "1,x2\n",
      "2,√\n",
      "2x1x2). (5.61)\n",
      "One can represent hin terms of the Morthogonal eigen-functions and\n",
      "eigenvalues of K,\n",
      "h(x) =VD1\n",
      "2γφ(x), (5.62)\n",
      "where Dγ= diag( γ1,γ2,... ,γ M), and VisM×Mand orthogonal.\n",
      "Suppose we wish to solve the penalized polynomial regression problem\n",
      "min\n",
      "{βm}M\n",
      "1N∑\n",
      "i=1(\n",
      "yi−M∑\n",
      "m=1βmhm(xi))2\n",
      "+λM∑\n",
      "m=1β2\n",
      "m. (5.63)\n",
      "Substituting (5.62) into (5.63), we get an expression of the form (5.53) to\n",
      "optimize (Exercise 5.16).\n",
      "The number of basis functions M=(p+d\n",
      "d)\n",
      "can be very large, often much\n",
      "larger than N. Equation (5.55) tells us that if we use the kernel represen-\n",
      "tation for the solution function, we have only to evaluate the kernel N2\n",
      "times, and can compute the solution in O(N3) operations.\n",
      "This simplicity is not without implications. Each of the polynomials hm\n",
      "in (5.61) inherits a scaling factor from the particular form of K, which has\n",
      "a bearing on the impact of the penalty in (5.63). We elaborate on this in\n",
      "the next section.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "172 5. Basis Expansions and Regularization\n",
      "−2 −1 0 1 2 3 40.0 0.4 0.8\n",
      "XRadial Kernel in I R1K(≤, xm)\n",
      "FIGURE 5.13. Radial kernels kk(x)for the mixture data, with scale parameter\n",
      "ν= 1. The kernels are centered at ﬁve points xmchosen at random from the 200.\n",
      "Gaussian Radial Basis Functions\n",
      "In the preceding example, the kernel is chosen because it represents an\n",
      "expansion of polynomials and can conveniently compute high-dimensional\n",
      "inner products. In this example the kernel is chosen because of its functional\n",
      "form in the representation (5.50).\n",
      "The Gaussian kernel K(x,y) =e−ν||x−y||2along with squared-error loss,\n",
      "for example, leads to a regression model that is an expansion in Gaussian\n",
      "radial basis functions,\n",
      "km(x) =e−ν||x−xm||2, m= 1,... ,N, (5.64)\n",
      "each one centered at one of the training feature vectors xm. The coeﬃcients\n",
      "are estimated using (5.54).\n",
      "Figure 5.13 illustrates radial kernels in IR1using the ﬁrst coordinate of\n",
      "the mixture example from Chapter 2. We show ﬁve of the 200 kernel basis\n",
      "functions km(x) =K(x,xm).\n",
      "Figure 5.14 illustrates the implicit feature space for the radial kernel\n",
      "withx∈IR1. We computed the 200 ×200 kernel matrix K, and its eigen-\n",
      "decomposition ΦDγΦT. We can think of the columns of Φand the corre-\n",
      "sponding eigenvalues in Dγas empirical estimates of the eigen expansion\n",
      "(5.45)2. Although the eigenvectors are discrete, we can represent them as\n",
      "functions on IR1(Exercise 5.17). Figure 5.15 shows the largest 50 eigenval-\n",
      "ues ofK. The leading eigenfunctions are smooth, and they are successively\n",
      "more wiggly as the order increases. This brings to life the penalty in (5.49) ,\n",
      "where we see the coeﬃcients of higher-order functions get penalized more\n",
      "than lower-order ones. The right panel in Figure 5.14 shows the correspond-\n",
      "2Theℓth column of Φis an estimate of φℓ, evaluated at each of the Nobservations.\n",
      "Alternatively, the ith row of Φis the estimated vector of basis functions φ(xi), evaluated\n",
      "at the point xi. Although in principle, there can be inﬁnitely many element s inφ, our\n",
      "estimate has at most Nelements.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.8 Regularization and Reproducing Kernel Hilbert Spaces 173\n",
      "*\n",
      "******* ** ** ******\n",
      "******\n",
      "*******\n",
      "******\n",
      "**********\n",
      "****\n",
      "** *******\n",
      "*****\n",
      "**\n",
      "***\n",
      "**********\n",
      "*****\n",
      "*****\n",
      "***\n",
      "**\n",
      "***\n",
      "*****\n",
      "*************\n",
      "******\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "***********\n",
      "**\n",
      "****\n",
      "**** *\n",
      "** ******* * **\n",
      "**************\n",
      "********* ***\n",
      "**\n",
      "*******\n",
      "***\n",
      "**\n",
      "****\n",
      "***\n",
      "***\n",
      "**\n",
      "***\n",
      "*****\n",
      "*******\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "*****\n",
      "**\n",
      "******\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "*********\n",
      "**\n",
      "**\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "****\n",
      "***\n",
      "**\n",
      "**\n",
      "*******\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "****\n",
      "**\n",
      "****\n",
      "**\n",
      "**\n",
      "****\n",
      "****\n",
      "**\n",
      "**\n",
      "*****\n",
      "****\n",
      "**\n",
      "****\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "****\n",
      "****\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "*********\n",
      "******\n",
      "*\n",
      "***\n",
      "** **\n",
      "****\n",
      "*\n",
      "***\n",
      "***\n",
      "***\n",
      "***\n",
      "*******\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "*******\n",
      "**\n",
      "***\n",
      "***\n",
      "******\n",
      "**\n",
      "***\n",
      "**\n",
      "******\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "****\n",
      "*** **\n",
      "*********\n",
      "****\n",
      "**\n",
      "****\n",
      "** **\n",
      "** ****\n",
      "****\n",
      "***\n",
      "***\n",
      "**\n",
      "** ***\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "* * * **\n",
      "****\n",
      "**\n",
      "**\n",
      "****\n",
      "*****\n",
      "****\n",
      "* **** ***\n",
      "**\n",
      "*** **** * *\n",
      "**\n",
      "* *\n",
      "**\n",
      "**\n",
      "******\n",
      "***\n",
      "***\n",
      "****\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "***\n",
      "****\n",
      "***\n",
      "********\n",
      "*******\n",
      "** ** **\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "*\n",
      "*********\n",
      "**********\n",
      "*\n",
      "****\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "*****\n",
      "****\n",
      "**\n",
      "**\n",
      "****\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "******\n",
      "*******\n",
      "***\n",
      "***\n",
      "*** *\n",
      "****\n",
      "***\n",
      "****\n",
      "* ***\n",
      "* * ** *\n",
      "* ***\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "*******\n",
      "**\n",
      "**\n",
      "*******\n",
      "**\n",
      "*\n",
      "* *********\n",
      "***\n",
      "******\n",
      "******\n",
      "** ***\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "* **\n",
      "***\n",
      "*\n",
      "***\n",
      "**\n",
      "***********\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "*********\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "* * **\n",
      "*****\n",
      "**\n",
      "**\n",
      "*****\n",
      "**\n",
      "**\n",
      "** ******\n",
      "**\n",
      "**\n",
      "***** ****\n",
      "***\n",
      "****\n",
      "* *****\n",
      "****\n",
      "****\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "* *******\n",
      "****\n",
      "****\n",
      "***\n",
      "**\n",
      "****\n",
      "*****\n",
      "******\n",
      "*****\n",
      "*\n",
      "*****\n",
      "***\n",
      "*\n",
      "***\n",
      "******\n",
      "*****\n",
      "***\n",
      "***********\n",
      "***\n",
      "***** **\n",
      "* *\n",
      "***\n",
      "*****\n",
      "*******\n",
      "*****\n",
      "**\n",
      "****\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "*****\n",
      "***\n",
      "*******\n",
      "******* *\n",
      "*****\n",
      "*\n",
      "**\n",
      "***\n",
      "*\n",
      "*****\n",
      "**\n",
      "*****\n",
      "**\n",
      "*** *\n",
      "***\n",
      "***\n",
      "******\n",
      "**\n",
      "***\n",
      "********\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "***** ****\n",
      "***\n",
      "**\n",
      "****\n",
      "********\n",
      "*******\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "*********\n",
      "*\n",
      "*******\n",
      "*****\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "******\n",
      "**\n",
      "*\n",
      "*****\n",
      "*\n",
      "***\n",
      "** **\n",
      "*\n",
      "***\n",
      "** ***\n",
      "****\n",
      "**\n",
      "*******\n",
      "****\n",
      "**\n",
      "****\n",
      "*****\n",
      "******\n",
      "**\n",
      "****\n",
      "**\n",
      "**\n",
      "** ***\n",
      "*****\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "****\n",
      "**\n",
      "*****\n",
      "***\n",
      "**\n",
      "****\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "*\n",
      "*****\n",
      "***\n",
      "******* ***\n",
      "**\n",
      "****\n",
      "* ***\n",
      "******\n",
      "*****\n",
      "*****\n",
      "***\n",
      "**\n",
      "* ** ***\n",
      "***\n",
      "***\n",
      "*\n",
      "***\n",
      "***\n",
      "****\n",
      "****\n",
      "***\n",
      "*****\n",
      "*****\n",
      "***\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "*****\n",
      "**\n",
      "******\n",
      "********* *\n",
      "**\n",
      "*\n",
      "****\n",
      "**\n",
      "*\n",
      "* *\n",
      "***\n",
      "**\n",
      "****\n",
      "***\n",
      "****\n",
      "*** ****\n",
      "****\n",
      "**\n",
      "*****\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "*****\n",
      "*\n",
      "***\n",
      "*** *\n",
      "******\n",
      "*\n",
      "***********\n",
      "**\n",
      "******\n",
      "**\n",
      "*******\n",
      "**\n",
      "****\n",
      "*\n",
      "**********\n",
      "*\n",
      "**** *\n",
      "**\n",
      "******\n",
      "*\n",
      "* **\n",
      "***\n",
      "*\n",
      "**\n",
      "*****\n",
      "*\n",
      "**\n",
      "*\n",
      "****\n",
      "**\n",
      "*\n",
      "***\n",
      "** *\n",
      "****\n",
      "******\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "**** ****\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "***\n",
      "*****\n",
      "**** * *\n",
      "**\n",
      "**\n",
      "******\n",
      "*****\n",
      "*****\n",
      "***\n",
      "** *\n",
      "****\n",
      "*\n",
      "****\n",
      "** *\n",
      "*****\n",
      "****\n",
      "**** *\n",
      "*****\n",
      "**\n",
      "****\n",
      "*****\n",
      "** **\n",
      "***\n",
      "****\n",
      "****\n",
      "**\n",
      "** *****\n",
      "**\n",
      "*****\n",
      "***\n",
      "**\n",
      "****\n",
      "*\n",
      "****\n",
      "****\n",
      "**\n",
      "*****\n",
      "***\n",
      "********\n",
      "*\n",
      "****\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "*****\n",
      "**\n",
      "***\n",
      "*****\n",
      "********\n",
      "****\n",
      "* *******\n",
      "******* * *\n",
      "***\n",
      "****\n",
      "**** *\n",
      "**\n",
      "**\n",
      "***\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "******\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "*****\n",
      "***\n",
      "****\n",
      "*****\n",
      "*\n",
      "***********\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "*******\n",
      "******\n",
      "***********\n",
      "***\n",
      "***\n",
      "*****\n",
      "****\n",
      "***\n",
      "* ***\n",
      "*****\n",
      "**\n",
      "*********\n",
      "*\n",
      "**\n",
      "***\n",
      "** ****\n",
      "** * *\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "*****\n",
      "***\n",
      "*****\n",
      "****\n",
      "**\n",
      "**\n",
      "***\n",
      "*****\n",
      "* * **\n",
      "**\n",
      "**\n",
      "* **\n",
      "***\n",
      "**\n",
      "*****\n",
      "***\n",
      "***\n",
      "***\n",
      "*******\n",
      "**\n",
      "*** *\n",
      "****\n",
      "*\n",
      "* ***\n",
      "****\n",
      "***\n",
      "***\n",
      "***\n",
      "***\n",
      "*****\n",
      "* ***\n",
      "****\n",
      "****\n",
      "***\n",
      "****\n",
      "*****\n",
      "*******\n",
      "***\n",
      "**\n",
      "***\n",
      "****\n",
      "*\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "*****\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "*** *\n",
      "******\n",
      "**\n",
      "* **\n",
      "****\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "*******\n",
      "**\n",
      "***\n",
      "**\n",
      "****\n",
      "***\n",
      "****\n",
      "* * *\n",
      "****\n",
      "* ***\n",
      "***\n",
      "**\n",
      "**\n",
      "******\n",
      "***\n",
      "**\n",
      "*****\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "**\n",
      "** *\n",
      "******\n",
      "*****\n",
      "***\n",
      "*\n",
      "***\n",
      "****\n",
      "**\n",
      "* **\n",
      "*******\n",
      "****\n",
      "*******\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "*****\n",
      "*\n",
      "********\n",
      "***\n",
      "*\n",
      "***\n",
      "***\n",
      "**\n",
      "*******\n",
      "**\n",
      "*\n",
      "*****\n",
      "***\n",
      "**\n",
      "***\n",
      "***\n",
      "****\n",
      "* ***\n",
      "******\n",
      "***\n",
      "***\n",
      "*****\n",
      "*** **\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "********\n",
      "**\n",
      "****\n",
      "**** *****\n",
      "****\n",
      "****\n",
      "**\n",
      "***\n",
      "**\n",
      "****\n",
      "** *****\n",
      "**\n",
      "**\n",
      "*****\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "****\n",
      "*****\n",
      "*** *\n",
      "*******\n",
      "****\n",
      "********\n",
      "**\n",
      "** **\n",
      "*****\n",
      "**\n",
      "***\n",
      "*\n",
      "******\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "****\n",
      "**\n",
      "***\n",
      "**\n",
      "******\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*******\n",
      "**\n",
      "****\n",
      "****\n",
      "*\n",
      "*******\n",
      "**** ******\n",
      "*\n",
      "****\n",
      "****\n",
      "***\n",
      "*\n",
      "**\n",
      "****\n",
      "** **\n",
      "** *\n",
      "*** *\n",
      "**\n",
      "****\n",
      "***************\n",
      "***\n",
      "***\n",
      "*****\n",
      "*****\n",
      "* *****\n",
      "***\n",
      "****\n",
      "*****\n",
      "**\n",
      "***\n",
      "**\n",
      "****\n",
      "**\n",
      "***\n",
      "*\n",
      "*****\n",
      "***\n",
      "**\n",
      "****\n",
      "***\n",
      "**\n",
      "**\n",
      "*******\n",
      "*****\n",
      "***\n",
      "*\n",
      "***\n",
      "*******\n",
      "*\n",
      "**\n",
      "**** ***\n",
      "****\n",
      "*\n",
      "****\n",
      "****\n",
      "*****\n",
      "*****\n",
      "***\n",
      "***\n",
      "*****\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "*****\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "**** *\n",
      "*\n",
      "**\n",
      "******\n",
      "****\n",
      "**\n",
      "****\n",
      "***\n",
      "*\n",
      "*\n",
      "****\n",
      "****\n",
      "****\n",
      "***\n",
      "***\n",
      "******\n",
      "***\n",
      "****\n",
      "*\n",
      "***\n",
      "**\n",
      "*****\n",
      "*\n",
      "* ****\n",
      "****\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "****\n",
      "**\n",
      "**\n",
      "***\n",
      "****\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "********* *\n",
      "**\n",
      "******\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "****\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "*******\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**Orthonormal Basis Φ\n",
      "*\n",
      "*****\n",
      "** *\n",
      "* ***\n",
      "*\n",
      "****\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "******\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "****\n",
      "**\n",
      "*\n",
      "**\n",
      "****\n",
      "*****\n",
      "**\n",
      "***\n",
      "**\n",
      "******\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "***\n",
      "*****\n",
      "*\n",
      "***\n",
      "*\n",
      "***\n",
      "*****\n",
      "***\n",
      "***\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "********\n",
      "**\n",
      "****\n",
      "*\n",
      "***\n",
      "*\n",
      "** *\n",
      "*\n",
      "***\n",
      "** ***\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "***\n",
      "***\n",
      "***\n",
      "***\n",
      "**\n",
      "***\n",
      "****\n",
      "***\n",
      "**\n",
      "****\n",
      "***\n",
      "***\n",
      "**\n",
      "***\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "****\n",
      "*\n",
      "**\n",
      "******\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "*\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "****\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "****\n",
      "****\n",
      "**\n",
      "**\n",
      "****\n",
      "*\n",
      "****\n",
      "**\n",
      "****\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "****\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "******\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "***\n",
      "*\n",
      "*\n",
      "***\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "****\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "****\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "*****\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "*****\n",
      "*****\n",
      "****\n",
      "****\n",
      "**\n",
      "*\n",
      "***\n",
      "** **\n",
      "***\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "** ***\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "** ***\n",
      "****\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "*******\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "*** **\n",
      "**\n",
      "* *\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "***\n",
      "***\n",
      "****\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "*\n",
      "***\n",
      "**\n",
      "*\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "*****\n",
      "*\n",
      "******\n",
      "** ****\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "****\n",
      "*\n",
      "****\n",
      "**\n",
      "********\n",
      "*\n",
      "****\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "*****\n",
      "****\n",
      "**\n",
      "**\n",
      "****\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "******\n",
      "*****\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "****\n",
      "****\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "* ***\n",
      "* *** *\n",
      "* ***\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "*******\n",
      "**\n",
      "**\n",
      "**\n",
      "*****\n",
      "**\n",
      "*\n",
      "* ****\n",
      "***\n",
      "**\n",
      "***\n",
      "*\n",
      "*****\n",
      "******\n",
      "** ***\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "* **\n",
      "***\n",
      "*\n",
      "***\n",
      "**\n",
      "*****\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "*******\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "* ***\n",
      "*****\n",
      "**\n",
      "**\n",
      "*****\n",
      "**\n",
      "**\n",
      "** ***\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "***** ****\n",
      "***\n",
      "****\n",
      "******\n",
      "****\n",
      "****\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "* *******\n",
      "**\n",
      "**\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "****\n",
      "*****\n",
      "******\n",
      "*****\n",
      "*\n",
      "*****\n",
      "***\n",
      "*\n",
      "***\n",
      "******\n",
      "*****\n",
      "***\n",
      "****\n",
      "*****\n",
      "**\n",
      "***\n",
      "***** **\n",
      "* *\n",
      "***\n",
      "*****\n",
      "*******\n",
      "****\n",
      "*\n",
      "**\n",
      "****\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "***\n",
      "****\n",
      "******* *\n",
      "*****\n",
      "*\n",
      "**\n",
      "***\n",
      "*\n",
      "*****\n",
      "**\n",
      "*****\n",
      "**\n",
      "*** *\n",
      "***\n",
      "***\n",
      "******\n",
      "**\n",
      "***\n",
      "********\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "***** ****\n",
      "***\n",
      "**\n",
      "****\n",
      "********\n",
      "*******\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "*********\n",
      "*\n",
      "*******\n",
      "*****\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "******\n",
      "**\n",
      "*\n",
      "*****\n",
      "*\n",
      "***\n",
      "** **\n",
      "*\n",
      "***\n",
      "** ***\n",
      "****\n",
      "**\n",
      "*******\n",
      "****\n",
      "**\n",
      "****\n",
      "*****\n",
      "******\n",
      "**\n",
      "****\n",
      "****\n",
      "** ********\n",
      "***\n",
      "*****\n",
      "**\n",
      "****\n",
      "**\n",
      "**********\n",
      "****\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "****\n",
      "*****\n",
      "***\n",
      "******* ***\n",
      "**\n",
      "****\n",
      "* ***\n",
      "******\n",
      "*****\n",
      "*****\n",
      "*****\n",
      "* ** ***\n",
      "***\n",
      "***\n",
      "*\n",
      "***\n",
      "***\n",
      "****\n",
      "*******\n",
      "*****\n",
      "*****\n",
      "***\n",
      "* *****\n",
      "**\n",
      "**\n",
      "*\n",
      "*****\n",
      "**\n",
      "******\n",
      "********* ****\n",
      "****\n",
      "***\n",
      "* ****\n",
      "**\n",
      "****\n",
      "***\n",
      "******* ** ********\n",
      "********\n",
      "**\n",
      "**\n",
      "********\n",
      "****\n",
      "*** *\n",
      "***** ***************\n",
      "******\n",
      "**\n",
      "*** ****\n",
      "******************** ** ****** ***\n",
      "** **\n",
      "***\n",
      "*\n",
      "**********\n",
      "*****\n",
      "******** *\n",
      "****\n",
      "** * ******\n",
      "********* * ***\n",
      "**\n",
      "********\n",
      "******* ***** * *\n",
      "****** ***** *** *********** ************ ** ***** ******* **\n",
      "**** ************** *** ********* **** * ******************* ********** ***************** *******\n",
      "*****\n",
      "***\n",
      "******** *************** ******* ** * ****\n",
      "******* * ******* ** ** * ***** *** ** ************* ******** ** ***** ** ***** **\n",
      "**** * ************************* ******************************** ** * ** *** *********** * ************** * ****** * *************** ***************\n",
      "****** * **** * ** ** *** ***** ***** ************* ******** ** ***** ** ************ ************* * ********** * **** ********* ********************* ** * *** *********** ** ********** ** * * ****** * **************** ****** *** ***********\n",
      "* **** * ** ** ** * **** * *** ** ************* ******** ********* ***** ****** * ************* * ** ******** * **** ********* *********** ** ****** *** ** *** *********** * * ***** * **** ** * * ****** * * ******* ******* * ****** ** * *********** * **** * ** ** ** * **** * *** ** ************* ******** ** ***** ** ***** ** **** * **** ********* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * ***** * **** ** ** ****** * * ************** * ****** ** * **** * ****** * ** ** * ** ** ** * **** * *** ** ************* ******** ** **** *** ***** ** **** * ************* * ** ******** * **** ********* *********** ** ****** ** * * * *** *********** * * ***** * **** ** * * ****** * * ************** * ****** ** * **** * ****** * **** * ** ** ** * **** * *** ** **** ***** **** ******** ** ***** ** ***** ** **** * ************* * ** ******** * **** ********* *** * ******* ** ****** ** * * * *** *********** ** ***** * **** ** * * * * **** * * ************** * ****** ** * **** * ******Feature Space H\n",
      "FIGURE 5.14. (Left panel) The ﬁrst 16normalized eigenvectors of K, the\n",
      "200×200kernel matrix for the ﬁrst coordinate of the mixture data. These a re\n",
      "viewed as estimates ˆφℓof the eigenfunctions in (5.45), and are represented as\n",
      "functions in I R1with the observed values superimposed in color. They are arr anged\n",
      "in rows, starting at the top left. (Right panel) Rescaled versi onshℓ=√ˆγℓˆφℓof\n",
      "the functions in the left panel, for which the kernel computes the “inner product.”\n",
      "0 10 20 30 40 501e−15 1e−11 1e−07 1e−03 1e+01Eigenvalue\n",
      "FIGURE 5.15. The largest 50eigenvalues of K; all those beyond the 30th are\n",
      "eﬀectively zero.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "174 5. Basis Expansions and Regularization\n",
      "ingfeature space representation of the eigenfunctions\n",
      "hℓ(x) =√\n",
      "ˆγℓˆφℓ(x), ℓ= 1,... ,N. (5.65)\n",
      "Note that ⟨h(xi),h(xi′)⟩=K(xi,xi′). The scaling by the eigenvalues quickly\n",
      "shrinks most of the functions down to zero, leaving an eﬀective dimension\n",
      "of about 12 in this case. The corresponding optimization problem is a stan-\n",
      "dard ridge regression, as in (5.63). So although in principle the implicit\n",
      "feature space is inﬁnite dimensional, the eﬀective dimension is dramat-\n",
      "ically lower because of the relative amounts of shrinkage applied to each\n",
      "basis function. The kernel scale parameter νplays a role here as well; larger\n",
      "νimplies more local kmfunctions, and increases the eﬀective dimension of\n",
      "the feature space. See Hastie and Zhu (2006) for more details.\n",
      "It is also known (Girosi et al., 1995) that a thin-plate spline (Section 5.7 )\n",
      "is an expansion in radial basis functions, generated by the kernel\n",
      "K(x,y) =∥x−y∥2log(∥x−y∥). (5.66)\n",
      "Radial basis functions are discussed in more detail in Section 6.7.\n",
      "Support Vector Classiﬁers\n",
      "The support vector machines of Chapter 12 for a two-class classiﬁcation\n",
      "problem have the form f(x) =α0+∑N\n",
      "i=1αiK(x,xi), where the parameters\n",
      "are chosen to minimize\n",
      "min\n",
      "α0,α{N∑\n",
      "i=1[1−yif(xi)]++λ\n",
      "2αTKα}\n",
      ", (5.67)\n",
      "where yi∈ {−1,1}, and [ z]+denotes the positive part of z. This can be\n",
      "viewed as a quadratic optimization problem with linear constraints, and\n",
      "requires a quadratic programming algorithm for its solution. The name\n",
      "support vector arises from the fact that typically many of the ˆ αi= 0 [due\n",
      "to the piecewise-zero nature of the loss function in (5.67)], and so ˆfis an\n",
      "expansion in a subset of the K(≤,xi). See Section 12.3.3 for more details.\n",
      "5.9 Wavelet Smoothing\n",
      "We have seen two diﬀerent modes of operation with dictionaries of basis\n",
      "functions. With regression splines, we select a subset of the bases, using\n",
      "either subject-matter knowledge, or else automatically. The more adaptive\n",
      "procedures such as MARS (Chapter 9) can capture both smooth and non-\n",
      "smooth behavior. With smoothing splines, we use a complete basis, but\n",
      "then shrink the coeﬃcients toward smoothness.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.9 Wavelet Smoothing 175\n",
      "Time0.0 0.2 0.4 0.6 0.8 1.0Haar Wavelets\n",
      "Time0.0 0.2 0.4 0.6 0.8 1.0Symmlet-8 Wavelets\n",
      "ψ1,0ψ2,1ψ2,3ψ3,2ψ3,5ψ4,4ψ4,9ψ5,1ψ5,15ψ6,15ψ6,35\n",
      "FIGURE 5.16. Some selected wavelets at diﬀerent translations and dilations\n",
      "for the Haar and symmlet families. The functions have been scale d to suit the\n",
      "display.\n",
      "Wavelets typically use a complete orthonormal basis to represent func-\n",
      "tions, but then shrink and select the coeﬃcients toward a sparse represen-\n",
      "tation. Just as a smooth function can be represented by a few spline basis\n",
      "functions, a mostly ﬂat function with a few isolated bumps can be repre-\n",
      "sented with a few (bumpy) basis functions. Wavelets bases are very popular\n",
      "in signal processing and compression, since they are able to represent both\n",
      "smooth and/or locally bumpy functions in an eﬃcient way—a phenomenon\n",
      "dubbed time and frequency localization . In contrast, the traditional Fourier\n",
      "basis allows only frequency localization.\n",
      "Before we give details, let’s look at the Haar wavelets in the left panel\n",
      "of Figure 5.16 to get an intuitive idea of how wavelet smoothing works.\n",
      "The vertical axis indicates the scale (frequency) of the wavelets, from low\n",
      "scale at the bottom to high scale at the top. At each scale the wavelets are\n",
      "“packed in” side-by-side to completely ﬁll the time axis: we have only shown\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "176 5. Basis Expansions and Regularization\n",
      "a selected subset. Wavelet smoothing ﬁts the coeﬃcients for this basis by\n",
      "least squares, and then thresholds (discards, ﬁlters) the smaller coeﬃcients.\n",
      "Since there are many basis functions at each scale, it can use bases where\n",
      "it needs them and discard the ones it does not need, to achieve time and\n",
      "frequency localization. The Haar wavelets are simple to understand, but not\n",
      "smooth enough for most purposes. The symmlet wavelets in the right panel\n",
      "of Figure 5.16 have the same orthonormal properties, but are smoother.\n",
      "Figure 5.17 displays an NMR (nuclear magnetic resonance) signal, which\n",
      "appears to be composed of smooth components and isolated spikes, plus\n",
      "some noise. The wavelet transform, using a symmlet basis, is shown in the\n",
      "lower left panel. The wavelet coeﬃcients are arranged in rows, from lowest\n",
      "scale at the bottom, to highest scale at the top. The length of each line\n",
      "segment indicates the size of the coeﬃcient. The bottom right panel shows\n",
      "the wavelet coeﬃcients after they have been thresholded. The threshold\n",
      "procedure, given below in equation (5.69), is the same soft-thresholding\n",
      "rule that arises in the lasso procedure for linear regression (Section 3.4.2).\n",
      "Notice that many of the smaller coeﬃcients have been set to zero. The\n",
      "green curve in the top panel shows the back-transform of the thresholded\n",
      "coeﬃcients: this is the smoothed version of the original signal. In the next\n",
      "section we give the details of this process, including the construction of\n",
      "wavelets and the thresholding rule.\n",
      "5.9.1 Wavelet Bases and the Wavelet Transform\n",
      "In this section we give details on the construction and ﬁltering of wavelets.\n",
      "Wavelet bases are generated by translations and dilations of a single scal-\n",
      "ing function φ(x) (also known as the father ). The red curves in Figure 5.18\n",
      "are the Haar andsymmlet-8 scaling functions. The Haar basis is particu-\n",
      "larly easy to understand, especially for anyone with experience in analysis\n",
      "of variance or trees, since it produces a piecewise-constant representation.\n",
      "Thus if φ(x) =I(x∈[0,1]), then φ0,k(x) =φ(x−k),kan integer, generates\n",
      "an orthonormal basis for functions with jumps at the integers. Call this ref-\n",
      "erence space V0. The dilations φ1,k(x) =√\n",
      "2φ(2x−k) form an orthonormal\n",
      "basis for a space V1⊃V0of functions piecewise constant on intervals of\n",
      "length1\n",
      "2. In fact, more generally we have ≤≤≤ ⊃ V1⊃V0⊃V−1⊃ ≤≤≤ where\n",
      "eachVjis spanned by φj,k= 2j/2φ(2jx−k).\n",
      "Now to the deﬁnition of wavelets. In analysis of variance, we often rep-\n",
      "resent a pair of means θ1andθ2by their grand mean θ=1\n",
      "2(θ1+θ2), and\n",
      "then a contrast α=1\n",
      "2(θ1−θ2). A simpliﬁcation occurs if the contrast αis\n",
      "very small, because then we can set it to zero. In a similar manner we might\n",
      "represent a function in Vj+1by a component in Vjplus the component in\n",
      "the orthogonal complement WjofVjtoVj+1, written as Vj+1=Vj⊕Wj.\n",
      "The component in Wjrepresents detail, and we might wish to set some ele-\n",
      "ments of this component to zero. It is easy to see that the functions ψ(x−k)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.9 Wavelet Smoothing 177\n",
      "NMR Signal\n",
      "0 200 400 600 800 10000 20 40 60\n",
      "0 200 400 600 800 1000Wavelet Transform - Original Signal\n",
      "0 200 400 600 800 1000Wavelet Transform - WaveShrunk Signal\n",
      "Signal Signal\n",
      "W9 W9\n",
      "W8 W8\n",
      "W7 W7\n",
      "W6 W6\n",
      "W5 W5\n",
      "W4 W4\n",
      "V4 V4\n",
      "FIGURE 5.17. The top panel shows an NMR signal, with the wavelet-shrunk\n",
      "version superimposed in green. The lower left panel represents the wavelet trans-\n",
      "form of the original signal, down to V4, using the symmlet-8 basis. Each coeﬃ-\n",
      "cient is represented by the height (positive or negative) of the vertical bar. The\n",
      "lower right panel represents the wavelet coeﬃcients after being shrunken using\n",
      "thewaveshrink function in S-PLUS, which implements the SureShrink method\n",
      "of wavelet adaptation of Donoho and Johnstone.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "178 5. Basis Expansions and Regularization\n",
      "Haar Basis Symmlet Basis\n",
      "φ(x) φ(x)\n",
      "ψ(x) ψ(x)\n",
      "FIGURE 5.18. TheHaarandsymmlet father (scaling) wavelet φ(x)and mother\n",
      "wavelet ψ(x).\n",
      "generated by the mother wavelet ψ(x) =φ(2x)−φ(2x−1) form an orthonor-\n",
      "mal basis for W0for the Haar family. Likewise ψj,k= 2j/2ψ(2jx−k) form\n",
      "a basis for Wj.\n",
      "NowVj+1=Vj⊕Wj=Vj−1⊕Wj−1⊕Wj, so besides representing a\n",
      "function by its level- jdetail and level- jrough components, the latter can\n",
      "be broken down to level-( j−1) detail and rough, and so on. Finally we get\n",
      "a representation of the form VJ=V0⊕W0⊕W1≤≤≤ ⊕WJ−1. Figure 5.16\n",
      "on page 175 shows particular wavelets ψj,k(x).\n",
      "Notice that since these spaces are orthogonal, all the basis functions are\n",
      "orthonormal. In fact, if the domain is discrete with N= 2J(time) points,\n",
      "this is as far as we can go. There are 2jbasis elements at level j, and\n",
      "adding up, we have a total of 2J−1 elements in the Wj, and one in V0.\n",
      "This structured orthonormal basis allows for a multiresolution analysis ,\n",
      "which we illustrate in the next section.\n",
      "While helpful for understanding the construction above, the Haar basis\n",
      "is often too coarse for practical purposes. Fortunately, many clever wavelet\n",
      "bases have been invented. Figures 5.16 and 5.18 include the Daubechies\n",
      "symmlet-8 basis. This basis has smoother elements than the corresponding\n",
      "Haar basis, but there is a tradeoﬀ:\n",
      "•Each wavelet has a support covering 15 consecutive time intervals,\n",
      "rather than one for the Haar basis. More generally, the symmlet- p\n",
      "family has a support of 2 p−1 consecutive intervals. The wider the\n",
      "support, the more time the wavelet has to die to zero, and so it can\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "5.9 Wavelet Smoothing 179\n",
      "achieve this more smoothly. Note that the eﬀective support seems to\n",
      "be much narrower.\n",
      "•The symmlet- pwavelet ψ(x) has pvanishing moments; that is,\n",
      "∫\n",
      "ψ(x)xjdx= 0, j= 0,... ,p −1.\n",
      "One implication is that any order- ppolynomial over the N= 2Jtimes\n",
      "points is reproduced exactly in V0(Exercise 5.18). In this sense V0\n",
      "is equivalent to the null space of the smoothing-spline penalty. The\n",
      "Haar wavelets have one vanishing moment, and V0can reproduce any\n",
      "constant function.\n",
      "The symmlet- pscaling functions are one of many families of wavelet\n",
      "generators. The operations are similar to those for the Haar basis:\n",
      "•IfV0is spanned by φ(x−k), then V1⊃V0is spanned by φ1,k(x) =√\n",
      "2φ(2x−k) and φ(x) =∑\n",
      "k∈Zh(k)φ1,k(x), for some ﬁlter coeﬃcients\n",
      "h(k).\n",
      "•W0is spanned by ψ(x) =∑\n",
      "k∈Zg(k)φ1,k(x), with ﬁlter coeﬃcients\n",
      "g(k) = (−1)1−kh(1−k).\n",
      "5.9.2 Adaptive Wavelet Filtering\n",
      "Wavelets are particularly useful when the data are measured on a uniform\n",
      "lattice, such as a discretized signal, image, or a time series. We will focus o n\n",
      "the one-dimensional case, and having N= 2Jlattice-points is convenient.\n",
      "Suppose yis the response vector, and Wis the N×Northonormal wavelet\n",
      "basis matrix evaluated at the Nuniformly spaced observations. Then y∗=\n",
      "WTyis called the wavelet transform ofy(and is the full least squares\n",
      "regression coeﬃcient). A popular method for adaptive wavelet ﬁtting is\n",
      "known as SURE shrinkage (Stein Unbiased Risk Estimation, Donoho and\n",
      "Johnstone (1994)). We start with the criterion\n",
      "min\n",
      "θ||y−Wθ||2\n",
      "2+ 2λ||θ||1, (5.68)\n",
      "which is the same as the lasso criterion in Chapter 3. Because Wis or-\n",
      "thonormal, this leads to the simple solution:\n",
      "ˆθj= sign( y∗\n",
      "j)(|y∗\n",
      "j| −λ)+. (5.69)\n",
      "The least squares coeﬃcients are translated toward zero, and truncated\n",
      "at zero. The ﬁtted function (vector) is then given by the inverse wavelet\n",
      "transform ˆf=Wˆθ.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "180 5. Basis Expansions and Regularization\n",
      "A simple choice for λisλ=σ√2logN, where σis an estimate of the\n",
      "standard deviation of the noise. We can give some motivation for this cho ice.\n",
      "SinceWis an orthonormal transformation, if the elements of yare white\n",
      "noise (independent Gaussian variates with mean 0 and variance σ2), then\n",
      "so arey∗. Furthermore if random variables Z1,Z2,... ,Z Nare white noise,\n",
      "the expected maximum of |Zj|,j= 1,... ,N is approximately σ√2logN.\n",
      "Hence all coeﬃcients below σ√2logNare likely to be noise and are set to\n",
      "zero.\n",
      "The space Wcould be any basis of orthonormal functions: polynomials,\n",
      "natural splines or cosinusoids. What makes wavelets special is the particular\n",
      "form of basis functions used, which allows for a representation localized in\n",
      "time and in frequency .\n",
      "Let’s look again at the NMR signal of Figure 5.17. The wavelet transfor m\n",
      "was computed using a symmlet −8 basis. Notice that the coeﬃcients do not\n",
      "descend all the way to V0, but stop at V4which has 16 basis functions.\n",
      "As we ascend to each level of detail, the coeﬃcients get smaller, except in\n",
      "locations where spiky behavior is present. The wavelet coeﬃcients represent\n",
      "characteristics of the signal localized in time (the basis functions at each\n",
      "level are translations of each other) and localized in frequency. Each dilation\n",
      "increases the detail by a factor of two, and in this sense corresponds to\n",
      "doubling the frequency in a traditional Fourier representation. In fact, a\n",
      "more mathematical understanding of wavelets reveals that the wavelets at\n",
      "a particular scale have a Fourier transform that is restricted to a limited\n",
      "range or octave of frequencies.\n",
      "The shrinking/truncation in the right panel was achieved using the SURE\n",
      "approach described in the introduction to this section. The orthonormal\n",
      "N×Nbasis matrix Whas columns which are the wavelet basis functions\n",
      "evaluated at the Ntime points. In particular, in this case there will be 16\n",
      "columns corresponding to the φ4,k(x), and the remainder devoted to the\n",
      "ψj,k(x), j= 4,... ,11. In practice λdepends on the noise variance, and has\n",
      "to be estimated from the data (such as the variance of the coeﬃcients at\n",
      "the highest level).\n",
      "Notice the similarity between the SURE criterion (5.68) on page 179,\n",
      "and the smoothing spline criterion (5.21) on page 156:\n",
      "•Both are hierarchically structured from coarse to ﬁne detail, although\n",
      "wavelets are also localized in time within each resolution level.\n",
      "•The splines build in a bias toward smooth functions by imposing\n",
      "diﬀerential shrinking constants dk. Early versions of SURE shrinkage\n",
      "treated all scales equally. The S+wavelets function waveshrink() has\n",
      "many options, some of which allow for diﬀerential shrinkage.\n",
      "•The spline L2penalty cause pure shrinkage, while the SURE L1\n",
      "penalty does shrinkage and selection.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 181\n",
      "More generally smoothing splines achieve compression of the original signal\n",
      "by imposing smoothness, while wavelets impose sparsity. Figure 5.19 co m-\n",
      "pares a wavelet ﬁt (using SURE shrinkage) to a smoothing spline ﬁt (using\n",
      "cross-validation) on two examples diﬀerent in nature. For the NMR data in\n",
      "the upper panel, the smoothing spline introduces detail everywhere in order\n",
      "to capture the detail in the isolated spikes; the wavelet ﬁt nicely localizes\n",
      "the spikes. In the lower panel, the true function is smooth, and the noise is\n",
      "relatively high. The wavelet ﬁt has let in some additional and unnecessary\n",
      "wiggles—a price it pays in variance for the additional adaptivity.\n",
      "The wavelet transform is not performed by matrix multiplication as in\n",
      "y∗=WTy. In fact, using clever pyramidal schemes y∗can be obtained\n",
      "inO(N) computations, which is even faster than the Nlog(N) of the fast\n",
      "Fourier transform (FFT). While the general construction is beyond the\n",
      "scope of this book, it is easy to see for the Haar basis (Exercise 5.19).\n",
      "Likewise, the inverse wavelet transform Wˆθis also O(N).\n",
      "This has been a very brief glimpse of this vast and growing ﬁeld. There is\n",
      "a very large mathematical and computational base built on wavelets. Mod-\n",
      "ern image compression is often performed using two-dimensional wavelet\n",
      "representations.\n",
      "Bibliographic Notes\n",
      "Splines and B-splines are discussed in detail in de Boor (1978). Green\n",
      "and Silverman (1994) and Wahba (1990) give a thorough treatment of\n",
      "smoothing splines and thin-plate splines; the latter also covers reproducing\n",
      "kernel Hilbert spaces. See also Girosi et al. (1995) and Evgeniou et al.\n",
      "(2000) for connections between many nonparametric regression techniques\n",
      "using RKHS approaches. Modeling functional data, as in Section 5.2.3, is\n",
      "covered in detail in Ramsay and Silverman (1997).\n",
      "Daubechies (1992) is a classic and mathematical treatment of wavelets.\n",
      "Other useful sources are Chui (1992) and Wickerhauser (1994). Donoho and\n",
      "Johnstone (1994) developed the SURE shrinkage and selection technology\n",
      "from a statistical estimation framework; see also Vidakovic (199 9). Bruce\n",
      "and Gao (1996) is a useful applied introduction, which also describes the\n",
      "wavelet software in S-PLUS.\n",
      "Exercises\n",
      "Ex. 5.1 Show that the truncated power basis functions in (5.3) represent a\n",
      "basis for a cubic spline with the two knots as indicated.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "182 5. Basis Expansions and Regularization\n",
      "NMR Signal0 200 400 600 800 10000 20 40 60spline\n",
      "wavelet\n",
      "Smooth Function (Simulated)n\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-4 -2 0 2 4spline\n",
      "wavelet\n",
      "true•\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•\n",
      "•\n",
      "••••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "••\n",
      "••\n",
      "••••••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••••\n",
      "•••••••\n",
      "••••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••••••\n",
      "••••\n",
      "••\n",
      "•\n",
      "•••••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "FIGURE 5.19. Wavelet smoothing compared with smoothing splines on two\n",
      "examples. Each panel compares the SURE-shrunk wavelet ﬁt to the cro ss-validated\n",
      "smoothing spline ﬁt.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 183\n",
      "Ex. 5.2 Suppose that Bi,M(x) is an order- M B-spline deﬁned in the Ap-\n",
      "pendix on page 186 through the sequence (5.77)–(5.78).\n",
      "(a) Show by induction that Bi,M(x) = 0 for x̸∈[τi,τi+M]. This shows, for\n",
      "example, that the support of cubic B-splines is at most 5 knots.\n",
      "(b) Show by induction that Bi,M(x)>0 forx∈(τi,τi+M). The B-splines\n",
      "are positive in the interior of their support.\n",
      "(c) Show by induction that∑K+M\n",
      "i=1Bi,M(x) = 1∀x∈[ξ0,ξK+1].\n",
      "(d) Show that Bi,Mis a piecewise polynomial of order M(degree M−1)\n",
      "on [ξ0,ξK+1], with breaks only at the knots ξ1,... ,ξ K.\n",
      "(e) Show that an order- M B-spline basis function is the density function\n",
      "of a convolution of Muniform random variables.\n",
      "Ex. 5.3 Write a program to reproduce Figure 5.3 on page 145.\n",
      "Ex. 5.4 Consider the truncated power series representation for cubic splines\n",
      "withKinterior knots. Let\n",
      "f(X) =3∑\n",
      "j=0βjXj+K∑\n",
      "k=1θk(X−ξk)3\n",
      "+. (5.70)\n",
      "Prove that the natural boundary conditions for natural cubic splines (Sec-\n",
      "tion 5.2.1) imply the following linear constraints on the coeﬃcients:\n",
      "β2= 0,∑K\n",
      "k=1θk= 0,\n",
      "β3= 0,∑K\n",
      "k=1ξkθk= 0.(5.71)\n",
      "Hence derive the basis (5.4) and (5.5).\n",
      "Ex. 5.5 Write a program to classify the phoneme data using a quadratic dis-\n",
      "criminant analysis (Section 4.3). Since there are many correlated features,\n",
      "you should ﬁlter them using a smooth basis of natural cubic splines (Sec-\n",
      "tion 5.2.3). Decide beforehand on a series of ﬁve diﬀerent choices for the\n",
      "number and position of the knots, and use tenfold cross-validation to make\n",
      "the ﬁnal selection. The phoneme data are available from the book website\n",
      "www-stat.stanford.edu/ElemStatLearn .\n",
      "Ex. 5.6 Suppose you wish to ﬁt a periodic function, with a known period T.\n",
      "Describe how you could modify the truncated power series basis to achieve\n",
      "this goal.\n",
      "Ex. 5.7 Derivation of smoothing splines (Green and Silverman, 1994). Sup-\n",
      "pose that N≥2, and that gis the natural cubic spline interpolant to the\n",
      "pairs{xi,zi}N\n",
      "1, with a < x 1<≤≤≤< x N< b. This is a natural spline\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "184 5. Basis Expansions and Regularization\n",
      "with a knot at every xi; being an N-dimensional space of functions, we can\n",
      "determine the coeﬃcients such that it interpolates the sequence ziexactly.\n",
      "Let ˜gbe any other diﬀerentiable function on [ a,b] that interpolates the N\n",
      "pairs.\n",
      "(a) Let h(x) = ˜g(x)−g(x). Use integration by parts and the fact that gis\n",
      "a natural cubic spline to show that\n",
      "∫b\n",
      "ag′′(x)h′′(x)dx=−N−1∑\n",
      "j=1g′′′(x+\n",
      "j){h(xj+1)−h(xj)}(5.72)\n",
      "= 0.\n",
      "(b) Hence show that∫b\n",
      "a˜g′′(t)2dt≥∫b\n",
      "ag′′(t)2dt,\n",
      "and that equality can only hold if his identically zero in [ a,b].\n",
      "(c) Consider the penalized least squares problem\n",
      "min\n",
      "f[N∑\n",
      "i=1(yi−f(xi))2+λ∫b\n",
      "af′′(t)2dt]\n",
      ".\n",
      "Use (b) to argue that the minimizer must be a cubic spline with knots\n",
      "at each of the xi.\n",
      "Ex. 5.8 In the appendix to this chapter we show how the smoothing spline\n",
      "computations could be more eﬃciently carried out using a ( N+ 4) dimen-\n",
      "sional basis of B-splines. Describe a slightly simpler scheme using a ( N+2)\n",
      "dimensional B-spline basis deﬁned on the N−2 interior knots.\n",
      "Ex. 5.9 Derive the Reinsch form Sλ= (I+λK)−1for the smoothing spline.\n",
      "Ex. 5.10 Derive an expression for Var( ˆfλ(x0)) and bias( ˆfλ(x0)). Using the\n",
      "example (5.22), create a version of Figure 5.9 where the mean and several\n",
      "(pointwise) quantiles of ˆfλ(x) are shown.\n",
      "Ex. 5.11 Prove that for a smoothing spline the null space of Kis spanned\n",
      "by functions linear in X.\n",
      "Ex. 5.12 Characterize the solution to the following problem,\n",
      "min\n",
      "fRSS(f,λ) =N∑\n",
      "i=1wi{yi−f(xi)}2+λ∫\n",
      "{f′′(t)}2dt, (5.73)\n",
      "where the wi≥0 are observation weights.\n",
      "Characterize the solution to the smoothing spline problem (5.9) when\n",
      "the training data have ties in X.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 185\n",
      "Ex. 5.13 You have ﬁtted a smoothing spline ˆfλto a sample of Npairs\n",
      "(xi,yi). Suppose you augment your original sample with the pair x0,ˆfλ(x0),\n",
      "and reﬁt; describe the result. Use this to derive the N-fold cross-validation\n",
      "formula (5.26).\n",
      "Ex. 5.14 Derive the constraints on the αjin the thin-plate spline expan-\n",
      "sion (5.39) to guarantee that the penalty J(f) is ﬁnite. How else could one\n",
      "ensure that the penalty was ﬁnite?\n",
      "Ex. 5.15 This exercise derives some of the results quoted in Section 5.8.1.\n",
      "Suppose K(x,y) satisfying the conditions (5.45) and let f(x)∈ H K. Show\n",
      "that\n",
      "(a)⟨K(≤,xi),f⟩HK=f(xi).\n",
      "(b)⟨K(≤,xi),K(≤,xj)⟩HK=K(xi,xj).\n",
      "(c) If g(x) =∑N\n",
      "i=1αiK(x,xi), then\n",
      "J(g) =N∑\n",
      "i=1N∑\n",
      "j=1K(xi,xj)αiαj.\n",
      "Suppose that ˜ g(x) =g(x) +ρ(x), with ρ(x)∈ H K, and orthogonal in HK\n",
      "to each of K(x,xi), i= 1,... ,N . Show that\n",
      "(d)\n",
      "N∑\n",
      "i=1L(yi,˜g(xi)) +λJ(˜g)≥N∑\n",
      "i=1L(yi,g(xi)) +λJ(g) (5.74)\n",
      "with equality iﬀ ρ(x) = 0.\n",
      "Ex. 5.16 Consider the ridge regression problem (5.53), and assume M≥N.\n",
      "Assume you have a kernel Kthat computes the inner product K(x,y) =∑M\n",
      "m=1hm(x)hm(y).\n",
      "(a) Derive (5.62) on page 171 in the text. How would you compute the\n",
      "matrices VandDγ, given K? Hence show that (5.63) is equivalent\n",
      "to (5.53).\n",
      "(b) Show that\n",
      "ˆf=Hˆβ\n",
      "=K(K+λI)−1y, (5.75)\n",
      "whereHis the N×Mmatrix of evaluations hm(xi), and K=HHT\n",
      "theN×Nmatrix of inner-products h(xi)Th(xj).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "186 5. Basis Expansions and Regularization\n",
      "(c) Show that\n",
      "ˆf(x) = h(x)Tˆβ\n",
      "=N∑\n",
      "i=1K(x,xi)ˆαi (5.76)\n",
      "andˆα= (K+λI)−1y.\n",
      "(d) How would you modify your solution if M < N ?\n",
      "Ex. 5.17 Show how to convert the discrete eigen-decomposition of Kin\n",
      "Section 5.8.2 to estimates of the eigenfunctions of K.\n",
      "Ex. 5.18 The wavelet function ψ(x) of the symmlet- pwavelet basis has\n",
      "vanishing moments up to order p. Show that this implies that polynomials\n",
      "of order pare represented exactly in V0, deﬁned on page 176.\n",
      "Ex. 5.19 Show that the Haar wavelet transform of a signal of length N= 2J\n",
      "can be computed in O(N) computations.\n",
      "Appendix: Computations for Splines\n",
      "In this Appendix, we describe the B-spline basis for representing polyno-\n",
      "mial splines. We also discuss their use in the computations of smoothing\n",
      "splines.\n",
      "B-splines\n",
      "Before we can get started, we need to augment the knot sequence deﬁned\n",
      "in Section 5.2. Let ξ0< ξ1andξK< ξK+1be two boundary knots, which\n",
      "typically deﬁne the domain over which we wish to evaluate our spline. We\n",
      "now deﬁne the augmented knot sequence τsuch that\n",
      "•τ1≤τ2≤ ≤≤≤ ≤ τM≤ξ0;\n",
      "•τj+M=ξj, j= 1,≤≤≤,K;\n",
      "•ξK+1≤τK+M+1≤τK+M+2≤ ≤≤≤ ≤ τK+2M.\n",
      "The actual values of these additional knots beyond the boundary are arbi-\n",
      "trary, and it is customary to make them all the same and equal to ξ0and\n",
      "ξK+1, respectively.\n",
      "Denote by Bi,m(x) the ithB-spline basis function of order mfor the\n",
      "knot-sequence τ,m≤M. They are deﬁned recursively in terms of divided\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Appendix: Computations for Splines 187\n",
      "diﬀerences as follows:\n",
      "Bi,1(x) ={\n",
      "1 ifτi≤x < τ i+1\n",
      "0 otherwise(5.77)\n",
      "fori= 1,... ,K + 2M−1. These are also known as Haar basis functions.\n",
      "Bi,m(x) =x−τi\n",
      "τi+m−1−τiBi,m−1(x) +τi+m−x\n",
      "τi+m−τi+1Bi+1,m−1(x)\n",
      "fori= 1,... ,K + 2M−m.\n",
      "(5.78)\n",
      "Thus with M= 4,Bi,4, i= 1,≤≤≤,K+ 4 are the K+ 4 cubic B-spline\n",
      "basis functions for the knot sequence ξ. This recursion can be contin-\n",
      "ued and will generate the B-spline basis for any order spline. Figure 5.20\n",
      "shows the sequence of B-splines up to order four with knots at the points\n",
      "0.0,0.1,... ,1.0. Since we have created some duplicate knots, some care\n",
      "has to be taken to avoid division by zero. If we adopt the convention\n",
      "thatBi,1= 0 if τi=τi+1, then by induction Bi,m= 0 if τi=τi+1=\n",
      "...=τi+m. Note also that in the construction above, only the subset\n",
      "Bi,m, i=M−m+ 1,... ,M +Kare required for the B-spline basis\n",
      "of order m < M with knots ξ.\n",
      "To fully understand the properties of these functions, and to show that\n",
      "they do indeed span the space of cubic splines for the knot sequence, re-\n",
      "quires additional mathematical machinery, including the properties of di-\n",
      "vided diﬀerences. Exercise 5.2 explores these issues.\n",
      "The scope of B-splines is in fact bigger than advertised here, and has to\n",
      "do with knot duplication. If we duplicate an interior knot in the construc-\n",
      "tion of the τsequence above, and then generate the B-spline sequence as\n",
      "before, the resulting basis spans the space of piecewise polynomials with\n",
      "one less continuous derivative at the duplicated knot. In general, if in ad-\n",
      "dition to the repeated boundary knots, we include the interior knot ξj\n",
      "1≤rj≤Mtimes, then the lowest-order derivative to be discontinuous\n",
      "atx=ξjwill be order M−rj. Thus for cubic splines with no repeats,\n",
      "rj= 1, j= 1,... ,K , and at each interior knot the third derivatives (4 −1)\n",
      "are discontinuous. Repeating the jth knot three times leads to a discontin-\n",
      "uous 1st derivative; repeating it four times leads to a discontinuous zeroth\n",
      "derivative, i.e., the function is discontinuous at x=ξj. This is exactly what\n",
      "happens at the boundary knots; we repeat the knots Mtimes, so the spline\n",
      "becomes discontinuous at the boundary knots (i.e., undeﬁned beyond the\n",
      "boundary).\n",
      "The local support of B-splines has important computational implica-\n",
      "tions, especially when the number of knots Kis large. Least squares com-\n",
      "putations with Nobservations and K+Mvariables (basis functions) take\n",
      "O(N(K+M)2+ (K+M)3) ﬂops (ﬂoating point operations.) If Kis some\n",
      "appreciable fraction of N, this leads to O(N3) algorithms which becomes\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "188 5. Basis Expansions and Regularization\n",
      "B-splines of Order 1\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\n",
      "B-splines of Order 2\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\n",
      "B-splines of Order 3\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\n",
      "B-splines of Order 4\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.4 0.8 1.2\n",
      "FIGURE 5.20. The sequence of B-splines up to order four with ten knots evenly\n",
      "spaced from 0to1. The B-splines have local support ; they are nonzero on an\n",
      "interval spanned by M+ 1knots.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Appendix: Computations for Splines 189\n",
      "unacceptable for large N. If the Nobservations are sorted, the N×(K+M)\n",
      "regression matrix consisting of the K+M B-spline basis functions evalu-\n",
      "ated at the Npoints has many zeros, which can be exploited to reduce the\n",
      "computational complexity back to O(N). We take this up further in the\n",
      "next section.\n",
      "Computations for Smoothing Splines\n",
      "Although natural splines (Section 5.2.1) provide a basis for smoothing\n",
      "splines, it is computationally more convenient to operate in the larger space\n",
      "of unconstrained B-splines. We write f(x) =∑N+4\n",
      "1γjBj(x), where γjare\n",
      "coeﬃcients and the Bjare the cubic B-spline basis functions. The solution\n",
      "looks the same as before,\n",
      "ˆγ= (BTB+λΩB)−1BTy, (5.79)\n",
      "except now the N×Nmatrix Nis replaced by the N×(N+ 4) matrix\n",
      "B, and similarly the ( N+ 4)×(N+ 4) penalty matrix ΩBreplaces the\n",
      "N×Ndimensional ΩN. Although at face value it seems that there are\n",
      "no boundary derivative constraints, it turns out that the penalty term\n",
      "automatically imposes them by giving eﬀectively inﬁnite weight to any non\n",
      "zero derivative beyond the boundary. In practice, ˆ γis restricted to a linear\n",
      "subspace for which the penalty is always ﬁnite.\n",
      "Since the columns of Bare the evaluated B-splines, in order from left\n",
      "to right and evaluated at the sorted values of X, and the cubic B-splines\n",
      "have local support, Bis lower 4-banded. Consequently the matrix M=\n",
      "(BTB+λΩ) is 4-banded and hence its Cholesky decomposition M=LLT\n",
      "can be computed easily. One then solves LLTγ=BTyby back-substitution\n",
      "to give γand hence the solution ˆfinO(N) operations.\n",
      "In practice, when Nis large, it is unnecessary to use all Ninterior knots,\n",
      "and any reasonable thinning strategy will save in computations and have\n",
      "negligible eﬀect on the ﬁt. For example, the smooth.spline function in S-\n",
      "PLUS uses an approximately logarithmic strategy: if N <50 all knots are\n",
      "included, but even at N= 5,000 only 204 knots are used.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "190 5. Basis Expansions and Regularization\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 191\n",
      "Printer: Opaque this\n",
      "6\n",
      "Kernel Smoothing Methods\n",
      "In this chapter we describe a class of regression techniques that achieve\n",
      "ﬂexibility in estimating the regression function f(X) over the domain IRp\n",
      "by ﬁtting a diﬀerent but simple model separately at each query point x0.\n",
      "This is done by using only those observations close to the target point x0to\n",
      "ﬁt the simple model, and in such a way that the resulting estimated function\n",
      "ˆf(X) issmooth in IRp. This localization is achieved via a weighting function\n",
      "orkernel Kλ(x0,xi), which assigns a weight to xibased on its distance from\n",
      "x0. The kernels Kλare typically indexed by a parameter λthat dictates\n",
      "the width of the neighborhood. These memory-based methods require in\n",
      "principle little or no training; all the work gets done at evaluation time.\n",
      "The only parameter that needs to be determined from the training data is\n",
      "λ. The model, however, is the entire training data set.\n",
      "We also discuss more general classes of kernel-based techniques , which\n",
      "tie in with structured methods in other chapters, and are useful for density\n",
      "estimation and classiﬁcation.\n",
      "The techniques in this chapter should not be confused with those asso-\n",
      "ciated with the more recent usage of the phrase “kernel methods”. In this\n",
      "chapter kernels are mostly used as a device for localization. We discuss ker-\n",
      "nel methods in Sections 5.8, 14.5.4, 18.5 and Chapter 12; in those contexts\n",
      "the kernel computes an inner product in a high-dimensional (implicit) fea-\n",
      "ture space, and is used for regularized nonlinear modeling. We make some\n",
      "connections to the methodology in this chapter at the end of Section 6.7.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "192 6. Kernel Smoothing Methods\n",
      "Nearest-Neighbor Kernel\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\n",
      "OOOO\n",
      "OO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "OO\n",
      "OOOOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOOO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO•\n",
      "x0ˆf(x0)Epanechnikov Kernel\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\n",
      "OOOO\n",
      "OO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "OO\n",
      "OOOOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "O\n",
      "OO•\n",
      "x0ˆf(x0)\n",
      "FIGURE 6.1. In each panel 100pairs xi, yiare generated at random from the\n",
      "blue curve with Gaussian errors: Y= sin(4 X)+ε,X∼U[0,1],ε∼N(0,1/3). In\n",
      "the left panel the green curve is the result of a 30-nearest-neighbor running-mean\n",
      "smoother. The red point is the ﬁtted constant ˆf(x0), and the red circles indicate\n",
      "those observations contributing to the ﬁt at x0. The solid yellow region indicates\n",
      "the weights assigned to observations. In the right panel, the gr een curve is the\n",
      "kernel-weighted average, using an Epanechnikov kernel with (hal f) window width\n",
      "λ= 0.2.\n",
      "6.1 One-Dimensional Kernel Smoothers\n",
      "In Chapter 2, we motivated the k–nearest-neighbor average\n",
      "ˆf(x) = Ave( yi|xi∈Nk(x)) (6.1)\n",
      "as an estimate of the regression function E( Y|X=x). Here Nk(x) is the set\n",
      "ofkpoints nearest to xin squared distance, and Ave denotes the average\n",
      "(mean). The idea is to relax the deﬁnition of conditional expectation, as\n",
      "illustrated in the left panel of Figure 6.1, and compute an average in a\n",
      "neighborhood of the target point. In this case we have used the 30-nearest\n",
      "neighborhood—the ﬁt at x0is the average of the 30 pairs whose xivalues\n",
      "are closest to x0. The green curve is traced out as we apply this deﬁnition\n",
      "at diﬀerent values x0. The green curve is bumpy, since ˆf(x) is discontinuous\n",
      "inx. As we move x0from left to right, the k-nearest neighborhood remains\n",
      "constant, until a point xito the right of x0becomes closer than the furthest\n",
      "point xi′in the neighborhood to the left of x0, at which time xireplaces xi′.\n",
      "The average in (6.1) changes in a discrete way, leading to a discontinuous\n",
      "ˆf(x).\n",
      "This discontinuity is ugly and unnecessary. Rather than give all the\n",
      "points in the neighborhood equal weight, we can assign weights that die\n",
      "oﬀ smoothly with distance from the target point. The right panel shows\n",
      "an example of this, using the so-called Nadaraya–Watson kernel-weighted\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.1 One-Dimensional Kernel Smoothers 193\n",
      "average\n",
      "ˆf(x0) =∑N\n",
      "i=1Kλ(x0,xi)yi∑N\n",
      "i=1Kλ(x0,xi), (6.2)\n",
      "with the Epanechnikov quadratic kernel\n",
      "Kλ(x0,x) =D(|x−x0|\n",
      "λ)\n",
      ", (6.3)\n",
      "with\n",
      "D(t) ={3\n",
      "4(1−t2) if|t| ≤1;\n",
      "0 otherwise .(6.4)\n",
      "The ﬁtted function is now continuous, and quite smooth in the right panel\n",
      "of Figure 6.1. As we move the target from left to right, points enter t he\n",
      "neighborhood initially with weight zero, and then their contribution slowly\n",
      "increases (see Exercise 6.1).\n",
      "In the right panel we used a metric window size λ= 0.2 for the kernel\n",
      "ﬁt, which does not change as we move the target point x0, while the size\n",
      "of the 30-nearest-neighbor smoothing window adapts to the local density\n",
      "of the xi. One can, however, also use such adaptive neighborhoods with\n",
      "kernels, but we need to use a more general notation. Let hλ(x0) be a width\n",
      "function (indexed by λ) that determines the width of the neighborhood at\n",
      "x0. Then more generally we have\n",
      "Kλ(x0,x) =D(|x−x0|\n",
      "hλ(x0))\n",
      ". (6.5)\n",
      "In (6.3), hλ(x0) =λis constant. For k-nearest neighborhoods, the neigh-\n",
      "borhood size kreplaces λ, and we have hk(x0) =|x0−x[k]|where x[k]is\n",
      "thekth closest xitox0.\n",
      "There are a number of details that one has to attend to in practice:\n",
      "•The smoothing parameter λ, which determines the width of the local\n",
      "neighborhood, has to be determined. Large λimplies lower variance\n",
      "(averages over more observations) but higher bias (we essentially as-\n",
      "sume the true function is constant within the window).\n",
      "•Metric window widths (constant hλ(x)) tend to keep the bias of the\n",
      "estimate constant, but the variance is inversely proportional to the\n",
      "local density. Nearest-neighbor window widths exhibit the opposite\n",
      "behavior; the variance stays constant and the absolute bias varies\n",
      "inversely with local density.\n",
      "•Issues arise with nearest-neighbors when there are ties in the xi. With\n",
      "most smoothing techniques one can simply reduce the data set by\n",
      "averaging the yiat tied values of X, and supplementing these new\n",
      "observations at the unique values of xiwith an additional weight wi\n",
      "(which multiples the kernel weight).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "194 6. Kernel Smoothing Methods\n",
      "-3 -2 -1 0 1 2 30.0 0.4 0.8Epanechnikov\n",
      "Tri-cube\n",
      "GaussianKλ(x0, x)\n",
      "FIGURE 6.2. A comparison of three popular kernels for local smoothing. Eac h\n",
      "has been calibrated to integrate to 1. The tri-cube kernel is compact and has two\n",
      "continuous derivatives at the boundary of its support, while th e Epanechnikov ker-\n",
      "nel has none. The Gaussian kernel is continuously diﬀerentiable, bu t has inﬁnite\n",
      "support.\n",
      "•This leaves a more general problem to deal with: observation weights\n",
      "wi. Operationally we simply multiply them by the kernel weights be-\n",
      "fore computing the weighted average. With nearest neighborhoods, it\n",
      "is now natural to insist on neighborhoods with a total weight content\n",
      "k(relative to∑wi). In the event of overﬂow (the last observation\n",
      "needed in a neighborhood has a weight wjwhich causes the sum of\n",
      "weights to exceed the budget k), then fractional parts can be used.\n",
      "•Boundary issues arise. The metric neighborhoods tend to contain less\n",
      "points on the boundaries, while the nearest-neighborhoods get wider.\n",
      "•The Epanechnikov kernel has compact support (needed when used\n",
      "with nearest-neighbor window size). Another popular compact kernel\n",
      "is based on the tri-cube function\n",
      "D(t) ={\n",
      "(1− |t|3)3if|t| ≤1;\n",
      "0 otherwise(6.6)\n",
      "This is ﬂatter on the top (like the nearest-neighbor box) and is diﬀer-\n",
      "entiable at the boundary of its support. The Gaussian density func-\n",
      "tionD(t) =φ(t) is a popular noncompact kernel, with the standard-\n",
      "deviation playing the role of the window size. Figure 6.2 compares\n",
      "the three.\n",
      "6.1.1 Local Linear Regression\n",
      "We have progressed from the raw moving average to a smoothly varying\n",
      "locally weighted average by using kernel weighting. The smooth kernel ﬁt\n",
      "still has problems, however, as exhibited in Figure 6.3 (left panel). Locally-\n",
      "weighted averages can be badly biased on the boundaries of the domain,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.1 One-Dimensional Kernel Smoothers 195\n",
      "N-W Kernel at Boundary\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OOOOO\n",
      "OOO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OOOO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "OOOO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OOOOO\n",
      "OOO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "•\n",
      "x0ˆf(x0)Local Linear Regression at Boundary\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OOOOO\n",
      "OOO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OOOO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "OOOO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OOOO\n",
      "O\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OOOOO\n",
      "OOO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "•\n",
      "x0ˆf(x0)\n",
      "FIGURE 6.3. The locally weighted average has bias problems at or near the\n",
      "boundaries of the domain. The true function is approximately line ar here, but\n",
      "most of the observations in the neighborhood have a higher mean than the target\n",
      "point, so despite weighting, their mean will be biased upwards . By ﬁtting a locally\n",
      "weighted linear regression (right panel), this bias is remove d to ﬁrst order\n",
      "because of the asymmetry of the kernel in that region. By ﬁtting straight\n",
      "lines rather than constants locally, we can remove this bias exactly to ﬁrst\n",
      "order; see Figure 6.3 (right panel). Actually, this bias can be present in the\n",
      "interior of the domain as well, if the Xvalues are not equally spaced (for\n",
      "the same reasons, but usually less severe). Again locally weighted linear\n",
      "regression will make a ﬁrst-order correction.\n",
      "Locally weighted regression solves a separate weighted least squares prob-\n",
      "lem at each target point x0:\n",
      "min\n",
      "α(x0),β(x0)N∑\n",
      "i=1Kλ(x0,xi)[yi−α(x0)−β(x0)xi]2. (6.7)\n",
      "The estimate is then ˆf(x0) = ˆα(x0) +ˆβ(x0)x0. Notice that although we ﬁt\n",
      "an entire linear model to the data in the region, we only use it to evaluate\n",
      "the ﬁt at the single point x0.\n",
      "Deﬁne the vector-valued function b(x)T= (1,x). Let Bbe the N×2\n",
      "regression matrix with ith row b(xi)T, andW(x0) the N×Ndiagonal\n",
      "matrix with ith diagonal element Kλ(x0,xi). Then\n",
      "ˆf(x0) = b(x0)T(BTW(x0)B)−1BTW(x0)y (6.8)\n",
      "=N∑\n",
      "i=1li(x0)yi. (6.9)\n",
      "Equation (6.8) gives an explicit expression for the local linear regression\n",
      "estimate, and (6.9) highlights the fact that the estimate is linear in the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "196 6. Kernel Smoothing Methods\n",
      "Local Linear Equivalent Kernel at Boundary\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOOOOOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOOOO\n",
      "OO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "OOOO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OOOOO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOOOOO\n",
      "•••••••••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••••••••••••• • ••••• •• • • • ••• • ••• • •• ••• • •• ••••• ••• • ••• •••• •• • •• • • •• • •• •••• ••• • • • ••••••\n",
      "x0ˆf(x0)Local Linear Equivalent Kernel in Interior\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OO\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOOOOOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOOOO\n",
      "OO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "OOOO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OOOOO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOOOO\n",
      "OO\n",
      "OO\n",
      "OOOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OOO•\n",
      "•• • ••• •••• •••• • •••••••• ••• ••••••••••••••••••••••••••••••••••••••\n",
      "•••\n",
      "••\n",
      "• •• • ••• • •• • •• •••• ••• • • • ••••••\n",
      "x0ˆf(x0)\n",
      "FIGURE 6.4. The green points show the equivalent kernel li(x0)for local re-\n",
      "gression. These are the weights in ˆf(x0) =PN\n",
      "i=1li(x0)yi, plotted against their\n",
      "corresponding xi. For display purposes, these have been rescaled, since in fac t\n",
      "they sum to 1. Since the yellow shaded region is the (rescaled) equivalent ke rnel\n",
      "for the Nadaraya–Watson local average, we see how local regr ession automati-\n",
      "cally modiﬁes the weighting kernel to correct for biases due to a symmetry in the\n",
      "smoothing window.\n",
      "yi(theli(x0) do not involve y). These weights li(x0) combine the weight-\n",
      "ing kernel Kλ(x0,≤) and the least squares operations, and are sometimes\n",
      "referred to as the equivalent kernel . Figure 6.4 illustrates the eﬀect of lo-\n",
      "cal linear regression on the equivalent kernel. Historically, the bias in the\n",
      "Nadaraya–Watson and other local average kernel methods were corrected\n",
      "by modifying the kernel. These modiﬁcations were based on theoretical\n",
      "asymptotic mean-square-error considerations, and besides being tedious to\n",
      "implement, are only approximate for ﬁnite sample sizes. Local linear re-\n",
      "gression automatically modiﬁes the kernel to correct the bias exactly to\n",
      "ﬁrst order, a phenomenon dubbed as automatic kernel carpentry . Consider\n",
      "the following expansion for E ˆf(x0), using the linearity of local regression\n",
      "and a series expansion of the true function faround x0,\n",
      "Eˆf(x0) =N∑\n",
      "i=1li(x0)f(xi)\n",
      "=f(x0)N∑\n",
      "i=1li(x0) +f′(x0)N∑\n",
      "i=1(xi−x0)li(x0)\n",
      "+f′′(x0)\n",
      "2N∑\n",
      "i=1(xi−x0)2li(x0) +R, (6.10)\n",
      "where the remainder term Rinvolves third- and higher-order derivatives of\n",
      "f, and is typically small under suitable smoothness assumptions. It can be\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.1 One-Dimensional Kernel Smoothers 197\n",
      "Local Linear in Interior\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOOOO\n",
      "OOO\n",
      "O\n",
      "O\n",
      "O\n",
      "OOO\n",
      "OOOO\n",
      "OOOO\n",
      "OOOOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOOOO\n",
      "O•ˆf(x0)Local Quadratic in Interior\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.5 0.0 0.5 1.0 1.5OOO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOOOO\n",
      "OOO\n",
      "O\n",
      "O\n",
      "O\n",
      "OOO\n",
      "OOOO\n",
      "OOOO\n",
      "OOOOO\n",
      "OO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "O\n",
      "OO\n",
      "OO\n",
      "OOO\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OOO\n",
      "OO\n",
      "OOO\n",
      "OO\n",
      "OO\n",
      "O\n",
      "OO\n",
      "O\n",
      "OOOOO\n",
      "O•ˆf(x0)\n",
      "FIGURE 6.5. Local linear ﬁts exhibit bias in regions of curvature of the true\n",
      "function. Local quadratic ﬁts tend to eliminate this bias.\n",
      "shown (Exercise 6.2) that for local linear regression,∑N\n",
      "i=1li(x0) = 1 and∑N\n",
      "i=1(xi−x0)li(x0) = 0. Hence the middle term equals f(x0), and since\n",
      "the bias is E ˆf(x0)−f(x0), we see that it depends only on quadratic and\n",
      "higher–order terms in the expansion of f.\n",
      "6.1.2 Local Polynomial Regression\n",
      "Why stop at local linear ﬁts? We can ﬁt local polynomial ﬁts of any de-\n",
      "greed,\n",
      "min\n",
      "α(x0),βj(x0), j=1,...,dN∑\n",
      "i=1Kλ(x0,xi)\n",
      "yi−α(x0)−d∑\n",
      "j=1βj(x0)xj\n",
      "i\n",
      "2\n",
      "(6.11)\n",
      "with solution ˆf(x0) = ˆα(x0)+∑d\n",
      "j=1ˆβj(x0)xj\n",
      "0. In fact, an expansion such as\n",
      "(6.10) will tell us that the bias will only have components of degree d+1 and\n",
      "higher (Exercise 6.2). Figure 6.5 illustrates local quadratic regression. Local\n",
      "linear ﬁts tend to be biased in regions of curvature of the true function, a\n",
      "phenomenon referred to as trimming the hills andﬁlling the valleys . Local\n",
      "quadratic regression is generally able to correct this bias.\n",
      "There is of course a price to be paid for this bias reduction, and that is\n",
      "increased variance. The ﬁt in the right panel of Figure 6.5 is slightly more\n",
      "wiggly, especially in the tails. Assuming the model yi=f(xi) +εi, with\n",
      "εiindependent and identically distributed with mean zero and variance\n",
      "σ2, Var( ˆf(x0)) =σ2||l(x0)||2, where l(x0) is the vector of equivalent kernel\n",
      "weights at x0. It can be shown (Exercise 6.3) that ||l(x0)||increases with d,\n",
      "and so there is a bias–variance tradeoﬀ in selecting the polynomial degree.\n",
      "Figure 6.6 illustrates these variance curves for degree zero, one and two\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "198 6. Kernel Smoothing Methods\n",
      "Variance\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5Constant\n",
      "Linear\n",
      "Quadratic\n",
      "FIGURE 6.6. The variances functions ||l(x)||2for local constant, linear and\n",
      "quadratic regression, for a metric bandwidth ( λ= 0.2) tri-cube kernel.\n",
      "local polynomials. To summarize some collected wisdom on this issue:\n",
      "•Local linear ﬁts can help bias dramatically at the boundaries at a\n",
      "modest cost in variance. Local quadratic ﬁts do little at the bound-\n",
      "aries for bias, but increase the variance a lot.\n",
      "•Local quadratic ﬁts tend to be most helpful in reducing bias due to\n",
      "curvature in the interior of the domain.\n",
      "•Asymptotic analysis suggest that local polynomials of odd degree\n",
      "dominate those of even degree. This is largely due to the fact that\n",
      "asymptotically the MSE is dominated by boundary eﬀects.\n",
      "While it may be helpful to tinker, and move from local linear ﬁts at the\n",
      "boundary to local quadratic ﬁts in the interior, we do not recommend such\n",
      "strategies. Usually the application will dictate the degree of the ﬁt. For\n",
      "example, if we are interested in extrapolation, then the boundary is of\n",
      "more interest, and local linear ﬁts are probably more reliable.\n",
      "6.2 Selecting the Width of the Kernel\n",
      "In each of the kernels Kλ,λis a parameter that controls its width:\n",
      "•For the Epanechnikov or tri-cube kernel with metric width, λis the\n",
      "radius of the support region.\n",
      "•For the Gaussian kernel, λis the standard deviation.\n",
      "•λis the number kof nearest neighbors in k-nearest neighborhoods,\n",
      "often expressed as a fraction or spank/Nof the total training sample.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.2 Selecting the Width of the Kernel 199\n",
      "•••\n",
      "••••••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••••\n",
      "••••• ••••••• •••• •• ••• ••••• • • ••••• •••• ••• • ••••• •••••••• • • • •• •• •••• • ••• • ••••••••••••••••\n",
      "••\n",
      "••\n",
      "••••••\n",
      "••\n",
      "•• ••••••• •••• •• ••• •••••• • ••••• ••••••• • ••••• ••••••••• • • •• •• •••• • ••• • •••••• •••••• •• ••••• •• •• • •••• •• •• ••••••••••••••••••••• •• ••••• •••••\n",
      "•\n",
      "••••••• ••••••••• • • •• •• •••• • ••• • •••••• •••••• ••••••• •• •• • •••••• ••••••••••••••••••••••• • • •••••••••••\n",
      "•••••••••••••••• •• •• •• •••• •••• • ••••••\n",
      "FIGURE 6.7. Equivalent kernels for a local linear regression smoother (tri -cube\n",
      "kernel; orange) and a smoothing spline (blue), with matching degre es of freedom.\n",
      "The vertical spikes indicates the target points.\n",
      "There is a natural bias–variance tradeoﬀ as we change the width of the\n",
      "averaging window, which is most explicit for local averages:\n",
      "•If the window is narrow, ˆf(x0) is an average of a small number of yi\n",
      "close to x0, and its variance will be relatively large—close to that of\n",
      "an individual yi. The bias will tend to be small, again because each\n",
      "of the E(yi) =f(xi) should be close to f(x0).\n",
      "•If the window is wide, the variance of ˆf(x0) will be small relative to\n",
      "the variance of any yi, because of the eﬀects of averaging. The bias\n",
      "will be higher, because we are now using observations xifurther from\n",
      "x0, and there is no guarantee that f(xi) will be close to f(x0).\n",
      "Similar arguments apply to local regression estimates, say local linear: a s\n",
      "the width goes to zero, the estimates approach a piecewise-linear function\n",
      "that interpolates the training data1; as the width gets inﬁnitely large, the\n",
      "ﬁt approaches the global linear least-squares ﬁt to the data.\n",
      "The discussion in Chapter 5 on selecting the regularization parameter for\n",
      "smoothing splines applies here, and will not be repeated. Local regression\n",
      "smoothers are linear estimators; the smoother matrix in ˆf=Sλyis built up\n",
      "from the equivalent kernels (6.8), and has ijth entry {Sλ}ij=li(xj). Leave-\n",
      "one-out cross-validation is particularly simple (Exercise 6.7), as is genera l-\n",
      "ized cross-validation, Cp(Exercise 6.10), and k-fold cross-validation. The\n",
      "eﬀective degrees of freedom is again deﬁned as trace( Sλ), and can be used\n",
      "to calibrate the amount of smoothing. Figure 6.7 compares the equivalent\n",
      "kernels for a smoothing spline and local linear regression. The local regres-\n",
      "sion smoother has a span of 40%, which results in df = trace( Sλ) = 5.86.\n",
      "The smoothing spline was calibrated to have the same df, and their equiv-\n",
      "alent kernels are qualitatively quite similar.\n",
      "1With uniformly spaced xi; with irregularly spaced xi, the behavior can deteriorate.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "200 6. Kernel Smoothing Methods\n",
      "6.3 Local Regression in I Rp\n",
      "Kernel smoothing and local regression generalize very naturally to two or\n",
      "more dimensions. The Nadaraya–Watson kernel smoother ﬁts a constant\n",
      "locally with weights supplied by a p-dimensional kernel. Local linear re-\n",
      "gression will ﬁt a hyperplane locally in X, by weighted least squares, with\n",
      "weights supplied by a p-dimensional kernel. It is simple to implement and\n",
      "is generally preferred to the local constant ﬁt for its superior performance\n",
      "on the boundaries.\n",
      "Letb(X) be a vector of polynomial terms in Xof maximum degree d.\n",
      "For example, with d= 1 and p= 2 we get b(X) = (1 ,X1,X2); with d= 2\n",
      "we get b(X) = (1 ,X1,X2,X2\n",
      "1,X2\n",
      "2,X1X2); and trivially with d= 0 we get\n",
      "b(X) = 1. At each x0∈IRpsolve\n",
      "min\n",
      "β(x0)N∑\n",
      "i=1Kλ(x0,xi)(yi−b(xi)Tβ(x0))2(6.12)\n",
      "to produce the ﬁt ˆf(x0) =b(x0)Tˆβ(x0). Typically the kernel will be a radial\n",
      "function, such as the radial Epanechnikov or tri-cube kernel\n",
      "Kλ(x0,x) =D(||x−x0||\n",
      "λ)\n",
      ", (6.13)\n",
      "where ||≤||is the Euclidean norm. Since the Euclidean norm depends on the\n",
      "units in each coordinate, it makes most sense to standardize each predictor,\n",
      "for example, to unit standard deviation, prior to smoothing.\n",
      "While boundary eﬀects are a problem in one-dimensional smoothing,\n",
      "they are a much bigger problem in two or higher dimensions, since the\n",
      "fraction of points on the boundary is larger. In fact, one of the manifesta -\n",
      "tions of the curse of dimensionality is that the fraction of points close to the\n",
      "boundary increases to one as the dimension grows. Directly modifying the\n",
      "kernel to accommodate two-dimensional boundaries becomes very messy,\n",
      "especially for irregular boundaries. Local polynomial regression seamless ly\n",
      "performs boundary correction to the desired order in any dimensions. Fig-\n",
      "ure 6.8 illustrates local linear regression on some measurements from an\n",
      "astronomical study with an unusual predictor design (star-shaped). Here\n",
      "the boundary is extremely irregular, and the ﬁtted surface must also inter-\n",
      "polate over regions of increasing data sparsity as we approach the boundary.\n",
      "Local regression becomes less useful in dimensions much higher than two\n",
      "or three. We have discussed in some detail the problems of dimensional-\n",
      "ity, for example, in Chapter 2. It is impossible to simultaneously main-\n",
      "tain localness ( ⇒low bias) and a sizable sample in the neighborhood ( ⇒\n",
      "low variance) as the dimension increases, without the total sample size in-\n",
      "creasing exponentially in p. Visualization of ˆf(X) also becomes diﬃcult in\n",
      "higher dimensions, and this is often one of the primary goals of smoothing.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.4 Structured Local Regression Models in I Rp201\n",
      "East-WestSouth-NorthVelocity\n",
      "East-WestSouth-NorthVelocity\n",
      "FIGURE 6.8. The left panel shows three-dimensional data, where the response\n",
      "is the velocity measurements on a galaxy, and the two predictors record positions\n",
      "on the celestial sphere. The unusual “star”-shaped design ind icates the way the\n",
      "measurements were made, and results in an extremely irregular b oundary. The\n",
      "right panel shows the results of local linear regression smoot hing in I R2, using a\n",
      "nearest-neighbor window with 15%of the data.\n",
      "Although the scatter-cloud and wire-frame pictures in Figure 6.8 look at-\n",
      "tractive, it is quite diﬃcult to interpret the results except at a gross level.\n",
      "From a data analysis perspective, conditional plots are far more useful.\n",
      "Figure 6.9 shows an analysis of some environmental data with three pre-\n",
      "dictors. The trellis display here shows ozone as a function of radiation,\n",
      "conditioned on the other two variables, temperature and wind speed. How-\n",
      "ever, conditioning on the value of a variable really implies local to that\n",
      "value (as in local regression). Above each of the panels in Figure 6.9 is an\n",
      "indication of the range of values present in that panel for each of the condi-\n",
      "tioning values. In the panel itself the data subsets are displayed (response\n",
      "versus remaining variable), and a one-dimensional local linear regression is\n",
      "ﬁt to the data. Although this is not quite the same as looking at slices of\n",
      "a ﬁtted three-dimensional surface, it is probably more useful in terms of\n",
      "understanding the joint behavior of the data.\n",
      "6.4 Structured Local Regression Models in I Rp\n",
      "When the dimension to sample-size ratio is unfavorable, local regression\n",
      "does not help us much, unless we are willing to make some structural as-\n",
      "sumptions about the model. Much of this book is about structured regres-\n",
      "sion and classiﬁcation models. Here we focus on some approaches directly\n",
      "related to kernel methods.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "202 6. Kernel Smoothing Methods\n",
      "12345TempWind\n",
      "0 50 150 250TempWind\n",
      "TempWind\n",
      "0 50 150 250TempWindTempWind\n",
      "TempWind\n",
      "TempWind\n",
      "12345TempWind12345TempWind\n",
      "TempWind\n",
      "TempWind\n",
      "TempWindTempWind\n",
      "TempWind0 50 150 250\n",
      "TempWind\n",
      "12345TempWind0 50 150 250\n",
      "Solar Radiation (langleys)Cube Root Ozone (cube root ppb)\n",
      "FIGURE 6.9. Three-dimensional smoothing example. The response is (cube-roo t\n",
      "of) ozone concentration, and the three predictors are temperatur e, wind speed and\n",
      "radiation. The trellis display shows ozone as a function of radiation, conditioned\n",
      "on intervals of temperature and wind speed (indicated by darker g reen or orange\n",
      "shaded bars). Each panel contains about 40%of the range of each of the condi-\n",
      "tioned variables. The curve in each panel is a univariate local l inear regression,\n",
      "ﬁt to the data in the panel.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.4 Structured Local Regression Models in I Rp203\n",
      "6.4.1 Structured Kernels\n",
      "One line of approach is to modify the kernel. The default spherical ker-\n",
      "nel (6.13) gives equal weight to each coordinate, and so a natural default\n",
      "strategy is to standardize each variable to unit standard deviation. A more\n",
      "general approach is to use a positive semideﬁnite matrix Ato weigh the\n",
      "diﬀerent coordinates:\n",
      "Kλ,A(x0,x) =D((x−x0)TA(x−x0)\n",
      "λ)\n",
      ". (6.14)\n",
      "Entire coordinates or directions can be downgraded or omitted by imposing\n",
      "appropriate restrictions on A. For example, if Ais diagonal, then we can\n",
      "increase or decrease the inﬂuence of individual predictors Xjby increasing\n",
      "or decreasing Ajj. Often the predictors are many and highly correlated,\n",
      "such as those arising from digitized analog signals or images. The covari ance\n",
      "function of the predictors can be used to tailor a metric Athat focuses less,\n",
      "say, on high-frequency contrasts (Exercise 6.4). Proposals have been made\n",
      "for learning the parameters for multidimensional kernels. For example, the\n",
      "projection-pursuit regression model discussed in Chapter 11 is of this ﬂavor,\n",
      "where low-rank versions of Aimply ridge functions for ˆf(X). More general\n",
      "models for Aare cumbersome, and we favor instead the structured forms\n",
      "for the regression function discussed next.\n",
      "6.4.2 Structured Regression Functions\n",
      "We are trying to ﬁt a regression function E(Y|X) =f(X1,X2,... ,X p) in\n",
      "IRp, in which every level of interaction is potentially present. It is natural\n",
      "to consider analysis-of-variance (ANOVA) decompositions of the form\n",
      "f(X1,X2,... ,X p) =α+∑\n",
      "jgj(Xj) +∑\n",
      "k<ℓgkℓ(Xk,Xℓ) +≤≤≤ (6.15)\n",
      "and then introduce structure by eliminating some of the higher-order terms.\n",
      "Additive models assume only main eﬀect terms: f(X) =α+∑p\n",
      "j=1gj(Xj);\n",
      "second-order models will have terms with interactions of order at most\n",
      "two, and so on. In Chapter 9, we describe iterative backﬁtting algorithms\n",
      "for ﬁtting such low-order interaction models. In the additive model, for\n",
      "example, if all but the kth term is assumed known, then we can estimate gk\n",
      "by local regression of Y−∑\n",
      "j̸=kgj(Xj) onXk. This is done for each function\n",
      "in turn, repeatedly, until convergence. The important detail is that at any\n",
      "stage, one-dimensional local regression is all that is needed. The same ideas\n",
      "can be used to ﬁt low-dimensional ANOVA decompositions.\n",
      "An important special case of these structured models are the class of\n",
      "varying coeﬃcient models . Suppose, for example, that we divide the ppre-\n",
      "dictors in Xinto a set ( X1,X2,... ,X q) with q < p, and the remainder of\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "204 6. Kernel Smoothing Methods\n",
      "1012141618202224DepthFemale\n",
      "20 30 40 50 60DepthFemale\n",
      "DepthFemale\n",
      "20 30 40 50 60DepthFemale\n",
      "DepthFemale\n",
      "20 30 40 50 60DepthFemaleDepthMale\n",
      "DepthMale20 30 40 50 60\n",
      "DepthMale\n",
      "DepthMale20 30 40 50 60\n",
      "DepthMale\n",
      "1012141618202224DepthMale20 30 40 50 60\n",
      "AgeDiameterAortic Diameter vs Age\n",
      "FIGURE 6.10. In each panel the aorta diameter is modeled as a linear func-\n",
      "tion ofage. The coeﬃcients of this model vary with gender anddepth down\n",
      "theaorta (left is near the top, right is low down). There is a clear trend in the\n",
      "coeﬃcients of the linear model.\n",
      "the variables we collect in the vector Z. We then assume the conditionally\n",
      "linear model\n",
      "f(X) =α(Z) +β1(Z)X1+≤≤≤+βq(Z)Xq. (6.16)\n",
      "For given Z, this is a linear model, but each of the coeﬃcients can vary\n",
      "withZ. It is natural to ﬁt such a model by locally weighted least squares:\n",
      "min\n",
      "α(z0),β(z0)N∑\n",
      "i=1Kλ(z0,zi)(yi−α(z0)−x1iβ1(z0)− ≤≤≤ − xqiβq(z0))2.\n",
      "(6.17)\n",
      "Figure 6.10 illustrates the idea on measurements of the human aorta.\n",
      "A longstanding claim has been that the aorta thickens with age. Here we\n",
      "model the diameter of the aorta as a linear function of age, but allow the\n",
      "coeﬃcients to vary with gender anddepth down the aorta. We used a local\n",
      "regression model separately for males and females. While the aorta clearly\n",
      "does thicken with age at the higher regions of the aorta, the relationship\n",
      "fades with distance down the aorta. Figure 6.11 shows the intercept and\n",
      "slope as a function of depth.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.5 Local Likelihood and Other Models 205\n",
      "MaleAge Intercept\n",
      "Distance Down AortaAge Slope\n",
      "0.0 0.2 0.4 0.6 0.8 1.0Female\n",
      "14 16 18 20\n",
      "Distance Down Aorta\n",
      "0.0 0.4 0.8 1.2\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "FIGURE 6.11. The intercept and slope of ageas a function of distance down\n",
      "the aorta, separately for males and females. The yellow bands i ndicate one stan-\n",
      "dard error.\n",
      "6.5 Local Likelihood and Other Models\n",
      "The concept of local regression and varying coeﬃcient models is extremely\n",
      "broad: any parametric model can be made local if the ﬁtting method ac-\n",
      "commodates observation weights. Here are some examples:\n",
      "•Associated with each observation yiis a parameter θi=θ(xi) =xT\n",
      "iβ\n",
      "linear in the covariate(s) xi, and inference for βis based on the log-\n",
      "likelihood l(β) =∑N\n",
      "i=1l(yi,xT\n",
      "iβ). We can model θ(X) more ﬂexibly\n",
      "by using the likelihood local to x0for inference of θ(x0) =xT\n",
      "0β(x0):\n",
      "l(β(x0)) =N∑\n",
      "i=1Kλ(x0,xi)l(yi,xT\n",
      "iβ(x0)).\n",
      "Many likelihood models, in particular the family of generalized linear\n",
      "models including logistic and log-linear models, involve the covariates\n",
      "in a linear fashion. Local likelihood allows a relaxation from a global ly\n",
      "linear model to one that is locally linear.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "206 6. Kernel Smoothing Methods\n",
      "•As above, except diﬀerent variables are associated with θfrom those\n",
      "used for deﬁning the local likelihood:\n",
      "l(θ(z0)) =N∑\n",
      "i=1Kλ(z0,zi)l(yi,η(xi,θ(z0))).\n",
      "For example, η(x,θ) =xTθcould be a linear model in x. This will ﬁt\n",
      "a varying coeﬃcient model θ(z) by maximizing the local likelihood.\n",
      "•Autoregressive time series models of order khave the form yt=\n",
      "β0+β1yt−1+β2yt−2+≤≤≤+βkyt−k+εt. Denoting the lag set by\n",
      "zt= (yt−1,yt−2,... ,y t−k), the model looks like a standard linear\n",
      "model yt=zT\n",
      "tβ+εt, and is typically ﬁt by least squares. Fitting\n",
      "by local least squares with a kernel K(z0,zt) allows the model to\n",
      "vary according to the short-term history of the series. This is to be\n",
      "distinguished from the more traditional dynamic linear models that\n",
      "vary by windowing time.\n",
      "As an illustration of local likelihood, we consider the local version of the\n",
      "multiclass linear logistic regression model (4.36) of Chapter 4. The data\n",
      "consist of features xiand an associated categorical response gi∈ {1,2,... ,J },\n",
      "and the linear model has the form\n",
      "Pr(G=j|X=x) =eβj0+βT\n",
      "jx\n",
      "1 +∑J−1\n",
      "k=1eβk0+βT\n",
      "kx. (6.18)\n",
      "The local log-likelihood for this Jclass model can be written\n",
      "N∑\n",
      "i=1Kλ(x0,xi){\n",
      "βgi0(x0) +βgi(x0)T(xi−x0)\n",
      "−log[\n",
      "1 +J−1∑\n",
      "k=1exp(\n",
      "βk0(x0) +βk(x0)T(xi−x0))]}\n",
      ".\n",
      "(6.19)\n",
      "Notice that\n",
      "•we have used gias a subscript in the ﬁrst line to pick out the appro-\n",
      "priate numerator;\n",
      "•βJ0= 0 and βJ= 0 by the deﬁnition of the model;\n",
      "•we have centered the local regressions at x0, so that the ﬁtted poste-\n",
      "rior probabilities at x0are simply\n",
      "ˆPr(G=j|X=x0) =eˆβj0(x0)\n",
      "1 +∑J−1\n",
      "k=1eˆβk0(x0). (6.20)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.5 Local Likelihood and Other Models 207\n",
      "Systolic Blood PressurePrevalence CHD\n",
      "100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0\n",
      "ObesityPrevalence CHD\n",
      "15 25 35 450.0 0.2 0.4 0.6 0.8 1.0\n",
      "FIGURE 6.12. Each plot shows the binary response CHD (coronary heart dis-\n",
      "ease) as a function of a risk factor for the South African heart d isease data.\n",
      "For each plot we have computed the ﬁtted prevalence of CHD using a local linear\n",
      "logistic regression model. The unexpected increase in the prev alence of CHD at\n",
      "the lower ends of the ranges is because these are retrospective data, and some of\n",
      "the subjects had already undergone treatment to reduce their bl ood pressure and\n",
      "weight. The shaded region in the plot indicates an estimated p ointwise standard\n",
      "error band.\n",
      "This model can be used for ﬂexible multiclass classiﬁcation in moderately\n",
      "low dimensions, although successes have been reported with the high-\n",
      "dimensional ZIP-code classiﬁcation problem. Generalized additive models\n",
      "(Chapter 9) using kernel smoothing methods are closely related, and avoid\n",
      "dimensionality problems by assuming an additive structure for the regres-\n",
      "sion function.\n",
      "As a simple illustration we ﬁt a two-class local linear logistic model to\n",
      "the heart disease data of Chapter 4. Figure 6.12 shows the univariate local\n",
      "logistic models ﬁt to two of the risk factors (separately). This is a useful\n",
      "screening device for detecting nonlinearities, when the data themselves have\n",
      "little visual information to oﬀer. In this case an unexpected anomaly is\n",
      "uncovered in the data, which may have gone unnoticed with traditional\n",
      "methods.\n",
      "SinceCHDis a binary indicator, we could estimate the conditional preva-\n",
      "lence Pr( G=j|x0) by simply smoothing this binary response directly with-\n",
      "out resorting to a likelihood formulation. This amounts to ﬁtting a locall y\n",
      "constant logistic regression model (Exercise 6.5). In order to enjoy the bia s-\n",
      "correction of local-linear smoothing, it is more natural to operate on the\n",
      "unrestricted logit scale.\n",
      "Typically with logistic regression, we compute parameter estimates as\n",
      "well as their standard errors. This can be done locally as well, and so\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "208 6. Kernel Smoothing Methods\n",
      "Systolic Blood Pressure (for CHD group)Density Estimate\n",
      "100 120 140 160 180 200 2200.0 0.005 0.010 0.015 0.020\n",
      "FIGURE 6.13. A kernel density estimate for systolic blood pressure (for the\n",
      "CHD group). The density estimate at each point is the average co ntribution from\n",
      "each of the kernels at that point. We have scaled the kernels down by a factor of\n",
      "10 to make the graph readable.\n",
      "we can produce, as shown in the plot, estimated pointwise standard-error\n",
      "bands about our ﬁtted prevalence.\n",
      "6.6 Kernel Density Estimation and Classiﬁcation\n",
      "Kernel density estimation is an unsupervised learning procedure, which\n",
      "historically precedes kernel regression. It also leads naturally to a simple\n",
      "family of procedures for nonparametric classiﬁcation.\n",
      "6.6.1 Kernel Density Estimation\n",
      "Suppose we have a random sample x1,... ,x Ndrawn from a probability\n",
      "density fX(x), and we wish to estimate fXat a point x0. For simplicity we\n",
      "assume for now that X∈IR. Arguing as before, a natural local estimate\n",
      "has the form\n",
      "ˆfX(x0) =#xi∈ N(x0)\n",
      "Nλ, (6.21)\n",
      "where N(x0) is a small metric neighborhood around x0of width λ. This\n",
      "estimate is bumpy, and the smooth Parzen estimate is preferred\n",
      "ˆfX(x0) =1\n",
      "NλN∑\n",
      "i=1Kλ(x0,xi), (6.22)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.6 Kernel Density Estimation and Classiﬁcation 209\n",
      "Systolic Blood PressureDensity Estimates\n",
      "100 140 180 2200.0 0.010 0.020CHD\n",
      "no CHD\n",
      "Systolic Blood PressurePosterior Estimate\n",
      "100 140 180 2200.0 0.2 0.4 0.6 0.8 1.0\n",
      "FIGURE 6.14. The left panel shows the two separate density estimates for\n",
      "systolic blood pressure in the CHD versus no-CHD groups, using a Gaussian\n",
      "kernel density estimate in each. The right panel shows the estim ated posterior\n",
      "probabilities for CHD, using (6.25).\n",
      "because it counts observations close to x0with weights that decrease with\n",
      "distance from x0. In this case a popular choice for Kλis the Gaussian kernel\n",
      "Kλ(x0,x) =φ(|x−x0|/λ). Figure 6.13 shows a Gaussian kernel density ﬁt\n",
      "to the sample values for systolic blood pressure for theCHDgroup. Letting\n",
      "φλdenote the Gaussian density with mean zero and standard-deviation λ,\n",
      "then (6.22) has the form\n",
      "ˆfX(x) =1\n",
      "NN∑\n",
      "i=1φλ(x−xi)\n",
      "= (ˆF ⋆ φ λ)(x), (6.23)\n",
      "the convolution of the sample empirical distribution ˆFwithφλ. The dis-\n",
      "tribution ˆF(x) puts mass 1 /Nat each of the observed xi, and is jumpy; in\n",
      "ˆfX(x) we have smoothed ˆFby adding independent Gaussian noise to each\n",
      "observation xi.\n",
      "The Parzen density estimate is the equivalent of the local average, and\n",
      "improvements have been proposed along the lines of local regression [on the\n",
      "log scale for densities; see Loader (1999)]. We will not pursue these here.\n",
      "In IRpthe natural generalization of the Gaussian density estimate amounts\n",
      "to using the Gaussian product kernel in (6.23),\n",
      "ˆfX(x0) =1\n",
      "N(2λ2π)p\n",
      "2N∑\n",
      "i=1e−1\n",
      "2(||xi−x0||/λ)2. (6.24)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "210 6. Kernel Smoothing Methods\n",
      "0.00.51.0\n",
      "FIGURE 6.15. The population class densities may have interesting structure\n",
      "(left) that disappears when the posterior probabilities ar e formed (right).\n",
      "6.6.2 Kernel Density Classiﬁcation\n",
      "One can use nonparametric density estimates for classiﬁcation in a straight-\n",
      "forward fashion using Bayes’ theorem. Suppose for a Jclass problem we ﬁt\n",
      "nonparametric density estimates ˆfj(X), j= 1,... ,J separately in each of\n",
      "the classes, and we also have estimates of the class priors ˆ πj(usually the\n",
      "sample proportions). Then\n",
      "ˆPr(G=j|X=x0) =ˆπjˆfj(x0)∑J\n",
      "k=1ˆπkˆfk(x0). (6.25)\n",
      "Figure 6.14 uses this method to estimate the prevalence of CHD for the\n",
      "heart risk factor study, and should be compared with the left panel of Fig-\n",
      "ure 6.12. The main diﬀerence occurs in the region of high SBP in the right\n",
      "panel of Figure 6.14. In this region the data are sparse for both classes, a nd\n",
      "since the Gaussian kernel density estimates use metric kernels, the density\n",
      "estimates are low and of poor quality (high variance) in these regions. The\n",
      "local logistic regression method (6.20) uses the tri-cube kernel with k-NN\n",
      "bandwidth; this eﬀectively widens the kernel in this region, and makes use\n",
      "of the local linear assumption to smooth out the estimate (on the logit\n",
      "scale).\n",
      "If classiﬁcation is the ultimate goal, then learning the separate class den-\n",
      "sities well may be unnecessary, and can in fact be misleading. Figure 6.15\n",
      "shows an example where the densities are both multimodal, but the pos-\n",
      "terior ratio is quite smooth. In learning the separate densities from data,\n",
      "one might decide to settle for a rougher, high-variance ﬁt to capture these\n",
      "features, which are irrelevant for the purposes of estimating the posterior\n",
      "probabilities. In fact, if classiﬁcation is the ultimate goal, then we need onl y\n",
      "to estimate the posterior well near the decision boundary (for two classes,\n",
      "this is the set {x|Pr(G= 1|X=x) =1\n",
      "2}).\n",
      "6.6.3 The Naive Bayes Classiﬁer\n",
      "This is a technique that has remained popular over the years, despite its\n",
      "name (also known as “Idiot’s Bayes”!) It is especially appropriate when\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.6 Kernel Density Estimation and Classiﬁcation 211\n",
      "the dimension pof the feature space is high, making density estimation\n",
      "unattractive. The naive Bayes model assumes that given a class G=j, the\n",
      "features Xkare independent:\n",
      "fj(X) =p∏\n",
      "k=1fjk(Xk). (6.26)\n",
      "While this assumption is generally not true, it does simplify the estimation\n",
      "dramatically:\n",
      "•The individual class-conditional marginal densities fjkcan each be\n",
      "estimated separately using one-dimensional kernel density estimates.\n",
      "This is in fact a generalization of the original naive Bayes procedures,\n",
      "which used univariate Gaussians to represent these marginals.\n",
      "•If a component XjofXis discrete, then an appropriate histogram\n",
      "estimate can be used. This provides a seamless way of mixing variable\n",
      "types in a feature vector.\n",
      "Despite these rather optimistic assumptions, naive Bayes classiﬁers often\n",
      "outperform far more sophisticated alternatives. The reasons are related to\n",
      "Figure 6.15: although the individual class density estimates may be biased,\n",
      "this bias might not hurt the posterior probabilities as much, especially\n",
      "near the decision regions. In fact, the problem may be able to withstand\n",
      "considerable bias for the savings in variance such a “naive” assumption\n",
      "earns.\n",
      "Starting from (6.26) we can derive the logit-transform (using class Jas\n",
      "the base):\n",
      "logPr(G=ℓ|X)\n",
      "Pr(G=J|X)= logπℓfℓ(X)\n",
      "πJfJ(X)\n",
      "= logπℓ∏p\n",
      "k=1fℓk(Xk)\n",
      "πJ∏p\n",
      "k=1fJk(Xk)\n",
      "= logπℓ\n",
      "πJ+p∑\n",
      "k=1logfℓk(Xk)\n",
      "fJk(Xk)\n",
      "=αℓ+p∑\n",
      "k=1gℓk(Xk).(6.27)\n",
      "This has the form of a generalized additive model , which is described in more\n",
      "detail in Chapter 9. The models are ﬁt in quite diﬀerent ways though; their\n",
      "diﬀerences are explored in Exercise 6.9. The relationship between naive\n",
      "Bayes and generalized additive models is analogous to that between linear\n",
      "discriminant analysis and logistic regression (Section 4.4.5).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "212 6. Kernel Smoothing Methods\n",
      "6.7 Radial Basis Functions and Kernels\n",
      "In Chapter 5, functions are represented as expansions in basis functions:\n",
      "f(x) =∑M\n",
      "j=1βjhj(x). The art of ﬂexible modeling using basis expansions\n",
      "consists of picking an appropriate family of basis functions, and then con-\n",
      "trolling the complexity of the representation by selection, regularization, or\n",
      "both. Some of the families of basis functions have elements that are deﬁned\n",
      "locally; for example, B-splines are deﬁned locally in IR. If more ﬂexibility\n",
      "is desired in a particular region, then that region needs to be represented\n",
      "by more basis functions (which in the case of B-splines translates to more\n",
      "knots). Tensor products of IR-local basis functions deliver basis functions\n",
      "local in IRp. Not all basis functions are local—for example, the truncated\n",
      "power bases for splines, or the sigmoidal basis functions σ(α0+αx) used\n",
      "in neural-networks (see Chapter 11). The composed function f(x) can nev-\n",
      "ertheless show local behavior, because of the particular signs and values\n",
      "of the coeﬃcients causing cancellations of global eﬀects. For example, the\n",
      "truncated power basis has an equivalent B-spline basis for the same space\n",
      "of functions; the cancellation is exact in this case.\n",
      "Kernel methods achieve ﬂexibility by ﬁtting simple models in a region\n",
      "local to the target point x0. Localization is achieved via a weighting kernel\n",
      "Kλ, and individual observations receive weights Kλ(x0,xi).\n",
      "Radial basis functions combine these ideas, by treating the kernel func-\n",
      "tionsKλ(ξ,x) as basis functions. This leads to the model\n",
      "f(x) =M∑\n",
      "j=1Kλj(ξj,x)βj\n",
      "=M∑\n",
      "j=1D(||x−ξj||\n",
      "λj)\n",
      "βj, (6.28)\n",
      "where each basis element is indexed by a location or prototype parameter ξj\n",
      "and a scale parameter λj. A popular choice for Dis the standard Gaussian\n",
      "density function. There are several approaches to learning the parameters\n",
      "{λj,ξj,βj}, j= 1,... ,M . For simplicity we will focus on least squares\n",
      "methods for regression, and use the Gaussian kernel.\n",
      "•Optimize the sum-of-squares with respect to all the parameters:\n",
      "min\n",
      "{λj,ξj,βj}M\n",
      "1N∑\n",
      "i=1\n",
      "yi−β0−M∑\n",
      "j=1βjexp{\n",
      "−(xi−ξj)T(xi−ξj)\n",
      "λ2\n",
      "j}\n",
      "2\n",
      ".\n",
      "(6.29)\n",
      "This model is commonly referred to as an RBF network, an alterna-\n",
      "tive to the sigmoidal neural network discussed in Chapter 11; the ξj\n",
      "andλjplaying the role of the weights. This criterion is nonconvex\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.7 Radial Basis Functions and Kernels 2130 2 4 6 8 0.0 0.4 0.8 1.2\n",
      "FIGURE 6.16. Gaussian radial basis functions in I Rwith ﬁxed width can leave\n",
      "holes (top panel). Renormalized Gaussian radial basis functio ns avoid this prob-\n",
      "lem, and produce basis functions similar in some respects to B-splines.\n",
      "with multiple local minima, and the algorithms for optimization are\n",
      "similar to those used for neural networks.\n",
      "•Estimate the {λj,ξj}separately from the βj. Given the former, the\n",
      "estimation of the latter is a simple least squares problem. Often the\n",
      "kernel parameters λjandξjare chosen in an unsupervised way using\n",
      "theXdistribution alone. One of the methods is to ﬁt a Gaussian\n",
      "mixture density model to the training xi, which provides both the\n",
      "centers ξjand the scales λj. Other even more adhoc approaches use\n",
      "clustering methods to locate the prototypes ξj, and treat λj=λ\n",
      "as a hyper-parameter. The obvious drawback of these approaches is\n",
      "that the conditional distribution Pr( Y|X) and in particular E(Y|X)\n",
      "is having no say in where the action is concentrated. On the positive\n",
      "side, they are much simpler to implement.\n",
      "While it would seem attractive to reduce the parameter set and assume\n",
      "a constant value for λj=λ, this can have an undesirable side eﬀect of\n",
      "creating holes—regions of IRpwhere none of the kernels has appreciable\n",
      "support, as illustrated in Figure 6.16 (upper panel). Renormalized radial\n",
      "basis functions,\n",
      "hj(x) =D(||x−ξj||/λ)∑M\n",
      "k=1D(||x−ξk||/λ), (6.30)\n",
      "avoid this problem (lower panel).\n",
      "The Nadaraya–Watson kernel regression estimator (6.2) in IRpcan be\n",
      "viewed as an expansion in renormalized radial basis functions,\n",
      "ˆf(x0) =∑N\n",
      "i=1yiKλ(x0,xi)PN\n",
      "i=1Kλ(x0,xi)\n",
      "=∑N\n",
      "i=1yihi(x0) (6.31)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "214 6. Kernel Smoothing Methods\n",
      "with a basis function hilocated at every observation and coeﬃcients yi;\n",
      "that is, ξi=xi,ˆβi=yi, i= 1,... ,N .\n",
      "Note the similarity between the expansion (6.31) and the solution (5.50)\n",
      "on page 169 to the regularization problem induced by the kernel K. Radial\n",
      "basis functions form the bridge between the modern “kernel methods” and\n",
      "local ﬁtting technology.\n",
      "6.8 Mixture Models for Density Estimation and\n",
      "Classiﬁcation\n",
      "The mixture model is a useful tool for density estimation, and can be viewed\n",
      "as a kind of kernel method. The Gaussian mixture model has the form\n",
      "f(x) =M∑\n",
      "m=1αmφ(x;θm,Σm) (6.32)\n",
      "with mixing proportions αm,∑\n",
      "mαm= 1, and each Gaussian density has\n",
      "a mean θmand covariance matrix Σm. In general, mixture models can use\n",
      "any component densities in place of the Gaussian in (6.32): the Gaussian\n",
      "mixture model is by far the most popular.\n",
      "The parameters are usually ﬁt by maximum likelihood, using the EM\n",
      "algorithm as described in Chapter 8. Some special cases arise:\n",
      "•If the covariance matrices are constrained to be scalar: Σm=σmI,\n",
      "then (6.32) has the form of a radial basis expansion.\n",
      "•If in addition σm=σ >0 is ﬁxed, and M↑N, then the max-\n",
      "imum likelihood estimate for (6.32) approaches the kernel density\n",
      "estimate (6.22) where ˆ αm= 1/Nand ˆθm=xm.\n",
      "Using Bayes’ theorem, separate mixture densities in each class lead to ﬂex-\n",
      "ible models for Pr( G|X); this is taken up in some detail in Chapter 12.\n",
      "Figure 6.17 shows an application of mixtures to the heart disease risk-\n",
      "factor study. In the top row are histograms of Agefor theno CHD andCHD\n",
      "groups separately, and then combined on the right. Using the combined\n",
      "data, we ﬁt a two-component mixture of the form (6.32) with the (scalars)\n",
      "Σ1andΣ2not constrained to be equal. Fitting was done via the EM\n",
      "algorithm (Chapter 8): note that the procedure does not use knowledge of\n",
      "theCHDlabels. The resulting estimates were\n",
      "ˆθ1= 36.4, ˆΣ1= 157 .7, ˆα1= 0.7,\n",
      "ˆθ2= 58.0, ˆΣ2= 15.6, ˆα2= 0.3.\n",
      "The component densities φ(ˆθ1,ˆΣ1) and φ(ˆθ2,ˆΣ2) are shown in the lower-\n",
      "left and middle panels. The lower-right panel shows these component den-\n",
      "sities (orange and blue) along with the estimated mixture density (green).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "6.8 Mixture Models for Density Estimation and Classiﬁcation 215\n",
      "No CHD\n",
      "AgeCount\n",
      "20 30 40 50 600 5 10 15 20CHD\n",
      "AgeCount\n",
      "20 30 40 50 600 5 10 15Combined\n",
      "AgeCount\n",
      "20 30 40 50 600 5 10 15 20 25 30\n",
      "20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\n",
      "AgeMixture Estimate\n",
      "20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\n",
      "AgeMixture Estimate\n",
      "20 30 40 50 600.00 0.02 0.04 0.06 0.08 0.10\n",
      "AgeMixture Estimate\n",
      "FIGURE 6.17. Application of mixtures to the heart disease risk-factor stu dy.\n",
      "(Top row:) Histograms of Agefor theno CHD andCHDgroups separately, and\n",
      "combined. (Bottom row:) estimated component densities from a Ga ussian mix-\n",
      "ture model, (bottom left, bottom middle); (bottom right:) E stimated component\n",
      "densities (blue and orange) along with the estimated mixture densi ty (green). The\n",
      "orange density has a very large standard deviation, and approximat es a uniform\n",
      "density.\n",
      "The mixture model also provides an estimate of the probability that\n",
      "observation ibelongs to component m,\n",
      "ˆrim=ˆαmφ(xi; ˆθm,ˆΣm)∑M\n",
      "k=1ˆαkφ(xi; ˆθk,ˆΣk), (6.33)\n",
      "where xiisAgein our example. Suppose we threshold each value ˆ ri2and\n",
      "hence deﬁne ˆδi=I(ˆri2>0.5). Then we can compare the classiﬁcation of\n",
      "each observation by CHDand the mixture model:\n",
      "Mixture model\n",
      "ˆδ= 0 ˆδ= 1\n",
      "CHD No 232 70\n",
      "Yes 76 84\n",
      "Although the mixture model did not use the CHDlabels, it has done a fair\n",
      "job in discovering the two CHDsubpopulations. Linear logistic regression,\n",
      "using the CHDas a response, achieves the same error rate (32%) when ﬁt to\n",
      "these data using maximum-likelihood (Section 4.4).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "216 6. Kernel Smoothing Methods\n",
      "6.9 Computational Considerations\n",
      "Kernel and local regression and density estimation are memory-based meth-\n",
      "ods: the model is the entire training data set, and the ﬁtting is done at\n",
      "evaluation or prediction time. For many real-time applications, this can\n",
      "make this class of methods infeasible.\n",
      "The computational cost to ﬁt at a single observation x0isO(N) ﬂops,\n",
      "except in oversimpliﬁed cases (such as square kernels). By comparison,\n",
      "an expansion in Mbasis functions costs O(M) for one evaluation, and\n",
      "typically M∼O(logN). Basis function methods have an initial cost of at\n",
      "leastO(NM2+M3).\n",
      "The smoothing parameter(s) λfor kernel methods are typically deter-\n",
      "mined oﬀ-line, for example using cross-validation, at a cost of O(N2) ﬂops.\n",
      "Popular implementations of local regression, such as the loess function in\n",
      "S-PLUS and Rand the locfit procedure (Loader, 1999), use triangulation\n",
      "schemes to reduce the computations. They compute the ﬁt exactly at M\n",
      "carefully chosen locations ( O(NM)), and then use blending techniques to\n",
      "interpolate the ﬁt elsewhere ( O(M) per evaluation).\n",
      "Bibliographic Notes\n",
      "There is a vast literature on kernel methods which we will not attempt to\n",
      "summarize. Rather we will point to a few good references that themselves\n",
      "have extensive bibliographies. Loader (1999) gives excellent coverage of lo-\n",
      "cal regression and likelihood, and also describes state-of-the-art software\n",
      "for ﬁtting these models. Fan and Gijbels (1996) cover these models from\n",
      "a more theoretical aspect. Hastie and Tibshirani (1990) discuss local re-\n",
      "gression in the context of additive modeling. Silverman (1986) gives a goo d\n",
      "overview of density estimation, as does Scott (1992).\n",
      "Exercises\n",
      "Ex. 6.1 Show that the Nadaraya–Watson kernel smooth with ﬁxed metric\n",
      "bandwidth λand a Gaussian kernel is diﬀerentiable. What can be said for\n",
      "the Epanechnikov kernel? What can be said for the Epanechnikov kernel\n",
      "with adaptive nearest-neighbor bandwidth λ(x0)?\n",
      "Ex. 6.2 Show that∑N\n",
      "i=1(xi−x0)li(x0) = 0 for local linear regression. Deﬁne\n",
      "bj(x0) =∑N\n",
      "i=1(xi−x0)jli(x0). Show that b0(x0) = 1 for local polynomial\n",
      "regression of any degree (including local constants). Show that bj(x0) = 0\n",
      "for all j∈ {1,2,... ,k }for local polynomial regression of degree k. What\n",
      "are the implications of this on the bias?\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 217\n",
      "Ex. 6.3 Show that ||l(x)||(Section 6.1.2) increases with the degree of the\n",
      "local polynomial.\n",
      "Ex. 6.4 Suppose that the ppredictors Xarise from sampling relatively\n",
      "smooth analog curves at puniformly spaced abscissa values. Denote by\n",
      "Cov(X|Y) =Σthe conditional covariance matrix of the predictors, and\n",
      "assume this does not change much with Y. Discuss the nature of Maha-\n",
      "lanobis choice A=Σ−1for the metric in (6.14). How does this compare\n",
      "withA=I? How might you construct a kernel Athat (a) downweights\n",
      "high-frequency components in the distance metric; (b) ignores them\n",
      "completely?\n",
      "Ex. 6.5 Show that ﬁtting a locally constant multinomial logit model of\n",
      "the form (6.19) amounts to smoothing the binary response indicators for\n",
      "each class separately using a Nadaraya–Watson kernel smoother with kernel\n",
      "weights Kλ(x0,xi).\n",
      "Ex. 6.6 Suppose that all you have is software for ﬁtting local regression,\n",
      "but you can specify exactly which monomials are included in the ﬁt. How\n",
      "could you use this software to ﬁt a varying-coeﬃcient model in some of the\n",
      "variables?\n",
      "Ex. 6.7 Derive an expression for the leave-one-out cross-validated residual\n",
      "sum-of-squares for local polynomial regression.\n",
      "Ex. 6.8 Suppose that for continuous response Yand predictor X, we model\n",
      "the joint density of X,Yusing a multivariate Gaussian kernel estimator.\n",
      "Note that the kernel in this case would be the product kernel φλ(X)φλ(Y).\n",
      "Show that the conditional mean E(Y|X) derived from this estimate is a\n",
      "Nadaraya–Watson estimator. Extend this result to classiﬁcation by pro-\n",
      "viding a suitable kernel for the estimation of the joint distribution of a\n",
      "continuous Xand discrete Y.\n",
      "Ex. 6.9 Explore the diﬀerences between the naive Bayes model (6.27) and\n",
      "a generalized additive logistic regression model, in terms of (a) model as-\n",
      "sumptions and (b) estimation. If all the variables Xkare discrete, what can\n",
      "you say about the corresponding GAM?\n",
      "Ex. 6.10 Suppose we have Nsamples generated from the model yi=f(xi)+\n",
      "εi, with εiindependent and identically distributed with mean zero and\n",
      "variance σ2, thexiassumed ﬁxed (non random). We estimate fusing a\n",
      "linear smoother (local regression, smoothing spline, etc.) with smoothing\n",
      "parameter λ. Thus the vector of ﬁtted values is given by ˆf=Sλy. Consider\n",
      "thein-sample prediction error\n",
      "PE(λ) = E1\n",
      "NN∑\n",
      "i=1(y∗\n",
      "i−ˆfλ(xi))2(6.34)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "218 6. Kernel Smoothing Methods\n",
      "for predicting new responses at the Ninput values. Show that the aver-\n",
      "age squared residual on the training data, ASR( λ), is a biased estimate\n",
      "(optimistic) for PE( λ), while\n",
      "Cλ= ASR( λ) +2σ2\n",
      "Ntrace(Sλ) (6.35)\n",
      "is unbiased.\n",
      "Ex. 6.11 Show that for the Gaussian mixture model (6.32) the likelihood\n",
      "is maximized at + ∞, and describe how.\n",
      "Ex. 6.12 Write a computer program to perform a local linear discrimi-\n",
      "nant analysis. At each query point x0, the training data receive weights\n",
      "Kλ(x0,xi) from a weighting kernel, and the ingredients for the linear deci-\n",
      "sion boundaries (see Section 4.3) are computed by weighted averages. Try\n",
      "out your program on the zipcode data, and show the training and test er-\n",
      "rors for a series of ﬁve pre-chosen values of λ. Thezipcode data are available\n",
      "from the book website www-stat.stanford.edu/ElemStatLearn .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 219\n",
      "Printer: Opaque this\n",
      "7\n",
      "Model Assessment and Selection\n",
      "7.1 Introduction\n",
      "Thegeneralization performance of a learning method relates to its predic-\n",
      "tion capability on independent test data. Assessment of this performance\n",
      "is extremely important in practice, since it guides the choice of learning\n",
      "method or model, and gives us a measure of the quality of the ultimately\n",
      "chosen model.\n",
      "In this chapter we describe and illustrate the key methods for perfor-\n",
      "mance assessment, and show how they are used to select models. We begin\n",
      "the chapter with a discussion of the interplay between bias, variance and\n",
      "model complexity.\n",
      "7.2 Bias, Variance and Model Complexity\n",
      "Figure 7.1 illustrates the important issue in assessing the ability of a learn-\n",
      "ing method to generalize. Consider ﬁrst the case of a quantitative or interval\n",
      "scale response. We have a target variable Y, a vector of inputs X, and a\n",
      "prediction model ˆf(X) that has been estimated from a training set T.\n",
      "The loss function for measuring errors between Yandˆf(X) is denoted by\n",
      "L(Y,ˆf(X)). Typical choices are\n",
      "L(Y,ˆf(X)) ={\n",
      "(Y−ˆf(X))2squared error\n",
      "|Y−ˆf(X)| absolute error .(7.1)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "220 7. Model Assessment and Selection\n",
      "0 5 10 15 20 25 30 350.0 0.2 0.4 0.6 0.8 1.0 1.2\n",
      "Model Complexity (df)Prediction ErrorHigh Bias Low Bias\n",
      "High Variance Low Variance\n",
      "FIGURE 7.1. Behavior of test sample and training sample error as the model\n",
      "complexity is varied. The light blue curves show the training er rorerr, while the\n",
      "light red curves show the conditional test error ErrTfor100training sets of size\n",
      "50each, as the model complexity is increased. The solid curves sh ow the expected\n",
      "test error Errand the expected training error E[err].\n",
      "Test error , also referred to as generalization error , is the prediction error\n",
      "over an independent test sample\n",
      "ErrT= E[L(Y,ˆf(X))|T] (7.2)\n",
      "where both XandYare drawn randomly from their joint distribution\n",
      "(population). Here the training set Tis ﬁxed, and test error refers to the\n",
      "error for this speciﬁc training set. A related quantity is the expected pre-\n",
      "diction error (or expected test error)\n",
      "Err = E[ L(Y,ˆf(X))] = E[Err T]. (7.3)\n",
      "Note that this expectation averages over everything that is random, includ-\n",
      "ing the randomness in the training set that produced ˆf.\n",
      "Figure 7.1 shows the prediction error (light red curves) Err Tfor 100\n",
      "simulated training sets each of size 50. The lasso (Section 3.4.2) was used\n",
      "to produce the sequence of ﬁts. The solid red curve is the average, and\n",
      "hence an estimate of Err.\n",
      "Estimation of Err Twill be our goal, although we will see that Err is\n",
      "more amenable to statistical analysis, and most methods eﬀectively esti-\n",
      "mate the expected error. It does not seem possible to estimate conditional\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.2 Bias, Variance and Model Complexity 221\n",
      "error eﬀectively, given only the information in the same training set. Some\n",
      "discussion of this point is given in Section 7.12.\n",
      "Training error is the average loss over the training sample\n",
      "err =1\n",
      "NN∑\n",
      "i=1L(yi,ˆf(xi)). (7.4)\n",
      "We would like to know the expected test error of our estimated model\n",
      "ˆf. As the model becomes more and more complex, it uses the training\n",
      "data more and is able to adapt to more complicated underlying structures.\n",
      "Hence there is a decrease in bias but an increase in variance. There is some\n",
      "intermediate model complexity that gives minimum expected test error.\n",
      "Unfortunately training error is not a good estimate of the test error,\n",
      "as seen in Figure 7.1. Training error consistently decreases with model\n",
      "complexity, typically dropping to zero if we increase the model complexity\n",
      "enough. However, a model with zero training error is overﬁt to the training\n",
      "data and will typically generalize poorly.\n",
      "The story is similar for a qualitative or categorical response Gtaking\n",
      "one of Kvalues in a set G, labeled for convenience as 1 ,2,... ,K . Typically\n",
      "we model the probabilities pk(X) = Pr( G=k|X) (or some monotone\n",
      "transformations fk(X)), and then ˆG(X) = arg max kˆpk(X). In some cases,\n",
      "such as 1-nearest neighbor classiﬁcation (Chapters 2 and 13) we produce\n",
      "ˆG(X) directly. Typical loss functions are\n",
      "L(G,ˆG(X)) = I(G̸=ˆG(X)) (0–1 loss) , (7.5)\n",
      "L(G,ˆp(X)) = −2K∑\n",
      "k=1I(G=k)log ˆpk(X)\n",
      "=−2log ˆpG(X) (−2×log-likelihood) .(7.6)\n",
      "The quantity −2×the log-likelihood is sometimes referred to as the\n",
      "deviance .\n",
      "Again, test error here is Err T= E[L(G,ˆG(X))|T], the population mis-\n",
      "classiﬁcation error of the classiﬁer trained on T, and Err is the expected\n",
      "misclassiﬁcation error.\n",
      "Training error is the sample analogue, for example,\n",
      "err =−2\n",
      "NN∑\n",
      "i=1log ˆpgi(xi), (7.7)\n",
      "the sample log-likelihood for the model.\n",
      "The log-likelihood can be used as a loss-function for general response\n",
      "densities, such as the Poisson, gamma, exponential, log-normal and others.\n",
      "If Pr θ(X)(Y) is the density of Y, indexed by a parameter θ(X) that depends\n",
      "on the predictor X, then\n",
      "L(Y,θ(X)) =−2≤log Pr θ(X)(Y). (7.8)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "222 7. Model Assessment and Selection\n",
      "The “−2” in the deﬁnition makes the log-likelihood loss for the Gaussian\n",
      "distribution match squared-error loss.\n",
      "For ease of exposition, for the remainder of this chapter we will use Yand\n",
      "f(X) to represent all of the above situations, since we focus mainly on the\n",
      "quantitative response (squared-error loss) setting. For the other situati ons,\n",
      "the appropriate translations are obvious.\n",
      "In this chapter we describe a number of methods for estimating the\n",
      "expected test error for a model. Typically our model will have a tuning\n",
      "parameter or parameters αand so we can write our predictions as ˆfα(x).\n",
      "The tuning parameter varies the complexity of our model, and we wish to\n",
      "ﬁnd the value of αthat minimizes error, that is, produces the minimum of\n",
      "the average test error curve in Figure 7.1. Having said this, for brevity w e\n",
      "will often suppress the dependence of ˆf(x) onα.\n",
      "It is important to note that there are in fact two separate goals that we\n",
      "might have in mind:\n",
      "Model selection: estimating the performance of diﬀerent models in order\n",
      "to choose the best one.\n",
      "Model assessment: having chosen a ﬁnal model, estimating its predic-\n",
      "tion error (generalization error) on new data.\n",
      "If we are in a data-rich situation, the best approach for both problems is\n",
      "to randomly divide the dataset into three parts: a training set, a validation\n",
      "set, and a test set. The training set is used to ﬁt the models; the validation\n",
      "set is used to estimate prediction error for model selection; the test set is\n",
      "used for assessment of the generalization error of the ﬁnal chosen model.\n",
      "Ideally, the test set should be kept in a “vault,” and be brought out only\n",
      "at the end of the data analysis. Suppose instead that we use the test-set\n",
      "repeatedly, choosing the model with smallest test-set error. Then the test\n",
      "set error of the ﬁnal chosen model will underestimate the true test error,\n",
      "sometimes substantially.\n",
      "It is diﬃcult to give a general rule on how to choose the number of\n",
      "observations in each of the three parts, as this depends on the signal-to-\n",
      "noise ratio in the data and the training sample size. A typical split might\n",
      "be 50% for training, and 25% each for validation and testing:\n",
      "Test Train Validation TestTrain Validation Test Validation Train Validation TestTrain\n",
      "The methods in this chapter are designed for situations where there is\n",
      "insuﬃcient data to split it into three parts. Again it is too diﬃcult to give\n",
      "a general rule on how much training data is enough; among other things,\n",
      "this depends on the signal-to-noise ratio of the underlying function, and\n",
      "the complexity of the models being ﬁt to the data.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.3 The Bias–Variance Decomposition 223\n",
      "The methods of this chapter approximate the validation step either an-\n",
      "alytically (AIC, BIC, MDL, SRM) or by eﬃcient sample re-use (cross-\n",
      "validation and the bootstrap). Besides their use in model selection, we also\n",
      "examine to what extent each method provides a reliable estimate of test\n",
      "error of the ﬁnal chosen model.\n",
      "Before jumping into these topics, we ﬁrst explore in more detail the\n",
      "nature of test error and the bias–variance tradeoﬀ.\n",
      "7.3 The Bias–Variance Decomposition\n",
      "As in Chapter 2, if we assume that Y=f(X) +εwhere E( ε) = 0 and\n",
      "Var(ε) =σ2\n",
      "ε, we can derive an expression for the expected prediction error\n",
      "of a regression ﬁt ˆf(X) at an input point X=x0, using squared-error loss:\n",
      "Err(x0) = E[(Y−ˆf(x0))2|X=x0]\n",
      "=σ2\n",
      "ε+ [Eˆf(x0)−f(x0)]2+E[ˆf(x0)−Eˆf(x0)]2\n",
      "=σ2\n",
      "ε+ Bias2(ˆf(x0)) + Var( ˆf(x0))\n",
      "= Irreducible Error + Bias2+ Variance . (7.9)\n",
      "The ﬁrst term is the variance of the target around its true mean f(x0), and\n",
      "cannot be avoided no matter how well we estimate f(x0), unless σ2\n",
      "ε= 0.\n",
      "The second term is the squared bias, the amount by which the average of\n",
      "our estimate diﬀers from the true mean; the last term is the variance; the\n",
      "expected squared deviation of ˆf(x0) around its mean. Typically the more\n",
      "complex we make the model ˆf, the lower the (squared) bias but the higher\n",
      "the variance.\n",
      "For the k-nearest-neighbor regression ﬁt, these expressions have the sim-\n",
      "ple form\n",
      "Err(x0) = E[(Y−ˆfk(x0))2|X=x0]\n",
      "=σ2\n",
      "ε+[\n",
      "f(x0)−1\n",
      "kk∑\n",
      "ℓ=1f(x(ℓ))]2\n",
      "+σ2\n",
      "ε\n",
      "k. (7.10)\n",
      "Here we assume for simplicity that training inputs xiare ﬁxed, and the ran-\n",
      "domness arises from the yi. The number of neighbors kis inversely related\n",
      "to the model complexity. For small k, the estimate ˆfk(x) can potentially\n",
      "adapt itself better to the underlying f(x). As we increase k, the bias—the\n",
      "squared diﬀerence between f(x0) and the average of f(x) at the k-nearest\n",
      "neighbors—will typically increase, while the variance decreases.\n",
      "For a linear model ﬁt ˆfp(x) =xTˆβ, where the parameter vector βwith\n",
      "pcomponents is ﬁt by least squares, we have\n",
      "Err(x0) = E[(Y−ˆfp(x0))2|X=x0]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "224 7. Model Assessment and Selection\n",
      "=σ2\n",
      "ε+ [f(x0)−Eˆfp(x0)]2+||h(x0)||2σ2\n",
      "ε.(7.11)\n",
      "Hereh(x0) =X(XTX)−1x0, theN-vector of linear weights that produce\n",
      "the ﬁt ˆfp(x0) =x0T(XTX)−1XTy, and hence Var[ ˆfp(x0)] =||h(x0)||2σ2\n",
      "ε.\n",
      "While this variance changes with x0, its average (with x0taken to be each\n",
      "of the sample values xi) is (p/N)σ2\n",
      "ε, and hence\n",
      "1\n",
      "NN∑\n",
      "i=1Err(xi) =σ2\n",
      "ε+1\n",
      "NN∑\n",
      "i=1[f(xi)−Eˆf(xi)]2+p\n",
      "Nσ2\n",
      "ε, (7.12)\n",
      "thein-sample error. Here model complexity is directly related to the num-\n",
      "ber of parameters p.\n",
      "The test error Err( x0) for a ridge regression ﬁt ˆfα(x0) is identical in\n",
      "form to (7.11), except the linear weights in the variance term are diﬀerent:\n",
      "h(x0) =X(XTX+αI)Tx0. The bias term will also be diﬀerent.\n",
      "For a linear model family such as ridge regression, we can break down\n",
      "the bias more ﬁnely. Let β∗denote the parameters of the best-ﬁtting linear\n",
      "approximation to f:\n",
      "β∗= arg min\n",
      "βE(\n",
      "f(X)−XTβ)2. (7.13)\n",
      "Here the expectation is taken with respect to the distribution of the input\n",
      "variables X. Then we can write the average squared bias as\n",
      "Ex0[\n",
      "f(x0)−Eˆfα(x0)]2\n",
      "= E x0[\n",
      "f(x0)−xT\n",
      "0β∗]2+ Ex0[\n",
      "xT\n",
      "0β∗−ExT\n",
      "0ˆβα]2\n",
      "= Ave[Model Bias]2+ Ave[Estimation Bias]2\n",
      "(7.14)\n",
      "The ﬁrst term on the right-hand side is the average squared model bias , the\n",
      "error between the best-ﬁtting linear approximation and the true function.\n",
      "The second term is the average squared estimation bias , the error between\n",
      "the average estimate E( xT\n",
      "0ˆβ) and the best-ﬁtting linear approximation.\n",
      "For linear models ﬁt by ordinary least squares, the estimation bias is zero.\n",
      "For restricted ﬁts, such as ridge regression, it is positive, and we trade i t oﬀ\n",
      "with the beneﬁts of a reduced variance. The model bias can only be reduced\n",
      "by enlarging the class of linear models to a richer collection of models, by\n",
      "including interactions and transformations of the variables in the model.\n",
      "Figure 7.2 shows the bias–variance tradeoﬀ schematically. In the case\n",
      "of linear models, the model space is the set of all linear predictions from\n",
      "pinputs and the black dot labeled “closest ﬁt” is xTβ∗. The blue-shaded\n",
      "region indicates the error σεwith which we see the truth in the training\n",
      "sample.\n",
      "Also shown is the variance of the least squares ﬁt, indicated by the large\n",
      "yellow circle centered at the black dot labeled “closest ﬁt in population,’\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.3 The Bias–Variance Decomposition 225\n",
      "RealizationClosest fit in population\n",
      "Estimation BiasSPACE\n",
      "VarianceEstimationClosest fit\n",
      "Truth\n",
      "Model bias\n",
      "RESTRICTEDShrunken fit\n",
      "MODELSPACEMODEL\n",
      "FIGURE 7.2. Schematic of the behavior of bias and variance. The model space\n",
      "is the set of all possible predictions from the model, with the “closest ﬁt” labeled\n",
      "with a black dot. The model bias from the truth is shown, along wi th the variance,\n",
      "indicated by the large yellow circle centered at the black dot l abeled “closest ﬁt\n",
      "in population.” A shrunken or regularized ﬁt is also shown, having additional\n",
      "estimation bias, but smaller prediction error due to its dec reased variance.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "226 7. Model Assessment and Selection\n",
      "Now if we were to ﬁt a model with fewer predictors, or regularize the coef-\n",
      "ﬁcients by shrinking them toward zero (say), we would get the “shrunken\n",
      "ﬁt” shown in the ﬁgure. This ﬁt has an additional estimation bias, due to\n",
      "the fact that it is not the closest ﬁt in the model space. On the other hand,\n",
      "it has smaller variance. If the decrease in variance exceeds the increase in\n",
      "(squared) bias, then this is worthwhile.\n",
      "7.3.1 Example: Bias–Variance Tradeoﬀ\n",
      "Figure 7.3 shows the bias–variance tradeoﬀ for two simulated examples.\n",
      "There are 80 observations and 20 predictors, uniformly distributed in the\n",
      "hypercube [0 ,1]20. The situations are as follows:\n",
      "Left panels: Yis 0 if X1≤1/2 and 1 if X1>1/2, and we apply k-nearest\n",
      "neighbors.\n",
      "Right panels: Yis 1 if∑10\n",
      "j=1Xjis greater than 5 and 0 otherwise, and we\n",
      "use best subset linear regression of size p.\n",
      "The top row is regression with squared error loss; the bottom row is cla ssi-\n",
      "ﬁcation with 0–1 loss. The ﬁgures show the prediction error (red), squared\n",
      "bias (green) and variance (blue), all computed for a large test sample.\n",
      "In the regression problems, bias and variance add to produce the predic-\n",
      "tion error curves, with minima at about k= 5 for k-nearest neighbors, and\n",
      "p≥10 for the linear model. For classiﬁcation loss (bottom ﬁgures), some\n",
      "interesting phenomena can be seen. The bias and variance curves are the\n",
      "same as in the top ﬁgures, and prediction error now refers to misclassiﬁ-\n",
      "cation rate. We see that prediction error is no longer the sum of squared\n",
      "bias and variance. For the k-nearest neighbor classiﬁer, prediction error\n",
      "decreases or stays the same as the number of neighbors is increased to 20,\n",
      "despite the fact that the squared bias is rising. For the linear model classi-\n",
      "ﬁer the minimum occurs for p≥10 as in regression, but the improvement\n",
      "over the p= 1 model is more dramatic. We see that bias and variance seem\n",
      "to interact in determining prediction error.\n",
      "Why does this happen? There is a simple explanation for the ﬁrst phe-\n",
      "nomenon. Suppose at a given input point, the true probability of class 1 is\n",
      "0.9 while the expected value of our estimate is 0 .6. Then the squared bias—\n",
      "(0.6−0.9)2—is considerable, but the prediction error is zero since we make\n",
      "the correct decision. In other words, estimation errors that leave us on the\n",
      "right side of the decision boundary don’t hurt. Exercise 7.2 demonstrates\n",
      "this phenomenon analytically, and also shows the interaction eﬀect between\n",
      "bias and variance.\n",
      "The overall point is that the bias–variance tradeoﬀ behaves diﬀerently\n",
      "for 0–1 loss than it does for squared error loss. This in turn means that\n",
      "the best choices of tuning parameters may diﬀer substantially in the two\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.3 The Bias–Variance Decomposition 2270.0 0.1 0.2 0.3 0.4\n",
      "Number of Neighbors k50 40 30 20 10 0k−NN − Regression\n",
      "5 10 15 200.0 0.1 0.2 0.3 0.4\n",
      "Subset Size pLinear Model − Regression0.0 0.1 0.2 0.3 0.4\n",
      "Number of Neighbors k50 40 30 20 10 0k−NN − Classification\n",
      "5 10 15 200.0 0.1 0.2 0.3 0.4\n",
      "Subset Size pLinear Model − Classification\n",
      "FIGURE 7.3. Expected prediction error (orange), squared bias (green) and va ri-\n",
      "ance (blue) for a simulated example. The top row is regression w ith squared error\n",
      "loss; the bottom row is classiﬁcation with 0–1loss. The models are k-nearest\n",
      "neighbors (left) and best subset regression of size p(right). The variance and bias\n",
      "curves are the same in regression and classiﬁcation, but the pre diction error curve\n",
      "is diﬀerent.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "228 7. Model Assessment and Selection\n",
      "settings. One should base the choice of tuning parameter on an estimate of\n",
      "prediction error, as described in the following sections.\n",
      "7.4 Optimism of the Training Error Rate\n",
      "Discussions of error rate estimation can be confusing, because we have\n",
      "to make clear which quantities are ﬁxed and which are random1. Before\n",
      "we continue, we need a few deﬁnitions, elaborating on the material of Sec-\n",
      "tion 7.2. Given a training set T={(x1,y1),(x2,y2),...(xN,yN)}the gen-\n",
      "eralization error of a model ˆfis\n",
      "ErrT= EX0,Y0[L(Y0,ˆf(X0))|T]; (7.15)\n",
      "Note that the training set Tis ﬁxed in expression (7.15). The point ( X0,Y0)\n",
      "is a new test data point, drawn from F, the joint distribution of the data.\n",
      "Averaging over training sets Tyields the expected error\n",
      "Err = E TEX0,Y0[L(Y0,ˆf(X0))|T], (7.16)\n",
      "which is more amenable to statistical analysis. As mentioned earlier, it\n",
      "turns out that most methods eﬀectively estimate the expected error rather\n",
      "than E T; see Section 7.12 for more on this point.\n",
      "Now typically, the training error\n",
      "err =1\n",
      "NN∑\n",
      "i=1L(yi,ˆf(xi)) (7.17)\n",
      "will be less than the true error Err T, because the same data is being used\n",
      "to ﬁt the method and assess its error (see Exercise 2.9). A ﬁtting method\n",
      "typically adapts to the training data, and hence the apparent or training\n",
      "error err will be an overly optimistic estimate of the generalization error\n",
      "ErrT.\n",
      "Part of the discrepancy is due to where the evaluation points occur. The\n",
      "quantity Err Tcan be thought of as extra-sample error, since the test input\n",
      "vectors don’t need to coincide with the training input vectors. The nature\n",
      "of the optimism in err is easiest to understand when we focus instead on\n",
      "thein-sample error\n",
      "Errin=1\n",
      "NN∑\n",
      "i=1EY0[L(Y0\n",
      "i,ˆf(xi))|T] (7.18)\n",
      "TheY0notation indicates that we observe Nnew response values at\n",
      "each of the training points xi, i= 1,2,... ,N . We deﬁne the optimism as\n",
      "1Indeed, in the ﬁrst edition of our book, this section wasn’t s uﬃciently clear.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.4 Optimism of the Training Error Rate 229\n",
      "the diﬀerence between Err inand the training error err:\n",
      "op≡Errin−err. (7.19)\n",
      "This is typically positive since err is usually biased downward as an estimate\n",
      "of prediction error. Finally, the average optimism is the expectation of the\n",
      "optimism over training sets\n",
      "ω≡Ey(op). (7.20)\n",
      "Here the predictors in the training set are ﬁxed, and the expectation is\n",
      "over the training set outcome values; hence we have used the notation E y\n",
      "instead of E T. We can usually estimate only the expected error ωrather\n",
      "than op, in the same way that we can estimate the expected error Err\n",
      "rather than the conditional error Err T.\n",
      "For squared error, 0–1, and other loss functions, one can show quite\n",
      "generally that\n",
      "ω=2\n",
      "NN∑\n",
      "i=1Cov(ˆyi,yi), (7.21)\n",
      "where Cov indicates covariance. Thus the amount by which err underesti-\n",
      "mates the true error depends on how strongly yiaﬀects its own prediction.\n",
      "The harder we ﬁt the data, the greater Cov(ˆ yi,yi) will be, thereby increas-\n",
      "ing the optimism. Exercise 7.4 proves this result for squared error loss where\n",
      "ˆyiis the ﬁtted value from the regression. For 0–1 loss, ˆ yi∈ {0,1}is the\n",
      "classiﬁcation at xi, and for entropy loss, ˆ yi∈[0,1] is the ﬁtted probability\n",
      "of class 1 at xi.\n",
      "In summary, we have the important relation\n",
      "Ey(Errin) = E y(err) +2\n",
      "NN∑\n",
      "i=1Cov(ˆyi,yi). (7.22)\n",
      "This expression simpliﬁes if ˆ yiis obtained by a linear ﬁt with dinputs\n",
      "or basis functions. For example,\n",
      "N∑\n",
      "i=1Cov(ˆyi,yi) =dσ2\n",
      "ε (7.23)\n",
      "for the additive error model Y=f(X) +ε, and so\n",
      "Ey(Errin) = E y(err) + 2 ≤d\n",
      "Nσ2\n",
      "ε. (7.24)\n",
      "Expression (7.23) is the basis for the deﬁnition of the eﬀective number of\n",
      "parameters discussed in Section 7.6 The optimism increases linearly with\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "230 7. Model Assessment and Selection\n",
      "the number dof inputs or basis functions we use, but decreases as the\n",
      "training sample size increases. Versions of (7.24) hold approximately for\n",
      "other error models, such as binary data and entropy loss.\n",
      "An obvious way to estimate prediction error is to estimate the optimism\n",
      "and then add it to the training error err. The methods described in the\n",
      "next section— Cp, AIC, BIC and others—work in this way, for a special\n",
      "class of estimates that are linear in their parameters.\n",
      "In contrast, cross-validation and bootstrap methods, described later in\n",
      "the chapter, are direct estimates of the extra-sample error Err. These gen-\n",
      "eral tools can be used with any loss function, and with nonlinear, adaptive\n",
      "ﬁtting techniques.\n",
      "In-sample error is not usually of direct interest since future values of the\n",
      "features are not likely to coincide with their training set values. But for\n",
      "comparison between models, in-sample error is convenient and often leads\n",
      "to eﬀective model selection. The reason is that the relative (rather than\n",
      "absolute) size of the error is what matters.\n",
      "7.5 Estimates of In-Sample Prediction Error\n",
      "The general form of the in-sample estimates is\n",
      "ˆErrin=err + ˆω, (7.25)\n",
      "where ˆ ωis an estimate of the average optimism.\n",
      "Using expression (7.24), applicable when dparameters are ﬁt under\n",
      "squared error loss, leads to a version of the so-called Cpstatistic,\n",
      "Cp=err + 2 ≤d\n",
      "Nˆσε2. (7.26)\n",
      "Here ˆσε2is an estimate of the noise variance, obtained from the mean-\n",
      "squared error of a low-bias model. Using this criterion we adjust the training\n",
      "error by a factor proportional to the number of basis functions used.\n",
      "TheAkaike information criterion is a similar but more generally appli-\n",
      "cable estimate of Err inwhen a log-likelihood loss function is used. It relies\n",
      "on a relationship similar to (7.24) that holds asymptotically as N→ ∞:\n",
      "−2≤E[log Pr ˆθ(Y)]≈ −2\n",
      "N≤E[loglik] + 2 ≤d\n",
      "N. (7.27)\n",
      "Here Pr θ(Y) is a family of densities for Y(containing the “true” density),\n",
      "ˆθis the maximum-likelihood estimate of θ, and “loglik” is the maximized\n",
      "log-likelihood:\n",
      "loglik =N∑\n",
      "i=1log Pr ˆθ(yi). (7.28)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.5 Estimates of In-Sample Prediction Error 231\n",
      "For example, for the logistic regression model, using the binomial log-\n",
      "likelihood, we have\n",
      "AIC = −2\n",
      "N≤loglik + 2 ≤d\n",
      "N. (7.29)\n",
      "For the Gaussian model (with variance σ2\n",
      "ε= ˆσε2assumed known), the AIC\n",
      "statistic is equivalent to Cp, and so we refer to them collectively as AIC.\n",
      "To use AIC for model selection, we simply choose the model giving small-\n",
      "est AIC over the set of models considered. For nonlinear and other complex\n",
      "models, we need to replace dby some measure of model complexity. We\n",
      "discuss this in Section 7.6.\n",
      "Given a set of models fα(x) indexed by a tuning parameter α, denote\n",
      "byerr(α) and d(α) the training error and number of parameters for each\n",
      "model. Then for this set of models we deﬁne\n",
      "AIC(α) =err(α) + 2≤d(α)\n",
      "Nˆσε2. (7.30)\n",
      "The function AIC( α) provides an estimate of the test error curve, and we\n",
      "ﬁnd the tuning parameter ˆ αthat minimizes it. Our ﬁnal chosen model\n",
      "isfˆα(x). Note that if the basis functions are chosen adaptively, (7.23) no\n",
      "longer holds. For example, if we have a total of pinputs, and we choose\n",
      "the best-ﬁtting linear model with d < p inputs, the optimism will exceed\n",
      "(2d/N)σ2\n",
      "ε. Put another way, by choosing the best-ﬁtting model with d\n",
      "inputs, the eﬀective number of parameters ﬁt is more than d.\n",
      "Figure 7.4 shows AIC in action for the phoneme recognition example\n",
      "of Section 5.2.3 on page 148. The input vector is the log-periodogram of\n",
      "the spoken vowel, quantized to 256 uniformly spaced frequencies. A lin-\n",
      "ear logistic regression model is used to predict the phoneme class, with\n",
      "coeﬃcient function β(f) =∑M\n",
      "m=1hm(f)θm, an expansion in Mspline ba-\n",
      "sis functions. For any given M, a basis of natural cubic splines is used\n",
      "for the hm, with knots chosen uniformly over the range of frequencies (so\n",
      "d(α) =d(M) =M). Using AIC to select the number of basis functions will\n",
      "approximately minimize Err( M) for both entropy and 0–1 loss.\n",
      "The simple formula\n",
      "(2/N)N∑\n",
      "i=1Cov(ˆyi,yi) = (2 d/N)σ2\n",
      "ε\n",
      "holds exactly for linear models with additive errors and squared error loss,\n",
      "and approximately for linear models and log-likelihoods. In particular, the\n",
      "formula does not hold in general for 0–1 loss (Efron, 1986), although many\n",
      "authors nevertheless use it in that context (right panel of Figure 7.4).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "232 7. Model Assessment and Selection\n",
      "Number of Basis FunctionsLog-likelihood\n",
      "0.5 1.0 1.5 2.0 2.5Log-likelihood Loss\n",
      "2 4 8 16 32 64 128O\n",
      "O\n",
      "O\n",
      "OOO\n",
      "O\n",
      "OO\n",
      "O\n",
      "O\n",
      "OOOOO\n",
      "O\n",
      "O\n",
      "O\n",
      "OOOOOTrain\n",
      "Test\n",
      "AIC\n",
      "Number of Basis FunctionsMisclassification Error\n",
      "0.10 0.15 0.20 0.25 0.30 0.350-1 Loss\n",
      "2 4 8 16 32 64 128O\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "O\n",
      "OO\n",
      "O\n",
      "O\n",
      "OOOOOO\n",
      "O\n",
      "O\n",
      "OOOOO\n",
      "FIGURE 7.4. AIC used for model selection for the phoneme recogni-\n",
      "tion example of Section 5.2.3. The logistic regression coeﬃci ent function\n",
      "β(f) =PM\n",
      "m=1hm(f)θmis modeled as an expansion in Mspline basis functions.\n",
      "In the left panel we see the AIC statistic used to estimate Errinusing log-likeli-\n",
      "hood loss. Included is an estimate of Errbased on an independent test sample. It\n",
      "does well except for the extremely over-parametrized case ( M= 256 parameters\n",
      "forN= 1000 observations). In the right panel the same is done for 0–1 loss.\n",
      "Although the AIC formula does not strictly apply here, it does a reasonable job in\n",
      "this case.\n",
      "7.6 The Eﬀective Number of Parameters\n",
      "The concept of “number of parameters” can be generalized, especially to\n",
      "models where regularization is used in the ﬁtting. Suppose we stack the\n",
      "outcomes y1,y2,... ,y Ninto a vector y, and similarly for the predictions\n",
      "ˆy. Then a linear ﬁtting method is one for which we can write\n",
      "ˆy=Sy, (7.31)\n",
      "whereSis anN×Nmatrix depending on the input vectors xibut not on\n",
      "theyi. Linear ﬁtting methods include linear regression on the original fea-\n",
      "tures or on a derived basis set, and smoothing methods that use quadratic\n",
      "shrinkage, such as ridge regression and cubic smoothing splines. Then the\n",
      "eﬀective number of parameters is deﬁned as\n",
      "df(S) = trace( S), (7.32)\n",
      "the sum of the diagonal elements of S(also known as the eﬀective degrees-\n",
      "of-freedom ). Note that if Sis an orthogonal-projection matrix onto a basis\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.7 The Bayesian Approach and BIC 233\n",
      "set spanned by Mfeatures, then trace( S) =M. It turns out that trace( S) is\n",
      "exactly the correct quantity to replace das the number of parameters in the\n",
      "Cpstatistic (7.26). If yarises from an additive-error model Y=f(X) +ε\n",
      "with Var( ε) =σ2\n",
      "ε, then one can show that∑N\n",
      "i=1Cov(ˆyi,yi) = trace( S)σ2\n",
      "ε,\n",
      "which motivates the more general deﬁnition\n",
      "df(ˆy) =∑N\n",
      "i=1Cov(ˆyi,yi)\n",
      "σ2ε(7.33)\n",
      "(Exercises 7.4 and 7.5). Section 5.4.1 on page 153 gives some more intuit ion\n",
      "for the deﬁnition df = trace( S) in the context of smoothing splines.\n",
      "For models like neural networks, in which we minimize an error function\n",
      "R(w) with weight decay penalty (regularization) α∑\n",
      "mw2\n",
      "m, the eﬀective\n",
      "number of parameters has the form\n",
      "df(α) =M∑\n",
      "m=1θm\n",
      "θm+α, (7.34)\n",
      "where the θmare the eigenvalues of the Hessian matrix ∂2R(w)/∂w∂wT.\n",
      "Expression (7.34) follows from (7.32) if we make a quadratic approx imation\n",
      "to the error function at the solution (Bishop, 1995).\n",
      "7.7 The Bayesian Approach and BIC\n",
      "The Bayesian information criterion (BIC), like AIC, is applicable in s ettings\n",
      "where the ﬁtting is carried out by maximization of a log-likelihood. The\n",
      "generic form of BIC is\n",
      "BIC = −2≤loglik + (log N)≤d. (7.35)\n",
      "The BIC statistic (times 1/2) is also known as the Schwarz criterion ( Schwarz,\n",
      "1978).\n",
      "Under the Gaussian model, assuming the variance σ2\n",
      "εis known, −2≤loglik\n",
      "equals (up to a constant)∑\n",
      "i(yi−ˆf(xi))2/σ2\n",
      "ε, which is N≤err/σ2\n",
      "εfor squared\n",
      "error loss. Hence we can write\n",
      "BIC =N\n",
      "σ2ε[\n",
      "err + (log N)≤d\n",
      "Nσ2\n",
      "ε]\n",
      ". (7.36)\n",
      "Therefore BIC is proportional to AIC ( Cp), with the factor 2 replaced\n",
      "by log N. Assuming N > e2≈7.4, BIC tends to penalize complex models\n",
      "more heavily, giving preference to simpler models in selection. As with AIC,\n",
      "σ2\n",
      "εis typically estimated by the mean squared error of a low-bias model.\n",
      "For classiﬁcation problems, use of the multinomial log-likelihood leads to a\n",
      "similar relationship with the AIC, using cross-entropy as the error measur e.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "234 7. Model Assessment and Selection\n",
      "Note however that the misclassiﬁcation error measure does not arise in the\n",
      "BIC context, since it does not correspond to the log-likelihood of the data\n",
      "under any probability model.\n",
      "Despite its similarity with AIC, BIC is motivated in quite a diﬀerent\n",
      "way. It arises in the Bayesian approach to model selection, which we now\n",
      "describe.\n",
      "Suppose we have a set of candidate models Mm,m= 1,... ,M and\n",
      "corresponding model parameters θm, and we wish to choose a best model\n",
      "from among them. Assuming we have a prior distribution Pr( θm|Mm) for\n",
      "the parameters of each model Mm, the posterior probability of a given\n",
      "model is\n",
      "Pr(Mm|Z)∝Pr(Mm)≤Pr(Z|Mm) (7.37)\n",
      "∝Pr(Mm)≤∫\n",
      "Pr(Z|θm,Mm)Pr(θm|Mm)dθm,\n",
      "where Zrepresents the training data {xi,yi}N\n",
      "1. To compare two models\n",
      "MmandMℓ, we form the posterior odds\n",
      "Pr(Mm|Z)\n",
      "Pr(Mℓ|Z)=Pr(Mm)\n",
      "Pr(Mℓ)≤Pr(Z|Mm)\n",
      "Pr(Z|Mℓ). (7.38)\n",
      "If the odds are greater than one we choose model m, otherwise we choose\n",
      "model ℓ. The rightmost quantity\n",
      "BF(Z) =Pr(Z|Mm)\n",
      "Pr(Z|Mℓ)(7.39)\n",
      "is called the Bayes factor , the contribution of the data toward the posterior\n",
      "odds.\n",
      "Typically we assume that the prior over models is uniform, so that\n",
      "Pr(Mm) is constant. We need some way of approximating Pr( Z|Mm).\n",
      "A so-called Laplace approximation to the integral followed by some other\n",
      "simpliﬁcations (Ripley, 1996, page 64) to (7.37) gives\n",
      "log Pr( Z|Mm) = log Pr( Z|ˆθm,Mm)−dm\n",
      "2≤logN+O(1).(7.40)\n",
      "Here ˆθmis a maximum likelihood estimate and dmis the number of free\n",
      "parameters in model Mm. If we deﬁne our loss function to be\n",
      "−2log Pr( Z|ˆθm,Mm),\n",
      "this is equivalent to the BIC criterion of equation (7.35).\n",
      "Therefore, choosing the model with minimum BIC is equivalent to choos-\n",
      "ing the model with largest (approximate) posterior probability. But this\n",
      "framework gives us more. If we compute the BIC criterion for a set of M,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.8 Minimum Description Length 235\n",
      "models, giving BIC m,m= 1,2,... ,M , then we can estimate the posterior\n",
      "probability of each model Mmas\n",
      "e−1\n",
      "2≤BIC m\n",
      "∑M\n",
      "ℓ=1e−1\n",
      "2≤BIC ℓ. (7.41)\n",
      "Thus we can estimate not only the best model, but also assess the relative\n",
      "merits of the models considered.\n",
      "For model selection purposes, there is no clear choice between AIC and\n",
      "BIC. BIC is asymptotically consistent as a selection criterion. What this\n",
      "means is that given a family of models, including the true model, the prob-\n",
      "ability that BIC will select the correct model approaches one as the sample\n",
      "sizeN→ ∞. This is not the case for AIC, which tends to choose models\n",
      "which are too complex as N→ ∞. On the other hand, for ﬁnite samples,\n",
      "BIC often chooses models that are too simple, because of its heavy penalty\n",
      "on complexity.\n",
      "7.8 Minimum Description Length\n",
      "The minimum description length (MDL) approach gives a selection cri-\n",
      "terion formally identical to the BIC approach, but is motivated from an\n",
      "optimal coding viewpoint. We ﬁrst review the theory of coding for data\n",
      "compression, and then apply it to model selection.\n",
      "We think of our datum zas a message that we want to encode and\n",
      "send to someone else (the “receiver”). We think of our model as a way of\n",
      "encoding the datum, and will choose the most parsimonious model, that is\n",
      "the shortest code, for the transmission.\n",
      "Suppose ﬁrst that the possible messages we might want to transmit are\n",
      "z1,z2,... ,z m. Our code uses a ﬁnite alphabet of length A: for example, we\n",
      "might use a binary code {0,1}of length A= 2. Here is an example with\n",
      "four possible messages and a binary coding:\n",
      "Message z1z2z3z4\n",
      "Code 010110 111(7.42)\n",
      "This code is known as an instantaneous preﬁx code: no code is the pre-\n",
      "ﬁx of any other, and the receiver (who knows all of the possible codes),\n",
      "knows exactly when the message has been completely sent. We restrict our\n",
      "discussion to such instantaneous preﬁx codes.\n",
      "One could use the coding in (7.42) or we could permute the codes, for\n",
      "example use codes 110 ,10,111,0 forz1,z2,z3,z4. How do we decide which\n",
      "to use? It depends on how often we will be sending each of the messages.\n",
      "If, for example, we will be sending z1most often, it makes sense to use the\n",
      "shortest code 0 for z1. Using this kind of strategy—shorter codes for more\n",
      "frequent messages—the average message length will be shorter.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "236 7. Model Assessment and Selection\n",
      "In general, if messages are sent with probabilities Pr( zi),i= 1,2,... ,4,\n",
      "a famous theorem due to Shannon says we should use code lengths li=\n",
      "−log2Pr(zi) and the average message length satisﬁes\n",
      "E(length) ≥ −∑\n",
      "Pr(zi)log2(Pr(zi)). (7.43)\n",
      "The right-hand side above is also called the entropy of the distribution\n",
      "Pr(zi). The inequality is an equality when the probabilities satisfy pi=\n",
      "A−li. In our example, if Pr( zi) = 1/2,1/4,1/8,1/8,respectively, then the\n",
      "coding shown in (7.42) is optimal and achieves the entropy lower bound.\n",
      "In general the lower bound cannot be achieved, but procedures like the\n",
      "Huﬀmann coding scheme can get close to the bound. Note that with an\n",
      "inﬁnite set of messages, the entropy is replaced by −∫\n",
      "Pr(z)log2Pr(z)dz.\n",
      "From this result we glean the following:\n",
      "To transmit a random variable zhaving probability density func-\n",
      "tionPr(z), we require about −log2Pr(z)bits of information.\n",
      "We henceforth change notation from log2Pr(z) to log Pr( z) = logePr(z);\n",
      "this is for convenience, and just introduces an unimportant multiplicative\n",
      "constant.\n",
      "Now we apply this result to the problem of model selection. We have\n",
      "a model Mwith parameters θ, and data Z= (X,y) consisting of both\n",
      "inputs and outputs. Let the (conditional) probability of the outputs under\n",
      "the model be Pr( y|θ,M,X), assume the receiver knows all of the inputs,\n",
      "and we wish to transmit the outputs. Then the message length required to\n",
      "transmit the outputs is\n",
      "length = −log Pr( y|θ,M,X)−log Pr( θ|M), (7.44)\n",
      "the log-probability of the target values given the inputs. The second term\n",
      "is the average code length for transmitting the model parameters θ, while\n",
      "the ﬁrst term is the average code length for transmitting the discrepancy\n",
      "between the model and actual target values. For example suppose we have\n",
      "a single target ywithy∼N(θ,σ2), parameter θ∼N(0,1) and no input\n",
      "(for simplicity). Then the message length is\n",
      "length = constant + log σ+(y−θ)2\n",
      "σ2+θ2\n",
      "2. (7.45)\n",
      "Note that the smaller σis, the shorter on average is the message length,\n",
      "sinceyis more concentrated around θ.\n",
      "The MDL principle says that we should choose the model that mini-\n",
      "mizes (7.44). We recognize (7.44) as the (negative) log-posterior distribu-\n",
      "tion, and hence minimizing description length is equivalent to maximizing\n",
      "posterior probability. Hence the BIC criterion, derived as approximation to\n",
      "log-posterior probability, can also be viewed as a device for (approximate)\n",
      "model choice by minimum description length.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.9 Vapnik–Chervonenkis Dimension 237\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-1.0 0.0 1.0\n",
      "xsin(50 ≤x)\n",
      "FIGURE 7.5. The solid curve is the function sin(50 x)forx∈[0,1]. The green\n",
      "(solid) and blue (hollow) points illustrate how the associate d indicator function\n",
      "I(sin(αx)>0)can shatter (separate) an arbitrarily large number of points b y\n",
      "choosing an appropriately high frequency α.\n",
      "Note that we have ignored the precision with which a random variable\n",
      "zis coded. With a ﬁnite code length we cannot code a continuous variable\n",
      "exactly. However, if we code zwithin a tolerance δz, the message length\n",
      "needed is the log of the probability in the interval [ z,z+δz] which is well ap-\n",
      "proximated by δzPr(z) ifδzis small. Since log δzPr(z) = log δz+log Pr( z),\n",
      "this means we can just ignore the constant log δzand use log Pr( z) as our\n",
      "measure of message length, as we did above.\n",
      "The preceding view of MDL for model selection says that we should\n",
      "choose the model with highest posterior probability. However, many Bayes-\n",
      "ians would instead do inference by sampling from the posterior distribution.\n",
      "7.9 Vapnik–Chervonenkis Dimension\n",
      "A diﬃculty in using estimates of in-sample error is the need to specify the\n",
      "number of parameters (or the complexity) dused in the ﬁt. Although the\n",
      "eﬀective number of parameters introduced in Section 7.6 is useful for some\n",
      "nonlinear models, it is not fully general. The Vapnik–Chervonenkis (VC)\n",
      "theory provides such a general measure of complexity, and gives associated\n",
      "bounds on the optimism. Here we give a brief review of this theory.\n",
      "Suppose we have a class of functions {f(x,α)}indexed by a parameter\n",
      "vector α, with x∈IRp. Assume for now that fis an indicator function,\n",
      "that is, takes the values 0 or 1. If α= (α0,α1) and fis the linear indi-\n",
      "cator function I(α0+αT\n",
      "1x >0), then it seems reasonable to say that the\n",
      "complexity of the class fis the number of parameters p+ 1. But what\n",
      "about f(x,α) =I(sinα≤x) where αis any real number and x∈IR? The\n",
      "function sin(50 ≤x) is shown in Figure 7.5. This is a very wiggly function\n",
      "that gets even rougher as the frequency αincreases, but it has only one\n",
      "parameter: despite this, it doesn’t seem reasonable to conclude that it has\n",
      "less complexity than the linear indicator function I(α0+α1x) inp= 1\n",
      "dimension.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "238 7. Model Assessment and Selection\n",
      "FIGURE 7.6. The ﬁrst three panels show that the class of lines in the plane\n",
      "can shatter three points. The last panel shows that this class c annot shatter four\n",
      "points, as no line will put the hollow points on one side and the solid points on\n",
      "the other. Hence the VC dimension of the class of straight lines i n the plane is\n",
      "three. Note that a class of nonlinear curves could shatter four p oints, and hence\n",
      "has VC dimension greater than three.\n",
      "The Vapnik–Chervonenkis dimension is a way of measuring the com-\n",
      "plexity of a class of functions by assessing how wiggly its members can\n",
      "be.\n",
      "TheVC dimension of the class {f(x,α)}is deﬁned to be the\n",
      "largest number of points (in some conﬁguration) that can be\n",
      "shattered by members of {f(x,α)}.\n",
      "A set of points is said to be shattered by a class of functions if, no matter\n",
      "how we assign a binary label to each point, a member of the class can\n",
      "perfectly separate them.\n",
      "Figure 7.6 shows that the VC dimension of linear indicator functions\n",
      "in the plane is 3 but not 4, since no four points can be shattered by a\n",
      "set of lines. In general, a linear indicator function in pdimensions has VC\n",
      "dimension p+1, which is also the number of free parameters. On the other\n",
      "hand, it can be shown that the family sin( αx) has inﬁnite VC dimension,\n",
      "as Figure 7.5 suggests. By appropriate choice of α, any set of points can be\n",
      "shattered by this class (Exercise 7.8).\n",
      "So far we have discussed the VC dimension only of indicator functions,\n",
      "but this can be extended to real-valued functions. The VC dimension of a\n",
      "class of real-valued functions {g(x,α)}is deﬁned to be the VC dimension\n",
      "of the indicator class {I(g(x,α)−β >0)}, where βtakes values over the\n",
      "range of g.\n",
      "One can use the VC dimension in constructing an estimate of (extra-\n",
      "sample) prediction error; diﬀerent types of results are available. Using the\n",
      "concept of VC dimension, one can prove results about the optimism of the\n",
      "training error when using a class of functions. An example of such a result is\n",
      "the following. If we ﬁt Ntraining points using a class of functions {f(x,α)}\n",
      "having VC dimension h, then with probability at least 1 −ηover training\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.9 Vapnik–Chervonenkis Dimension 239\n",
      "sets:\n",
      "ErrT≤err +ǫ\n",
      "2(\n",
      "1 +√\n",
      "1 +4≤err\n",
      "ǫ)\n",
      "(binary classiﬁcation)\n",
      "ErrT≤err\n",
      "(1−c√ǫ)+(regression) (7.46)\n",
      "where ǫ=a1h[log (a2N/h) + 1]−log (η/4)\n",
      "N,\n",
      "and 0 < a1≤4,0< a2≤2\n",
      "These bounds hold simultaneously for all members f(x,α), and are taken\n",
      "from Cherkassky and Mulier (2007, pages 116–118). They recommend the\n",
      "value c= 1. For regression they suggest a1=a2= 1, and for classiﬁcation\n",
      "they make no recommendation, with a1= 4 and a2= 2 corresponding\n",
      "to worst-case scenarios. They also give an alternative practical bound for\n",
      "regression\n",
      "ErrT≤err(\n",
      "1−√\n",
      "ρ−ρlogρ+logN\n",
      "2N)−1\n",
      "+(7.47)\n",
      "withρ=h\n",
      "N, which is free of tuning constants. The bounds suggest that the\n",
      "optimism increases with hand decreases with Nin qualitative agreement\n",
      "with the AIC correction d/Ngiven is (7.24). However, the results in (7.46)\n",
      "are stronger: rather than giving the expected optimism for each ﬁxed func-\n",
      "tionf(x,α), they give probabilistic upper bounds for all functions f(x,α),\n",
      "and hence allow for searching over the class.\n",
      "Vapnik’s structural risk minimization (SRM) approach ﬁts a nested se-\n",
      "quence of models of increasing VC dimensions h1< h2<≤≤≤, and then\n",
      "chooses the model with the smallest value of the upper bound.\n",
      "We note that upper bounds like the ones in (7.46) are often very loose,\n",
      "but that doesn’t rule them out as good criteria for model selection, where\n",
      "the relative (not absolute) size of the test error is important. The main\n",
      "drawback of this approach is the diﬃculty in calculating the VC dimension\n",
      "of a class of functions. Often only a crude upper bound for VC dimension\n",
      "is obtainable, and this may not be adequate. An example in which the\n",
      "structural risk minimization program can be successfully carried out is the\n",
      "support vector classiﬁer, discussed in Section 12.2.\n",
      "7.9.1 Example (Continued)\n",
      "Figure 7.7 shows the results when AIC, BIC and SRM are used to select\n",
      "the model size for the examples of Figure 7.3. For the examples labeled KNN,\n",
      "the model index αrefers to neighborhood size, while for those labeled REGα\n",
      "refers to subset size. Using each selection method (e.g., AIC) we estimated\n",
      "the best model ˆ αand found its true prediction error Err T(ˆα) on a test\n",
      "set. For the same training set we computed the prediction error of the best\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "240 7. Model Assessment and Selection\n",
      "reg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestAIC\n",
      "reg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBIC\n",
      "reg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestSRM\n",
      "FIGURE 7.7. Boxplots show the distribution of the relative error\n",
      "100×[Err T(ˆα)−min αErrT(α)]/[max αErrT(α)−min αErrT(α)]over the four\n",
      "scenarios of Figure 7.3. This is the error in using the chosen mo del relative to\n",
      "the best model. There are 100training sets each of size 80represented in each\n",
      "boxplot, with the errors computed on test sets of size 10,000.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.10 Cross-Validation 241\n",
      "and worst possible model choices: min αErrT(α) and max αErrT(α). The\n",
      "boxplots show the distribution of the quantity\n",
      "100×ErrT(ˆα)−minαErrT(α)\n",
      "max αErrT(α)−minαErrT(α),\n",
      "which represents the error in using the chosen model relative to the best\n",
      "model. For linear regression the model complexity was measured by the\n",
      "number of features; as mentioned in Section 7.5, this underestimates the\n",
      "df, since it does not charge for the search for the best model of that size.\n",
      "This was also used for the VC dimension of the linear classiﬁer. For k-\n",
      "nearest neighbors, we used the quantity N/k. Under an additive-error re-\n",
      "gression model, this can be justiﬁed as the exact eﬀective degrees of free-\n",
      "dom (Exercise 7.6); we do not know if it corresponds to the VC dimen-\n",
      "sion. We used a1=a2= 1 for the constants in (7.46); the results for SRM\n",
      "changed with diﬀerent constants, and this choice gave the most favorable re-\n",
      "sults. We repeated the SRM selection using the alternative practical bound\n",
      "(7.47), and got almost identical results. For misclassiﬁcation error w e used\n",
      "ˆσε2= [N/(N−d)]≤err(α) for the least restrictive model ( k= 5 for KNN,\n",
      "sincek= 1 results in zero training error). The AIC criterion seems to work\n",
      "well in all four scenarios, despite the lack of theoretical support with 0–1\n",
      "loss. BIC does nearly as well, while the performance of SRM is mixed.\n",
      "7.10 Cross-Validation\n",
      "Probably the simplest and most widely used method for estimating predic-\n",
      "tion error is cross-validation. This method directly estimates the expected\n",
      "extra-sample error Err = E[ L(Y,ˆf(X))], the average generalization error\n",
      "when the method ˆf(X) is applied to an independent test sample from the\n",
      "joint distribution of XandY. As mentioned earlier, we might hope that\n",
      "cross-validation estimates the conditional error, with the training set T\n",
      "held ﬁxed. But as we will see in Section 7.12, cross-validation typically\n",
      "estimates well only the expected prediction error.\n",
      "7.10.1 K-Fold Cross-Validation\n",
      "Ideally, if we had enough data, we would set aside a validation set and use\n",
      "it to assess the performance of our prediction model. Since data are often\n",
      "scarce, this is usually not possible. To ﬁnesse the problem, K-fold cross-\n",
      "validation uses part of the available data to ﬁt the model, and a diﬀerent\n",
      "part to test it. We split the data into Kroughly equal-sized parts; for\n",
      "example, when K= 5, the scenario looks like this:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "242 7. Model Assessment and Selection\n",
      "Validation Train1 2 3 4 5\n",
      "Train Train Train\n",
      "For the kth part (third above), we ﬁt the model to the other K−1 parts\n",
      "of the data, and calculate the prediction error of the ﬁtted model when\n",
      "predicting the kth part of the data. We do this for k= 1,2,... ,K and\n",
      "combine the Kestimates of prediction error.\n",
      "Here are more details. Let κ:{1,... ,N } ↦→ { 1,... ,K }be an indexing\n",
      "function that indicates the partition to which observation iis allocated by\n",
      "the randomization. Denote by ˆf−k(x) the ﬁtted function, computed with\n",
      "thekth part of the data removed. Then the cross-validation estimate of\n",
      "prediction error is\n",
      "CV(ˆf) =1\n",
      "NN∑\n",
      "i=1L(yi,ˆf−κ(i)(xi)). (7.48)\n",
      "Typical choices of Kare 5 or 10 (see below). The case K=Nis known\n",
      "asleave-one-out cross-validation. In this case κ(i) =i, and for the ith\n",
      "observation the ﬁt is computed using all the data except the ith.\n",
      "Given a set of models f(x,α) indexed by a tuning parameter α, denote\n",
      "byˆf−k(x,α) theαth model ﬁt with the kth part of the data removed. Then\n",
      "for this set of models we deﬁne\n",
      "CV(ˆf,α) =1\n",
      "NN∑\n",
      "i=1L(yi,ˆf−κ(i)(xi,α)). (7.49)\n",
      "The function CV( ˆf,α) provides an estimate of the test error curve, and we\n",
      "ﬁnd the tuning parameter ˆ αthat minimizes it. Our ﬁnal chosen model is\n",
      "f(x,ˆα), which we then ﬁt to all the data.\n",
      "It is interesting to wonder about what quantity K-fold cross-validation\n",
      "estimates. With K= 5 or 10, we might guess that it estimates the ex-\n",
      "pected error Err, since the training sets in each fold are quite diﬀerent\n",
      "from the original training set. On the other hand, if K=Nwe might\n",
      "guess that cross-validation estimates the conditional error Err T. It turns\n",
      "out that cross-validation only estimates eﬀectively the average error Err,\n",
      "as discussed in Section 7.12.\n",
      "What value should we choose for K? With K=N, the cross-validation\n",
      "estimator is approximately unbiased for the true (expected) prediction er-\n",
      "ror, but can have high variance because the N“training sets” are so similar\n",
      "to one another. The computational burden is also considerable, requiring\n",
      "Napplications of the learning method. In certain special problems, this\n",
      "computation can be done quickly—see Exercises 7.3 and 5.13.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.10 Cross-Validation 243\n",
      "Size of Training Set1-Err\n",
      "0 50 100 150 2000.0 0.2 0.4 0.6 0.8\n",
      "FIGURE 7.8. Hypothetical learning curve for a classiﬁer on a given task: a\n",
      "plot of 1−Errversus the size of the training set N. With a dataset of 200\n",
      "observations, 5-fold cross-validation would use training sets of size 160, which\n",
      "would behave much like the full set. However, with a dataset o f50observations\n",
      "ﬁvefold cross-validation would use training sets of size 40, and this would result\n",
      "in a considerable overestimate of prediction error.\n",
      "On the other hand, with K= 5 say, cross-validation has lower variance.\n",
      "But bias could be a problem, depending on how the performance of the\n",
      "learning method varies with the size of the training set. Figure 7.8 shows\n",
      "a hypothetical “learning curve” for a classiﬁer on a given task, a plot of\n",
      "1−Err versus the size of the training set N. The performance of the\n",
      "classiﬁer improves as the training set size increases to 100 observations;\n",
      "increasing the number further to 200 brings only a small beneﬁt. If our\n",
      "training set had 200 observations, ﬁvefold cross-validation would estimat e\n",
      "the performance of our classiﬁer over training sets of size 160, which from\n",
      "Figure 7.8 is virtually the same as the performance for training set size\n",
      "200. Thus cross-validation would not suﬀer from much bias. However if the\n",
      "training set had 50 observations, ﬁvefold cross-validation would estimate\n",
      "the performance of our classiﬁer over training sets of size 40, and from the\n",
      "ﬁgure that would be an underestimate of 1 −Err. Hence as an estimate of\n",
      "Err, cross-validation would be biased upward.\n",
      "To summarize, if the learning curve has a considerable slope at the given\n",
      "training set size, ﬁve- or tenfold cross-validation will overestimate the tr ue\n",
      "prediction error. Whether this bias is a drawback in practice depends on\n",
      "the objective. On the other hand, leave-one-out cross-validation has low\n",
      "bias but can have high variance. Overall, ﬁve- or tenfold cross-validation\n",
      "are recommended as a good compromise: see Breiman and Spector (1992)\n",
      "and Kohavi (1995).\n",
      "Figure 7.9 shows the prediction error and tenfold cross-validation curve\n",
      "estimated from a single training set, from the scenario in the bottom righ t\n",
      "panel of Figure 7.3. This is a two-class classiﬁcation problem, using a lin-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "244 7. Model Assessment and Selection\n",
      "Subset Size pMisclassification Error\n",
      "5 10 15 200.0 0.1 0.2 0.3 0.4 0.5 0.6•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "•••••• • •••••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••• • •• ••• • • •\n",
      "FIGURE 7.9. Prediction error (orange) and tenfold cross-validation curve\n",
      "(blue) estimated from a single training set, from the scenario in the bottom right\n",
      "panel of Figure 7.3.\n",
      "ear model with best subsets regression of subset size p. Standard error bars\n",
      "are shown, which are the standard errors of the individual misclassiﬁcation\n",
      "error rates for each of the ten parts. Both curves have minima at p= 10,\n",
      "although the CV curve is rather ﬂat beyond 10. Often a “one-standard\n",
      "error” rule is used with cross-validation, in which we choose the most par-\n",
      "simonious model whose error is no more than one standard error above\n",
      "the error of the best model. Here it looks like a model with about p= 9\n",
      "predictors would be chosen, while the true model uses p= 10.\n",
      "Generalized cross-validation provides a convenient approximation to leave-\n",
      "one out cross-validation, for linear ﬁtting under squared-error loss. As de-\n",
      "ﬁned in Section 7.6, a linear ﬁtting method is one for which we can write\n",
      "ˆy=Sy. (7.50)\n",
      "Now for many linear ﬁtting methods,\n",
      "1\n",
      "NN∑\n",
      "i=1[yi−ˆf−i(xi)]2=1\n",
      "NN∑\n",
      "i=1[yi−ˆf(xi)\n",
      "1−Sii]2\n",
      ", (7.51)\n",
      "where Siiis the ith diagonal element of S(see Exercise 7.3). The GCV\n",
      "approximation is\n",
      "GCV( ˆf) =1\n",
      "NN∑\n",
      "i=1[\n",
      "yi−ˆf(xi)\n",
      "1−trace(S)/N]2\n",
      ". (7.52)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.10 Cross-Validation 245\n",
      "The quantity trace( S) is the eﬀective number of parameters, as deﬁned in\n",
      "Section 7.6.\n",
      "GCV can have a computational advantage in some settings, where the\n",
      "trace of Scan be computed more easily than the individual elements Sii.\n",
      "In smoothing problems, GCV can also alleviate the tendency of cross-\n",
      "validation to undersmooth. The similarity between GCV and AIC can be\n",
      "seen from the approximation 1 /(1−x)2≈1 + 2x(Exercise 7.7).\n",
      "7.10.2 The Wrong and Right Way to Do Cross-validation\n",
      "Consider a classiﬁcation problem with a large number of predictors, as may\n",
      "arise, for example, in genomic or proteomic applications. A typical strategy\n",
      "for analysis might be as follows:\n",
      "1. Screen the predictors: ﬁnd a subset of “good” predictors that show\n",
      "fairly strong (univariate) correlation with the class labels\n",
      "2. Using just this subset of predictors, build a multivariate classiﬁer.\n",
      "3. Use cross-validation to estimate the unknown tuning parameters and\n",
      "to estimate the prediction error of the ﬁnal model.\n",
      "Is this a correct application of cross-validation? Consider a scenario with\n",
      "N= 50 samples in two equal-sized classes, and p= 5000 quantitative\n",
      "predictors (standard Gaussian) that are independent of the class labels.\n",
      "The true (test) error rate of any classiﬁer is 50%. We carried out the above\n",
      "recipe, choosing in step (1) the 100 predictors having highest correlation\n",
      "with the class labels, and then using a 1-nearest neighbor classiﬁer, based\n",
      "on just these 100 predictors, in step (2). Over 50 simulations from this\n",
      "setting, the average CV error rate was 3%. This is far lower than the true\n",
      "error rate of 50%.\n",
      "What has happened? The problem is that the predictors have an unfair\n",
      "advantage, as they were chosen in step (1) on the basis of all of the samples .\n",
      "Leaving samples out afterthe variables have been selected does not cor-\n",
      "rectly mimic the application of the classiﬁer to a completely independent\n",
      "test set, since these predictors “have already seen” the left out samples.\n",
      "Figure 7.10 (top panel) illustrates the problem. We selected the 100 pre-\n",
      "dictors having largest correlation with the class labels over all 50 sampl es.\n",
      "Then we chose a random set of 10 samples, as we would do in ﬁve-fold cross-\n",
      "validation, and computed the correlations of the pre-selected 100 predictors\n",
      "with the class labels over just these 10 samples (top panel). We see that\n",
      "the correlations average about 0.28, rather than 0, as one might expect.\n",
      "Here is the correct way to carry out cross-validation in this example:\n",
      "1. Divide the samples into Kcross-validation folds (groups) at random.\n",
      "2. For each fold k= 1,2,... ,K\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "246 7. Model Assessment and Selection\n",
      "Correlations of Selected Predictors with OutcomeFrequency\n",
      "−1.0 −0.5 0.0 0.5 1.00 10 20 30Wrong way\n",
      "Correlations of Selected Predictors with OutcomeFrequency\n",
      "−1.0 −0.5 0.0 0.5 1.00 10 20 30Right way\n",
      "FIGURE 7.10. Cross-validation the wrong and right way: histograms shows th e\n",
      "correlation of class labels, in 10randomly chosen samples, with the 100predic-\n",
      "tors chosen using the incorrect (upper red) and correct (lower g reen) versions of\n",
      "cross-validation.\n",
      "(a) Find a subset of “good” predictors that show fairly strong (uni-\n",
      "variate) correlation with the class labels, using all of the samples\n",
      "except those in fold k.\n",
      "(b) Using just this subset of predictors, build a multivariate classi-\n",
      "ﬁer, using all of the samples except those in fold k.\n",
      "(c) Use the classiﬁer to predict the class labels for the samples in\n",
      "foldk.\n",
      "The error estimates from step 2(c) are then accumulated over all Kfolds, to\n",
      "produce the cross-validation estimate of prediction error. The lower panel\n",
      "of Figure 7.10 shows the correlations of class labels with the 100 predictor s\n",
      "chosen in step 2(a) of the correct procedure, over the samples in a typical\n",
      "foldk. We see that they average about zero, as they should.\n",
      "In general, with a multistep modeling procedure, cross-validation must\n",
      "be applied to the entire sequence of modeling steps. In particular, samples\n",
      "must be “left out” before any selection or ﬁltering steps are applied. There\n",
      "is one qualiﬁcation: initial unsupervised screening steps can be done be-\n",
      "fore samples are left out. For example, we could select the 1000 predictors\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.10 Cross-Validation 247\n",
      "with highest variance across all 50 samples, before starting cross-valida tion.\n",
      "Since this ﬁltering does not involve the class labels, it does not give the\n",
      "predictors an unfair advantage.\n",
      "While this point may seem obvious to the reader, we have seen this\n",
      "blunder committed many times in published papers in top rank journals.\n",
      "With the large numbers of predictors that are so common in genomic and\n",
      "other areas, the potential consequences of this error have also increased\n",
      "dramatically; see Ambroise and McLachlan (2002) for a detailed discussion\n",
      "of this issue.\n",
      "7.10.3 Does Cross-Validation Really Work?\n",
      "We once again examine the behavior of cross-validation in a high-dimensional\n",
      "classiﬁcation problem. Consider a scenario with N= 20 samples in two\n",
      "equal-sized classes, and p= 500 quantitative predictors that are indepen-\n",
      "dent of the class labels. Once again, the true error rate of any classiﬁer is\n",
      "50%. Consider a simple univariate classiﬁer: a single split that minimizes\n",
      "the misclassiﬁcation error (a “stump”). Stumps are trees with a single split ,\n",
      "and are used in boosting methods (Chapter 10). A simple argument sug-\n",
      "gests that cross-validation will not work properly in this setting2:\n",
      "Fitting to the entire training set, we will ﬁnd a predictor th at\n",
      "splits the data very well If we do 5-fold cross-validation, thi s\n",
      "same predictor should split any 4/5ths and 1/5th of the data\n",
      "well too, and hence its cross-validation error will be small ( much\n",
      "less than 50%) Thus CV does not give an accurate estimate of\n",
      "error.\n",
      "To investigate whether this argument is correct, Figure 7.11 shows the\n",
      "result of a simulation from this setting. There are 500 predictors and 20\n",
      "samples, in each of two equal-sized classes, with all predictors having a\n",
      "standard Gaussian distribution. The panel in the top left shows the number\n",
      "of training errors for each of the 500 stumps ﬁt to the training data. We\n",
      "have marked in color the six predictors yielding the fewest errors. In the top\n",
      "right panel, the training errors are shown for stumps ﬁt to a random 4 /5ths\n",
      "partition of the data (16 samples), and tested on the remaining 1 /5th (four\n",
      "samples). The colored points indicate the same predictors marked in the\n",
      "top left panel. We see that the stump for the blue predictor (whose stump\n",
      "was the best in the top left panel), makes two out of four test errors (50%),\n",
      "and is no better than random.\n",
      "What has happened? The preceding argument has ignored the fact that\n",
      "in cross-validation, the model must be completely retrained for each fold\n",
      "2This argument was made to us by a scientist at a proteomics lab meeting, and led\n",
      "to material in this section.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "248 7. Model Assessment and Selection\n",
      "0 100 200 300 400 5002 3 4 5 6 7 8 9\n",
      "PredictorError on Full Training Set\n",
      "1 2 3 4 5 6 7 80 1 2 3 4\n",
      "Error on 4/5Error on 1/5\n",
      "−1 0 1 2\n",
      "Predictor 436 (blue)Class Label\n",
      "0 1\n",
      "full\n",
      "4/5\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "CV Errors\n",
      "FIGURE 7.11. Simulation study to investigate the performance of cross vali-\n",
      "dation in a high-dimensional problem where the predictors are independent of the\n",
      "class labels. The top-left panel shows the number of errors mad e by individual\n",
      "stump classiﬁers on the full training set ( 20observations). The top right panel\n",
      "shows the errors made by individual stumps trained on a random sp lit of the\n",
      "dataset into 4/5ths (16observations) and tested on the remaining 1/5th (4ob-\n",
      "servations). The best performers are depicted by colored dot s in each panel. The\n",
      "bottom left panel shows the eﬀect of re-estimating the split po int in each fold: the\n",
      "colored points correspond to the four samples in the 4/5ths validation set. The\n",
      "split point derived from the full dataset classiﬁes all four sa mples correctly, but\n",
      "when the split point is re-estimated on the 4/5ths data (as it should be), it com-\n",
      "mits two errors on the four validation samples. In the bottom right we see the\n",
      "overall result of ﬁve-fold cross-validation applied to 50simulated datasets. The\n",
      "average error rate is about 50%, as it should be.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.11 Bootstrap Methods 249\n",
      "of the process. In the present example, this means that the best predictor\n",
      "and corresponding split point are found from 4 /5ths of the data. The eﬀect\n",
      "of predictor choice is seen in the top right panel. Since the class labels are\n",
      "independent of the predictors, the performance of a stump on the 4 /5ths\n",
      "training data contains no information about its performance in the remain-\n",
      "ing 1/5th. The eﬀect of the choice of split point is shown in the bottom left\n",
      "panel. Here we see the data for predictor 436, corresponding to the blue\n",
      "dot in the top left plot. The colored points indicate the 1 /5th data, while\n",
      "the remaining points belong to the 4 /5ths. The optimal split points for this\n",
      "predictor based on both the full training set and 4 /5ths data are indicated.\n",
      "The split based on the full data makes no errors on the 1 /5ths data. But\n",
      "cross-validation must base its split on the 4 /5ths data, and this incurs two\n",
      "errors out of four samples.\n",
      "The results of applying ﬁve-fold cross-validation to each of 50 simulated\n",
      "datasets is shown in the bottom right panel. As we would hope, the average\n",
      "cross-validation error is around 50%, which is the true expected prediction\n",
      "error for this classiﬁer. Hence cross-validation has behaved as it should.\n",
      "On the other hand, there is considerable variability in the error, underscor-\n",
      "ing the importance of reporting the estimated standard error of the CV\n",
      "estimate. See Exercise 7.10 for another variation of this problem.\n",
      "7.11 Bootstrap Methods\n",
      "The bootstrap is a general tool for assessing statistical accuracy. Firs t we\n",
      "describe the bootstrap in general, and then show how it can be used to\n",
      "estimate extra-sample prediction error. As with cross-validation, the boo t-\n",
      "strap seeks to estimate the conditional error Err T, but typically estimates\n",
      "well only the expected prediction error Err.\n",
      "Suppose we have a model ﬁt to a set of training data. We denote the\n",
      "training set by Z= (z1,z2,... ,z N) where zi= (xi,yi). The basic idea is\n",
      "to randomly draw datasets with replacement from the training data, each\n",
      "sample the same size as the original training set. This is done Btimes\n",
      "(B= 100 say), producing Bbootstrap datasets, as shown in Figure 7.12.\n",
      "Then we reﬁt the model to each of the bootstrap datasets, and examine\n",
      "the behavior of the ﬁts over the Breplications.\n",
      "In the ﬁgure, S(Z) is any quantity computed from the data Z, for ex-\n",
      "ample, the prediction at some input point. From the bootstrap sampling\n",
      "we can estimate any aspect of the distribution of S(Z), for example, its\n",
      "variance,\n",
      "ˆVar[S(Z)] =1\n",
      "B−1B∑\n",
      "b=1(S(Z∗b)−¯S∗)2, (7.53)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "250 7. Model Assessment and Selection\n",
      "  Bootstrap\n",
      "Bootstrapreplications\n",
      "samples\n",
      "sampleTrainingZ= (z1,z2,... ,z N)Z∗1Z∗2Z∗BS(Z∗1) S(Z∗2) S(Z∗B)\n",
      "FIGURE 7.12. Schematic of the bootstrap process. We wish to assess the sta-\n",
      "tistical accuracy of a quantity S(Z)computed from our dataset. Btraining sets\n",
      "Z∗b, b= 1, . . . , B each of size Nare drawn with replacement from the original\n",
      "dataset. The quantity of interest S(Z)is computed from each bootstrap training\n",
      "set, and the values S(Z∗1), . . . , S (Z∗B)are used to assess the statistical accuracy\n",
      "ofS(Z).\n",
      "where ¯S∗=∑\n",
      "bS(Z∗b)/B. Note that ˆVar[S(Z)] can be thought of as a\n",
      "Monte-Carlo estimate of the variance of S(Z) under sampling from the\n",
      "empirical distribution function ˆFfor the data ( z1,z2,... ,z N).\n",
      "How can we apply the bootstrap to estimate prediction error? One ap-\n",
      "proach would be to ﬁt the model in question on a set of bootstrap samples,\n",
      "and then keep track of how well it predicts the original training set. If\n",
      "ˆf∗b(xi) is the predicted value at xi, from the model ﬁtted to the bth boot-\n",
      "strap dataset, our estimate is\n",
      "ˆErrboot=1\n",
      "B1\n",
      "NB∑\n",
      "b=1N∑\n",
      "i=1L(yi,ˆf∗b(xi)). (7.54)\n",
      "However, it is easy to see that ˆErrbootdoes not provide a good estimate in\n",
      "general. The reason is that the bootstrap datasets are acting as the training\n",
      "samples, while the original training set is acting as the test sample, and\n",
      "these two samples have observations in common. This overlap can make\n",
      "overﬁt predictions look unrealistically good, and is the reason that cross-\n",
      "validation explicitly uses non-overlapping data for the training and test\n",
      "samples. Consider for example a 1-nearest neighbor classiﬁer applied to a\n",
      "two-class classiﬁcation problem with the same number of observations in\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.11 Bootstrap Methods 251\n",
      "each class, in which the predictors and class labels are in fact independent.\n",
      "Then the true error rate is 0 .5. But the contributions to the bootstrap\n",
      "estimate ˆErrbootwill be zero unless the observation idoes not appear in the\n",
      "bootstrap sample b. In this latter case it will have the correct expectation\n",
      "0.5. Now\n",
      "Pr{observation i∈bootstrap sample b}= 1−(\n",
      "1−1\n",
      "N)N\n",
      "≈1−e−1\n",
      "= 0.632. (7.55)\n",
      "Hence the expectation of ˆErrbootis about 0 .5×0.368 = 0 .184, far below\n",
      "the correct error rate 0 .5.\n",
      "By mimicking cross-validation, a better bootstrap estimate can be ob-\n",
      "tained. For each observation, we only keep track of predictions from boot-\n",
      "strap samples not containing that observation. The leave-one-out bootstrap\n",
      "estimate of prediction error is deﬁned by\n",
      "ˆErr(1)=1\n",
      "NN∑\n",
      "i=11\n",
      "|C−i|∑\n",
      "b∈C−iL(yi,ˆf∗b(xi)). (7.56)\n",
      "HereC−iis the set of indices of the bootstrap samples bthat do notcontain\n",
      "observation i, and|C−i|is the number of such samples. In computing ˆErr(1),\n",
      "we either have to choose Blarge enough to ensure that all of the |C−i|are\n",
      "greater than zero, or we can just leave out the terms in (7.56) corresponding\n",
      "to|C−i|’s that are zero.\n",
      "The leave-one out bootstrap solves the overﬁtting problem suﬀered by\n",
      "ˆErrboot, but has the training-set-size bias mentioned in the discussion of\n",
      "cross-validation. The average number of distinct observations in each boot-\n",
      "strap sample is about 0 .632≤N, so its bias will roughly behave like that of\n",
      "twofold cross-validation. Thus if the learning curve has considerable slope\n",
      "at sample size N/2, the leave-one out bootstrap will be biased upward as\n",
      "an estimate of the true error.\n",
      "The “ .632 estimator” is designed to alleviate this bias. It is deﬁned by\n",
      "ˆErr(.632)=.368≤err +.632≤ˆErr(1). (7.57)\n",
      "The derivation of the .632 estimator is complex; intuitively it pulls the\n",
      "leave-one out bootstrap estimate down toward the training error rate, and\n",
      "hence reduces its upward bias. The use of the constant .632 relates to (7.55).\n",
      "The.632 estimator works well in “light ﬁtting” situations, but can break\n",
      "down in overﬁt ones. Here is an example due to Breiman et al. (1984).\n",
      "Suppose we have two equal-size classes, with the targets independent of\n",
      "the class labels, and we apply a one-nearest neighbor rule. Then err = 0,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "252 7. Model Assessment and Selection\n",
      "ˆErr(1)= 0.5 and so ˆErr(.632)=.632×0.5 =.316. However, the true error\n",
      "rate is 0.5.\n",
      "One can improve the .632 estimator by taking into account the amount\n",
      "of overﬁtting. First we deﬁne γto be the no-information error rate : this\n",
      "is the error rate of our prediction rule if the inputs and class labels were\n",
      "independent. An estimate of γis obtained by evaluating the prediction rule\n",
      "on all possible combinations of targets yiand predictors xi′\n",
      "ˆγ=1\n",
      "N2N∑\n",
      "i=1N∑\n",
      "i′=1L(yi,ˆf(xi′)). (7.58)\n",
      "For example, consider the dichotomous classiﬁcation problem: let ˆ p1be\n",
      "the observed proportion of responses yiequaling 1, and let ˆ q1be the ob-\n",
      "served proportion of predictions ˆf(xi′) equaling 1. Then\n",
      "ˆγ= ˆp1(1−ˆq1) + (1 −ˆp1)ˆq1. (7.59)\n",
      "With a rule like 1-nearest neighbors for which ˆ q1= ˆp1the value of ˆ γis\n",
      "2ˆp1(1−ˆp1). The multi-category generalization of (7.59) is ˆ γ=∑\n",
      "ℓˆpℓ(1−ˆqℓ).\n",
      "Using this, the relative overﬁtting rate is deﬁned to be\n",
      "ˆR=ˆErr(1)−err\n",
      "ˆγ−err, (7.60)\n",
      "a quantity that ranges from 0 if there is no overﬁtting ( ˆErr(1)=err) to 1\n",
      "if the overﬁtting equals the no-information value ˆ γ−err. Finally, we deﬁne\n",
      "the “.632+” estimator by\n",
      "ˆErr(.632+)= (1 −ˆw)≤err + ˆw≤ˆErr(1)(7.61)\n",
      "with ˆw=.632\n",
      "1−.368ˆR.\n",
      "The weight wranges from .632 if ˆR= 0 to 1 if ˆR= 1, so ˆErr(.632+)\n",
      "ranges from ˆErr(.632)toˆErr(1). Again, the derivation of (7.61) is compli-\n",
      "cated: roughly speaking, it produces a compromise between the leave-one-\n",
      "out bootstrap and the training error rate that depends on the amount of\n",
      "overﬁtting. For the 1-nearest-neighbor problem with class labels indepen-\n",
      "dent of the inputs, ˆ w=ˆR= 1, so ˆErr(.632+)=ˆErr(1), which has the correct\n",
      "expectation of 0.5. In other problems with less overﬁtting, ˆErr(.632+)will\n",
      "lie somewhere between err and ˆErr(1).\n",
      "7.11.1 Example (Continued)\n",
      "Figure 7.13 shows the results of tenfold cross-validation and the .632+ bo ot-\n",
      "strap estimate in the same four problems of Figures 7.7. As in that ﬁgure,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.11 Bootstrap Methods 253\n",
      "reg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestCross−validation\n",
      "reg/KNN reg/linear class/KNN class/linear0 20 40 60 80 100% Increase Over BestBootstrap\n",
      "FIGURE 7.13. Boxplots show the distribution of the relative error\n",
      "100≤[Errˆα−min αErr(α)]/[max αErr(α)−min αErr(α)]over the four scenar-\n",
      "ios of Figure 7.3. This is the error in using the chosen model re lative to the best\n",
      "model. There are 100training sets represented in each boxplot.\n",
      "Figure 7.13 shows boxplots of 100 ≤[Errˆα−minαErr(α)]/[max αErr(α)−\n",
      "minαErr(α)], the error in using the chosen model relative to the best model.\n",
      "There are 100 diﬀerent training sets represented in each boxplot. Both mea-\n",
      "sures perform well overall, perhaps the same or slightly worse that the AI C\n",
      "in Figure 7.7.\n",
      "Our conclusion is that for these particular problems and ﬁtting methods,\n",
      "minimization of either AIC, cross-validation or bootstrap yields a model\n",
      "fairly close to the best available. Note that for the purpose of model selec-\n",
      "tion, any of the measures could be biased and it wouldn’t aﬀect things, as\n",
      "long as the bias did not change the relative performance of the methods.\n",
      "For example, the addition of a constant to any of the measures would not\n",
      "change the resulting chosen model. However, for many adaptive, nonlinear\n",
      "techniques (like trees), estimation of the eﬀective number of parameters is\n",
      "very diﬃcult. This makes methods like AIC impractical and leaves us with\n",
      "cross-validation or bootstrap as the methods of choice.\n",
      "A diﬀerent question is: how well does each method estimate test error?\n",
      "On the average the AIC criterion overestimated prediction error of its cho-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "254 7. Model Assessment and Selection\n",
      "sen model by 38%, 37%, 51%, and 30%, respectively, over the four scenarios,\n",
      "with BIC performing similarly. In contrast, cross-validation over estimated\n",
      "the error by 1%, 4%, 0%, and 4%, with the bootstrap doing about the\n",
      "same. Hence the extra work involved in computing a cross-validation or\n",
      "bootstrap measure is worthwhile, if an accurate estimate of test error i s\n",
      "required. With other ﬁtting methods like trees, cross-validation and boot-\n",
      "strap can underestimate the true error by 10%, because the search for best\n",
      "tree is strongly aﬀected by the validation set. In these situations only a\n",
      "separate test set will provide an unbiased estimate of test error.\n",
      "7.12 Conditional or Expected Test Error?\n",
      "Figures 7.14 and 7.15 examine the question of whether cross-validation does\n",
      "a good job in estimating Err T, the error conditional on a given training set\n",
      "T(expression (7.15) on page 228), as opposed to the expected test error.\n",
      "For each of 100 training sets generated from the “reg/linear” setting in\n",
      "the top-right panel of Figure 7.3, Figure 7.14 shows the conditional error\n",
      "curves Err Tas a function of subset size (top left). The next two panels show\n",
      "10-fold and N-fold cross-validation, the latter also known as leave-one-out\n",
      "(LOO). The thick red curve in each plot is the expected error Err, while\n",
      "the thick black curves are the expected cross-validation curves. The lower\n",
      "right panel shows how well cross-validation approximates the conditional\n",
      "and expected error.\n",
      "One might have expected N-fold CV to approximate Err Twell, since it\n",
      "almost uses the full training sample to ﬁt a new test point. 10-fold CV, on\n",
      "the other hand, might be expected to estimate Err well, since it averages\n",
      "over somewhat diﬀerent training sets. From the ﬁgure it appears 10-fold\n",
      "does a better job than N-fold in estimating Err T, and estimates Err even\n",
      "better. Indeed, the similarity of the two black curves with the red curve\n",
      "suggests both CV curves are approximately unbiased for Err, with 10-fold\n",
      "having less variance. Similar trends were reported by Efron (1983).\n",
      "Figure 7.15 shows scatterplots of both 10-fold and N-fold cross-validation\n",
      "error estimates versus the true conditional error for the 100 simulations.\n",
      "Although the scatterplots do not indicate much correlation, the lower right\n",
      "panel shows that for the most part the correlations are negative, a curi-\n",
      "ous phenomenon that has been observed before. This negative correlation\n",
      "explains why neither form of CV estimates Err Twell. The broken lines in\n",
      "each plot are drawn at Err( p), the expected error for the best subset of\n",
      "sizep. We see again that both forms of CV are approximately unbiased for\n",
      "expected error, but the variation in test error for diﬀerent training sets is\n",
      "quite substantial.\n",
      "Among the four experimental conditions in 7.3, this “reg/linear” scenario\n",
      "showed the highest correlation between actual and predicted test error. This\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "7.12 Conditional or Expected Test Error? 255\n",
      "5 10 15 200.1 0.2 0.3 0.4Prediction Error\n",
      "Subset Size pError\n",
      "5 10 15 200.1 0.2 0.3 0.410−Fold CV Error\n",
      "Subset Size pError\n",
      "5 10 15 200.1 0.2 0.3 0.4Leave−One−Out CV Error\n",
      "Subset Size pError\n",
      "5 10 15 200.015 0.025 0.035 0.045Approximation Error\n",
      "Subset Size pMean Absolute DeviationET|CV10−Err|\n",
      "ET|CV10−ErrT|\n",
      "ET|CVN−ErrT|\n",
      "FIGURE 7.14. Conditional prediction-error ErrT,10-fold cross-validation, and\n",
      "leave-one-out cross-validation curves for a 100simulations from the top-right\n",
      "panel in Figure 7.3. The thick red curve is the expected predict ion error Err,\n",
      "while the thick black curves are the expected CV curves ETCV10andETCVN.\n",
      "The lower-right panel shows the mean absolute deviation of th e CV curves from\n",
      "the conditional error, ET|CVK−ErrT|forK= 10(blue) and K=N(green),\n",
      "as well as from the expected error ET|CV10−Err|(orange).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "256 7. Model Assessment and Selection\n",
      "0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 1\n",
      "Prediction ErrorCV Error\n",
      "0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 5\n",
      "Prediction ErrorCV Error\n",
      "0.10 0.15 0.20 0.25 0.30 0.35 0.400.10 0.20 0.30 0.40Subset Size 10\n",
      "Prediction ErrorCV Error\n",
      "5 10 15 20−0.6 −0.4 −0.2 0.0 0.2\n",
      "Subset SizeCorrelation\n",
      "Leave−one−out\n",
      "10−Fold\n",
      "FIGURE 7.15. Plots of the CV estimates of error versus the true conditional\n",
      "error for each of the 100training sets, for the simulation setup in the top right\n",
      "panel Figure 7.3. Both 10-fold and leave-one-out CV are depicted in diﬀerent\n",
      "colors. The ﬁrst three panels correspond to diﬀerent subset size sp, and vertical\n",
      "and horizontal lines are drawn at Err(p). Although there appears to be little cor-\n",
      "relation in these plots, we see in the lower right panel that fo r the most part the\n",
      "correlation is negative .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 257\n",
      "phenomenon also occurs for bootstrap estimates of error, and we would\n",
      "guess, for any other estimate of conditional prediction error.\n",
      "We conclude that estimation of test error for a particular training set is\n",
      "not easy in general, given just the data from that same training set. Instead,\n",
      "cross-validation and related methods may provide reasonable estimates of\n",
      "theexpected error Err.\n",
      "Bibliographic Notes\n",
      "Key references for cross-validation are Stone (1974), Stone (1977) and\n",
      "Allen (1974). The AIC was proposed by Akaike (1973), while the BIC\n",
      "was introduced by Schwarz (1978). Madigan and Raftery (1994) give an\n",
      "overview of Bayesian model selection. The MDL criterion is due to Rissa-\n",
      "nen (1983). Cover and Thomas (1991) contains a good description of coding\n",
      "theory and complexity. VC dimension is described in Vapnik (1996). Stone\n",
      "(1977) showed that the AIC and leave-one out cross-validation are asymp-\n",
      "totically equivalent. Generalized cross-validation is described by Golub et\n",
      "al. (1979) and Wahba (1980); a further discussion of the topic may be found\n",
      "in the monograph by Wahba (1990). See also Hastie and Tibshirani (1990),\n",
      "Chapter 3. The bootstrap is due to Efron (1979); see Efron and Tibshi-\n",
      "rani (1993) for an overview. Efron (1983) proposes a number of bootst rap\n",
      "estimates of prediction error, including the optimism and .632 estimates.\n",
      "Efron (1986) compares CV, GCV and bootstrap estimates of error rates.\n",
      "The use of cross-validation and the bootstrap for model selection is stud-\n",
      "ied by Breiman and Spector (1992), Breiman (1992), Shao (1996), Zhang\n",
      "(1993) and Kohavi (1995). The .632+ estimator was proposed by Efron\n",
      "and Tibshirani (1997).\n",
      "Cherkassky and Ma (2003) published a study on the performance of\n",
      "SRM for model selection in regression, in response to our study of section\n",
      "7.9.1. They complained that we had been unfair to SRM because had not\n",
      "applied it properly. Our response can be found in the same issue of the\n",
      "journal (Hastie et al. (2003)).\n",
      "Exercises\n",
      "Ex. 7.1 Derive the estimate of in-sample error (7.24).\n",
      "Ex. 7.2 For 0–1 loss with Y∈ {0,1}and Pr( Y= 1|x0) =f(x0), show that\n",
      "Err(x0) = Pr( Y̸=ˆG(x0)|X=x0)\n",
      "= Err B(x0) +|2f(x0)−1|Pr(ˆG(x0)̸=G(x0)|X=x0),\n",
      "(7.62)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "258 7. Model Assessment and Selection\n",
      "where ˆG(x) =I(ˆf(x)>1\n",
      "2),G(x) =I(f(x)>1\n",
      "2) is the Bayes classiﬁer,\n",
      "and Err B(x0) = Pr( Y̸=G(x0)|X=x0), the irreducible Bayes error atx0.\n",
      "Using the approximation ˆf(x0)∼N(Eˆf(x0),Var(ˆf(x0)), show that\n",
      "Pr(ˆG(x0)̸=G(x0)|X=x0)≈Φ(\n",
      "sign(1\n",
      "2−f(x0))(Eˆf(x0)−1\n",
      "2)√\n",
      "Var(ˆf(x0)))\n",
      ".(7.63)\n",
      "In the above,\n",
      "Φ(t) =1√\n",
      "2π∫t\n",
      "−∞exp(−t2/2)dt,\n",
      "the cumulative Gaussian distribution function. This is an increasing func-\n",
      "tion, with value 0 at t=−∞and value 1 at t= +∞.\n",
      "We can think of sign(1\n",
      "2−f(x0))(Eˆf(x0)−1\n",
      "2) as a kind of boundary-\n",
      "biasterm, as it depends on the true f(x0) only through which side of the\n",
      "boundary (1\n",
      "2) that it lies. Notice also that the bias and variance combine\n",
      "in a multiplicative rather than additive fashion. If E ˆf(x0) is on the same\n",
      "side of1\n",
      "2asf(x0), then the bias is negative, and decreasing the variance\n",
      "will decrease the misclassiﬁcation error. On the other hand, if E ˆf(x0) is\n",
      "on the opposite side of1\n",
      "2tof(x0), then the bias is positive and it pays to\n",
      "increase the variance! Such an increase will improve the chance that ˆf(x0)\n",
      "falls on the correct side of1\n",
      "2(Friedman, 1997).\n",
      "Ex. 7.3 Letˆf=Sybe a linear smoothing of y.\n",
      "(a) IfSiiis the ith diagonal element of S, show that for Sarising from least\n",
      "squares projections and cubic smoothing splines, the cross-validated\n",
      "residual can be written as\n",
      "yi−ˆf−i(xi) =yi−ˆf(xi)\n",
      "1−Sii. (7.64)\n",
      "(b) Use this result to show that |yi−ˆf−i(xi)| ≥ |yi−ˆf(xi)|.\n",
      "(c) Find general conditions on any smoother Sto make result (7.64) hold.\n",
      "Ex. 7.4 Consider the in-sample prediction error (7.18) and the training\n",
      "errorerr in the case of squared-error loss:\n",
      "Errin=1\n",
      "NN∑\n",
      "i=1EY0(Y0\n",
      "i−ˆf(xi))2\n",
      "err =1\n",
      "NN∑\n",
      "i=1(yi−ˆf(xi))2.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 259\n",
      "Add and subtract f(xi) and E ˆf(xi) in each expression and expand. Hence\n",
      "establish that the average optimism in the training error is\n",
      "2\n",
      "NN∑\n",
      "i=1Cov(ˆyi,yi),\n",
      "as given in (7.21).\n",
      "Ex. 7.5 For a linear smoother ˆy=Sy, show that\n",
      "N∑\n",
      "i=1Cov(ˆyi,yi) = trace( S)σ2\n",
      "ε, (7.65)\n",
      "which justiﬁes its use as the eﬀective number of parameters.\n",
      "Ex. 7.6 Show that for an additive-error model, the eﬀective degrees-of-\n",
      "freedom for the k-nearest-neighbors regression ﬁt is N/k.\n",
      "Ex. 7.7 Use the approximation 1 /(1−x)2≈1+2xto expose the relationship\n",
      "between Cp/AIC (7.26) and GCV (7.52), the main diﬀerence being the\n",
      "model used to estimate the noise variance σ2\n",
      "ε.\n",
      "Ex. 7.8 Show that the set of functions {I(sin(αx)>0)}can shatter the\n",
      "following points on the line:\n",
      "z1= 10−1,... ,zℓ= 10−ℓ, (7.66)\n",
      "for any ℓ. Hence the VC dimension of the class {I(sin(αx)>0)}is inﬁnite.\n",
      "Ex. 7.9 For the prostate data of Chapter 3, carry out a best-subset linear\n",
      "regression analysis, as in Table 3.3 (third column from left). Compute t he\n",
      "AIC, BIC, ﬁve- and tenfold cross-validation, and bootstrap .632 estimat es\n",
      "of prediction error. Discuss the results.\n",
      "Ex. 7.10 Referring to the example in Section 7.10.3, suppose instead that\n",
      "all of the ppredictors are binary, and hence there is no need to estimate\n",
      "split points. The predictors are independent of the class labels as before.\n",
      "Then if pis very large, we can probably ﬁnd a predictor that splits the\n",
      "entire training data perfectly, and hence would split the validation data\n",
      "(one-ﬁfth of data) perfectly as well. This predictor would therefore have\n",
      "zero cross-validation error. Does this mean that cross-validation does not\n",
      "provide a good estimate of test error in this situation? [This question wa s\n",
      "suggested by Li Ma.]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "260 7. Model Assessment and Selection\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 261\n",
      "Printer: Opaque this\n",
      "8\n",
      "Model Inference and Averaging\n",
      "8.1 Introduction\n",
      "For most of this book, the ﬁtting (learning) of models has been achieved by\n",
      "minimizing a sum of squares for regression, or by minimizing cross-entropy\n",
      "for classiﬁcation. In fact, both of these minimizations are instances of the\n",
      "maximum likelihood approach to ﬁtting.\n",
      "In this chapter we provide a general exposition of the maximum likeli-\n",
      "hood approach, as well as the Bayesian method for inference. The boot-\n",
      "strap, introduced in Chapter 7, is discussed in this context, and its relation\n",
      "to maximum likelihood and Bayes is described. Finally, we present some\n",
      "related techniques for model averaging and improvement, including com-\n",
      "mittee methods, bagging, stacking and bumping.\n",
      "8.2 The Bootstrap and Maximum Likelihood\n",
      "Methods\n",
      "8.2.1 A Smoothing Example\n",
      "The bootstrap method provides a direct computational way of assessing\n",
      "uncertainty, by sampling from the training data. Here we illustrate the\n",
      "bootstrap in a simple one-dimensional smoothing problem, and show its\n",
      "connection to maximum likelihood.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "262 8. Model Inference and Averaging\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\n",
      "xy\n",
      "••••••\n",
      "••••••••••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "•••••••••\n",
      "••••••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.00.0 0.2 0.4 0.6 0.8 1.0\n",
      "xB-spline Basis\n",
      "FIGURE 8.1. (Left panel): Data for smoothing example. (Right panel:) Set of\n",
      "seven B-spline basis functions. The broken vertical lines indicate the p lacement\n",
      "of the three knots.\n",
      "Denote the training data by Z={z1,z2,... ,z N}, with zi= (xi,yi),\n",
      "i= 1,2,... ,N . Here xiis a one-dimensional input, and yithe outcome,\n",
      "either continuous or categorical. As an example, consider the N= 50 data\n",
      "points shown in the left panel of Figure 8.1.\n",
      "Suppose we decide to ﬁt a cubic spline to the data, with three knots\n",
      "placed at the quartiles of the Xvalues. This is a seven-dimensional lin-\n",
      "ear space of functions, and can be represented, for example, by a linear\n",
      "expansion of B-spline basis functions (see Section 5.9.2):\n",
      "θ(x) =7∑\n",
      "j=1βjhj(x). (8.1)\n",
      "Here the hj(x),j= 1,2,... ,7 are the seven functions shown in the right\n",
      "panel of Figure 8.1. We can think of θ(x) as representing the conditional\n",
      "mean E( Y|X=x).\n",
      "LetHbe the N×7 matrix with ijth element hj(xi). The usual estimate\n",
      "ofβ, obtained by minimizing the squared error over the training set, is\n",
      "given by\n",
      "ˆβ= (HTH)−1HTy. (8.2)\n",
      "The corresponding ﬁt ˆ θ(x) =∑7\n",
      "j=1ˆβjhj(x) is shown in the top left panel\n",
      "of Figure 8.2.\n",
      "The estimated covariance matrix of ˆβis\n",
      "ˆVar(ˆβ) = (HTH)−1ˆσ2, (8.3)\n",
      "where we have estimated the noise variance by ˆ σ2=∑N\n",
      "i=1(yi−ˆθ(xi))2/N.\n",
      "Letting h(x)T= (h1(x),h2(x),... ,h 7(x)), the standard error of a predic-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.2 The Bootstrap and Maximum Likelihood Methods 263\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\n",
      "••••••••••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "•••••••••\n",
      "••••••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "xy\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\n",
      "xy\n",
      "••••••\n",
      "••••••••••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "•••••••••\n",
      "••••••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\n",
      "xy\n",
      "••••••\n",
      "••••••••••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "•••••••••\n",
      "••••••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5\n",
      "xy\n",
      "••••••\n",
      "••••••••••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "•••••••••\n",
      "••••••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "FIGURE 8.2. (Top left:) B-spline smooth of data. (Top right:) B-spline smooth\n",
      "plus and minus 1.96×standard error bands. (Bottom left:) Ten bootstrap repli-\n",
      "cates of the B-spline smooth. (Bottom right:) B-spline smooth with 95% standard\n",
      "error bands computed from the bootstrap distribution.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "264 8. Model Inference and Averaging\n",
      "tion ˆθ(x) =h(x)Tˆβis\n",
      "ˆse[ˆθ(x)] = [h(x)T(HTH)−1h(x)]1\n",
      "2ˆσ. (8.4)\n",
      "In the top right panel of Figure 8.2 we have plotted ˆ θ(x)±1.96≤ˆse[ˆθ(x)].\n",
      "Since 1.96 is the 97.5% point of the standard normal distribution, these\n",
      "represent approximate 100 −2×2.5% = 95% pointwise conﬁdence bands\n",
      "forθ(x).\n",
      "Here is how we could apply the bootstrap in this example. We draw B\n",
      "datasets each of size N= 50 with replacement from our training data, the\n",
      "sampling unit being the pair zi= (xi,yi). To each bootstrap dataset Z∗\n",
      "we ﬁt a cubic spline ˆ θ∗(x); the ﬁts from ten such samples are shown in the\n",
      "bottom left panel of Figure 8.2. Using B= 200 bootstrap samples, we can\n",
      "form a 95% pointwise conﬁdence band from the percentiles at each x: we\n",
      "ﬁnd the 2 .5%×200 = ﬁfth largest and smallest values at each x. These are\n",
      "plotted in the bottom right panel of Figure 8.2. The bands look similar to\n",
      "those in the top right, being a little wider at the endpoints.\n",
      "There is actually a close connection between the least squares estimates\n",
      "(8.2) and (8.3), the bootstrap, and maximum likelihood. Suppose we further\n",
      "assume that the model errors are Gaussian,\n",
      "Y=θ(X) +ε;ε∼N(0,σ2),\n",
      "θ(x) =7∑\n",
      "j=1βjhj(x). (8.5)\n",
      "The bootstrap method described above, in which we sample with re-\n",
      "placement from the training data, is called the nonparametric bootstrap .\n",
      "This really means that the method is “model-free,” since it uses the raw\n",
      "data, not a speciﬁc parametric model, to generate new datasets. Consider\n",
      "a variation of the bootstrap, called the parametric bootstrap , in which we\n",
      "simulate new responses by adding Gaussian noise to the predicted values:\n",
      "y∗\n",
      "i= ˆθ(xi) +ε∗\n",
      "i;ε∗\n",
      "i∼N(0,ˆσ2);i= 1,2,... ,N. (8.6)\n",
      "This process is repeated Btimes, where B= 200 say. The resulting boot-\n",
      "strap datasets have the form ( x1,y∗\n",
      "1),... ,(xN,y∗\n",
      "N) and we recompute the\n",
      "B-spline smooth on each. The conﬁdence bands from this method will ex-\n",
      "actly equal the least squares bands in the top right panel, as the number of\n",
      "bootstrap samples goes to inﬁnity. A function estimated from a bootstrap\n",
      "sample y∗is given by ˆ θ∗(x) =h(x)T(HTH)−1HTy∗, and has distribution\n",
      "ˆθ∗(x)∼N(ˆθ(x),h(x)T(HTH)−1h(x)ˆσ2). (8.7)\n",
      "Notice that the mean of this distribution is the least squares estimate, and\n",
      "the standard deviation is the same as the approximate formula (8.4).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.2 The Bootstrap and Maximum Likelihood Methods 265\n",
      "8.2.2 Maximum Likelihood Inference\n",
      "It turns out that the parametric bootstrap agrees with least squares in the\n",
      "previous example because the model (8.5) has additive Gaussian errors. In\n",
      "general, the parametric bootstrap agrees not with least squares but with\n",
      "maximum likelihood, which we now review.\n",
      "We begin by specifying a probability density or probability mass function\n",
      "for our observations\n",
      "zi∼gθ(z). (8.8)\n",
      "In this expression θrepresents one or more unknown parameters that gov-\n",
      "ern the distribution of Z. This is called a parametric model forZ. As an\n",
      "example, if Zhas a normal distribution with mean θand variance σ2, then\n",
      "θ= (θ,σ2), (8.9)\n",
      "and\n",
      "gθ(z) =1√\n",
      "2πσe−1\n",
      "2(z−θ)2/σ2. (8.10)\n",
      "Maximum likelihood is based on the likelihood function , given by\n",
      "L(θ;Z) =N∏\n",
      "i=1gθ(zi), (8.11)\n",
      "the probability of the observed data under the model gθ. The likelihood is\n",
      "deﬁned only up to a positive multiplier, which we have taken to be one.\n",
      "We think of L(θ;Z) as a function of θ, with our data Zﬁxed.\n",
      "Denote the logarithm of L(θ;Z) by\n",
      "ℓ(θ;Z) =N∑\n",
      "i=1ℓ(θ;zi)\n",
      "=N∑\n",
      "i=1loggθ(zi), (8.12)\n",
      "which we will sometimes abbreviate as ℓ(θ). This expression is called the\n",
      "log-likelihood, and each value ℓ(θ;zi) = log gθ(zi) is called a log-likelihood\n",
      "component. The method of maximum likelihood chooses the value θ=ˆθ\n",
      "to maximize ℓ(θ;Z).\n",
      "The likelihood function can be used to assess the precision of ˆθ. We need\n",
      "a few more deﬁnitions. The score function is deﬁned by\n",
      "˙ℓ(θ;Z) =N∑\n",
      "i=1˙ℓ(θ;zi), (8.13)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "266 8. Model Inference and Averaging\n",
      "where ˙ℓ(θ;zi) =∂ℓ(θ;zi)/∂θ. Assuming that the likelihood takes its maxi-\n",
      "mum in the interior of the parameter space, ˙ℓ(ˆθ;Z) = 0. The information\n",
      "matrix is\n",
      "I(θ) =−N∑\n",
      "i=1∂2ℓ(θ;zi)\n",
      "∂θ∂θT. (8.14)\n",
      "WhenI(θ) is evaluated at θ=ˆθ, it is often called the observed information .\n",
      "TheFisher information (or expected information) is\n",
      "i(θ) = E θ[I(θ)]. (8.15)\n",
      "Finally, let θ0denote the true value of θ.\n",
      "A standard result says that the sampling distribution of the maximum\n",
      "likelihood estimator has a limiting normal distribution\n",
      "ˆθ→N(θ0,i(θ0)−1), (8.16)\n",
      "asN→ ∞. Here we are independently sampling from gθ0(z). This suggests\n",
      "that the sampling distribution of ˆθmay be approximated by\n",
      "N(ˆθ,i(ˆθ)−1) orN(ˆθ,I(ˆθ)−1), (8.17)\n",
      "where ˆθrepresents the maximum likelihood estimate from the observed\n",
      "data.\n",
      "The corresponding estimates for the standard errors of ˆθjare obtained\n",
      "from\n",
      "√\n",
      "i(ˆθ)−1\n",
      "jj and√\n",
      "I(ˆθ)−1\n",
      "jj. (8.18)\n",
      "Conﬁdence points for θjcan be constructed from either approximation\n",
      "in (8.17). Such a conﬁdence point has the form\n",
      "ˆθj−z(1−α)≤√\n",
      "i(ˆθ)−1\n",
      "jj or ˆθj−z(1−α)≤√\n",
      "I(ˆθ)−1\n",
      "jj,\n",
      "respectively, where z(1−α)is the 1 −αpercentile of the standard normal\n",
      "distribution. More accurate conﬁdence intervals can be derived from the\n",
      "likelihood function, by using the chi-squared approximation\n",
      "2[ℓ(ˆθ)−ℓ(θ0)]∼χ2\n",
      "p, (8.19)\n",
      "where pis the number of components in θ. The resulting 1 −2αconﬁ-\n",
      "dence interval is the set of all θ0such that 2[ ℓ(ˆθ)−ℓ(θ0)]≤χ2\n",
      "p(1−2α),\n",
      "where χ2\n",
      "p(1−2α)is the 1 −2αpercentile of the chi-squared distribution with\n",
      "pdegrees of freedom.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.3 Bayesian Methods 267\n",
      "Let’s return to our smoothing example to see what maximum likelihood\n",
      "yields. The parameters are θ= (β,σ2). The log-likelihood is\n",
      "ℓ(θ) =−N\n",
      "2logσ22π−1\n",
      "2σ2N∑\n",
      "i=1(yi−h(xi)Tβ)2. (8.20)\n",
      "The maximum likelihood estimate is obtained by setting ∂ℓ/∂β = 0 and\n",
      "∂ℓ/∂σ2= 0, giving\n",
      "ˆβ= (HTH)−1HTy,\n",
      "ˆσ2=1\n",
      "N∑\n",
      "(yi−ˆθ(xi))2,(8.21)\n",
      "which are the same as the usual estimates given in (8.2) and below (8.3).\n",
      "The information matrix for θ= (β,σ2) is block-diagonal, and the block\n",
      "corresponding to βis\n",
      "I(β) = (HTH)/σ2, (8.22)\n",
      "so that the estimated variance ( HTH)−1ˆσ2agrees with the least squares\n",
      "estimate (8.3).\n",
      "8.2.3 Bootstrap versus Maximum Likelihood\n",
      "In essence the bootstrap is a computer implementation of nonparametric or\n",
      "parametric maximum likelihood. The advantage of the bootstrap over the\n",
      "maximum likelihood formula is that it allows us to compute maximum like-\n",
      "lihood estimates of standard errors and other quantities in settings where\n",
      "no formulas are available.\n",
      "In our example, suppose that we adaptively choose by cross-validation\n",
      "the number and position of the knots that deﬁne the B-splines, rather\n",
      "than ﬁx them in advance. Denote by λthe collection of knots and their\n",
      "positions. Then the standard errors and conﬁdence bands should account\n",
      "for the adaptive choice of λ, but there is no way to do this analytically.\n",
      "With the bootstrap, we compute the B-spline smooth with an adaptive\n",
      "choice of knots for each bootstrap sample. The percentiles of the resulting\n",
      "curves capture the variability from both the noise in the targets as well as\n",
      "that from ˆλ. In this particular example the conﬁdence bands (not shown)\n",
      "don’t look much diﬀerent than the ﬁxed λbands. But in other problems,\n",
      "where more adaptation is used, this can be an important eﬀect to capture.\n",
      "8.3 Bayesian Methods\n",
      "In the Bayesian approach to inference, we specify a sampling model Pr( Z|θ)\n",
      "(density or probability mass function) for our data given the parameters,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "268 8. Model Inference and Averaging\n",
      "and a prior distribution for the parameters Pr( θ) reﬂecting our knowledge\n",
      "about θbefore we see the data. We then compute the posterior distribution\n",
      "Pr(θ|Z) =Pr(Z|θ)≤Pr(θ)∫\n",
      "Pr(Z|θ)≤Pr(θ)dθ, (8.23)\n",
      "which represents our updated knowledge about θafter we see the data. To\n",
      "understand this posterior distribution, one might draw samples from it or\n",
      "summarize by computing its mean or mode. The Bayesian approach diﬀers\n",
      "from the standard (“frequentist”) method for inference in its use of a prior\n",
      "distribution to express the uncertainty present before seeing the data, and\n",
      "to allow the uncertainty remaining after seeing the data to be expressed in\n",
      "the form of a posterior distribution.\n",
      "The posterior distribution also provides the basis for predicting the values\n",
      "of a future observation znew, via the predictive distribution :\n",
      "Pr(znew|Z) =∫\n",
      "Pr(znew|θ)≤Pr(θ|Z)dθ. (8.24)\n",
      "In contrast, the maximum likelihood approach would use Pr( znew|ˆθ),\n",
      "the data density evaluated at the maximum likelihood estimate, to predict\n",
      "future data. Unlike the predictive distribution (8.24), this does not account\n",
      "for the uncertainty in estimating θ.\n",
      "Let’s walk through the Bayesian approach in our smoothing example.\n",
      "We start with the parametric model given by equation (8.5), and assume\n",
      "for the moment that σ2is known. We assume that the observed feature\n",
      "values x1,x2,... ,x Nare ﬁxed, so that the randomness in the data comes\n",
      "solely from yvarying around its mean θ(x).\n",
      "The second ingredient we need is a prior distribution. Distributions on\n",
      "functions are fairly complex entities: one approach is to use a Gaussian\n",
      "process prior in which we specify the prior covariance between any two\n",
      "function values θ(x) and θ(x′) (Wahba, 1990; Neal, 1996).\n",
      "Here we take a simpler route: by considering a ﬁnite B-spline basis for\n",
      "θ(x), we can instead provide a prior for the coeﬃcients β, and this implicitly\n",
      "deﬁnes a prior for θ(x). We choose a Gaussian prior centered at zero\n",
      "β∼N(0,τΣ) (8.25)\n",
      "with the choices of the prior correlation matrix Σand variance τto be\n",
      "discussed below. The implicit process prior for θ(x) is hence Gaussian,\n",
      "with covariance kernel\n",
      "K(x,x′) = cov[ θ(x),θ(x′)]\n",
      "=τ≤h(x)TΣh(x′). (8.26)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.3 Bayesian Methods 269\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.0-3 -2 -1 0 1 2 3θ(x)\n",
      "x\n",
      "FIGURE 8.3. Smoothing example: Ten draws from the Gaussian prior distri-\n",
      "bution for the function θ(x).\n",
      "The posterior distribution for βis also Gaussian, with mean and covariance\n",
      "E(β|Z) =(\n",
      "HTH+σ2\n",
      "τΣ−1)−1\n",
      "HTy,\n",
      "cov(β|Z) =(\n",
      "HTH+σ2\n",
      "τΣ−1)−1\n",
      "σ2,(8.27)\n",
      "with the corresponding posterior values for θ(x),\n",
      "E(θ(x)|Z) =h(x)T(\n",
      "HTH+σ2\n",
      "τΣ−1)−1\n",
      "HTy,\n",
      "cov[θ(x),θ(x′)|Z] =h(x)T(\n",
      "HTH+σ2\n",
      "τΣ−1)−1\n",
      "h(x′)σ2.(8.28)\n",
      "How do we choose the prior correlation matrix Σ? In some settings the\n",
      "prior can be chosen from subject matter knowledge about the parameters.\n",
      "Here we are willing to say the function θ(x) should be smooth, and have\n",
      "guaranteed this by expressing θin a smooth low-dimensional basis of B-\n",
      "splines. Hence we can take the prior correlation matrix to be the identity\n",
      "Σ=I. When the number of basis functions is large, this might not be suf-\n",
      "ﬁcient, and additional smoothness can be enforced by imposing restrictions\n",
      "onΣ; this is exactly the case with smoothing splines (Section 5.8.1).\n",
      "Figure 8.3 shows ten draws from the corresponding prior for θ(x). To\n",
      "generate posterior values of the function θ(x), we generate values β′from its\n",
      "posterior (8.27), giving corresponding posterior value θ′(x) =∑7\n",
      "1β′\n",
      "jhj(x).\n",
      "Ten such posterior curves are shown in Figure 8.4. Two diﬀerent values\n",
      "were used for the prior variance τ, 1 and 1000. Notice how similar the\n",
      "right panel looks to the bootstrap distribution in the bottom left panel\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "270 8. Model Inference and Averaging\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\n",
      "••••••••••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "•••••••••\n",
      "••••••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.0-1 0 1 2 3 4 5••••••\n",
      "••••••••••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•••\n",
      "•••••••••\n",
      "••••••••\n",
      "••••\n",
      "••\n",
      "••θ(x)θ(x)\n",
      "x xτ= 1 τ= 1000\n",
      "FIGURE 8.4. Smoothing example: Ten draws from the posterior distribution\n",
      "for the function θ(x), for two diﬀerent values of the prior variance τ. The purple\n",
      "curves are the posterior means.\n",
      "of Figure 8.2 on page 263. This similarity is no accident. As τ→ ∞, the\n",
      "posterior distribution (8.27) and the bootstrap distribution (8.7) co incide.\n",
      "On the other hand, for τ= 1, the posterior curves θ(x) in the left panel\n",
      "of Figure 8.4 are smoother than the bootstrap curves, because we have\n",
      "imposed more prior weight on smoothness.\n",
      "The distribution (8.25) with τ→ ∞ is called a noninformative prior for\n",
      "θ. In Gaussian models, maximum likelihood and parametric bootstrap anal -\n",
      "yses tend to agree with Bayesian analyses that use a noninformative prior\n",
      "for the free parameters. These tend to agree, because with a constant prior,\n",
      "the posterior distribution is proportional to the likelihood. This corresp on-\n",
      "dence also extends to the nonparametric case, where the nonparametric\n",
      "bootstrap approximates a noninformative Bayes analysis; Section 8.4 has\n",
      "the details.\n",
      "We have, however, done some things that are not proper from a Bayesian\n",
      "point of view. We have used a noninformative (constant) prior for σ2and\n",
      "replaced it with the maximum likelihood estimate ˆ σ2in the posterior. A\n",
      "more standard Bayesian analysis would also put a prior on σ(typically\n",
      "g(σ)∝1/σ), calculate a joint posterior for θ(x) and σ, and then integrate\n",
      "outσ, rather than just extract the maximum of the posterior distribution\n",
      "(“MAP” estimate).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.4 Relationship Between the Bootstrap and Bayesian Inference 271\n",
      "8.4 Relationship Between the Bootstrap and\n",
      "Bayesian Inference\n",
      "Consider ﬁrst a very simple example, in which we observe a single obser-\n",
      "vation zfrom a normal distribution\n",
      "z∼N(θ,1). (8.29)\n",
      "To carry out a Bayesian analysis for θ, we need to specify a prior. The\n",
      "most convenient and common choice would be θ∼N(0,τ) giving posterior\n",
      "distribution\n",
      "θ|z∼N(z\n",
      "1 + 1/τ,1\n",
      "1 + 1/τ)\n",
      ". (8.30)\n",
      "Now the larger we take τ, the more concentrated the posterior becomes\n",
      "around the maximum likelihood estimate ˆθ=z. In the limit as τ→ ∞ we\n",
      "obtain a noninformative (constant) prior, and the posterior distribution is\n",
      "θ|z∼N(z,1). (8.31)\n",
      "This is the same as a parametric bootstrap distribution in which we gen-\n",
      "erate bootstrap values z∗from the maximum likelihood estimate of the\n",
      "sampling density N(z,1).\n",
      "There are three ingredients that make this correspondence work:\n",
      "1. The choice of noninformative prior for θ.\n",
      "2. The dependence of the log-likelihood ℓ(θ;Z) on the data Zonly\n",
      "through the maximum likelihood estimate ˆθ. Hence we can write the\n",
      "log-likelihood as ℓ(θ;ˆθ).\n",
      "3. The symmetry of the log-likelihood in θandˆθ, that is, ℓ(θ;ˆθ) =\n",
      "ℓ(ˆθ;θ) + constant.\n",
      "Properties (2) and (3) essentially only hold for the Gaussian distribu-\n",
      "tion. However, they also hold approximately for the multinomial distribu-\n",
      "tion, leading to a correspondence between the nonparametric bootstrap\n",
      "and Bayes inference, which we outline next.\n",
      "Assume that we have a discrete sample space with Lcategories. Let wjbe\n",
      "the probability that a sample point falls in category j, and ˆwjthe observed\n",
      "proportion in category j. Letw= (w1,w2,... ,w L),ˆw= ( ˆw1,ˆw2,... ,ˆwL).\n",
      "Denote our estimator by S( ˆw); take as a prior distribution for wa sym-\n",
      "metric Dirichlet distribution with parameter a:\n",
      "w∼DiL(a1), (8.32)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "272 8. Model Inference and Averaging\n",
      "that is, the prior probability mass function is proportional to∏L\n",
      "ℓ=1wa−1\n",
      "ℓ.\n",
      "Then the posterior density of wis\n",
      "w∼DiL(a1 +Nˆw), (8.33)\n",
      "where Nis the sample size. Letting a→0 to obtain a noninformative prior\n",
      "gives\n",
      "w∼DiL(Nˆw). (8.34)\n",
      "Now the bootstrap distribution, obtained by sampling with replacement\n",
      "from the data, can be expressed as sampling the category proportions from\n",
      "a multinomial distribution. Speciﬁcally,\n",
      "Nˆw∗∼Mult( N,ˆw), (8.35)\n",
      "where Mult( N,ˆw) denotes a multinomial distribution, having probability\n",
      "mass function(N\n",
      "Nˆw∗\n",
      "1,...,N ˆw∗\n",
      "L)∏ˆwNˆw∗\n",
      "ℓ\n",
      "ℓ. This distribution is similar to the pos-\n",
      "terior distribution above, having the same support, same mean, and nearly\n",
      "the same covariance matrix. Hence the bootstrap distribution of S( ˆw∗) will\n",
      "closely approximate the posterior distribution of S(w).\n",
      "In this sense, the bootstrap distribution represents an (approximate)\n",
      "nonparametric, noninformative posterior distribution for our parameter.\n",
      "But this bootstrap distribution is obtained painlessly—without having to\n",
      "formally specify a prior and without having to sample from the posterior\n",
      "distribution. Hence we might think of the bootstrap distribution as a “poor\n",
      "man’s” Bayes posterior. By perturbing the data, the bootstrap approxi-\n",
      "mates the Bayesian eﬀect of perturbing the parameters, and is typically\n",
      "much simpler to carry out.\n",
      "8.5 The EM Algorithm\n",
      "The EM algorithm is a popular tool for simplifying diﬃcult maximum\n",
      "likelihood problems. We ﬁrst describe it in the context of a simple mixture\n",
      "model.\n",
      "8.5.1 Two-Component Mixture Model\n",
      "In this section we describe a simple mixture model for density estimation,\n",
      "and the associated EM algorithm for carrying out maximum likelihood\n",
      "estimation. This has a natural connection to Gibbs sampling methods for\n",
      "Bayesian inference. Mixture models are discussed and demonstrated in sev-\n",
      "eral other parts of the book, in particular Sections 6.8, 12.7 and 13.2.3.\n",
      "The left panel of Figure 8.5 shows a histogram of the 20 ﬁctitious data\n",
      "points in Table 8.1.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.5 The EM Algorithm 273\n",
      "0 2 4 60.0 0.2 0.4 0.6 0.8 1.0\n",
      "y ydensity\n",
      "0 2 4 60.0 0.2 0.4 0.6 0.8 1.0• •• •••••••\n",
      "•\n",
      "•\n",
      "••••• •• •\n",
      "FIGURE 8.5. Mixture example. (Left panel:) Histogram of data. (Right panel: )\n",
      "Maximum likelihood ﬁt of Gaussian densities (solid red) and resp onsibility (dotted\n",
      "green) of the left component density for observation y, as a function of y.\n",
      "TABLE 8.1. Twenty ﬁctitious data points used in the two-component mixture\n",
      "example in Figure 8.5.\n",
      "-0.39 0.12 0.94 1.67 1.76 2.44 3.72 4.28 4.92 5.53\n",
      "0.06 0.48 1.01 1.68 1.80 3.25 4.12 4.60 5.28 6.22\n",
      "We would like to model the density of the data points, and due to the\n",
      "apparent bi-modality, a Gaussian distribution would not be appropriate.\n",
      "There seems to be two separate underlying regimes, so instead we model\n",
      "Yas a mixture of two normal distributions:\n",
      "Y1∼N(θ1,σ2\n",
      "1),\n",
      "Y2∼N(θ2,σ2\n",
      "2), (8.36)\n",
      "Y= (1 −∆)≤Y1+ ∆≤Y2,\n",
      "where ∆ ∈ {0,1}with Pr(∆ = 1) = π. This generative representation is\n",
      "explicit: generate a ∆ ∈ {0,1}with probability π, and then depending on\n",
      "the outcome, deliver either Y1orY2. Letφθ(x) denote the normal density\n",
      "with parameters θ= (θ,σ2). Then the density of Yis\n",
      "gY(y) = (1 −π)φθ1(y) +πφθ2(y). (8.37)\n",
      "Now suppose we wish to ﬁt this model to the data in Figure 8.5 by maxi-\n",
      "mum likelihood. The parameters are\n",
      "θ= (π,θ1,θ2) = (π,θ1,σ2\n",
      "1,θ2,σ2\n",
      "2). (8.38)\n",
      "The log-likelihood based on the Ntraining cases is\n",
      "ℓ(θ;Z) =N∑\n",
      "i=1log[(1 −π)φθ1(yi) +πφθ2(yi)]. (8.39)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "274 8. Model Inference and Averaging\n",
      "Direct maximization of ℓ(θ;Z) is quite diﬃcult numerically, because of\n",
      "the sum of terms inside the logarithm. There is, however, a simpler ap-\n",
      "proach. We consider unobserved latent variables ∆ itaking values 0 or 1 as\n",
      "in (8.36): if ∆ i= 1 then Yicomes from model 2, otherwise it comes from\n",
      "model 1. Suppose we knew the values of the ∆ i’s. Then the log-likelihood\n",
      "would be\n",
      "ℓ0(θ;Z,∆) =N∑\n",
      "i=1[(1−∆i)logφθ1(yi) + ∆ ilogφθ2(yi)]\n",
      "+N∑\n",
      "i=1[(1−∆i)log(1 −π) + ∆ ilogπ],(8.40)\n",
      "and the maximum likelihood estimates of θ1andσ2\n",
      "1would be the sample\n",
      "mean and variance for those data with ∆ i= 0, and similarly those for θ2\n",
      "andσ2\n",
      "2would be the sample mean and variance of the data with ∆ i= 1.\n",
      "The estimate of πwould be the proportion of ∆ i= 1.\n",
      "Since the values of the ∆ i’s are actually unknown, we proceed in an\n",
      "iterative fashion, substituting for each ∆ iin (8.40) its expected value\n",
      "γi(θ) = E(∆ i|θ,Z) = Pr(∆ i= 1|θ,Z), (8.41)\n",
      "also called the responsibility of model 2 for observation i. We use a proce-\n",
      "dure called the EM algorithm, given in Algorithm 8.1 for the special case of\n",
      "Gaussian mixtures. In the expectation step, we do a soft assignment of each\n",
      "observation to each model: the current estimates of the parameters are used\n",
      "to assign responsibilities according to the relative density of the training\n",
      "points under each model. In the maximization step, these responsibilities\n",
      "are used in weighted maximum-likelihood ﬁts to update the estimates of\n",
      "the parameters.\n",
      "A good way to construct initial guesses for ˆ θ1and ˆθ2is simply to choose\n",
      "two of the yiat random. Both ˆ σ2\n",
      "1and ˆσ2\n",
      "2can be set equal to the overall\n",
      "sample variance∑N\n",
      "i=1(yi−¯y)2/N. The mixing proportion ˆ πcan be started\n",
      "at the value 0 .5.\n",
      "Note that the actual maximizer of the likelihood occurs when we put a\n",
      "spike of inﬁnite height at any one data point, that is, ˆ θ1=yifor some\n",
      "iand ˆσ2\n",
      "1= 0. This gives inﬁnite likelihood, but is not a useful solution.\n",
      "Hence we are actually looking for a good local maximum of the likelihood,\n",
      "one for which ˆ σ2\n",
      "1,ˆσ2\n",
      "2>0. To further complicate matters, there can be\n",
      "more than one local maximum having ˆ σ2\n",
      "1,ˆσ2\n",
      "2>0. In our example, we\n",
      "ran the EM algorithm with a number of diﬀerent initial guesses for the\n",
      "parameters, all having ˆ σ2\n",
      "k>0.5, and chose the run that gave us the highest\n",
      "maximized likelihood. Figure 8.6 shows the progress of the EM algorithm in\n",
      "maximizing the log-likelihood. Table 8.2 shows ˆ π=∑\n",
      "iˆγi/N, the maximum\n",
      "likelihood estimate of the proportion of observations in class 2, at sel ected\n",
      "iterations of the EM procedure.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.5 The EM Algorithm 275\n",
      "Algorithm 8.1 EM Algorithm for Two-component Gaussian Mixture.\n",
      "1. Take initial guesses for the parameters ˆ θ1,ˆσ2\n",
      "1,ˆθ2,ˆσ2\n",
      "2,ˆπ(see text).\n",
      "2.Expectation Step : compute the responsibilities\n",
      "ˆγi=ˆπφˆθ2(yi)\n",
      "(1−ˆπ)φˆθ1(yi) + ˆπφˆθ2(yi), i= 1,2,... ,N. (8.42)\n",
      "3.Maximization Step : compute the weighted means and variances:\n",
      "ˆθ1=∑N\n",
      "i=1(1−ˆγi)yi∑N\n",
      "i=1(1−ˆγi), ˆσ2\n",
      "1=∑N\n",
      "i=1(1−ˆγi)(yi−ˆθ1)2\n",
      "∑N\n",
      "i=1(1−ˆγi),\n",
      "ˆθ2=∑N\n",
      "i=1ˆγiyi∑N\n",
      "i=1ˆγi, ˆσ2\n",
      "2=∑N\n",
      "i=1ˆγi(yi−ˆθ2)2\n",
      "∑N\n",
      "i=1ˆγi,\n",
      "and the mixing probability ˆ π=∑N\n",
      "i=1ˆγi/N.\n",
      "4. Iterate steps 2 and 3 until convergence.\n",
      "TABLE 8.2. Selected iterations of the EM algorithm for mixture example.\n",
      "Iteration ˆ π\n",
      "1 0.485\n",
      "5 0.493\n",
      "10 0.523\n",
      "15 0.544\n",
      "20 0.546\n",
      "The ﬁnal maximum likelihood estimates are\n",
      "ˆθ1= 4.62, ˆσ2\n",
      "1= 0.87,\n",
      "ˆθ2= 1.06, ˆσ2\n",
      "2= 0.77,\n",
      "ˆπ= 0.546.\n",
      "The right panel of Figure 8.5 shows the estimated Gaussian mixture density\n",
      "from this procedure (solid red curve), along with the responsibilities (dotted\n",
      "green curve). Note that mixtures are also useful for supervised learning; in\n",
      "Section 6.7 we show how the Gaussian mixture model leads to a version of\n",
      "radial basis functions.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "276 8. Model Inference and Averaging\n",
      "IterationObserved Data Log-likelihood\n",
      "5 10 15 20-44 -43 -42 -41 -40 -39\n",
      "ooooooooooooooo o o o o o\n",
      "FIGURE 8.6. EM algorithm: observed data log-likelihood as a function of t he\n",
      "iteration number.\n",
      "8.5.2 The EM Algorithm in General\n",
      "The above procedure is an example of the EM (or Baum–Welch) algorithm\n",
      "for maximizing likelihoods in certain classes of problems. These problems\n",
      "are ones for which maximization of the likelihood is diﬃcult, but made\n",
      "easier by enlarging the sample with latent (unobserved) data. This is called\n",
      "data augmentation . Here the latent data are the model memberships ∆ i.\n",
      "In other problems, the latent data are actual data that should have been\n",
      "observed but are missing.\n",
      "Algorithm 8.2 gives the general formulation of the EM algorithm. Our\n",
      "observed data is Z, having log-likelihood ℓ(θ;Z) depending on parameters\n",
      "θ. The latent or missing data is Zm, so that the complete data is T=\n",
      "(Z,Zm) with log-likelihood ℓ0(θ;T),ℓ0based on the complete density. In\n",
      "the mixture problem ( Z,Zm) = (y,∆), and ℓ0(θ;T) is given in (8.40).\n",
      "In our mixture example, E( ℓ0(θ′;T)|Z,ˆθ(j)) is simply (8.40) with the ∆ i\n",
      "replaced by the responsibilities ˆ γi(ˆθ), and the maximizers in step 3 are just\n",
      "weighted means and variances.\n",
      "We now give an explanation of why the EM algorithm works in general.\n",
      "Since\n",
      "Pr(Zm|Z,θ′) =Pr(Zm,Z|θ′)\n",
      "Pr(Z|θ′), (8.44)\n",
      "we can write\n",
      "Pr(Z|θ′) =Pr(T|θ′)\n",
      "Pr(Zm|Z,θ′). (8.45)\n",
      "In terms of log-likelihoods, we have ℓ(θ′;Z) =ℓ0(θ′;T)−ℓ1(θ′;Zm|Z), where\n",
      "ℓ1is based on the conditional density Pr( Zm|Z,θ′). Taking conditional\n",
      "expectations with respect to the distribution of T|Zgoverned by parameter\n",
      "θgives\n",
      "ℓ(θ′;Z) = E[ ℓ0(θ′;T)|Z,θ]−E[ℓ1(θ′;Zm|Z)|Z,θ]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.5 The EM Algorithm 277\n",
      "Algorithm 8.2 The EM Algorithm.\n",
      "1. Start with initial guesses for the parameters ˆθ(0).\n",
      "2.Expectation Step : at the jth step, compute\n",
      "Q(θ′,ˆθ(j)) = E( ℓ0(θ′;T)|Z,ˆθ(j)) (8.43)\n",
      "as a function of the dummy argument θ′.\n",
      "3.Maximization Step : determine the new estimate ˆθ(j+1)as the maxi-\n",
      "mizer of Q(θ′,ˆθ(j)) over θ′.\n",
      "4. Iterate steps 2 and 3 until convergence.\n",
      "≡Q(θ′,θ)−R(θ′,θ). (8.46)\n",
      "In the Mstep, the EM algorithm maximizes Q(θ′,θ) over θ′, rather than\n",
      "the actual objective function ℓ(θ′;Z). Why does it succeed in maximizing\n",
      "ℓ(θ′;Z)? Note that R(θ∗,θ) is the expectation of a log-likelihood of a density\n",
      "(indexed by θ∗), with respect to the same density indexed by θ, and hence\n",
      "(by Jensen’s inequality) is maximized as a function of θ∗, when θ∗=θ(see\n",
      "Exercise 8.1). So if θ′maximizes Q(θ′,θ), we see that\n",
      "ℓ(θ′;Z)−ℓ(θ;Z) = [ Q(θ′,θ)−Q(θ,θ)]−[R(θ′,θ)−R(θ,θ)]\n",
      "≥0. (8.47)\n",
      "Hence the EM iteration never decreases the log-likelihood.\n",
      "This argument also makes it clear that a full maximization in the M\n",
      "step is not necessary: we need only to ﬁnd a value ˆθ(j+1)so that Q(θ′,ˆθ(j))\n",
      "increases as a function of the ﬁrst argument, that is, Q(ˆθ(j+1),ˆθ(j))>\n",
      "Q(ˆθ(j),ˆθ(j)). Such procedures are called GEM (generalized EM) algorithms.\n",
      "The EM algorithm can also be viewed as a minorization procedure: see\n",
      "Exercise 8.7.\n",
      "8.5.3 EM as a Maximization–Maximization Procedure\n",
      "Here is a diﬀerent view of the EM procedure, as a joint maximization\n",
      "algorithm. Consider the function\n",
      "F(θ′,˜P) = E ˜P[ℓ0(θ′;T)]−E˜P[log˜P(Zm)]. (8.48)\n",
      "Here ˜P(Zm) is any distribution over the latent data Zm. In the mixture\n",
      "example, ˜P(Zm) comprises the set of probabilities γi= Pr(∆ i= 1|θ,Z).\n",
      "Note that Fevaluated at ˜P(Zm) = Pr( Zm|Z,θ′), is the log-likelihood of\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "278 8. Model Inference and Averaging\n",
      "1 2 3 4 50 1 2 3 40.10.3\n",
      "0.50.7\n",
      "0.9Model Parameters\n",
      "Latent Data ParametersEMEM\n",
      "FIGURE 8.7. Maximization–maximization view of the EM algorithm. Shown\n",
      "are the contours of the (augmented) observed data log-likelih oodF(θ′,˜P). The\n",
      "Estep is equivalent to maximizing the log-likelihood over the pa rameters of the\n",
      "latent data distribution. The Mstep maximizes it over the parameters of the\n",
      "log-likelihood. The red curve corresponds to the observed da ta log-likelihood, a\n",
      "proﬁle obtained by maximizing F(θ′,˜P)for each value of θ′.\n",
      "the observed data, from (8.46)1. The function Fexpands the domain of\n",
      "the log-likelihood, to facilitate its maximization.\n",
      "The EM algorithm can be viewed as a joint maximization method for F\n",
      "overθ′and˜P(Zm), by ﬁxing one argument and maximizing over the other.\n",
      "The maximizer over ˜P(Zm) for ﬁxed θ′can be shown to be\n",
      "˜P(Zm) = Pr( Zm|Z,θ′) (8.49)\n",
      "(Exercise 8.2). This is the distribution computed by the Estep, for example,\n",
      "(8.42) in the mixture example. In the Mstep, we maximize F(θ′,˜P) over θ′\n",
      "with˜Pﬁxed: this is the same as maximizing the ﬁrst term E ˜P[ℓ0(θ′;T)|Z,θ]\n",
      "since the second term does not involve θ′.\n",
      "Finally, since F(θ′,˜P) and the observed data log-likelihood agree when\n",
      "˜P(Zm) = Pr( Zm|Z,θ′), maximization of the former accomplishes maxi-\n",
      "mization of the latter. Figure 8.7 shows a schematic view of this process.\n",
      "This view of the EM algorithm leads to alternative maximization proce-\n",
      "1(8.46) holds for all θ, including θ=θ′.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.6 MCMC for Sampling from the Posterior 279\n",
      "Algorithm 8.3 Gibbs Sampler.\n",
      "1. Take some initial values U(0)\n",
      "k,k= 1,2,... ,K .\n",
      "2. Repeat for t= 1,2,... ,. :\n",
      "Fork= 1,2,... ,K generate U(t)\n",
      "kfrom\n",
      "Pr(U(t)\n",
      "k|U(t)\n",
      "1,... ,U(t)\n",
      "k−1,U(t−1)\n",
      "k+1,... ,U(t−1)\n",
      "K).\n",
      "3. Continue step 2 until the joint distribution of ( U(t)\n",
      "1,U(t)\n",
      "2,... ,U(t)\n",
      "K)\n",
      "does not change.\n",
      "dures. For example, one does not need to maximize with respect to all of\n",
      "the latent data parameters at once, but could instead maximize over one\n",
      "of them at a time, alternating with the Mstep.\n",
      "8.6 MCMC for Sampling from the Posterior\n",
      "Having deﬁned a Bayesian model, one would like to draw samples from\n",
      "the resulting posterior distribution, in order to make inferences about the\n",
      "parameters. Except for simple models, this is often a diﬃcult computa-\n",
      "tional problem. In this section we discuss the Markov chain Monte Carlo\n",
      "(MCMC) approach to posterior sampling. We will see that Gibbs sampling,\n",
      "an MCMC procedure, is closely related to the EM algorithm: the main dif-\n",
      "ference is that it samples from the conditional distributions rather than\n",
      "maximizing over them.\n",
      "Consider ﬁrst the following abstract problem. We have random variables\n",
      "U1,U2,... ,U Kand we wish to draw a sample from their joint distribution.\n",
      "Suppose this is diﬃcult to do, but it is easy to simulate from the conditional\n",
      "distributions Pr( Uj|U1,U2,... ,U j−1,Uj+1,... ,U K), j= 1,2,... ,K . The\n",
      "Gibbs sampling procedure alternatively simulates from each of these distri-\n",
      "butions and when the process stabilizes, provides a sample from the desired\n",
      "joint distribution. The procedure is deﬁned in Algorithm 8.3.\n",
      "Under regularity conditions it can be shown that this procedure even-\n",
      "tually stabilizes, and the resulting random variables are indeed a sample\n",
      "from the joint distribution of U1,U2,... ,U K. This occurs despite the fact\n",
      "that the samples ( U(t)\n",
      "1,U(t)\n",
      "2,... ,U(t)\n",
      "K) are clearly not independent for dif-\n",
      "ferent t. More formally, Gibbs sampling produces a Markov chain whose\n",
      "stationary distribution is the true joint distribution, and hence the term\n",
      "“Markov chain Monte Carlo.” It is not surprising that the true joint dis -\n",
      "tribution is stationary under this process, as the successive steps leave the\n",
      "marginal distributions of the Uk’s unchanged.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "280 8. Model Inference and Averaging\n",
      "Note that we don’t need to know the explicit form of the conditional\n",
      "densities, but just need to be able to sample from them. After the procedure\n",
      "reaches stationarity, the marginal density of any subset of the variables\n",
      "can be approximated by a density estimate applied to the sample values.\n",
      "However if the explicit form of the conditional density Pr( Uk,|Uℓ,ℓ̸=k)\n",
      "is available, a better estimate of say the marginal density of Ukcan be\n",
      "obtained from (Exercise 8.3):\n",
      "ˆPrUk(u) =1\n",
      "(M−m+ 1)M∑\n",
      "t=mPr(u|U(t)\n",
      "ℓ,ℓ̸=k). (8.50)\n",
      "Here we have averaged over the last M−m+ 1 members of the sequence,\n",
      "to allow for an initial “burn-in” period before stationarity is reached.\n",
      "Now getting back to Bayesian inference, our goal is to draw a sample from\n",
      "the joint posterior of the parameters given the data Z. Gibbs sampling will\n",
      "be helpful if it is easy to sample from the conditional distribution of each\n",
      "parameter given the other parameters and Z. An example—the Gaussian\n",
      "mixture problem—is detailed next.\n",
      "There is a close connection between Gibbs sampling from a posterior and\n",
      "the EM algorithm in exponential family models. The key is to consider the\n",
      "latent data Zmfrom the EM procedure to be another parameter for the\n",
      "Gibbs sampler. To make this explicit for the Gaussian mixture problem,\n",
      "we take our parameters to be ( θ,Zm). For simplicity we ﬁx the variances\n",
      "σ2\n",
      "1,σ2\n",
      "2and mixing proportion πat their maximum likelihood values so that\n",
      "the only unknown parameters in θare the means θ1andθ2. The Gibbs\n",
      "sampler for the mixture problem is given in Algorithm 8.4. We see that\n",
      "steps 2(a) and 2(b) are the same as the EandMsteps of the EM pro-\n",
      "cedure, except that we sample rather than maximize. In step 2(a), rather\n",
      "than compute the maximum likelihood responsibilities γi= E(∆ i|θ,Z),\n",
      "the Gibbs sampling procedure simulates the latent data ∆ ifrom the distri-\n",
      "butions Pr(∆ i|θ,Z). In step 2(b), rather than compute the maximizers of\n",
      "the posterior Pr( θ1,θ2,∆|Z) we simulate from the conditional distribution\n",
      "Pr(θ1,θ2|∆,Z).\n",
      "Figure 8.8 shows 200 iterations of Gibbs sampling, with the mean param-\n",
      "etersθ1(lower) and θ2(upper) shown in the left panel, and the proportion\n",
      "of class 2 observations∑\n",
      "i∆i/Non the right. Horizontal broken lines have\n",
      "been drawn at the maximum likelihood estimate values ˆ θ1,ˆθ2and∑\n",
      "iˆγi/N\n",
      "in each case. The values seem to stabilize quite quickly, and are distributed\n",
      "evenly around the maximum likelihood values.\n",
      "The above mixture model was simpliﬁed, in order to make the clear\n",
      "connection between Gibbs sampling and the EM algorithm. More realisti-\n",
      "cally, one would put a prior distribution on the variances σ2\n",
      "1,σ2\n",
      "2and mixing\n",
      "proportion π, and include separate Gibbs sampling steps in which we sam-\n",
      "ple from their posterior distributions, conditional on the other parameters.\n",
      "One can also incorporate proper (informative) priors for the mean param-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.6 MCMC for Sampling from the Posterior 281\n",
      "Algorithm 8.4 Gibbs sampling for mixtures.\n",
      "1. Take some initial values θ(0)= (θ(0)\n",
      "1,θ(0)\n",
      "2).\n",
      "2. Repeat for t= 1,2,... ,.\n",
      "(a) For i= 1,2,... ,N generate ∆(t)\n",
      "i∈ {0,1}with Pr(∆(t)\n",
      "i= 1) =\n",
      "ˆγi(θ(t)), from equation (8.42).\n",
      "(b) Set\n",
      "ˆθ1=∑N\n",
      "i=1(1−∆(t)\n",
      "i)≤yi∑N\n",
      "i=1(1−∆(t)\n",
      "i),\n",
      "ˆθ2=∑N\n",
      "i=1∆(t)\n",
      "i≤yi∑N\n",
      "i=1∆(t)\n",
      "i,\n",
      "and generate θ(t)\n",
      "1∼N(ˆθ1,ˆσ2\n",
      "1) and θ(t)\n",
      "2∼N(ˆθ2,ˆσ2\n",
      "2).\n",
      "3. Continue step 2 until the joint distribution of ( ∆(t),θ(t)\n",
      "1,θ(t)\n",
      "2) doesn’t\n",
      "change\n",
      "Gibbs IterationMean Parameters\n",
      "0 50 100 150 2000 2 4 6 8\n",
      "Gibbs IterationMixing Proportion\n",
      "0 50 100 150 2000.3 0.4 0.5 0.6 0.7\n",
      "FIGURE 8.8. Mixture example. (Left panel:) 200values of the two mean param-\n",
      "eters from Gibbs sampling; horizontal lines are drawn at the maxi mum likelihood\n",
      "estimates ˆθ1,ˆθ2. (Right panel:) Proportion of values with ∆i= 1, for each of the\n",
      "200Gibbs sampling iterations; a horizontal line is drawn atP\n",
      "iˆγi/N.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "282 8. Model Inference and Averaging\n",
      "eters. These priors must not be improper as this will lead to a degenerate\n",
      "posterior, with all the mixing weight on one component.\n",
      "Gibbs sampling is just one of a number of recently developed procedures\n",
      "for sampling from posterior distributions. It uses conditional sampling of\n",
      "each parameter given the rest, and is useful when the structure of the prob-\n",
      "lem makes this sampling easy to carry out. Other methods do not require\n",
      "such structure, for example the Metropolis–Hastings algorithm. These and\n",
      "other computational Bayesian methods have been applied to sophisticated\n",
      "learning algorithms such as Gaussian process models and neural networks.\n",
      "Details may be found in the references given in the Bibliographic Notes at\n",
      "the end of this chapter.\n",
      "8.7 Bagging\n",
      "Earlier we introduced the bootstrap as a way of assessing the accuracy of a\n",
      "parameter estimate or a prediction. Here we show how to use the bootstrap\n",
      "to improve the estimate or prediction itself. In Section 8.4 we investigat ed\n",
      "the relationship between the bootstrap and Bayes approaches, and found\n",
      "that the bootstrap mean is approximately a posterior average. Bagging\n",
      "further exploits this connection.\n",
      "Consider ﬁrst the regression problem. Suppose we ﬁt a model to our\n",
      "training data Z={(x1,y1),(x2,y2),... ,(xN,yN)}, obtaining the predic-\n",
      "tionˆf(x) at input x. Bootstrap aggregation or bagging averages this predic-\n",
      "tion over a collection of bootstrap samples, thereby reducing its variance.\n",
      "For each bootstrap sample Z∗b,b= 1,2,... ,B , we ﬁt our model, giving\n",
      "prediction ˆf∗b(x). The bagging estimate is deﬁned by\n",
      "ˆfbag(x) =1\n",
      "BB∑\n",
      "b=1ˆf∗b(x). (8.51)\n",
      "Denote by ˆPthe empirical distribution putting equal probability 1 /Non\n",
      "each of the data points ( xi,yi). In fact the “true” bagging estimate is\n",
      "deﬁned by E ˆPˆf∗(x), where Z∗= (x∗\n",
      "1,y∗\n",
      "1),(x∗\n",
      "2,y∗\n",
      "2),... ,(x∗\n",
      "N,y∗\n",
      "N) and each\n",
      "(x∗\n",
      "i,y∗\n",
      "i)∼ˆP. Expression (8.51) is a Monte Carlo estimate of the true\n",
      "bagging estimate, approaching it as B→ ∞.\n",
      "The bagged estimate (8.51) will diﬀer from the original estimate ˆf(x)\n",
      "only when the latter is a nonlinear or adaptive function of the data. For\n",
      "example, to bag the B-spline smooth of Section 8.2.1, we average the curves\n",
      "in the bottom left panel of Figure 8.2 at each value of x. The B-spline\n",
      "smoother is linear in the data if we ﬁx the inputs; hence if we sample using\n",
      "the parametric bootstrap in equation (8.6), then ˆfbag(x)→ˆf(x) asB→ ∞\n",
      "(Exercise 8.4). Hence bagging just reproduces the original smooth in the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.7 Bagging 283\n",
      "top left panel of Figure 8.2. The same is approximately true if we were to\n",
      "bag using the nonparametric bootstrap.\n",
      "A more interesting example is a regression tree, where ˆf(x) denotes the\n",
      "tree’s prediction at input vector x(regression trees are described in Chap-\n",
      "ter 9). Each bootstrap tree will typically involve diﬀerent features tha n the\n",
      "original, and might have a diﬀerent number of terminal nodes. The bagged\n",
      "estimate is the average prediction at xfrom these Btrees.\n",
      "Now suppose our tree produces a classiﬁer ˆG(x) for a K-class response.\n",
      "Here it is useful to consider an underlying indicator-vector function ˆf(x),\n",
      "with value a single one and K−1 zeroes, such that ˆG(x) = arg max kˆf(x).\n",
      "Then the bagged estimate ˆfbag(x) (8.51) is a K-vector [ p1(x),p2(x),... ,\n",
      "pK(x)], with pk(x) equal to the proportion of trees predicting class katx.\n",
      "The bagged classiﬁer selects the class with the most “votes” from the B\n",
      "trees, ˆGbag(x) = arg max kˆfbag(x).\n",
      "Often we require the class-probability estimates at x, rather than the\n",
      "classiﬁcations themselves. It is tempting to treat the voting proportions\n",
      "pk(x) as estimates of these probabilities. A simple two-class example shows\n",
      "that they fail in this regard. Suppose the true probability of class 1 at xis\n",
      "0.75, and each of the bagged classiﬁers accurately predict a 1. Then p1(x) =\n",
      "1, which is incorrect. For many classiﬁers ˆG(x), however, there is already\n",
      "an underlying function ˆf(x) that estimates the class probabilities at x(for\n",
      "trees, the class proportions in the terminal node). An alternative bagging\n",
      "strategy is to average these instead, rather than the vote indicator vectors.\n",
      "Not only does this produce improved estimates of the class probabilities,\n",
      "but it also tends to produce bagged classiﬁers with lower variance, especially\n",
      "for small B(see Figure 8.10 in the next example).\n",
      "8.7.1 Example: Trees with Simulated Data\n",
      "We generated a sample of size N= 30, with two classes and p= 5 features,\n",
      "each having a standard Gaussian distribution with pairwise correlation\n",
      "0.95. The response Ywas generated according to Pr( Y= 1|x1≤0.5) = 0 .2,\n",
      "Pr(Y= 1|x1>0.5) = 0 .8. The Bayes error is 0 .2. A test sample of size 2000\n",
      "was also generated from the same population. We ﬁt classiﬁcation trees to\n",
      "the training sample and to each of 200 bootstrap samples (classiﬁcation\n",
      "trees are described in Chapter 9). No pruning was used. Figure 8.9 shows\n",
      "the original tree and eleven bootstrap trees. Notice how the trees are all\n",
      "diﬀerent, with diﬀerent splitting features and cutpoints. The test error for\n",
      "the original tree and the bagged tree is shown in Figure 8.10. In this ex-\n",
      "ample the trees have high variance due to the correlation in the predictors.\n",
      "Bagging succeeds in smoothing out this variance and hence reducing the\n",
      "test error.\n",
      "Bagging can dramatically reduce the variance of unstable procedures\n",
      "like trees, leading to improved prediction. A simple argument shows why\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "284 8. Model Inference and Averaging\n",
      "|x.1 < 0.395\n",
      "010\n",
      "101\n",
      "10Original Tree\n",
      "|x.1 < 0.555\n",
      "0\n",
      "1001b = 1\n",
      "|x.2 < 0.205\n",
      "0101\n",
      "01b = 2\n",
      "|x.2 < 0.285\n",
      "1 1010b = 3\n",
      "|x.3 < 0.985\n",
      "0\n",
      "1\n",
      "011 1b = 4\n",
      "|x.4 < −1.36\n",
      "0\n",
      "1\n",
      "1010\n",
      "10b = 5\n",
      "|x.1 < 0.395\n",
      "1 10 01b = 6\n",
      "|x.1 < 0.395\n",
      "01011b = 7\n",
      "|x.3 < 0.985\n",
      "010 010b = 8\n",
      "|x.1 < 0.395\n",
      "0\n",
      "1\n",
      "0110b = 9\n",
      "|x.1 < 0.555\n",
      "101\n",
      "01b = 10\n",
      "|x.1 < 0.555\n",
      "0 101b = 11\n",
      "FIGURE 8.9. Bagging trees on simulated dataset. The top left panel shows th e\n",
      "original tree. Eleven trees grown on bootstrap samples are sh own. For each tree,\n",
      "the top split is annotated.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.7 Bagging 285\n",
      "0 50 100 150 2000.20 0.25 0.30 0.35 0.40 0.45 0.50\n",
      "Number of Bootstrap SamplesTest ErrorBagged TreesOriginal Tree\n",
      "BayesConsensus\n",
      "Probability\n",
      "FIGURE 8.10. Error curves for the bagging example of Figure 8.9. Shown is\n",
      "the test error of the original tree and bagged trees as a function of the number of\n",
      "bootstrap samples. The orange points correspond to the consensus vote, while the\n",
      "green points average the probabilities.\n",
      "bagging helps under squared-error loss, in short because averaging reduces\n",
      "variance and leaves bias unchanged.\n",
      "Assume our training observations ( xi,yi), i= 1,... ,N are indepen-\n",
      "dently drawn from a distribution P, and consider the ideal aggregate es-\n",
      "timator fag(x) = E Pˆf∗(x). Here xis ﬁxed and the bootstrap dataset Z∗\n",
      "consists of observations x∗\n",
      "i,y∗\n",
      "i,i= 1,2,... ,N sampled from P. Note that\n",
      "fag(x) is a bagging estimate, drawing bootstrap samples from the actual\n",
      "population Prather than the data. It is not an estimate that we can use\n",
      "in practice, but is convenient for analysis. We can write\n",
      "EP[Y−ˆf∗(x)]2= E P[Y−fag(x) +fag(x)−ˆf∗(x)]2\n",
      "= E P[Y−fag(x)]2+ EP[ˆf∗(x)−fag(x)]2\n",
      "≥EP[Y−fag(x)]2. (8.52)\n",
      "The extra error on the right-hand side comes from the variance of ˆf∗(x)\n",
      "around its mean fag(x). Therefore true population aggregation never in-\n",
      "creases mean squared error. This suggests that bagging—drawing samples\n",
      "from the training data— will often decrease mean-squared error.\n",
      "The above argument does not hold for classiﬁcation under 0-1 loss, be-\n",
      "cause of the nonadditivity of bias and variance. In that setting, bagging a\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "286 8. Model Inference and Averaging\n",
      "good classiﬁer can make it better, but bagging a bad classiﬁer can make it\n",
      "worse. Here is a simple example, using a randomized rule. Suppose Y= 1\n",
      "for all x, and the classiﬁer ˆG(x) predicts Y= 1 (for all x) with proba-\n",
      "bility 0.4 and predicts Y= 0 (for all x) with probability 0.6. Then the\n",
      "misclassiﬁcation error of ˆG(x) is 0.6 but that of the bagged classiﬁer is 1.0.\n",
      "For classiﬁcation we can understand the bagging eﬀect in terms of a\n",
      "consensus of independent weak learners (Dietterich, 2000a). Let the Bayes\n",
      "optimal decision at xbeG(x) = 1 in a two-class example. Suppose each\n",
      "of the weak learners G∗\n",
      "bhave an error-rate eb=e <0.5, and let S1(x) =∑B\n",
      "b=1I(G∗\n",
      "b(x) = 1) be the consensus vote for class 1. Since the weak learn-\n",
      "ers are assumed to be independent, S1(x)∼Bin(B,1−e), and Pr( S1>\n",
      "B/2)→1 asBgets large. This concept has been popularized outside of\n",
      "statistics as the “Wisdom of Crowds” (Surowiecki, 2004) — the collective\n",
      "knowledge of a diverse and independent body of people typically exceeds\n",
      "the knowledge of any single individual, and can be harnessed by voting.\n",
      "Of course, the main caveat here is “independent,” and bagged trees are\n",
      "not. Figure 8.11 illustrates the power of a consensus vote in a simulated\n",
      "example, where only 30% of the voters have some knowledge.\n",
      "In Chapter 15 we see how random forests improve on bagging by reducing\n",
      "the correlation between the sampled trees.\n",
      "Note that when we bag a model, any simple structure in the model is\n",
      "lost. As an example, a bagged tree is no longer a tree. For interpretation\n",
      "of the model this is clearly a drawback. More stable procedures like near-\n",
      "est neighbors are typically not aﬀected much by bagging. Unfortunately,\n",
      "the unstable models most helped by bagging are unstable because of the\n",
      "emphasis on interpretability, and this is lost in the bagging process.\n",
      "Figure 8.12 shows an example where bagging doesn’t help. The 100 data\n",
      "points shown have two features and two classes, separated by the gray\n",
      "linear boundary x1+x2= 1. We choose as our classiﬁer ˆG(x) a single\n",
      "axis-oriented split, choosing the split along either x1orx2that produces\n",
      "the largest decrease in training misclassiﬁcation error.\n",
      "The decision boundary obtained from bagging the 0-1 decision rule over\n",
      "B= 50 bootstrap samples is shown by the blue curve in the left panel.\n",
      "It does a poor job of capturing the true boundary. The single split rule,\n",
      "derived from the training data, splits near 0 (the middle of the range of x1\n",
      "orx2), and hence has little contribution away from the center. Averaging\n",
      "the probabilities rather than the classiﬁcations does not help here. Bagging\n",
      "estimates the expected class probabilities from the single split rule, that is,\n",
      "averaged over many replications. Note that the expected class probabilities\n",
      "computed by bagging cannot be realized on any single replication, in the\n",
      "same way that a woman cannot have 2.4 children. In this sense, bagging\n",
      "increases somewhat the space of models of the individual base classiﬁer.\n",
      "However, it doesn’t help in this and many other examples where a greater\n",
      "enlargement of the model class is needed. “Boosting” is a way of doing this\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.7 Bagging 2870 2 4 6 8 10\n",
      "P −  Probability of Informed Person Being CorrectExpected Correct out of 10Wisdom of Crowds\n",
      "Consensus\n",
      "Individual\n",
      "0.25 0.50 0.75 1.00\n",
      "FIGURE 8.11. Simulated academy awards voting. 50members vote in 10 cat-\n",
      "egories, each with 4nominations. For any category, only 15voters have some\n",
      "knowledge, represented by their probability of selecting the “ correct” candidate in\n",
      "that category (so P= 0.25means they have no knowledge). For each category, the\n",
      "15experts are chosen at random from the 50. Results show the expected correct\n",
      "(based on 50simulations) for the consensus, as well as for the individuals. T he\n",
      "error bars indicate one standard deviation. We see, for example, t hat if the 15\n",
      "informed for a category have a 50% chance of selecting the correct candidate, the\n",
      "consensus doubles the expected performance of an individual.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "288 8. Model Inference and Averaging\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "••••\n",
      "•••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•• •• •••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•• • •••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••••\n",
      "•\n",
      "• ••\n",
      "••\n",
      "••\n",
      "••• •••\n",
      "••••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••••\n",
      "•\n",
      "••\n",
      "•••••\n",
      "••\n",
      "•••\n",
      "•••••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•Bagged Decision Rule\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "••••\n",
      "•••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•• •• •••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•• • •••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••••\n",
      "•\n",
      "• ••\n",
      "••\n",
      "••\n",
      "••• •••\n",
      "••••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••••\n",
      "•\n",
      "••\n",
      "•••••\n",
      "••\n",
      "•••\n",
      "•••••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•Boosted Decision Rule\n",
      "FIGURE 8.12. Data with two features and two classes, separated by a linear\n",
      "boundary. (Left panel:) Decision boundary estimated from bagg ing the decision\n",
      "rule from a single split, axis-oriented classiﬁer. (Right panel: ) Decision boundary\n",
      "from boosting the decision rule of the same classiﬁer. The test error rates are\n",
      "0.166, and 0.065, respectively. Boosting is described in Chapter 10.\n",
      "and is described in Chapter 10. The decision boundary in the right panel is\n",
      "the result of the boosting procedure, and it roughly captures the diagonal\n",
      "boundary.\n",
      "8.8 Model Averaging and Stacking\n",
      "In Section 8.4 we viewed bootstrap values of an estimator as approximate\n",
      "posterior values of a corresponding parameter, from a kind of nonparamet-\n",
      "ric Bayesian analysis. Viewed in this way, the bagged estimate (8.51) i s\n",
      "an approximate posterior Bayesian mean. In contrast, the training sampl e\n",
      "estimate ˆf(x) corresponds to the mode of the posterior. Since the posterior\n",
      "mean (not mode) minimizes squared-error loss, it is not surprising that\n",
      "bagging can often reduce mean squared-error.\n",
      "Here we discuss Bayesian model averaging more generally. We have a\n",
      "set of candidate models Mm, m= 1,... ,M for our training set Z. These\n",
      "models may be of the same type with diﬀerent parameter values (e.g.,\n",
      "subsets in linear regression), or diﬀerent models for the same task (e.g.,\n",
      "neural networks and regression trees).\n",
      "Suppose ζis some quantity of interest, for example, a prediction f(x) at\n",
      "some ﬁxed feature value x. The posterior distribution of ζis\n",
      "Pr(ζ|Z) =M∑\n",
      "m=1Pr(ζ|Mm,Z)Pr(Mm|Z), (8.53)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.8 Model Averaging and Stacking 289\n",
      "with posterior mean\n",
      "E(ζ|Z) =M∑\n",
      "m=1E(ζ|Mm,Z)Pr(Mm|Z). (8.54)\n",
      "This Bayesian prediction is a weighted average of the individual predictions,\n",
      "with weights proportional to the posterior probability of each model.\n",
      "This formulation leads to a number of diﬀerent model-averaging strate-\n",
      "gies.Committee methods take a simple unweighted average of the predic-\n",
      "tions from each model, essentially giving equal probability to each model.\n",
      "More ambitiously, the development in Section 7.7 shows the BIC criterion\n",
      "can be used to estimate posterior model probabilities. This is applicable\n",
      "in cases where the diﬀerent models arise from the same parametric model,\n",
      "with diﬀerent parameter values. The BIC gives weight to each model de-\n",
      "pending on how well it ﬁts and how many parameters it uses. One can also\n",
      "carry out the Bayesian recipe in full. If each model Mmhas parameters\n",
      "θm, we write\n",
      "Pr(Mm|Z)∝Pr(Mm)≤Pr(Z|Mm)\n",
      "∝Pr(Mm)≤∫\n",
      "Pr(Z|θm,Mm)Pr(θm|Mm)dθm.\n",
      "(8.55)\n",
      "In principle one can specify priors Pr( θm|Mm) and numerically com-\n",
      "pute the posterior probabilities from (8.55), to be used as model-averaging\n",
      "weights. However, we have seen no real evidence that this is worth all of\n",
      "the eﬀort, relative to the much simpler BIC approximation.\n",
      "How can we approach model averaging from a frequentist viewpoint?\n",
      "Given predictions ˆf1(x),ˆf2(x),... ,ˆfM(x), under squared-error loss, we can\n",
      "seek the weights w= (w1,w2,... ,w M) such that\n",
      "ˆw= argmin\n",
      "wEP[\n",
      "Y−M∑\n",
      "m=1wmˆfm(x)]2\n",
      ". (8.56)\n",
      "Here the input value xis ﬁxed and the Nobservations in the dataset Z(and\n",
      "the target Y) are distributed according to P. The solution is the population\n",
      "linear regression of YonˆF(x)T≡[ˆf1(x),ˆf2(x),... ,ˆfM(x)]:\n",
      "ˆw= EP[ˆF(x)ˆF(x)T]−1EP[ˆF(x)Y]. (8.57)\n",
      "Now the full regression has smaller error than any single model\n",
      "EP[\n",
      "Y−M∑\n",
      "m=1ˆwmˆfm(x)]2\n",
      "≤EP[\n",
      "Y−ˆfm(x)]2\n",
      "∀m (8.58)\n",
      "so combining models never makes things worse, at the population level.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "290 8. Model Inference and Averaging\n",
      "Of course the population linear regression (8.57) is not available, and it\n",
      "is natural to replace it with the linear regression over the training set. But\n",
      "there are simple examples where this does not work well. For example, if\n",
      "ˆfm(x), m= 1,2,... ,M represent the prediction from the best subset of\n",
      "inputs of size mamong Mtotal inputs, then linear regression would put all\n",
      "of the weight on the largest model, that is, ˆ wM= 1,ˆwm= 0, m < M . The\n",
      "problem is that we have not put each of the models on the same footing\n",
      "by taking into account their complexity (the number of inputs min this\n",
      "example).\n",
      "Stacked generalization , orstacking , is a way of doing this. Let ˆf−i\n",
      "m(x)\n",
      "be the prediction at x, using model m, applied to the dataset with the\n",
      "ith training observation removed. The stacking estimate of the weights is\n",
      "obtained from the least squares linear regression of yionˆf−i\n",
      "m(xi), m=\n",
      "1,2,... ,M . In detail the stacking weights are given by\n",
      "ˆwst= argmin\n",
      "wN∑\n",
      "i=1[\n",
      "yi−M∑\n",
      "m=1wmˆf−i\n",
      "m(xi)]2\n",
      ". (8.59)\n",
      "The ﬁnal prediction is∑\n",
      "mˆwst\n",
      "mˆfm(x). By using the cross-validated pre-\n",
      "dictions ˆf−i\n",
      "m(x), stacking avoids giving unfairly high weight to models with\n",
      "higher complexity. Better results can be obtained by restricting the weights\n",
      "to be nonnegative, and to sum to 1. This seems like a reasonable restriction\n",
      "if we interpret the weights as posterior model probabilities as in equation\n",
      "(8.54), and it leads to a tractable quadratic programming problem.\n",
      "There is a close connection between stacking and model selection via\n",
      "leave-one-out cross-validation (Section 7.10). If we restrict the minimizatio n\n",
      "in (8.59) to weight vectors wthat have one unit weight and the rest zero,\n",
      "this leads to a model choice ˆ mwith smallest leave-one-out cross-validation\n",
      "error. Rather than choose a single model, stacking combines them with\n",
      "estimated optimal weights. This will often lead to better prediction, but\n",
      "less interpretability than the choice of only one of the Mmodels.\n",
      "The stacking idea is actually more general than described above. One\n",
      "can use any learning method, not just linear regression, to combine the\n",
      "models as in (8.59); the weights could also depend on the input location\n",
      "x. In this way, learning methods are “stacked” on top of one another, to\n",
      "improve prediction performance.\n",
      "8.9 Stochastic Search: Bumping\n",
      "The ﬁnal method described in this chapter does not involve averaging or\n",
      "combining models, but rather is a technique for ﬁnding a better single\n",
      "model. Bumping uses bootstrap sampling to move randomly through model\n",
      "space. For problems where ﬁtting method ﬁnds many local minima, bump-\n",
      "ing can help the method to avoid getting stuck in poor solutions.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "8.9 Stochastic Search: Bumping 291\n",
      "Regular 4-Node Tree\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••••••\n",
      "••••\n",
      "•\n",
      "•\n",
      "• ••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "• •••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "••••• •\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••••\n",
      "•\n",
      "••••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••••\n",
      "•\n",
      "•• •••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•• •\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••••••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••••\n",
      "•••••\n",
      "•\n",
      "• •• ••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "••Bumped 4-Node Tree\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••••••\n",
      "••••\n",
      "•\n",
      "•\n",
      "• ••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "• •••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "••••• •\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••••\n",
      "•\n",
      "••••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••••\n",
      "•\n",
      "•• •••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•• •\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••••••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••••\n",
      "•••••\n",
      "•\n",
      "• •• ••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "FIGURE 8.13. Data with two features and two classes (blue and orange), dis-\n",
      "playing a pure interaction. The left panel shows the partition fo und by three splits\n",
      "of a standard, greedy, tree-growing algorithm. The vertical g rey line near the left\n",
      "edge is the ﬁrst split, and the broken lines are the two subsequent splits. The al-\n",
      "gorithm has no idea where to make a good initial split, and makes a poor choice.\n",
      "The right panel shows the near-optimal splits found by bumping th e tree-growing\n",
      "algorithm 20times.\n",
      "As in bagging, we draw bootstrap samples and ﬁt a model to each. But\n",
      "rather than average the predictions, we choose the model estimated from a\n",
      "bootstrap sample that best ﬁts the training data. In detail, we draw boot-\n",
      "strap samples Z∗1,... ,Z∗Band ﬁt our model to each, giving predictions\n",
      "ˆf∗b(x), b= 1,2,... ,B at input point x. We then choose the model that\n",
      "produces the smallest prediction error, averaged over the original training\n",
      "set. For squared error, for example, we choose the model obtained from\n",
      "bootstrap sample ˆb, where\n",
      "ˆb= arg min\n",
      "bN∑\n",
      "i=1[yi−ˆf∗b(xi)]2. (8.60)\n",
      "The corresponding model predictions are ˆf∗ˆb(x). By convention we also\n",
      "include the original training sample in the set of bootstrap samples, so that\n",
      "the method is free to pick the original model if it has the lowest training\n",
      "error.\n",
      "By perturbing the data, bumping tries to move the ﬁtting procedure\n",
      "around to good areas of model space. For example, if a few data points are\n",
      "causing the procedure to ﬁnd a poor solution, any bootstrap sample that\n",
      "omits those data points should procedure a better solution.\n",
      "For another example, consider the classiﬁcation data in Figure 8.13, the\n",
      "notorious exclusive or (XOR) problem. There are two classes (blue and\n",
      "orange) and two input features, with the features exhibiting a pure inter-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "292 8. Model Inference and Averaging\n",
      "action. By splitting the data at x1= 0 and then splitting each resulting\n",
      "strata at x2= 0, (or vice versa) a tree-based classiﬁer could achieve per-\n",
      "fect discrimination. However, the greedy, short-sighted CART algorithm\n",
      "(Section 9.2) tries to ﬁnd the best split on either feature, and then splits\n",
      "the resulting strata. Because of the balanced nature of the data, all initial\n",
      "splits on x1orx2appear to be useless, and the procedure essentially gener-\n",
      "ates a random split at the top level. The actual split found for these data is\n",
      "shown in the left panel of Figure 8.13. By bootstrap sampling from the data ,\n",
      "bumping breaks the balance in the classes, and with a reasonable number\n",
      "of bootstrap samples (here 20), it will by chance produce at least one tree\n",
      "with initial split near either x1= 0 or x2= 0. Using just 20 bootstrap\n",
      "samples, bumping found the near optimal splits shown in the right panel\n",
      "of Figure 8.13. This shortcoming of the greedy tree-growing algorithm is\n",
      "exacerbated if we add a number of noise features that are independent of\n",
      "the class label. Then the tree-growing algorithm cannot distinguish x1or\n",
      "x2from the others, and gets seriously lost.\n",
      "Since bumping compares diﬀerent models on the training data, one must\n",
      "ensure that the models have roughly the same complexity. In the case of\n",
      "trees, this would mean growing trees with the same number of terminal\n",
      "nodes on each bootstrap sample. Bumping can also help in problems where\n",
      "it is diﬃcult to optimize the ﬁtting criterion, perhaps because of a lack of\n",
      "smoothness. The trick is to optimize a diﬀerent, more convenient criterion\n",
      "over the bootstrap samples, and then choose the model producing the best\n",
      "results for the desired criterion on the training sample.\n",
      "Bibliographic Notes\n",
      "There are many books on classical statistical inference: Cox and Hink-\n",
      "ley (1974) and Silvey (1975) give nontechnical accounts. The bootstrap\n",
      "is due to Efron (1979) and is described more fully in Efron and Tibshi-\n",
      "rani (1993) and Hall (1992). A good modern book on Bayesian inference\n",
      "is Gelman et al. (1995). A lucid account of the application of Bayesian\n",
      "methods to neural networks is given in Neal (1996). The statistical appli-\n",
      "cation of Gibbs sampling is due to Geman and Geman (1984), and Gelfand\n",
      "and Smith (1990), with related work by Tanner and Wong (1987). Markov\n",
      "chain Monte Carlo methods, including Gibbs sampling and the Metropolis–\n",
      "Hastings algorithm, are discussed in Spiegelhalter et al. (1996). The EM\n",
      "algorithm is due to Dempster et al. (1977); as the discussants in that pa-\n",
      "per make clear, there was much related, earlier work. The view of EM as\n",
      "a joint maximization scheme for a penalized complete-data log-likelihood\n",
      "was elucidated by Neal and Hinton (1998); they credit Csiszar and Tusn´ ady\n",
      "(1984) and Hathaway (1986) as having noticed this connection earlier. Bag-\n",
      "ging was proposed by Breiman (1996a). Stacking is due to Wolpert (1992) ;\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 293\n",
      "Breiman (1996b) contains an accessible discussion for statisticians. Lebla nc\n",
      "and Tibshirani (1996) describe variations on stacking based on the boot-\n",
      "strap. Model averaging in the Bayesian framework has been recently advo-\n",
      "cated by Madigan and Raftery (1994). Bumping was proposed by Tibshi-\n",
      "rani and Knight (1999).\n",
      "Exercises\n",
      "Ex. 8.1 Letr(y) and q(y) be probability density functions. Jensen’s in-\n",
      "equality states that for a random variable Xand a convex function φ(x),\n",
      "E[φ(X)]≥φ[E(X)]. Use Jensen’s inequality to show that\n",
      "Eqlog[r(Y)/q(Y)] (8.61)\n",
      "is maximized as a function of r(y) when r(y) =q(y). Hence show that\n",
      "R(θ,θ)≥R(θ′,θ) as stated below equation (8.46).\n",
      "Ex. 8.2 Consider the maximization of the log-likelihood (8.48), over dis-\n",
      "tributions ˜P(Zm) such that ˜P(Zm)≥0 and∑\n",
      "Zm˜P(Zm) = 1. Use La-\n",
      "grange multipliers to show that the solution is the conditional distribution\n",
      "˜P(Zm) = Pr( Zm|Z,θ′), as in (8.49).\n",
      "Ex. 8.3 Justify the estimate (8.50), using the relationship\n",
      "Pr(A) =∫\n",
      "Pr(A|B)d(Pr(B)).\n",
      "Ex. 8.4 Consider the bagging method of Section 8.7. Let our estimate ˆf(x)\n",
      "be the B-spline smoother ˆ θ(x) of Section 8.2.1. Consider the parametric\n",
      "bootstrap of equation (8.6), applied to this estimator. Show that if we ba g\n",
      "ˆf(x), using the parametric bootstrap to generate the bootstrap samples,\n",
      "the bagging estimate ˆfbag(x) converges to the original estimate ˆf(x) as\n",
      "B→ ∞.\n",
      "Ex. 8.5 Suggest generalizations of each of the loss functions in Figure 10.4\n",
      "to more than two classes, and design an appropriate plot to compare them.\n",
      "Ex. 8.6 Consider the bone mineral density data of Figure 5.6.\n",
      "(a) Fit a cubic smooth spline to the relative change in spinal BMD, as a\n",
      "function of age. Use cross-validation to estimate the optimal amount\n",
      "of smoothing. Construct pointwise 90% conﬁdence bands for the un-\n",
      "derlying function.\n",
      "(b) Compute the posterior mean and covariance for the true function via\n",
      "(8.28), and compare the posterior bands to those obtained in (a).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "294 8. Model Inference and Averaging\n",
      "(c) Compute 100 bootstrap replicates of the ﬁtted curves, as in the bottom\n",
      "left panel of Figure 8.2. Compare the results to those obtained in (a)\n",
      "and (b).\n",
      "Ex. 8.7 EM as a minorization algorithm (Hunter and Lange, 2004; Wu and\n",
      "Lange, 2007). A function g(x,y) to said to minorize a function f(x) if\n",
      "g(x,y)≤f(x), g(x,x) =f(x) (8.62)\n",
      "for all x,yin the domain. This is useful for maximizing f(x) since is easy\n",
      "to show that f(x) is non-decreasing under the update\n",
      "xs+1= argmaxxg(x,xs) (8.63)\n",
      "There are analogous deﬁnitions for majorization , for minimizing a function\n",
      "f(x). The resulting algorithms are known as MMalgorithms, for “Minorize-\n",
      "Maximize” or “Majorize-Minimize.”\n",
      "Show that the EM algorithm (Section 8.5.2) is an example of an MM al-\n",
      "gorithm, using Q(θ′,θ)+log Pr( Z|θ)−Q(θ,θ) to minorize the observed data\n",
      "log-likelihood ℓ(θ′;Z). (Note that only the ﬁrst term involves the relevant\n",
      "parameter θ′).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 295\n",
      "Printer: Opaque this\n",
      "9\n",
      "Additive Models, Trees, and Related\n",
      "Methods\n",
      "In this chapter we begin our discussion of some speciﬁc methods for super-\n",
      "vised learning. These techniques each assume a (diﬀerent) structured form\n",
      "for the unknown regression function, and by doing so they ﬁnesse the curse\n",
      "of dimensionality. Of course, they pay the possible price of misspecifying\n",
      "the model, and so in each case there is a tradeoﬀ that has to be made. They\n",
      "take oﬀ where Chapters 3–6 left oﬀ. We describe ﬁve related techniques:\n",
      "generalized additive models, trees, multivariate adaptive regression splines,\n",
      "the patient rule induction method, and hierarchical mixtures of experts.\n",
      "9.1 Generalized Additive Models\n",
      "Regression models play an important role in many data analyses, providi ng\n",
      "prediction and classiﬁcation rules, and data analytic tools for understand-\n",
      "ing the importance of diﬀerent inputs.\n",
      "Although attractively simple, the traditional linear model often fails in\n",
      "these situations: in real life, eﬀects are often not linear. In earlier chapters\n",
      "we described techniques that used predeﬁned basis functions to achieve\n",
      "nonlinearities. This section describes more automatic ﬂexible statistical\n",
      "methods that may be used to identify and characterize nonlinear regression\n",
      "eﬀects. These methods are called “generalized additive models.”\n",
      "In the regression setting, a generalized additive model has the form\n",
      "E(Y|X1,X2,... ,X p) =α+f1(X1) +f2(X2) +≤≤≤+fp(Xp).(9.1)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "296 9. Additive Models, Trees, and Related Methods\n",
      "As usual X1,X2,... ,X prepresent predictors and Yis the outcome; the fj’s\n",
      "are unspeciﬁed smooth (“nonparametric”) functions. If we were to model\n",
      "each function using an expansion of basis functions (as in Chapter 5), the\n",
      "resulting model could then be ﬁt by simple least squares. Our approach\n",
      "here is diﬀerent: we ﬁt each function using a scatterplot smoother (e.g., a\n",
      "cubic smoothing spline or kernel smoother), and provide an algorithm for\n",
      "simultaneously estimating all pfunctions (Section 9.1.1).\n",
      "For two-class classiﬁcation, recall the logistic regression model for binar y\n",
      "data discussed in Section 4.4. We relate the mean of the binary response\n",
      "θ(X) = Pr( Y= 1|X) to the predictors via a linear regression model and\n",
      "thelogitlink function:\n",
      "log(θ(X)\n",
      "1−θ(X))\n",
      "=α+β1X1+≤≤≤+βpXp. (9.2)\n",
      "Theadditive logistic regression model replaces each linear term by a more\n",
      "general functional form\n",
      "log(θ(X)\n",
      "1−θ(X))\n",
      "=α+f1(X1) +≤≤≤+fp(Xp), (9.3)\n",
      "where again each fjis an unspeciﬁed smooth function. While the non-\n",
      "parametric form for the functions fjmakes the model more ﬂexible, the\n",
      "additivity is retained and allows us to interpret the model in much the\n",
      "same way as before. The additive logistic regression model is an example\n",
      "of a generalized additive model. In general, the conditional mean θ(X) of\n",
      "a response Yis related to an additive function of the predictors via a link\n",
      "function g:\n",
      "g[θ(X)] =α+f1(X1) +≤≤≤+fp(Xp). (9.4)\n",
      "Examples of classical link functions are the following:\n",
      "•g(θ) =θis the identity link, used for linear and additive models for\n",
      "Gaussian response data.\n",
      "•g(θ) = logit( θ) as above, or g(θ) = probit( θ), theprobit link function,\n",
      "for modeling binomial probabilities. The probit function is the inverse\n",
      "Gaussian cumulative distribution function: probit( θ) = Φ−1(θ).\n",
      "•g(θ) = log( θ) for log-linear or log-additive models for Poisson count\n",
      "data.\n",
      "All three of these arise from exponential family sampling models, which\n",
      "in addition include the gamma and negative-binomial distributions. These\n",
      "families generate the well-known class of generalized linear models, which\n",
      "are all extended in the same way to generalized additive models.\n",
      "The functions fjare estimated in a ﬂexible manner, using an algorithm\n",
      "whose basic building block is a scatterplot smoother. The estimated func-\n",
      "tionˆfjcan then reveal possible nonlinearities in the eﬀect of Xj. Not all\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.1 Generalized Additive Models 297\n",
      "of the functions fjneed to be nonlinear. We can easily mix in linear and\n",
      "other parametric forms with the nonlinear terms, a necessity when some of\n",
      "the inputs are qualitative variables (factors). The nonlinear terms are not\n",
      "restricted to main eﬀects either; we can have nonlinear components in two\n",
      "or more variables, or separate curves in Xjfor each level of the factor Xk.\n",
      "Thus each of the following would qualify:\n",
      "•g(θ) =XTβ+αk+f(Z)—asemiparametric model, where Xis a\n",
      "vector of predictors to be modeled linearly, αkthe eﬀect for the kth\n",
      "level of a qualitative input V, and the eﬀect of predictor Zis modeled\n",
      "nonparametrically.\n",
      "•g(θ) =f(X) +gk(Z)—again kindexes the levels of a qualitative\n",
      "input V, and thus creates an interaction term g(V,Z) =gk(Z) for\n",
      "the eﬀect of VandZ.\n",
      "•g(θ) =f(X) +g(Z,W) where gis a nonparametric function in two\n",
      "features.\n",
      "Additive models can replace linear models in a wide variety of settings,\n",
      "for example an additive decomposition of time series,\n",
      "Yt=St+Tt+εt, (9.5)\n",
      "where Stis a seasonal component, Ttis a trend and εis an error term.\n",
      "9.1.1 Fitting Additive Models\n",
      "In this section we describe a modular algorithm for ﬁtting additive models\n",
      "and their generalizations. The building block is the scatterplot smoother\n",
      "for ﬁtting nonlinear eﬀects in a ﬂexible way. For concreteness we use as our\n",
      "scatterplot smoother the cubic smoothing spline described in Chapter 5.\n",
      "The additive model has the form\n",
      "Y=α+p∑\n",
      "j=1fj(Xj) +ε, (9.6)\n",
      "where the error term εhas mean zero. Given observations xi,yi, a criterion\n",
      "like the penalized sum of squares (5.9) of Section 5.4 can be speciﬁed for\n",
      "this problem,\n",
      "PRSS( α,f1,f2,... ,f p) =N∑\n",
      "i=1(\n",
      "yi−α−p∑\n",
      "j=1fj(xij))2\n",
      "+p∑\n",
      "j=1λj∫\n",
      "f′′\n",
      "j(tj)2dtj,\n",
      "(9.7)\n",
      "where the λj≥0 are tuning parameters. It can be shown that the minimizer\n",
      "of (9.7) is an additive cubic spline model; each of the functions fjis a\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "298 9. Additive Models, Trees, and Related Methods\n",
      "Algorithm 9.1 The Backﬁtting Algorithm for Additive Models.\n",
      "1. Initialize: ˆ α=1\n",
      "N∑N\n",
      "1yi,ˆfj≡0,∀i,j.\n",
      "2. Cycle: j= 1,2,... ,p,... , 1,2,... ,p,... ,\n",
      "ˆfj← S j[\n",
      "{yi−ˆα−∑\n",
      "k̸=jˆfk(xik)}N\n",
      "1]\n",
      ",\n",
      "ˆfj←ˆfj−1\n",
      "NN∑\n",
      "i=1ˆfj(xij).\n",
      "until the functions ˆfjchange less than a prespeciﬁed threshold.\n",
      "cubic spline in the component Xj, with knots at each of the unique values\n",
      "ofxij, i= 1,... ,N . However, without further restrictions on the model,\n",
      "the solution is not unique. The constant αis not identiﬁable, since we\n",
      "can add or subtract any constants to each of the functions fj, and adjust\n",
      "αaccordingly. The standard convention is to assume that∑N\n",
      "1fj(xij) =\n",
      "0∀j—the functions average zero over the data. It is easily seen that ˆ α=\n",
      "ave(yi) in this case. If in addition to this restriction, the matrix of input\n",
      "values (having ijth entry xij) has full column rank, then (9.7) is a strictly\n",
      "convex criterion and the minimizer is unique. If the matrix is singular, then\n",
      "thelinear part of the components fjcannot be uniquely determined (while\n",
      "the nonlinear parts can!)(Buja et al., 1989).\n",
      "Furthermore, a simple iterative procedure exists for ﬁnding the solution.\n",
      "We set ˆ α= ave( yi), and it never changes. We apply a cubic smoothing\n",
      "spline Sjto the targets {yi−ˆα−∑\n",
      "k̸=jˆfk(xik)}N\n",
      "1, as a function of xij,\n",
      "to obtain a new estimate ˆfj. This is done for each predictor in turn, using\n",
      "the current estimates of the other functions ˆfkwhen computing yi−ˆα−∑\n",
      "k̸=jˆfk(xik). The process is continued until the estimates ˆfjstabilize. This\n",
      "procedure, given in detail in Algorithm 9.1, is known as “backﬁtting” and\n",
      "the resulting ﬁt is analogous to a multiple regression for linear models.\n",
      "In principle, the second step in (2) of Algorithm 9.1 is not needed, since\n",
      "the smoothing spline ﬁt to a mean-zero response has mean zero (Exer-\n",
      "cise 9.1). In practice, machine rounding can cause slippage, and the ad-\n",
      "justment is advised.\n",
      "This same algorithm can accommodate other ﬁtting methods in exactly\n",
      "the same way, by specifying appropriate smoothing operators Sj:\n",
      "•other univariate regression smoothers such as local polynomial re-\n",
      "gression and kernel methods;\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.1 Generalized Additive Models 299\n",
      "•linear regression operators yielding polynomial ﬁts, piecewise con-\n",
      "stant ﬁts, parametric spline ﬁts, series and Fourier ﬁts;\n",
      "•more complicated operators such as surface smoothers for second or\n",
      "higher-order interactions or periodic smoothers for seasonal eﬀects.\n",
      "If we consider the operation of smoother Sjonly at the training points, it\n",
      "can be represented by an N×Noperator matrix Sj(see Section 5.4.1).\n",
      "Then the degrees of freedom for the jth term are (approximately) computed\n",
      "as df j= trace[ Sj]−1, by analogy with degrees of freedom for smoothers\n",
      "discussed in Chapters 5 and 6.\n",
      "For a large class of linear smoothers Sj, backﬁtting is equivalent to a\n",
      "Gauss–Seidel algorithm for solving a certain linear system of equations.\n",
      "Details are given in Exercise 9.2.\n",
      "For the logistic regression model and other generalized additive models,\n",
      "the appropriate criterion is a penalized log-likelihood. To maximize it, the\n",
      "backﬁtting procedure is used in conjunction with a likelihood maximizer.\n",
      "The usual Newton–Raphson routine for maximizing log-likelihoods in gen-\n",
      "eralized linear models can be recast as an IRLS (iteratively reweighted\n",
      "least squares) algorithm. This involves repeatedly ﬁtting a weighted linear\n",
      "regression of a working response variable on the covariates; each regress ion\n",
      "yields a new value of the parameter estimates, which in turn give new work-\n",
      "ing responses and weights, and the process is iterated (see Section 4.4.1).\n",
      "In the generalized additive model, the weighted linear regression is simply\n",
      "replaced by a weighted backﬁtting algorithm. We describe the algorithm in\n",
      "more detail for logistic regression below, and more generally in Chapter 6\n",
      "of Hastie and Tibshirani (1990).\n",
      "9.1.2 Example: Additive Logistic Regression\n",
      "Probably the most widely used model in medical research is the logistic\n",
      "model for binary data. In this model the outcome Ycan be coded as 0\n",
      "or 1, with 1 indicating an event (like death or relapse of a disease) and\n",
      "0 indicating no event. We wish to model Pr( Y= 1|X), the probability of\n",
      "an event given values of the prognostic factors XT= (X1,... ,X p). The\n",
      "goal is usually to understand the roles of the prognostic factors, rather\n",
      "than to classify new individuals. Logistic models are also used in applica-\n",
      "tions where one is interested in estimating the class probabilities, for use\n",
      "in risk screening. Apart from medical applications, credit risk screening is\n",
      "a popular application.\n",
      "The generalized additive logistic model has the form\n",
      "logPr(Y= 1|X)\n",
      "Pr(Y= 0|X)=α+f1(X1) +≤≤≤+fp(Xp). (9.8)\n",
      "The functions f1,f2,... ,f pare estimated by a backﬁtting algorithm\n",
      "within a Newton–Raphson procedure, shown in Algorithm 9.2.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "300 9. Additive Models, Trees, and Related Methods\n",
      "Algorithm 9.2 Local Scoring Algorithm for the Additive Logistic Regres-\n",
      "sion Model.\n",
      "1. Compute starting values: ˆ α= log[¯ y/(1−¯y)], where ¯ y= ave( yi), the\n",
      "sample proportion of ones, and set ˆfj≡0∀j.\n",
      "2. Deﬁne ˆ ηi= ˆα+∑\n",
      "jˆfj(xij) and ˆ pi= 1/[1 + exp( −ˆηi)].\n",
      "Iterate:\n",
      "(a) Construct the working target variable\n",
      "zi= ˆηi+(yi−ˆpi)\n",
      "ˆpi(1−ˆpi).\n",
      "(b) Construct weights wi= ˆpi(1−ˆpi)\n",
      "(c) Fit an additive model to the targets ziwith weights wi, us-\n",
      "ing a weighted backﬁtting algorithm. This gives new estimates\n",
      "ˆα,ˆfj,∀j\n",
      "3. Continue step 2. until the change in the functions falls below a pre-\n",
      "speciﬁed threshold.\n",
      "The additive model ﬁtting in step (2) of Algorithm 9.2 requires a weighted\n",
      "scatterplot smoother. Most smoothing procedures can accept observation\n",
      "weights (Exercise 5.12); see Chapter 3 of Hastie and Tibshirani (1990) fo r\n",
      "further details.\n",
      "The additive logistic regression model can be generalized further to han-\n",
      "dle more than two classes, using the multilogit formulation as outlined in\n",
      "Section 4.4. While the formulation is a straightforward extension of ( 9.8),\n",
      "the algorithms for ﬁtting such models are more complex. See Yee and Wild\n",
      "(1996) for details, and the VGAMsoftware currently available from:\n",
      "http://www.stat.auckland.ac.nz/ ∼yee.\n",
      "Example: Predicting Email Spam\n",
      "We apply a generalized additive model to the spam data introduced in\n",
      "Chapter 1. The data consists of information from 4601 email messages, in\n",
      "a study to screen email for “spam” (i.e., junk email). The data is publicly\n",
      "available at ftp.ics.uci.edu , and was donated by George Forman from\n",
      "Hewlett-Packard laboratories, Palo Alto, California.\n",
      "The response variable is binary, with values email orspam, and there are\n",
      "57 predictors as described below:\n",
      "•48 quantitative predictors—the percentage of words in the email that\n",
      "match a given word. Examples include business ,address ,internet ,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.1 Generalized Additive Models 301\n",
      "TABLE 9.1. Test data confusion matrix for the additive logistic regress ion model\n",
      "ﬁt to the spam training data. The overall test error rate is 5.5%.\n",
      "Predicted Class\n",
      "True Class email (0)spam(1)\n",
      "email (0) 58.3% 2.5%\n",
      "spam(1) 3.0% 36.3%\n",
      "free, andgeorge . The idea was that these could be customized for\n",
      "individual users.\n",
      "•6 quantitative predictors—the percentage of characters in the email\n",
      "that match a given character. The characters are ch;,ch(,ch[,ch!,\n",
      "ch$, andch#.\n",
      "•The average length of uninterrupted sequences of capital letters:\n",
      "CAPAVE .\n",
      "•The length of the longest uninterrupted sequence of capital letters:\n",
      "CAPMAX .\n",
      "•The sum of the length of uninterrupted sequences of capital letters:\n",
      "CAPTOT .\n",
      "We coded spamas 1 and email as zero. A test set of size 1536 was randomly\n",
      "chosen, leaving 3065 observations in the training set. A generalized additive\n",
      "model was ﬁt, using a cubic smoothing spline with a nominal four degrees of\n",
      "freedom for each predictor. What this means is that for each predictor Xj,\n",
      "the smoothing-spline parameter λjwas chosen so that trace[ Sj(λj)]−1 = 4,\n",
      "whereSj(λ) is the smoothing spline operator matrix constructed using the\n",
      "observed values xij, i= 1,... ,N . This is a convenient way of specifying\n",
      "the amount of smoothing in such a complex model.\n",
      "Most of the spampredictors have a very long-tailed distribution. Before\n",
      "ﬁtting the GAM model, we log-transformed each variable (actually log( x+\n",
      "0.1)), but the plots in Figure 9.1 are shown as a function of the original\n",
      "variables.\n",
      "The test error rates are shown in Table 9.1; the overall error rate is 5.3 %.\n",
      "By comparison, a linear logistic regression has a test error rate of 7.6% .\n",
      "Table 9.2 shows the predictors that are highly signiﬁcant in the additive\n",
      "model.\n",
      "For ease of interpretation, in Table 9.2 the contribution for each variabl e\n",
      "is decomposed into a linear component and the remaining nonlinear com-\n",
      "ponent. The top block of predictors are positively correlated with spam,\n",
      "while the bottom block is negatively correlated. The linear component is a\n",
      "weighted least squares linear ﬁt of the ﬁtted curve on the predictor, while\n",
      "the nonlinear part is the residual. The linear component of an estimated\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "302 9. Additive Models, Trees, and Related Methods\n",
      "TABLE 9.2. Signiﬁcant predictors from the additive model ﬁt to the spam train-\n",
      "ing data. The coeﬃcients represent the linear part of ˆfj, along with their standard\n",
      "errors and Z-score. The nonlinear P-value is for a test of nonlineari ty of ˆfj.\n",
      "Name Num. df Coeﬃcient Std. Error ZScore Nonlinear\n",
      "P-value\n",
      "Positive eﬀects\n",
      "our 5 3.9 0.566 0.114 4.970 0.052\n",
      "over 6 3.9 0.244 0.195 1.249 0.004\n",
      "remove 7 4.0 0.949 0.183 5.201 0.093\n",
      "internet 8 4.0 0.524 0.176 2.974 0.028\n",
      "free 16 3.9 0.507 0.127 4.010 0.065\n",
      "business 17 3.8 0.779 0.186 4.179 0.194\n",
      "hpl 26 3.8 0.045 0.250 0.181 0.002\n",
      "ch! 52 4.0 0.674 0.128 5.283 0.164\n",
      "ch$ 53 3.9 1.419 0.280 5.062 0.354\n",
      "CAPMAX 56 3.8 0.247 0.228 1.080 0.000\n",
      "CAPTOT 57 4.0 0.755 0.165 4.566 0.063\n",
      "Negative eﬀects\n",
      "hp 25 3.9 −1.404 0.224 −6.262 0.140\n",
      "george 27 3.7 −5.003 0.744 −6.722 0.045\n",
      "1999 37 3.8 −0.672 0.191 −3.512 0.011\n",
      "re 45 3.9 −0.620 0.133 −4.649 0.597\n",
      "edu 46 4.0 −1.183 0.209 −5.647 0.000\n",
      "function is summarized by the coeﬃcient, standard error and Z-score; the\n",
      "latter is the coeﬃcient divided by its standard error, and is considered\n",
      "signiﬁcant if it exceeds the appropriate quantile of a standard normal dis-\n",
      "tribution. The column labeled nonlinear P-value is a test of nonlinearity\n",
      "of the estimated function. Note, however, that the eﬀect of each predictor\n",
      "is fully adjusted for the entire eﬀects of the other predictors, not just for\n",
      "their linear parts. The predictors shown in the table were judged signiﬁ-\n",
      "cant by at least one of the tests (linear or nonlinear) at the p= 0.01 level\n",
      "(two-sided).\n",
      "Figure 9.1 shows the estimated functions for the signiﬁcant predictors\n",
      "appearing in Table 9.2. Many of the nonlinear eﬀects appear to account for\n",
      "a strong discontinuity at zero. For example, the probability of spamdrops\n",
      "signiﬁcantly as the frequency of george increases from zero, but then does\n",
      "not change much after that. This suggests that one might replace each of\n",
      "the frequency predictors by an indicator variable for a zero count, and resort\n",
      "to a linear logistic model. This gave a test error rate of 7 .4%; including the\n",
      "linear eﬀects of the frequencies as well dropped the test error to 6 .6%. It\n",
      "appears that the nonlinearities in the additive model have an additional\n",
      "predictive power.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.1 Generalized Additive Models 303\n",
      "0 2 4 6 8-5 0 5\n",
      "0 1 2 3-5 0 5\n",
      "0 2 4 6-5 0 5 10\n",
      "0 2 4 6 8 10-5 0 5 10\n",
      "0 2 4 6 8 10-5 0 5 10\n",
      "0 2 4 6-5 0 5 10\n",
      "0 5 10 15 20-10 -5 0\n",
      "0 5 10-10 -5 0\n",
      "0 10 20 30-10 -5 0 5\n",
      "0 2 4 6-5 0 5\n",
      "0 5 10 15 20-10 -5 0 5\n",
      "0 5 10 15-10 -5 0\n",
      "0 10 20 30-5 0 5 10\n",
      "0 1 2 3 4 5 6-5 0 5 10\n",
      "0 2000 6000 10000-5 0 5\n",
      "0 5000 10000 15000-5 0 5our over remove internet\n",
      "free business hp hpl\n",
      "george 1999 re edu\n",
      "ch! ch$ CAPMAX CAPTOTˆf(our)\n",
      "ˆf(over)\n",
      "ˆf(remove )\n",
      "ˆf(internet )ˆf(free)\n",
      "ˆf(business )\n",
      "ˆf(hp)\n",
      "ˆf(hpl)ˆf(george )\n",
      "ˆf(1999)\n",
      "ˆf(re)\n",
      "ˆf(edu)ˆf(ch!)\n",
      "ˆf(ch$)\n",
      "ˆf(CAPMAX )\n",
      "ˆf(CAPTOT )\n",
      "FIGURE 9.1. Spam analysis: estimated functions for signiﬁcant predictors. The\n",
      "rug plot along the bottom of each frame indicates the observed values of the cor-\n",
      "responding predictor. For many of the predictors the nonlinearity picks up the\n",
      "discontinuity at zero.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "304 9. Additive Models, Trees, and Related Methods\n",
      "It is more serious to classify a genuine email message as spam, since then\n",
      "a good email would be ﬁltered out and would not reach the user. We can\n",
      "alter the balance between the class error rates by changing the losses (see\n",
      "Section 2.4). If we assign a loss L01for predicting a true class 0 as class 1,\n",
      "andL10for predicting a true class 1 as class 0, then the estimated Bayes\n",
      "rule predicts class 1 if its probability is greater than L01/(L01+L10). For\n",
      "example, if we take L01= 10,L10= 1 then the (true) class 0 and class 1\n",
      "error rates change to 0.8% and 8.7%.\n",
      "More ambitiously, we can encourage the model to ﬁt better data in the\n",
      "class 0 by using weights L01for the class 0 observations and L10for the\n",
      "class 1 observations. As above, we then use the estimated Bayes rule to\n",
      "predict. This gave error rates of 1.2% and 8.0% in (true) class 0 and class 1,\n",
      "respectively. We discuss below the issue of unequal losses further, in the\n",
      "context of tree-based models.\n",
      "After ﬁtting an additive model, one should check whether the inclusion\n",
      "of some interactions can signiﬁcantly improve the ﬁt. This can be done\n",
      "“manually,” by inserting products of some or all of the signiﬁcant inputs,\n",
      "or automatically via the MARS procedure (Section 9.4).\n",
      "This example uses the additive model in an automatic fashion. As a data\n",
      "analysis tool, additive models are often used in a more interactive fashi on,\n",
      "adding and dropping terms to determine their eﬀect. By calibrating the\n",
      "amount of smoothing in terms of df j, one can move seamlessly between\n",
      "linear models (df j= 1) and partially linear models, where some terms are\n",
      "modeled more ﬂexibly. See Hastie and Tibshirani (1990) for more details.\n",
      "9.1.3 Summary\n",
      "Additive models provide a useful extension of linear models, making them\n",
      "more ﬂexible while still retaining much of their interpretability. The famil iar\n",
      "tools for modeling and inference in linear models are also available for\n",
      "additive models, seen for example in Table 9.2. The backﬁtting procedure\n",
      "for ﬁtting these models is simple and modular, allowing one to choose a\n",
      "ﬁtting method appropriate for each input variable. As a result they have\n",
      "become widely used in the statistical community.\n",
      "However additive models can have limitations for large data-mining ap-\n",
      "plications. The backﬁtting algorithm ﬁts all predictors, which is not feasi-\n",
      "ble or desirable when a large number are available. The BRUTO procedure\n",
      "(Hastie and Tibshirani, 1990, Chapter 9) combines backﬁtting with selec-\n",
      "tion of inputs, but is not designed for large data-mining problems. There\n",
      "has also been recent work using lasso-type penalties to estimate sparse ad-\n",
      "ditive models, for example the COSSO procedure of Lin and Zhang (2006)\n",
      "and the SpAM proposal of Ravikumar et al. (2008). For large problems a\n",
      "forward stagewise approach such as boosting (Chapter 10) is more eﬀectiv e,\n",
      "and also allows for interactions to be included in the model.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.2 Tree-Based Methods 305\n",
      "9.2 Tree-Based Methods\n",
      "9.2.1 Background\n",
      "Tree-based methods partition the feature space into a set of rectangles, and\n",
      "then ﬁt a simple model (like a constant) in each one. They are conceptually\n",
      "simple yet powerful. We ﬁrst describe a popular method for tree-based\n",
      "regression and classiﬁcation called CART, and later contrast it with C4. 5,\n",
      "a major competitor.\n",
      "Let’s consider a regression problem with continuous response Yand in-\n",
      "putsX1andX2, each taking values in the unit interval. The top left panel\n",
      "of Figure 9.2 shows a partition of the feature space by lines that are parall el\n",
      "to the coordinate axes. In each partition element we can model Ywith a\n",
      "diﬀerent constant. However, there is a problem: although each partitioning\n",
      "line has a simple description like X1=c, some of the resulting regions are\n",
      "complicated to describe.\n",
      "To simplify matters, we restrict attention to recursive binary partitio ns\n",
      "like that in the top right panel of Figure 9.2. We ﬁrst split the space into\n",
      "two regions, and model the response by the mean of Yin each region.\n",
      "We choose the variable and split-point to achieve the best ﬁt. Then one\n",
      "or both of these regions are split into two more regions, and this process\n",
      "is continued, until some stopping rule is applied. For example, in the top\n",
      "right panel of Figure 9.2, we ﬁrst split at X1=t1. Then the region X1≤t1\n",
      "is split at X2=t2and the region X1> t1is split at X1=t3. Finally, the\n",
      "region X1> t3is split at X2=t4. The result of this process is a partition\n",
      "into the ﬁve regions R1,R2,... ,R 5shown in the ﬁgure. The corresponding\n",
      "regression model predicts Ywith a constant cmin region Rm, that is,\n",
      "ˆf(X) =5∑\n",
      "m=1cmI{(X1,X2)∈Rm}. (9.9)\n",
      "This same model can be represented by the binary tree in the bottom left\n",
      "panel of Figure 9.2. The full dataset sits at the top of the tree. Observations\n",
      "satisfying the condition at each junction are assigned to the left branch,\n",
      "and the others to the right branch. The terminal nodes or leaves of the\n",
      "tree correspond to the regions R1,R2,... ,R 5. The bottom right panel of\n",
      "Figure 9.2 is a perspective plot of the regression surface from this model.\n",
      "For illustration, we chose the node means c1=−5,c2=−7,c3= 0,c4=\n",
      "2,c5= 4 to make this plot.\n",
      "A key advantage of the recursive binary tree is its interpretability. The\n",
      "feature space partition is fully described by a single tree. With more than\n",
      "two inputs, partitions like that in the top right panel of Figure 9.2 are\n",
      "diﬃcult to draw, but the binary tree representation works in the same\n",
      "way. This representation is also popular among medical scientists, perhaps\n",
      "because it mimics the way that a doctor thinks. The tree stratiﬁes the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "306 9. Additive Models, Trees, and Related Methods\n",
      "|t1t2\n",
      "t3t4\n",
      "R1R1\n",
      "R2R2\n",
      "R3R3\n",
      "R4R4\n",
      "R5R5\n",
      "X1X1 X1\n",
      "X2X2X2\n",
      "X1≤t1\n",
      "X2≤t2 X1≤t3\n",
      "X2≤t4\n",
      "FIGURE 9.2. Partitions and CART. Top right panel shows a partition of a\n",
      "two-dimensional feature space by recursive binary splitting, a s used in CART,\n",
      "applied to some fake data. Top left panel shows a general partit ion that cannot\n",
      "be obtained from recursive binary splitting. Bottom left panel s hows the tree cor-\n",
      "responding to the partition in the top right panel, and a perspect ive plot of the\n",
      "prediction surface appears in the bottom right panel.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.2 Tree-Based Methods 307\n",
      "population into strata of high and low outcome, on the basis of patient\n",
      "characteristics.\n",
      "9.2.2 Regression Trees\n",
      "We now turn to the question of how to grow a regression tree. Our data\n",
      "consists of pinputs and a response, for each of Nobservations: that is,\n",
      "(xi,yi) for i= 1,2,... ,N , with xi= (xi1,xi2,... ,x ip). The algorithm\n",
      "needs to automatically decide on the splitting variables and split points,\n",
      "and also what topology (shape) the tree should have. Suppose ﬁrst that we\n",
      "have a partition into Mregions R1,R2,... ,R M, and we model the response\n",
      "as a constant cmin each region:\n",
      "f(x) =M∑\n",
      "m=1cmI(x∈Rm). (9.10)\n",
      "If we adopt as our criterion minimization of the sum of squares∑(yi−\n",
      "f(xi))2, it is easy to see that the best ˆ cmis just the average of yiin region\n",
      "Rm:\n",
      "ˆcm= ave( yi|xi∈Rm). (9.11)\n",
      "Now ﬁnding the best binary partition in terms of minimum sum of squares\n",
      "is generally computationally infeasible. Hence we proceed with a greedy\n",
      "algorithm. Starting with all of the data, consider a splitting variable jand\n",
      "split point s, and deﬁne the pair of half-planes\n",
      "R1(j,s) ={X|Xj≤s}andR2(j,s) ={X|Xj> s}. (9.12)\n",
      "Then we seek the splitting variable jand split point sthat solve\n",
      "min\n",
      "j, s[\n",
      "min\n",
      "c1∑\n",
      "xi∈R1(j,s)(yi−c1)2+ min\n",
      "c2∑\n",
      "xi∈R2(j,s)(yi−c2)2]\n",
      ". (9.13)\n",
      "For any choice jands, the inner minimization is solved by\n",
      "ˆc1= ave( yi|xi∈R1(j,s)) and ˆ c2= ave( yi|xi∈R2(j,s)). (9.14)\n",
      "For each splitting variable, the determination of the split point scan\n",
      "be done very quickly and hence by scanning through all of the inputs,\n",
      "determination of the best pair ( j,s) is feasible.\n",
      "Having found the best split, we partition the data into the two resulting\n",
      "regions and repeat the splitting process on each of the two regions. Then\n",
      "this process is repeated on all of the resulting regions.\n",
      "How large should we grow the tree? Clearly a very large tree might overﬁt\n",
      "the data, while a small tree might not capture the important structure.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "308 9. Additive Models, Trees, and Related Methods\n",
      "Tree size is a tuning parameter governing the model’s complexity, and the\n",
      "optimal tree size should be adaptively chosen from the data. One approach\n",
      "would be to split tree nodes only if the decrease in sum-of-squares due to the\n",
      "split exceeds some threshold. This strategy is too short-sighted, however,\n",
      "since a seemingly worthless split might lead to a very good split below it.\n",
      "The preferred strategy is to grow a large tree T0, stopping the splitting\n",
      "process only when some minimum node size (say 5) is reached. Then this\n",
      "large tree is pruned using cost-complexity pruning , which we now describe.\n",
      "We deﬁne a subtree T⊂T0to be any tree that can be obtained by\n",
      "pruning T0, that is, collapsing any number of its internal (non-terminal)\n",
      "nodes. We index terminal nodes by m, with node mrepresenting region\n",
      "Rm. Let|T|denote the number of terminal nodes in T. Letting\n",
      "Nm= #{xi∈Rm},\n",
      "ˆcm=1\n",
      "Nm∑\n",
      "xi∈Rmyi,\n",
      "Qm(T) =1\n",
      "Nm∑\n",
      "xi∈Rm(yi−ˆcm)2,(9.15)\n",
      "we deﬁne the cost complexity criterion\n",
      "Cα(T) =|T|∑\n",
      "m=1NmQm(T) +α|T|. (9.16)\n",
      "The idea is to ﬁnd, for each α, the subtree Tα⊆T0to minimize Cα(T).\n",
      "The tuning parameter α≥0 governs the tradeoﬀ between tree size and its\n",
      "goodness of ﬁt to the data. Large values of αresult in smaller trees Tα, and\n",
      "conversely for smaller values of α. As the notation suggests, with α= 0 the\n",
      "solution is the full tree T0. We discuss how to adaptively choose αbelow.\n",
      "For each αone can show that there is a unique smallest subtree Tαthat\n",
      "minimizes Cα(T). To ﬁnd Tαwe use weakest link pruning : we successively\n",
      "collapse the internal node that produces the smallest per-node increase in∑\n",
      "mNmQm(T), and continue until we produce the single-node (root) tree.\n",
      "This gives a (ﬁnite) sequence of subtrees, and one can show this sequence\n",
      "must contain Tα. See Breiman et al. (1984) or Ripley (1996) for details.\n",
      "Estimation of αis achieved by ﬁve- or tenfold cross-validation: we choose\n",
      "the value ˆ αto minimize the cross-validated sum of squares. Our ﬁnal tree\n",
      "isTˆα.\n",
      "9.2.3 Classiﬁcation Trees\n",
      "If the target is a classiﬁcation outcome taking values 1 ,2,... ,K , the only\n",
      "changes needed in the tree algorithm pertain to the criteria for splitting\n",
      "nodes and pruning the tree. For regression we used the squared-error node\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.2 Tree-Based Methods 309\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5\n",
      "pEntropy\n",
      "Gini index\n",
      "Misclassification error\n",
      "FIGURE 9.3. Node impurity measures for two-class classiﬁcation, as a funct ion\n",
      "of the proportion pin class 2. Cross-entropy has been scaled to pass through\n",
      "(0.5,0.5).\n",
      "impurity measure Qm(T) deﬁned in (9.15), but this is not suitable for\n",
      "classiﬁcation. In a node m, representing a region RmwithNmobservations,\n",
      "let\n",
      "ˆpmk=1\n",
      "Nm∑\n",
      "xi∈RmI(yi=k),\n",
      "the proportion of class kobservations in node m. We classify the obser-\n",
      "vations in node mto class k(m) = arg max kˆpmk, the majority class in\n",
      "nodem. Diﬀerent measures Qm(T) of node impurity include the following:\n",
      "Misclassiﬁcation error:1\n",
      "Nm∑\n",
      "i∈RmI(yi̸=k(m)) = 1 −ˆpmk(m).\n",
      "Gini index:∑\n",
      "k̸=k′ˆpmkˆpmk′=∑K\n",
      "k=1ˆpmk(1−ˆpmk).\n",
      "Cross-entropy or deviance: −∑K\n",
      "k=1ˆpmklog ˆpmk.\n",
      "(9.17)\n",
      "For two classes, if pis the proportion in the second class, these three mea-\n",
      "sures are 1 −max(p,1−p), 2p(1−p) and −plogp−(1−p)log (1 −p),\n",
      "respectively. They are shown in Figure 9.3. All three are similar, but cross -\n",
      "entropy and the Gini index are diﬀerentiable, and hence more amenable to\n",
      "numerical optimization. Comparing (9.13) and (9.15), we see that we need\n",
      "to weight the node impurity measures by the number NmLandNmRof\n",
      "observations in the two child nodes created by splitting node m.\n",
      "In addition, cross-entropy and the Gini index are more sensitive to changes\n",
      "in the node probabilities than the misclassiﬁcation rate. For example, in\n",
      "a two-class problem with 400 observations in each class (denote this by\n",
      "(400,400)), suppose one split created nodes (300 ,100) and (100 ,300), while\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "310 9. Additive Models, Trees, and Related Methods\n",
      "the other created nodes (200 ,400) and (200 ,0). Both splits produce a mis-\n",
      "classiﬁcation rate of 0.25, but the second split produces a pure node and is\n",
      "probably preferable. Both the Gini index and cross-entropy are lower for the\n",
      "second split. For this reason, either the Gini index or cross-entropy should\n",
      "be used when growing the tree. To guide cost-complexity pruning, any of\n",
      "the three measures can be used, but typically it is the misclassiﬁcation rate.\n",
      "The Gini index can be interpreted in two interesting ways. Rather than\n",
      "classify observations to the majority class in the node, we could classify\n",
      "them to class kwith probability ˆ pmk. Then the training error rate of this\n",
      "rule in the node is∑\n",
      "k̸=k′ˆpmkˆpmk′—the Gini index. Similarly, if we code\n",
      "each observation as 1 for class kand zero otherwise, the variance over the\n",
      "node of this 0-1 response is ˆ pmk(1−ˆpmk). Summing over classes kagain\n",
      "gives the Gini index.\n",
      "9.2.4 Other Issues\n",
      "Categorical Predictors\n",
      "When splitting a predictor having qpossible unordered values, there are\n",
      "2q−1−1 possible partitions of the qvalues into two groups, and the com-\n",
      "putations become prohibitive for large q. However, with a 0 −1 outcome,\n",
      "this computation simpliﬁes. We order the predictor classes according to the\n",
      "proportion falling in outcome class 1. Then we split this predictor as if it\n",
      "were an ordered predictor. One can show this gives the optimal split, in\n",
      "terms of cross-entropy or Gini index, among all possible 2q−1−1 splits. This\n",
      "result also holds for a quantitative outcome and square error loss—the cat-\n",
      "egories are ordered by increasing mean of the outcome. Although intuitive,\n",
      "the proofs of these assertions are not trivial. The proof for binary outcomes\n",
      "is given in Breiman et al. (1984) and Ripley (1996); the proof for quanti ta-\n",
      "tive outcomes can be found in Fisher (1958). For multicategory outcomes,\n",
      "no such simpliﬁcations are possible, although various approximations have\n",
      "been proposed (Loh and Vanichsetakul, 1988).\n",
      "The partitioning algorithm tends to favor categorical predictors with\n",
      "many levels q; the number of partitions grows exponentially in q, and the\n",
      "more choices we have, the more likely we can ﬁnd a good one for the data\n",
      "at hand. This can lead to severe overﬁtting if qis large, and such variables\n",
      "should be avoided.\n",
      "The Loss Matrix\n",
      "In classiﬁcation problems, the consequences of misclassifying observations\n",
      "are more serious in some classes than others. For example, it is probably\n",
      "worse to predict that a person will not have a heart attack when he/she\n",
      "actually will, than vice versa. To account for this, we deﬁne a K×Kloss\n",
      "matrix L, with Lkk′being the loss incurred for classifying a class kobser-\n",
      "vation as class k′. Typically no loss is incurred for correct classiﬁcations,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.2 Tree-Based Methods 311\n",
      "that is, Lkk= 0∀k. To incorporate the losses into the modeling process,\n",
      "we could modify the Gini index to∑\n",
      "k̸=k′Lkk′ˆpmkˆpmk′; this would be the\n",
      "expected loss incurred by the randomized rule. This works for the multi-\n",
      "class case, but in the two-class case has no eﬀect, since the coeﬃcient of\n",
      "ˆpmkˆpmk′isLkk′+Lk′k. For two classes a better approach is to weight the\n",
      "observations in class kbyLkk′. This can be used in the multiclass case only\n",
      "if, as a function of k,Lkk′doesn’t depend on k′. Observation weighting can\n",
      "be used with the deviance as well. The eﬀect of observation weighting is to\n",
      "alter the prior probability on the classes. In a terminal node, the empirical\n",
      "Bayes rule implies that we classify to class k(m) = arg min k∑\n",
      "ℓLℓkˆpmℓ.\n",
      "Missing Predictor Values\n",
      "Suppose our data has some missing predictor values in some or all of the\n",
      "variables. We might discard any observation with some missing values, but\n",
      "this could lead to serious depletion of the training set. Alternatively we\n",
      "might try to ﬁll in (impute) the missing values, with say the mean of that\n",
      "predictor over the nonmissing observations. For tree-based models, there\n",
      "are two better approaches. The ﬁrst is applicable to categorical predictors:\n",
      "we simply make a new category for “missing.” From this we might dis-\n",
      "cover that observations with missing values for some measurement behave\n",
      "diﬀerently than those with nonmissing values. The second more general\n",
      "approach is the construction of surrogate variables. When considering a\n",
      "predictor for a split, we use only the observations for which that predictor\n",
      "is not missing. Having chosen the best (primary) predictor and split point,\n",
      "we form a list of surrogate predictors and split points. The ﬁrst surroga te\n",
      "is the predictor and corresponding split point that best mimics the split of\n",
      "the training data achieved by the primary split. The second surrogate is\n",
      "the predictor and corresponding split point that does second best, and so\n",
      "on. When sending observations down the tree either in the training phase\n",
      "or during prediction, we use the surrogate splits in order, if the primary\n",
      "splitting predictor is missing. Surrogate splits exploit correlations between\n",
      "predictors to try and alleviate the eﬀect of missing data. The higher the cor-\n",
      "relation between the missing predictor and the other predictors, the smaller\n",
      "the loss of information due to the missing value. The general problem of\n",
      "missing data is discussed in Section 9.6.\n",
      "Why Binary Splits?\n",
      "Rather than splitting each node into just two groups at each stage (as\n",
      "above), we might consider multiway splits into more than two groups. Whil e\n",
      "this can sometimes be useful, it is not a good general strategy. The problem\n",
      "is that multiway splits fragment the data too quickly, leaving insuﬃcient\n",
      "data at the next level down. Hence we would want to use such splits only\n",
      "when needed. Since multiway splits can be achieved by a series of binary\n",
      "splits, the latter are preferred.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "312 9. Additive Models, Trees, and Related Methods\n",
      "Other Tree-Building Procedures\n",
      "The discussion above focuses on the CART (classiﬁcation and regression\n",
      "tree) implementation of trees. The other popular methodology is ID3 and\n",
      "its later versions, C4.5 and C5.0 (Quinlan, 1993). Early versions of the\n",
      "program were limited to categorical predictors, and used a top-down rule\n",
      "with no pruning. With more recent developments, C5.0 has become quite\n",
      "similar to CART. The most signiﬁcant feature unique to C5.0 is a scheme\n",
      "for deriving rule sets. After a tree is grown, the splitting rules that deﬁne the\n",
      "terminal nodes can sometimes be simpliﬁed: that is, one or more condition\n",
      "can be dropped without changing the subset of observations that fall in\n",
      "the node. We end up with a simpliﬁed set of rules deﬁning each terminal\n",
      "node; these no longer follow a tree structure, but their simplicity might\n",
      "make them more attractive to the user.\n",
      "Linear Combination Splits\n",
      "Rather than restricting splits to be of the form Xj≤s, one can allow splits\n",
      "along linear combinations of the form∑ajXj≤s. The weights ajand\n",
      "split point sare optimized to minimize the relevant criterion (such as the\n",
      "Gini index). While this can improve the predictive power of the tree, it can\n",
      "hurt interpretability. Computationally, the discreteness of the split point\n",
      "search precludes the use of a smooth optimization for the weights. A better\n",
      "way to incorporate linear combination splits is in the hierarchical mixtures\n",
      "of experts (HME) model, the topic of Section 9.5.\n",
      "Instability of Trees\n",
      "One major problem with trees is their high variance. Often a small change\n",
      "in the data can result in a very diﬀerent series of splits, making interpre-\n",
      "tation somewhat precarious. The major reason for this instability is the\n",
      "hierarchical nature of the process: the eﬀect of an error in the top split\n",
      "is propagated down to all of the splits below it. One can alleviate this to\n",
      "some degree by trying to use a more stable split criterion, but the inherent\n",
      "instability is not removed. It is the price to be paid for estimating a simple,\n",
      "tree-based structure from the data. Bagging (Section 8.7) averages many\n",
      "trees to reduce this variance.\n",
      "Lack of Smoothness\n",
      "Another limitation of trees is the lack of smoothness of the prediction sur-\n",
      "face, as can be seen in the bottom right panel of Figure 9.2. In classiﬁcation\n",
      "with 0/1 loss, this doesn’t hurt much, since bias in estimation of the class\n",
      "probabilities has a limited eﬀect. However, this can degrade performance\n",
      "in the regression setting, where we would normally expect the underlying\n",
      "function to be smooth. The MARS procedure, described in Section 9.4,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.2 Tree-Based Methods 313\n",
      "TABLE 9.3. Spam data: confusion rates for the 17-node tree (chosen by cross–\n",
      "validation) on the test data. Overall error rate is 9.3%.\n",
      "Predicted\n",
      "Trueemail spam\n",
      "email 57.3% 4.0%\n",
      "spam 5.3% 33.4%\n",
      "can be viewed as a modiﬁcation of CART designed to alleviate this lack of\n",
      "smoothness.\n",
      "Diﬃculty in Capturing Additive Structure\n",
      "Another problem with trees is their diﬃculty in modeling additive struc-\n",
      "ture. In regression, suppose, for example, that Y=c1I(X1< t1)+c2I(X2<\n",
      "t2) +εwhere εis zero-mean noise. Then a binary tree might make its ﬁrst\n",
      "split on X1neart1. At the next level down it would have to split both nodes\n",
      "onX2att2in order to capture the additive structure. This might happen\n",
      "with suﬃcient data, but the model is given no special encouragement to ﬁnd\n",
      "such structure. If there were ten rather than two additive eﬀects, it would\n",
      "take many fortuitous splits to recreate the structure, and the data analyst\n",
      "would be hard pressed to recognize it in the estimated tree. The “blame”\n",
      "here can again be attributed to the binary tree structure, which has both\n",
      "advantages and drawbacks. Again the MARS method (Section 9.4) gives\n",
      "up this tree structure in order to capture additive structure.\n",
      "9.2.5 Spam Example (Continued)\n",
      "We applied the classiﬁcation tree methodology to the spamexample intro-\n",
      "duced earlier. We used the deviance measure to grow the tree and mis-\n",
      "classiﬁcation rate to prune it. Figure 9.4 shows the 10-fold cross-validat ion\n",
      "error rate as a function of the size of the pruned tree, along with ±2 stan-\n",
      "dard errors of the mean, from the ten replications. The test error curve is\n",
      "shown in orange. Note that the cross-validation error rates are indexed by\n",
      "a sequence of values of αandnottree size; for trees grown in diﬀerent folds,\n",
      "a value of αmight imply diﬀerent sizes. The sizes shown at the base of the\n",
      "plot refer to |Tα|, the sizes of the pruned original tree.\n",
      "The error ﬂattens out at around 17 terminal nodes, giving the pruned tree\n",
      "in Figure 9.5. Of the 13 distinct features chosen by the tree, 11 overlap with\n",
      "the 16 signiﬁcant features in the additive model (Table 9.2). The overall\n",
      "error rate shown in Table 9.3 is about 50% higher than for the additive\n",
      "model in Table 9.1.\n",
      "Consider the rightmost branches of the tree. We branch to the right\n",
      "with aspamwarning if more than 5.5% of the characters are the $ sign.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "314 9. Additive Models, Trees, and Related Methods\n",
      "0 10 20 30 400.0 0.1 0.2 0.3 0.4\n",
      "Tree SizeMisclassification Rate176 21 7 5 3 2 0α\n",
      "FIGURE 9.4. Results for spamexample. The blue curve is the 10-fold cross-val-\n",
      "idation estimate of misclassiﬁcation rate as a function of tre e size, with standard\n",
      "error bars. The minimum occurs at a tree size with about 17terminal nodes (using\n",
      "the “one-standard-error” rule). The orange curve is the test er ror, which tracks\n",
      "the CV error quite closely. The cross-validation is indexed by values of α, shown\n",
      "above. The tree sizes shown below refer to |Tα|, the size of the original tree indexed\n",
      "byα.\n",
      "However, if in addition the phrase hpoccurs frequently, then this is likely\n",
      "to be company business and we classify as email. All of the 22 cases in\n",
      "the test set satisfying these criteria were correctly classiﬁed. If the second\n",
      "condition is not met, and in addition the average length of repeated capital\n",
      "lettersCAPAVE is larger than 2.9, then we classify as spam. Of the 227 test\n",
      "cases, only seven were misclassiﬁed.\n",
      "In medical classiﬁcation problems, the terms sensitivity andspeciﬁcity\n",
      "are used to characterize a rule. They are deﬁned as follows:\n",
      "Sensitivity: probability of predicting disease given true state is disease.\n",
      "Speciﬁcity: probability of predicting non-disease given true state is non-\n",
      "disease.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.2 Tree-Based Methods 315\n",
      "600/1536\n",
      "280/1177\n",
      "180/1065\n",
      " 80/861\n",
      " 80/652\n",
      " 77/423\n",
      " 20/238\n",
      " 19/236   1/2 57/185\n",
      " 48/113\n",
      " 37/101   1/12  9/72  3/229  0/209100/204\n",
      " 36/123\n",
      " 16/94\n",
      " 14/89   3/5  9/29 16/81  9/112\n",
      "  6/109   0/3 48/359\n",
      " 26/337\n",
      " 19/110\n",
      " 18/109   0/1  7/227  0/22\n",
      "spam\n",
      "spamspamspamspam\n",
      "spamspam\n",
      "spam\n",
      "spam\n",
      "spam\n",
      "spamspamemail\n",
      "emailemail\n",
      "emailemailemailemail\n",
      "email\n",
      "email\n",
      "email\n",
      "emailemailemail\n",
      "emailemailemailemailemailemailemailemail\n",
      "ch$<0.0555\n",
      "remove<0.06\n",
      "ch!<0.191\n",
      "george<0.005\n",
      "hp<0.03\n",
      "CAPMAX<10.5\n",
      "receive<0.125 edu<0.045\n",
      "our<1.2CAPAVE<2.7505\n",
      "free<0.065\n",
      "business<0.145george<0.15hp<0.405\n",
      "CAPAVE<2.907\n",
      "1999<0.58ch$>0.0555\n",
      "remove>0.06\n",
      "ch!>0.191\n",
      "george>0.005\n",
      "hp>0.03\n",
      "CAPMAX>10.5\n",
      "receive>0.125 edu>0.045\n",
      "our>1.2CAPAVE>2.7505\n",
      "free>0.065\n",
      "business>0.145george>0.15hp>0.405\n",
      "CAPAVE>2.907\n",
      "1999>0.58\n",
      "FIGURE 9.5. The pruned tree for the spamexample. The split variables are\n",
      "shown in blue on the branches, and the classiﬁcation is shown in e very node.The\n",
      "numbers under the terminal nodes indicate misclassiﬁcation rates on the test data.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "316 9. Additive Models, Trees, and Related Methods\n",
      "SpecificitySensitivity\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0• •\n",
      "•• •\n",
      "•\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "•••• ••••••••••••••• ••••• ••••••••• •••••••••••••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•Tree (0.95)\n",
      "GAM (0.98)\n",
      "Weighted Tree (0.90)\n",
      "FIGURE 9.6. ROC curves for the classiﬁcation rules ﬁt to the spamdata. Curves\n",
      "that are closer to the northeast corner represent better classi ﬁers. In this case the\n",
      "GAM classiﬁer dominates the trees. The weighted tree achieves better sensitivity\n",
      "for higher speciﬁcity than the unweighted tree. The numbers in t he legend repre-\n",
      "sent the area under the curve.\n",
      "If we think of spamandemail as the presence and absence of disease, re-\n",
      "spectively, then from Table 9.3 we have\n",
      "Sensitivity = 100 ×33.4\n",
      "33.4 + 5.3= 86.3%,\n",
      "Speciﬁcity = 100 ×57.3\n",
      "57.3 + 4.0= 93.4%.\n",
      "In this analysis we have used equal losses. As before let Lkk′be the\n",
      "loss associated with predicting a class kobject as class k′. By varying the\n",
      "relative sizes of the losses L01andL10, we increase the sensitivity and\n",
      "decrease the speciﬁcity of the rule, or vice versa. In this example, we want\n",
      "to avoid marking good email asspam, and thus we want the speciﬁcity to\n",
      "be very high. We can achieve this by setting L01>1 say, with L10= 1.\n",
      "The Bayes’ rule in each terminal node classiﬁes to class 1 ( spam) if the\n",
      "proportion of spamis≥L01/(L10+L01), and class zero otherwise. The\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.3 PRIM: Bump Hunting 317\n",
      "receiver operating characteristic curve (ROC) is a commonly used summary\n",
      "for assessing the tradeoﬀ between sensitivity and speciﬁcity. It is a plot of\n",
      "the sensitivity versus speciﬁcity as we vary the parameters of a classiﬁcation\n",
      "rule. Varying the loss L01between 0.1 and 10, and applying Bayes’ rule to\n",
      "the 17-node tree selected in Figure 9.4, produced the ROC curve shown\n",
      "in Figure 9.6. The standard error of each curve near 0.9 is approximately√\n",
      "0.9(1−0.9)/1536 = 0 .008, and hence the standard error of the diﬀerence\n",
      "is about 0 .01. We see that in order to achieve a speciﬁcity of close to 100%,\n",
      "the sensitivity has to drop to about 50%. The area under the curve is a\n",
      "commonly used quantitative summary; extending the curve linearly in each\n",
      "direction so that it is deﬁned over [0 ,100], the area is approximately 0 .95.\n",
      "For comparison, we have included the ROC curve for the GAM model ﬁt\n",
      "to these data in Section 9.2; it gives a better classiﬁcation rule for any los s,\n",
      "with an area of 0 .98.\n",
      "Rather than just modifying the Bayes rule in the nodes, it is better to\n",
      "take full account of the unequal losses in growing the tree, as was done\n",
      "in Section 9.2. With just two classes 0 and 1, losses may be incorporated\n",
      "into the tree-growing process by using weight Lk,1−kfor an observation in\n",
      "classk. Here we chose L01= 5,L10= 1 and ﬁt the same size tree as before\n",
      "(|Tα|= 17). This tree has higher sensitivity at high values of the speciﬁcity\n",
      "than the original tree, but does more poorly at the other extreme. Its top\n",
      "few splits are the same as the original tree, and then it departs from it.\n",
      "For this application the tree grown using L01= 5 is clearly better than the\n",
      "original tree.\n",
      "The area under the ROC curve, used above, is sometimes called the c-\n",
      "statistic . Interestingly, it can be shown that the area under the ROC curve\n",
      "is equivalent to the Mann-Whitney U statistic (or Wilcoxon rank-sum test),\n",
      "for the median diﬀerence between the prediction scores in the two groups\n",
      "(Hanley and McNeil, 1982). For evaluating the contribution of an additional\n",
      "predictor when added to a standard model, the c-statistic may not be an\n",
      "informative measure. The new predictor can be very signiﬁcant in terms\n",
      "of the change in model deviance, but show only a small increase in the c-\n",
      "statistic. For example, removal of the highly signiﬁcant term george from\n",
      "the model of Table 9.2 results in a decrease in the c-statistic of less than\n",
      "0.01. Instead, it is useful to examine how the additional predictor changes\n",
      "the classiﬁcation on an individual sample basis. A good discussion of this\n",
      "point appears in Cook (2007).\n",
      "9.3 PRIM: Bump Hunting\n",
      "Tree-based methods (for regression) partition the feature space into box-\n",
      "shaped regions, to try to make the response averages in each box as diﬀer-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "318 9. Additive Models, Trees, and Related Methods\n",
      "ent as possible. The splitting rules deﬁning the boxes are related to each\n",
      "through a binary tree, facilitating their interpretation.\n",
      "The patient rule induction method (PRIM) also ﬁnds boxes in the feature\n",
      "space, but seeks boxes in which the response average is high. Hence it looks\n",
      "for maxima in the target function, an exercise known as bump hunting . (If\n",
      "minima rather than maxima are desired, one simply works with the negative\n",
      "response values.)\n",
      "PRIM also diﬀers from tree-based partitioning methods in that the box\n",
      "deﬁnitions are not described by a binary tree. This makes interpretation of\n",
      "the collection of rules more diﬃcult; however, by removing the binary tree\n",
      "constraint, the individual rules are often simpler.\n",
      "The main box construction method in PRIM works from the top down,\n",
      "starting with a box containing all of the data. The box is compressed along\n",
      "one face by a small amount, and the observations then falling outside the\n",
      "box are peeled oﬀ. The face chosen for compression is the one resulting in\n",
      "the largest box mean, after the compression is performed. Then the process\n",
      "is repeated, stopping when the current box contains some minimum number\n",
      "of data points.\n",
      "This process is illustrated in Figure 9.7. There are 200 data points uni-\n",
      "formly distributed over the unit square. The color-coded plot indicates the\n",
      "response Ytaking the value 1 (red) when 0 .5< X1<0.8 and 0 .4< X2<\n",
      "0.6. and zero (blue) otherwise. The panels shows the successive boxes found\n",
      "by the top-down peeling procedure, peeling oﬀ a proportion α= 0.1 of the\n",
      "remaining data points at each stage.\n",
      "Figure 9.8 shows the mean of the response values in the box, as the box\n",
      "is compressed.\n",
      "After the top-down sequence is computed, PRIM reverses the process,\n",
      "expanding along any edge, if such an expansion increases the box mean.\n",
      "This is called pasting . Since the top-down procedure is greedy at each step,\n",
      "such an expansion is often possible.\n",
      "The result of these steps is a sequence of boxes, with diﬀerent numbers\n",
      "of observation in each box. Cross-validation, combined with the judgment\n",
      "of the data analyst, is used to choose the optimal box size.\n",
      "Denote by B1the indices of the observations in the box found in step 1.\n",
      "The PRIM procedure then removes the observations in B1from the training\n",
      "set, and the two-step process—top down peeling, followed by bottom-up\n",
      "pasting—is repeated on the remaining dataset. This entire process is re-\n",
      "peated several times, producing a sequence of boxes B1,B2,... ,B k. Each\n",
      "box is deﬁned by a set of rules involving a subset of predictors like\n",
      "(a1≤X1≤b1) and ( b1≤X3≤b2).\n",
      "A summary of the PRIM procedure is given Algorithm 9.3.\n",
      "PRIM can handle a categorical predictor by considering all partitions of\n",
      "the predictor, as in CART. Missing values are also handled in a manner\n",
      "similar to CART. PRIM is designed for regression (quantitative respo nse\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.3 PRIM: Bump Hunting 319\n",
      "1\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o2\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o3\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o4\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "5\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o6\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o7\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o8\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "12\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o17\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o22\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o27\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "o oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooo o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "FIGURE 9.7. Illustration of PRIM algorithm. There are two classes, indica ted\n",
      "by the blue (class 0) and red (class 1) points. The procedure starts with a rectangle\n",
      "(broken black lines) surrounding all of the data, and then peels a way points along\n",
      "one edge by a prespeciﬁed amount in order to maximize the mean of th e points\n",
      "remaining in the box. Starting at the top left panel, the sequence of p eelings is\n",
      "shown, until a pure red region is isolated in the bottom right pa nel. The iteration\n",
      "number is indicated at the top of each panel.\n",
      "Number of Observations in BoxBox Mean\n",
      "50 100 1500.2 0.4 0.6 0.8 1.0\n",
      "•••••••••••••••••••••••••••\n",
      "FIGURE 9.8. Box mean as a function of number of observations in the box.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "320 9. Additive Models, Trees, and Related Methods\n",
      "Algorithm 9.3 Patient Rule Induction Method.\n",
      "1. Start with all of the training data, and a maximal box containing all\n",
      "of the data.\n",
      "2. Consider shrinking the box by compressing one face, so as to peel oﬀ\n",
      "the proportion αof observations having either the highest values of\n",
      "a predictor Xj, or the lowest. Choose the peeling that produces the\n",
      "highest response mean in the remaining box. (Typically α= 0.05 or\n",
      "0.10.)\n",
      "3. Repeat step 2 until some minimal number of observations (say 10)\n",
      "remain in the box.\n",
      "4. Expand the box along any face, as long as the resulting box mean\n",
      "increases.\n",
      "5. Steps 1–4 give a sequence of boxes, with diﬀerent numbers of obser-\n",
      "vations in each box. Use cross-validation to choose a member of the\n",
      "sequence. Call the box B1.\n",
      "6. Remove the data in box B1from the dataset and repeat steps 2–5 to\n",
      "obtain a second box, and continue to get as many boxes as desired.\n",
      "variable); a two-class outcome can be handled simply by coding it as 0 and\n",
      "1. There is no simple way to deal with k >2 classes simultaneously: one\n",
      "approach is to run PRIM separately for each class versus a baseline class.\n",
      "An advantage of PRIM over CART is its patience. Because of its bi-\n",
      "nary splits, CART fragments the data quite quickly. Assuming splits of\n",
      "equal size, with Nobservations it can only make log2(N)−1 splits before\n",
      "running out of data. If PRIM peels oﬀ a proportion αof training points\n",
      "at each stage, it can perform approximately −log(N)/log(1−α) peeling\n",
      "steps before running out of data. For example, if N= 128 and α= 0.10,\n",
      "then log2(N)−1 = 6 while −log(N)/log(1−α)≈46. Taking into account\n",
      "that there must be an integer number of observations at each stage, PRIM\n",
      "in fact can peel only 29 times. In any case, the ability of PRIM to be more\n",
      "patient should help the top-down greedy algorithm ﬁnd a better solution.\n",
      "9.3.1 Spam Example (Continued)\n",
      "We applied PRIM to the spamdata, with the response coded as 1 for spam\n",
      "and 0 for email.\n",
      "The ﬁrst two boxes found by PRIM are summarized below:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines 321\n",
      "Rule 1 Global Mean Box Mean Box Support\n",
      "Training 0.3931 0.9607 0.1413\n",
      "Test 0.3958 1.0000 0.1536\n",
      "Rule 1\n",
      "\n",
      "ch!>0.029\n",
      "CAPAVE >2.331\n",
      "your >0.705\n",
      "1999 <0.040\n",
      "CAPTOT >79.50\n",
      "edu <0.070\n",
      "re<0.535\n",
      "ch;<0.030\n",
      "Rule 2 Remain Mean Box Mean Box Support\n",
      "Training 0.2998 0.9560 0.1043\n",
      "Test 0.2862 0.9264 0.1061\n",
      "Rule 2{\n",
      "remove >0.010\n",
      "george <0.110\n",
      "The box support is the proportion of observations falling in the box.\n",
      "The ﬁrst box is purely spam, and contains about 15% of the test data.\n",
      "The second box contains 10.6% of the test observations, 92.6% of which\n",
      "arespam. Together the two boxes contain 26% of the data and are about\n",
      "97%spam. The next few boxes (not shown) are quite small, containing only\n",
      "about 3% of the data.\n",
      "The predictors are listed in order of importance. Interestingly the top\n",
      "splitting variables in the CART tree (Figure 9.5) do not appear in PRIM’s\n",
      "ﬁrst box.\n",
      "9.4 MARS: Multivariate Adaptive Regression\n",
      "Splines\n",
      "MARS is an adaptive procedure for regression, and is well suited for high-\n",
      "dimensional problems (i.e., a large number of inputs). It can be viewed as a\n",
      "generalization of stepwise linear regression or a modiﬁcation of the CART\n",
      "method to improve the latter’s performance in the regression setting. We\n",
      "introduce MARS from the ﬁrst point of view, and later make the connection\n",
      "to CART.\n",
      "MARS uses expansions in piecewise linear basis functions of the form\n",
      "(x−t)+and (t−x)+. The “+” means positive part, so\n",
      "(x−t)+={\n",
      "x−t,ifx > t,\n",
      "0,otherwise,and ( t−x)+={\n",
      "t−x,ifx < t,\n",
      "0,otherwise .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "322 9. Additive Models, Trees, and Related Methods\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4 0.5(x−t)+ (t−x)+\n",
      "xtBasis Function\n",
      "FIGURE 9.9. The basis functions (x−t)+(solid orange) and (t−x)+(broken\n",
      "blue) used by MARS.\n",
      "As an example, the functions ( x−0.5)+and (0 .5−x)+are shown in Fig-\n",
      "ure 9.9.\n",
      "Each function is piecewise linear, with a knotat the value t. In the\n",
      "terminology of Chapter 5, these are linear splines. We call the two functions\n",
      "areﬂected pair in the discussion below. The idea is to form reﬂected pairs\n",
      "for each input Xjwith knots at each observed value xijof that input.\n",
      "Therefore, the collection of basis functions is\n",
      "C={(Xj−t)+,(t−Xj)+}t∈ {x1j, x2j, . . . , x Nj}\n",
      "j= 1,2, . . . , p.(9.18)\n",
      "If all of the input values are distinct, there are 2 Npbasis functions alto-\n",
      "gether. Note that although each basis function depends only on a single\n",
      "Xj, for example, h(X) = (Xj−t)+, it is considered as a function over the\n",
      "entire input space IRp.\n",
      "The model-building strategy is like a forward stepwise linear regression,\n",
      "but instead of using the original inputs, we are allowed to use functions\n",
      "from the set Cand their products. Thus the model has the form\n",
      "f(X) =β0+M∑\n",
      "m=1βmhm(X), (9.19)\n",
      "where each hm(X) is a function in C, or a product of two or more such\n",
      "functions.\n",
      "Given a choice for the hm, the coeﬃcients βmare estimated by minimiz-\n",
      "ing the residual sum-of-squares, that is, by standard linear regression. The\n",
      "real art, however, is in the construction of the functions hm(x). We start\n",
      "with only the constant function h0(X) = 1 in our model, and all functions\n",
      "in the set Care candidate functions. This is depicted in Figure 9.10.\n",
      "At each stage we consider as a new basis function pair all products of a\n",
      "function hmin the model set Mwith one of the reﬂected pairs in C. We\n",
      "add to the model Mthe term of the form\n",
      "ˆβM+1hℓ(X)≤(Xj−t)++ˆβM+2hℓ(X)≤(t−Xj)+, hℓ∈ M,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines 323\n",
      "X1\n",
      "X1X1\n",
      "X1\n",
      "X2X2\n",
      "X2X2\n",
      "X2\n",
      "XpXpXpConstant\n",
      "FIGURE 9.10. Schematic of the MARS forward model-building procedure. On\n",
      "the left are the basis functions currently in the model: initiall y, this is the constant\n",
      "function h(X) = 1. On the right are all candidate basis functions to be considered\n",
      "in building the model. These are pairs of piecewise linear basi s functions as in\n",
      "Figure 9.9, with knots tat all unique observed values xijof each predictor Xj.\n",
      "At each stage we consider all products of a candidate pair with a basis function\n",
      "in the model. The product that decreases the residual error t he most is added into\n",
      "the current model. Above we illustrate the ﬁrst three steps of t he procedure, with\n",
      "the selected functions shown in red.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "324 9. Additive Models, Trees, and Related Methods\n",
      "X1X2h(X1, X2)\n",
      "FIGURE 9.11. The function h(X1, X2) = (X1−x51)+≤(x72−X2)+, resulting\n",
      "from multiplication of two piecewise linear MARS basis functio ns.\n",
      "that produces the largest decrease in training error. Here ˆβM+1andˆβM+2\n",
      "are coeﬃcients estimated by least squares, along with all the other M+ 1\n",
      "coeﬃcients in the model. Then the winning products are added to the\n",
      "model and the process is continued until the model set Mcontains some\n",
      "preset maximum number of terms.\n",
      "For example, at the ﬁrst stage we consider adding to the model a function\n",
      "of the form β1(Xj−t)++β2(t−Xj)+;t∈ {xij}, since multiplication by\n",
      "the constant function just produces the function itself. Suppose the best\n",
      "choice is ˆβ1(X2−x72)++ˆβ2(x72−X2)+. Then this pair of basis functions\n",
      "is added to the set M, and at the next stage we consider including a pair\n",
      "of products the form\n",
      "hm(X)≤(Xj−t)+and hm(X)≤(t−Xj)+, t∈ {xij},\n",
      "where for hmwe have the choices\n",
      "h0(X) = 1 ,\n",
      "h1(X) = ( X2−x72)+,or\n",
      "h2(X) = ( x72−X2)+.\n",
      "The third choice produces functions such as ( X1−x51)+≤(x72−X2)+,\n",
      "depicted in Figure 9.11.\n",
      "At the end of this process we have a large model of the form (9.19). This\n",
      "model typically overﬁts the data, and so a backward deletion procedure\n",
      "is applied. The term whose removal causes the smallest increase in resid-\n",
      "ual squared error is deleted from the model at each stage, producing an\n",
      "estimated best model ˆfλof each size (number of terms) λ. One could use\n",
      "cross-validation to estimate the optimal value of λ, but for computational\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines 325\n",
      "savings the MARS procedure instead uses generalized cross-validation. This\n",
      "criterion is deﬁned as\n",
      "GCV( λ) =∑N\n",
      "i=1(yi−ˆfλ(xi))2\n",
      "(1−M(λ)/N)2. (9.20)\n",
      "The value M(λ) is the eﬀective number of parameters in the model: this\n",
      "accounts both for the number of terms in the models, plus the number\n",
      "of parameters used in selecting the optimal positions of the knots. Some\n",
      "mathematical and simulation results suggest that one should pay a price\n",
      "of three parameters for selecting a knot in a piecewise linear regression.\n",
      "Thus if there are rlinearly independent basis functions in the model, and\n",
      "Kknots were selected in the forward process, the formula is M(λ) =r+cK,\n",
      "where c= 3. (When the model is restricted to be additive—details below—\n",
      "a penalty of c= 2 is used). Using this, we choose the model along the\n",
      "backward sequence that minimizes GCV( λ).\n",
      "Why these piecewise linear basis functions, and why this particular model\n",
      "strategy? A key property of the functions of Figure 9.9 is their ability to\n",
      "operate locally; they are zero over part of their range. When they are mul-\n",
      "tiplied together, as in Figure 9.11, the result is nonzero only over the small\n",
      "part of the feature space where both component functions are nonzero. As\n",
      "a result, the regression surface is built up parsimoniously, using nonzero\n",
      "components locally—only where they are needed. This is important, since\n",
      "one should “spend” parameters carefully in high dimensions, as they can\n",
      "run out quickly. The use of other basis functions such as polynomials, would\n",
      "produce a nonzero product everywhere, and would not work as well.\n",
      "The second important advantage of the piecewise linear basis function\n",
      "concerns computation. Consider the product of a function in Mwith each\n",
      "of the Nreﬂected pairs for an input Xj. This appears to require the ﬁtting\n",
      "ofNsingle-input linear regression models, each of which uses O(N) oper-\n",
      "ations, making a total of O(N2) operations. However, we can exploit the\n",
      "simple form of the piecewise linear function. We ﬁrst ﬁt the reﬂected pair\n",
      "with rightmost knot. As the knot is moved successively one position at a\n",
      "time to the left, the basis functions diﬀer by zero over the left part of the\n",
      "domain, and by a constant over the right part. Hence after each such move\n",
      "we can update the ﬁt in O(1) operations. This allows us to try every knot\n",
      "in only O(N) operations.\n",
      "The forward modeling strategy in MARS is hierarchical, in the sense that\n",
      "multiway products are built up from products involving terms already in\n",
      "the model. For example, a four-way product can only be added to the model\n",
      "if one of its three-way components is already in the model. The philosophy\n",
      "here is that a high-order interaction will likely only exist if some of its lo wer-\n",
      "order “footprints” exist as well. This need not be true, but is a reasonable\n",
      "working assumption and avoids the search over an exponentially growing\n",
      "space of alternatives.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "326 9. Additive Models, Trees, and Related Methods\n",
      "Rank of ModelTest Misclassification Error\n",
      "0 20 40 60 80 1000.1 0.2 0.3 0.4\n",
      "• • • • • • • • • • •••••• •• •• • • •••• • ••• • • •• • ••••• ••••••• ••••••••••••••• ••• ••••• ••••••••••••••••••••••••••••••\n",
      "0.055GCV choice\n",
      "FIGURE 9.12. Spam data: test error misclassiﬁcation rate for the MARS pro-\n",
      "cedure, as a function of the rank (number of independent basis functi ons) in the\n",
      "model.\n",
      "There is one restriction put on the formation of model terms: each input\n",
      "can appear at most once in a product. This prevents the formation of\n",
      "higher-order powers of an input, which increase or decrease too sharply\n",
      "near the boundaries of the feature space. Such powers can be approximated\n",
      "in a more stable way with piecewise linear functions.\n",
      "A useful option in the MARS procedure is to set an upper limit on\n",
      "the order of interaction. For example, one can set a limit of two, allowing\n",
      "pairwise products of piecewise linear functions, but not three- or higher-\n",
      "way products. This can aid in the interpretation of the ﬁnal model. An\n",
      "upper limit of one results in an additive model.\n",
      "9.4.1 Spam Example (Continued)\n",
      "We applied MARS to the “spam” data analyzed earlier in this chapter. To\n",
      "enhance interpretability, we restricted MARS to second-degree interactions.\n",
      "Although the target is a two-class variable, we used the squared-error loss\n",
      "function nonetheless (see Section 9.4.3). Figure 9.12 shows the test error\n",
      "misclassiﬁcation rate as a function of the rank (number of independent ba-\n",
      "sis functions) in the model. The error rate levels oﬀ at about 5 .5%, which is\n",
      "slightly higher than that of the generalized additive model (5 .3%) discussed\n",
      "earlier. GCV chose a model size of 60, which is roughly the smallest model\n",
      "giving optimal performance. The leading interactions found by MARS in-\n",
      "volved inputs ( ch$, remove ), (ch$, free ) and (hp, CAPTOT ). However, these\n",
      "interactions give no improvement in performance over the generalized ad-\n",
      "ditive model.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.4 MARS: Multivariate Adaptive Regression Splines 327\n",
      "9.4.2 Example (Simulated Data)\n",
      "Here we examine the performance of MARS in three contrasting scenarios.\n",
      "There are N= 100 observations, and the predictors X1,X2,... ,X pand\n",
      "errors εhave independent standard normal distributions.\n",
      "Scenario 1: The data generation model is\n",
      "Y= (X1−1)++ (X1−1)+≤(X2−.8)++ 0.12≤ε. (9.21)\n",
      "The noise standard deviation 0.12 was chosen so that the signal-to-\n",
      "noise ratio was about 5. We call this the tensor-product scenario; the\n",
      "product term gives a surface that looks like that of Figure 9.11.\n",
      "Scenario 2: This is the same as scenario 1, but with p= 20 total predictors;\n",
      "that is, there are 18 inputs that are independent of the response.\n",
      "Scenario 3: This has the structure of a neural network:\n",
      "ℓ1=X1+X2+X3+X4+X5,\n",
      "ℓ2=X6−X7+X8−X9+X10,\n",
      "σ(t) = 1 /(1 +e−t),\n",
      "Y=σ(ℓ1) +σ(ℓ2) + 0.12≤ε.(9.22)\n",
      "Scenarios 1 and 2 are ideally suited for MARS, while scenario 3 contains\n",
      "high-order interactions and may be diﬃcult for MARS to approximate. We\n",
      "ran ﬁve simulations from each model, and recorded the results.\n",
      "In scenario 1, MARS typically uncovered the correct model almost per-\n",
      "fectly. In scenario 2, it found the correct structure but also found a few\n",
      "extraneous terms involving other predictors.\n",
      "Letθ(x) be the true mean of Y, and let\n",
      "MSE 0= ave x∈Test(¯y−θ(x))2,\n",
      "MSE = ave x∈Test(ˆf(x)−θ(x))2.(9.23)\n",
      "These represent the mean-square error of the constant model and the ﬁtted\n",
      "MARS model, estimated by averaging at the 1000 test values of x. Table 9.4\n",
      "shows the proportional decrease in model error or R2for each scenario:\n",
      "R2=MSE 0−MSE\n",
      "MSE 0. (9.24)\n",
      "The values shown are means and standard error over the ﬁve simulations.\n",
      "The performance of MARS is degraded only slightly by the inclusion of the\n",
      "useless inputs in scenario 2; it performs substantially worse in scenario 3.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "328 9. Additive Models, Trees, and Related Methods\n",
      "TABLE 9.4. Proportional decrease in model error ( R2) when MARS is applied\n",
      "to three diﬀerent scenarios.\n",
      "Scenario Mean (S.E.)\n",
      "1: Tensor product p= 2 0.97 (0.01)\n",
      "2: Tensor product p= 20 0.96 (0.01)\n",
      "3: Neural network 0.79 (0.01)\n",
      "9.4.3 Other Issues\n",
      "MARS for Classiﬁcation\n",
      "The MARS method and algorithm can be extended to handle classiﬁcation\n",
      "problems. Several strategies have been suggested.\n",
      "For two classes, one can code the output as 0/1 and treat the problem as\n",
      "a regression; we did this for the spamexample. For more than two classes,\n",
      "one can use the indicator response approach described in Section 4.2. One\n",
      "codes the Kresponse classes via 0/1 indicator variables, and then per-\n",
      "forms a multi-response MARS regression. For the latter we use a common\n",
      "set of basis functions for all response variables. Classiﬁcation is made to\n",
      "the class with the largest predicted response value. There are, however, po-\n",
      "tential masking problems with this approach, as described in Section 4.2.\n",
      "A generally superior approach is the “optimal scoring” method discussed\n",
      "in Section 12.5.\n",
      "Stone et al. (1997) developed a hybrid of MARS called PolyMARS specif-\n",
      "ically designed to handle classiﬁcation problems. It uses the multiple logistic\n",
      "framework described in Section 4.4. It grows the model in a forward stage-\n",
      "wise fashion like MARS, but at each stage uses a quadratic approximation\n",
      "to the multinomial log-likelihood to search for the next basis-function pair.\n",
      "Once found, the enlarged model is ﬁt by maximum likelihood, and the\n",
      "process is repeated.\n",
      "Relationship of MARS to CART\n",
      "Although they might seem quite diﬀerent, the MARS and CART strategies\n",
      "actually have strong similarities. Suppose we take the MARS procedure and\n",
      "make the following changes:\n",
      "•Replace the piecewise linear basis functions by step functions I(x−t >\n",
      "0) and I(x−t≤0).\n",
      "•When a model term is involved in a multiplication by a candidate\n",
      "term, it gets replaced by the interaction, and hence is not available\n",
      "for further interactions.\n",
      "With these changes, the MARS forward procedure is the same as the CART\n",
      "tree-growing algorithm. Multiplying a step function by a pair of reﬂected\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.5 Hierarchical Mixtures of Experts 329\n",
      "step functions is equivalent to splitting a node at the step. The second\n",
      "restriction implies that a node may not be split more than once, and leads\n",
      "to the attractive binary-tree representation of the CART model. On the\n",
      "other hand, it is this restriction that makes it diﬃcult for CART to model\n",
      "additive structures. MARS forgoes the tree structure and gains the ability\n",
      "to capture additive eﬀects.\n",
      "Mixed Inputs\n",
      "Mars can handle “mixed” predictors—quantitative and qualitative—in a\n",
      "natural way, much like CART does. MARS considers all possible binary\n",
      "partitions of the categories for a qualitative predictor into two groups.\n",
      "Each such partition generates a pair of piecewise constant basis functions—\n",
      "indicator functions for the two sets of categories. This basis pair is now\n",
      "treated as any other, and is used in forming tensor products with other\n",
      "basis functions already in the model.\n",
      "9.5 Hierarchical Mixtures of Experts\n",
      "The hierarchical mixtures of experts (HME) procedure can be viewed as a\n",
      "variant of tree-based methods. The main diﬀerence is that the tree splits\n",
      "are not hard decisions but rather soft probabilistic ones. At each node an\n",
      "observation goes left or right with probabilities depending on its input val-\n",
      "ues. This has some computational advantages since the resulting parameter\n",
      "optimization problem is smooth, unlike the discrete split point search in the\n",
      "tree-based approach. The soft splits might also help in prediction accuracy\n",
      "and provide a useful alternative description of the data.\n",
      "There are other diﬀerences between HMEs and the CART implementa-\n",
      "tion of trees. In an HME, a linear (or logistic regression) model is ﬁt in\n",
      "each terminal node, instead of a constant as in CART. The splits can be\n",
      "multiway, not just binary, and the splits are probabilistic functions of a\n",
      "linear combination of inputs, rather than a single input as in the standard\n",
      "use of CART. However, the relative merits of these choices are not clear,\n",
      "and most were discussed at the end of Section 9.2.\n",
      "A simple two-level HME model in shown in Figure 9.13. It can be thought\n",
      "of as a tree with soft splits at each non-terminal node. However, the inven-\n",
      "tors of this methodology use a diﬀerent terminology. The terminal nodes\n",
      "are called experts , and the non-terminal nodes are called gating networks .\n",
      "The idea is that each expert provides an opinion (prediction) about the\n",
      "response, and these are combined together by the gating networks. As we\n",
      "will see, the model is formally a mixture model, and the two-level model\n",
      "in the ﬁgure can be extend to multiple levels, hence the name hierarchical\n",
      "mixtures of experts .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "330 9. Additive Models, Trees, and Related Methods\n",
      "g1 g2\n",
      "g1|1 g2|1g1|2 g2|2Gating GatingGatingGating\n",
      "Gating GatingGating GatingGating\n",
      "Network Network NetworkNetwork\n",
      "Network\n",
      "NetworkNetwork\n",
      "Network Network NetworkNetwork\n",
      "NetworkNetwork NetworkNetwork Network\n",
      "Network Network NetworkNetwork\n",
      "Expert Expert Expert Expert Expert Expert Expert Expert Expert Expert Expert\n",
      "Pr(y|x, θ11) Pr( y|x, θ21) Pr( y|x, θ12) Pr( y|x, θ22)\n",
      "FIGURE 9.13. A two-level hierarchical mixture of experts (HME) model.\n",
      "Consider the regression or classiﬁcation problem, as described earlier in\n",
      "the chapter. The data is ( xi,yi),i= 1,2,... ,N , with yieither a continuous\n",
      "or binary-valued response, and xia vector-valued input. For ease of nota-\n",
      "tion we assume that the ﬁrst element of xiis one, to account for intercepts.\n",
      "Here is how an HME is deﬁned. The top gating network has the output\n",
      "gj(x,γj) =eγT\n",
      "jx\n",
      "∑K\n",
      "k=1eγT\n",
      "kx, j= 1,2,... ,K, (9.25)\n",
      "where each γjis a vector of unknown parameters. This represents a soft\n",
      "K-way split ( K= 2 in Figure 9.13.) Each gj(x,γj) is the probability of\n",
      "assigning an observation with feature vector xto the jth branch. Notice\n",
      "that with K= 2 groups, if we take the coeﬃcient of one of the elements of\n",
      "xto be + ∞, then we get a logistic curve with inﬁnite slope. In this case,\n",
      "the gating probabilities are either 0 or 1, corresponding to a hard split on\n",
      "that input.\n",
      "At the second level, the gating networks have a similar form:\n",
      "gℓ|j(x,γjℓ) =eγT\n",
      "jℓx\n",
      "∑K\n",
      "k=1eγT\n",
      "jkx, ℓ= 1,2,... ,K. (9.26)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.5 Hierarchical Mixtures of Experts 331\n",
      "This is the probability of assignment to the ℓth branch, given assignment\n",
      "to the jth branch at the level above.\n",
      "At each expert (terminal node), we have a model for the response variable\n",
      "of the form\n",
      "Y∼Pr(y|x,θjℓ). (9.27)\n",
      "This diﬀers according to the problem.\n",
      "Regression: The Gaussian linear regression model is used, with θjℓ=\n",
      "(βjℓ,σ2\n",
      "jℓ):\n",
      "Y=βT\n",
      "jℓx+εandε∼N(0,σ2\n",
      "jℓ). (9.28)\n",
      "Classiﬁcation: The linear logistic regression model is used:\n",
      "Pr(Y= 1|x,θjℓ) =1\n",
      "1 +e−θT\n",
      "jℓx. (9.29)\n",
      "Denoting the collection of all parameters by Ψ = {γj,γjℓ,θjℓ}, the total\n",
      "probability that Y=yis\n",
      "Pr(y|x,Ψ) =K∑\n",
      "j=1gj(x,γj)K∑\n",
      "ℓ=1gℓ|j(x,γjℓ)Pr(y|x,θjℓ). (9.30)\n",
      "This is a mixture model, with the mixture probabilities determined by the\n",
      "gating network models.\n",
      "To estimate the parameters, we maximize the log-likelihood of the data,∑\n",
      "ilog Pr( yi|xi,Ψ), over the parameters in Ψ. The most convenient method\n",
      "for doing this is the EM algorithm, which we describe for mixtures in\n",
      "Section 8.5. We deﬁne latent variables ∆ j, all of which are zero except for\n",
      "a single one. We interpret these as the branching decisions made by the top\n",
      "level gating network. Similarly we deﬁne latent variables ∆ ℓ|jto describe\n",
      "the gating decisions at the second level.\n",
      "In the E-step, the EM algorithm computes the expectations of the ∆ j\n",
      "and ∆ ℓ|jgiven the current values of the parameters. These expectations\n",
      "are then used as observation weights in the M-step of the procedure, to\n",
      "estimate the parameters in the expert networks. The parameters in the\n",
      "internal nodes are estimated by a version of multiple logistic regression.\n",
      "The expectations of the ∆ jor ∆ ℓ|jare probability proﬁles, and these are\n",
      "used as the response vectors for these logistic regressions.\n",
      "The hierarchical mixtures of experts approach is a promising competitor\n",
      "to CART trees. By using soft splits rather than hard decision rules it can\n",
      "capture situations where the transition from low to high response is gradual .\n",
      "The log-likelihood is a smooth function of the unknown weights and hence\n",
      "is amenable to numerical optimization. The model is similar to CART with\n",
      "linear combination splits, but the latter is more diﬃcult to optimize. On\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "332 9. Additive Models, Trees, and Related Methods\n",
      "the other hand, to our knowledge there are no methods for ﬁnding a good\n",
      "tree topology for the HME model, as there are in CART. Typically one uses\n",
      "a ﬁxed tree of some depth, possibly the output of the CART procedure.\n",
      "The emphasis in the research on HMEs has been on prediction rather than\n",
      "interpretation of the ﬁnal model. A close cousin of the HME is the latent\n",
      "class model (Lin et al., 2000), which typically has only one layer; here\n",
      "the nodes or latent classes are interpreted as groups of subjects that show\n",
      "similar response behavior.\n",
      "9.6 Missing Data\n",
      "It is quite common to have observations with missing values for one or mor e\n",
      "input features. The usual approach is to impute (ﬁll-in) the missing values\n",
      "in some way.\n",
      "However, the ﬁrst issue in dealing with the problem is determining wheth-\n",
      "er the missing data mechanism has distorted the observed data. Roughly\n",
      "speaking, data are missing at random if the mechanism resulting in its\n",
      "omission is independent of its (unobserved) value. A more precise deﬁnition\n",
      "is given in Little and Rubin (2002). Suppose yis the response vector and X\n",
      "is the N×pmatrix of inputs (some of which are missing). Denote by Xobs\n",
      "the observed entries in Xand let Z= (y,X),Zobs= (y,Xobs). Finally, if R\n",
      "is an indicator matrix with ijth entry 1 if xijis missing and zero otherwise,\n",
      "then the data is said to be missing at random (MAR) if the distribution of\n",
      "Rdepends on the data Zonly through Zobs:\n",
      "Pr(R|Z,θ) = Pr( R|Zobs,θ). (9.31)\n",
      "Hereθare any parameters in the distribution of R. Data are said to be\n",
      "missing completely at random (MCAR) if the distribution of Rdoesn’t\n",
      "depend on the observed or missing data:\n",
      "Pr(R|Z,θ) = Pr( R|θ). (9.32)\n",
      "MCAR is a stronger assumption than MAR: most imputation methods rely\n",
      "on MCAR for their validity.\n",
      "For example, if a patient’s measurement was not taken because the doctor\n",
      "felt he was too sick, that observation would not be MAR or MCAR. In this\n",
      "case the missing data mechanism causes our observed training data to give a\n",
      "distorted picture of the true population, and data imputation is dangerous\n",
      "in this instance. Often the determination of whether features are MCAR\n",
      "must be made from information about the data collection process. For\n",
      "categorical features, one way to diagnose this problem is to code “missing”\n",
      "as an additional class. Then we ﬁt our model to the training data and see\n",
      "if class “missing” is predictive of the response.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "9.6 Missing Data 333\n",
      "Assuming the features are missing completely at random, there are a\n",
      "number of ways of proceeding:\n",
      "1. Discard observations with any missing values.\n",
      "2. Rely on the learning algorithm to deal with missing values in its\n",
      "training phase.\n",
      "3. Impute all missing values before training.\n",
      "Approach (1) can be used if the relative amount of missing data is small,\n",
      "but otherwise should be avoided. Regarding (2), CART is one learning\n",
      "algorithm that deals eﬀectively with missing values, through surrogate splits\n",
      "(Section 9.2.4). MARS and PRIM use similar approaches. In generalized\n",
      "additive modeling, all observations missing for a given input feature are\n",
      "omitted when the partial residuals are smoothed against that feature in\n",
      "the backﬁtting algorithm, and their ﬁtted values are set to zero. Since the\n",
      "ﬁtted curves have mean zero (when the model includes an intercept), this\n",
      "amounts to assigning the average ﬁtted value to the missing observations.\n",
      "For most learning methods, the imputation approach (3) is necessary.\n",
      "The simplest tactic is to impute the missing value with the mean or median\n",
      "of the nonmissing values for that feature. (Note that the above procedure\n",
      "for generalized additive models is analogous to this.)\n",
      "If the features have at least some moderate degree of dependence, one\n",
      "can do better by estimating a predictive model for each feature given the\n",
      "other features and then imputing each missing value by its prediction from\n",
      "the model. In choosing the learning method for imputation of the features,\n",
      "one must remember that this choice is distinct from the method used for\n",
      "predicting yfromX. Thus a ﬂexible, adaptive method will often be pre-\n",
      "ferred, even for the eventual purpose of carrying out a linear regression of y\n",
      "onX. In addition, if there are many missing feature values in the training\n",
      "set, the learning method must itself be able to deal with missing feature\n",
      "values. CART therefore is an ideal choice for this imputation “engine.”\n",
      "After imputation, missing values are typically treated as if they were ac-\n",
      "tually observed. This ignores the uncertainty due to the imputation, which\n",
      "will itself introduce additional uncertainty into estimates and predictions\n",
      "from the response model. One can measure this additional uncertainty by\n",
      "doing multiple imputations and hence creating many diﬀerent training sets.\n",
      "The predictive model for ycan be ﬁt to each training set, and the variation\n",
      "across training sets can be assessed. If CART was used for the imputation\n",
      "engine, the multiple imputations could be done by sampling from the values\n",
      "in the corresponding terminal nodes.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "334 9. Additive Models, Trees, and Related Methods\n",
      "9.7 Computational Considerations\n",
      "With Nobservations and ppredictors, additive model ﬁtting requires some\n",
      "number mpof applications of a one-dimensional smoother or regression\n",
      "method. The required number of cycles mof the backﬁtting algorithm is\n",
      "usually less than 20 and often less than 10, and depends on the amount\n",
      "of correlation in the inputs. With cubic smoothing splines, for example,\n",
      "NlogNoperations are needed for an initial sort and Noperations for the\n",
      "spline ﬁt. Hence the total operations for an additive model ﬁt is pNlogN+\n",
      "mpN.\n",
      "Trees require pNlogNoperations for an initial sort for each predictor,\n",
      "and typically another pNlogNoperations for the split computations. If the\n",
      "splits occurred near the edges of the predictor ranges, this number could\n",
      "increase to N2p.\n",
      "MARS requires Nm2+pmN operations to add a basis function to a\n",
      "model with mterms already present, from a pool of ppredictors. Hence to\n",
      "build an M-term model requires NM3+pM2Ncomputations, which can\n",
      "be quite prohibitive if Mis a reasonable fraction of N.\n",
      "Each of the components of an HME are typically inexpensive to ﬁt at\n",
      "each M-step: Np2for the regressions, and Np2K2for a K-class logistic\n",
      "regression. The EM algorithm, however, can take a long time to converge,\n",
      "and so sizable HME models are considered costly to ﬁt.\n",
      "Bibliographic Notes\n",
      "The most comprehensive source for generalized additive models is the text\n",
      "of that name by Hastie and Tibshirani (1990). Diﬀerent applications of\n",
      "this work in medical problems are discussed in Hastie et al. (1989) and\n",
      "Hastie and Herman (1990), and the software implementation in Splus is\n",
      "described in Chambers and Hastie (1991). Green and Silverman (1994)\n",
      "discuss penalization and spline models in a variety of settings. Efron and\n",
      "Tibshirani (1991) give an exposition of modern developments in statisti cs\n",
      "(including generalized additive models), for a nonmathematical audience.\n",
      "Classiﬁcation and regression trees date back at least as far as Morgan and\n",
      "Sonquist (1963). We have followed the modern approaches of Breiman et\n",
      "al. (1984) and Quinlan (1993). The PRIM method is due to Friedman\n",
      "and Fisher (1999), while MARS is introduced in Friedman (1991), with an\n",
      "additive precursor in Friedman and Silverman (1989). Hierarchical mixtures\n",
      "of experts were proposed in Jordan and Jacobs (1994); see also Jacobs et\n",
      "al. (1991).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 335\n",
      "Exercises\n",
      "Ex. 9.1 Show that a smoothing spline ﬁt of yitoxipreserves the linear\n",
      "partof the ﬁt. In other words, if yi= ˆyi+ri, where ˆ yirepresents the\n",
      "linear regression ﬁts, and Sis the smoothing matrix, then Sy=ˆy+Sr.\n",
      "Show that the same is true for local linear regression (Section 6.1.1). Hence\n",
      "argue that the adjustment step in the second line of (2) in Algorithm 9.1\n",
      "is unnecessary.\n",
      "Ex. 9.2 LetAbe a known k×kmatrix, bbe a known k-vector, and z\n",
      "be an unknown k-vector. A Gauss–Seidel algorithm for solving the linear\n",
      "system of equations Az=bworks by successively solving for element zjin\n",
      "thejth equation, ﬁxing all other zj’s at their current guesses. This process\n",
      "is repeated for j= 1,2,... ,k, 1,2,... ,k,... , until convergence (Golub and\n",
      "Van Loan, 1983).\n",
      "(a) Consider an additive model with Nobservations and pterms, with\n",
      "thejth term to be ﬁt by a linear smoother Sj. Consider the following\n",
      "system of equations:\n",
      "\n",
      "I S 1S1≤≤≤S1\n",
      "S2I S 2≤≤≤S2\n",
      "...............\n",
      "SpSpSp≤≤≤I\n",
      "\n",
      "f1\n",
      "f2\n",
      "...\n",
      "fp\n",
      "=\n",
      "S1y\n",
      "S2y\n",
      "...\n",
      "Spy\n",
      ". (9.33)\n",
      "Here each fjis an N-vector of evaluations of the jth function at\n",
      "the data points, and yis anN-vector of the response values. Show\n",
      "that backﬁtting is a blockwise Gauss–Seidel algorithm for solving this\n",
      "system of equations.\n",
      "(b) Let S1andS2be symmetric smoothing operators (matrices) with\n",
      "eigenvalues in [0 ,1). Consider a backﬁtting algorithm with response\n",
      "vector yand smoothers S1,S2. Show that with any starting values,\n",
      "the algorithm converges and give a formula for the ﬁnal iterates.\n",
      "Ex. 9.3 Backﬁtting equations. Consider a backﬁtting procedure with orthog-\n",
      "onal projections, and let Dbe the overall regression matrix whose columns\n",
      "spanV=Lcol(S1)⊕ Lcol(S2)⊕ ≤≤≤ ⊕ L col(Sp), where Lcol(S) denotes the\n",
      "column space of a matrix S. Show that the estimating equations\n",
      "\n",
      "I S 1S1≤≤≤S1\n",
      "S2I S 2≤≤≤S2\n",
      "...............\n",
      "SpSpSp≤≤≤I\n",
      "\n",
      "f1\n",
      "f2\n",
      "...\n",
      "fp\n",
      "=\n",
      "S1y\n",
      "S2y\n",
      "...\n",
      "Spy\n",
      "\n",
      "are equivalent to the least squares normal equations DTDβ=DTywhere\n",
      "βis the vector of coeﬃcients.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "336 9. Additive Models, Trees, and Related Methods\n",
      "Ex. 9.4 Suppose the same smoother Sis used to estimate both terms in a\n",
      "two-term additive model (i.e., both variables are identical). Assume that S\n",
      "is symmetric with eigenvalues in [0 ,1). Show that the backﬁtting residual\n",
      "converges to ( I+S)−1(I−S)y, and that the residual sum of squares con-\n",
      "verges upward. Can the residual sum of squares converge upward in less\n",
      "structured situations? How does this ﬁt compare to the ﬁt with a single\n",
      "term ﬁt by S? [Hint: Use the eigen-decomposition of Sto help with this\n",
      "comparison.]\n",
      "Ex. 9.5 Degrees of freedom of a tree . Given data yiwith mean f(xi) and\n",
      "variance σ2, and a ﬁtting operation y→ˆy, let’s deﬁne the degrees of\n",
      "freedom of a ﬁt by∑\n",
      "icov(yi,ˆyi)/σ2.\n",
      "Consider a ﬁt ˆyestimated by a regression tree, ﬁt to a set of predictors\n",
      "X1,X2,... ,X p.\n",
      "(a) In terms of the number of terminal nodes m, give a rough formula for\n",
      "the degrees of freedom of the ﬁt.\n",
      "(b) Generate 100 observations with predictors X1,X2,... ,X 10as inde-\n",
      "pendent standard Gaussian variates and ﬁx these values.\n",
      "(c) Generate response values also as standard Gaussian ( σ2= 1), indepen-\n",
      "dent of the predictors. Fit regression trees to the data of ﬁxed size 1,5\n",
      "and 10 terminal nodes and hence estimate the degrees of freedom of\n",
      "each ﬁt. [Do ten simulations of the response and average the results,\n",
      "to get a good estimate of degrees of freedom.]\n",
      "(d) Compare your estimates of degrees of freedom in (a) and (c) and\n",
      "discuss.\n",
      "(e) If the regression tree ﬁt were a linear operation, we could write ˆy=Sy\n",
      "for some matrix S. Then the degrees of freedom would be tr( S).\n",
      "Suggest a way to compute an approximate Smatrix for a regression\n",
      "tree, compute it and compare the resulting degrees of freedom to\n",
      "those in (a) and (c).\n",
      "Ex. 9.6 Consider the ozone data of Figure 6.9.\n",
      "(a) Fit an additive model to the cube root of ozone concentration. as a\n",
      "function of temperature, wind speed, and radiation. Compare your\n",
      "results to those obtained via the trellis display in Figure 6.9.\n",
      "(b) Fit trees, MARS, and PRIM to the same data, and compare the results\n",
      "to those found in (a) and in Figure 6.9.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 337\n",
      "Printer: Opaque this\n",
      "10\n",
      "Boosting and Additive Trees\n",
      "10.1 Boosting Methods\n",
      "Boosting is one of the most powerful learning ideas introduced in the last\n",
      "twenty years. It was originally designed for classiﬁcation problems, but as\n",
      "will be seen in this chapter, it can proﬁtably be extended to regression\n",
      "as well. The motivation for boosting was a procedure that combines the\n",
      "outputs of many “weak” classiﬁers to produce a powerful “committee.”\n",
      "From this perspective boosting bears a resemblance to bagging and other\n",
      "committee-based approaches (Section 8.8). However we shall see that the\n",
      "connection is at best superﬁcial and that boosting is fundamentally diﬀer-\n",
      "ent.\n",
      "We begin by describing the most popular boosting algorithm due to\n",
      "Freund and Schapire (1997) called “AdaBoost.M1.” Consider a two-class\n",
      "problem, with the output variable coded as Y∈ {−1,1}. Given a vector of\n",
      "predictor variables X, a classiﬁer G(X) produces a prediction taking one\n",
      "of the two values {−1,1}. The error rate on the training sample is\n",
      "err =1\n",
      "NN∑\n",
      "i=1I(yi̸=G(xi)),\n",
      "and the expected error rate on future predictions is E XYI(Y̸=G(X)).\n",
      "A weak classiﬁer is one whose error rate is only slightly better than\n",
      "random guessing. The purpose of boosting is to sequentially apply the\n",
      "weak classiﬁcation algorithm to repeatedly modiﬁed versions of the data,\n",
      "thereby producing a sequence of weak classiﬁers Gm(x),m= 1,2,... ,M .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "338 10. Boosting and Additive Trees\n",
      "Training  SampleWeighted  SampleWeighted  SampleWeighted  Sample\n",
      "Training  SampleWeighted  SampleWeighted  SampleWeighted  SampleWeighted  Sample\n",
      "Training  SampleWeighted  Sample\n",
      "Training  SampleWeighted  SampleWeighted  SampleWeighted  Sample\n",
      "Weighted  SampleWeighted  SampleWeighted  Sample\n",
      "Training  SampleWeighted  SampleG(x) = sign[∑M\n",
      "m=1αmGm(x)]\n",
      "GM(x)\n",
      "G3(x)\n",
      "G2(x)\n",
      "G1(x)Final Classifier\n",
      "FIGURE 10.1. Schematic of AdaBoost. Classiﬁers are trained on weighted ver-\n",
      "sions of the dataset, and then combined to produce a ﬁnal predictio n.\n",
      "The predictions from all of them are then combined through a weighted\n",
      "majority vote to produce the ﬁnal prediction:\n",
      "G(x) = sign(M∑\n",
      "m=1αmGm(x))\n",
      ". (10.1)\n",
      "Hereα1,α2,... ,α Mare computed by the boosting algorithm, and weight\n",
      "the contribution of each respective Gm(x). Their eﬀect is to give higher\n",
      "inﬂuence to the more accurate classiﬁers in the sequence. Figure 10.1 shows\n",
      "a schematic of the AdaBoost procedure.\n",
      "The data modiﬁcations at each boosting step consist of applying weights\n",
      "w1,w2,... ,w Nto each of the training observations ( xi,yi), i= 1,2,... ,N .\n",
      "Initially all of the weights are set to wi= 1/N, so that the ﬁrst step simply\n",
      "trains the classiﬁer on the data in the usual manner. For each successive\n",
      "iteration m= 2,3,... ,M the observation weights are individually modi-\n",
      "ﬁed and the classiﬁcation algorithm is reapplied to the weighted observa-\n",
      "tions. At step m, those observations that were misclassiﬁed by the classiﬁer\n",
      "Gm−1(x) induced at the previous step have their weights increased, whereas\n",
      "the weights are decreased for those that were classiﬁed correctly. Thus as\n",
      "iterations proceed, observations that are diﬃcult to classify correctly re-\n",
      "ceive ever-increasing inﬂuence. Each successive classiﬁer is thereby forced\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.1 Boosting Methods 339\n",
      "Algorithm 10.1 AdaBoost.M1.\n",
      "1. Initialize the observation weights wi= 1/N, i = 1,2,... ,N .\n",
      "2. For m= 1 to M:\n",
      "(a) Fit a classiﬁer Gm(x) to the training data using weights wi.\n",
      "(b) Compute\n",
      "errm=∑N\n",
      "i=1wiI(yi̸=Gm(xi))\n",
      "∑N\n",
      "i=1wi.\n",
      "(c) Compute αm= log((1 −errm)/errm).\n",
      "(d) Set wi←wi≤exp[αm≤I(yi̸=Gm(xi))], i= 1,2,... ,N .\n",
      "3. Output G(x) = sign[∑M\n",
      "m=1αmGm(x)]\n",
      ".\n",
      "to concentrate on those training observations that are missed by previous\n",
      "ones in the sequence.\n",
      "Algorithm 10.1 shows the details of the AdaBoost.M1 algorithm. The\n",
      "current classiﬁer Gm(x) is induced on the weighted observations at line 2a.\n",
      "The resulting weighted error rate is computed at line 2b. Line 2c calculates\n",
      "the weight αmgiven to Gm(x) in producing the ﬁnal classiﬁer G(x) (line\n",
      "3). The individual weights of each of the observations are updated for the\n",
      "next iteration at line 2d. Observations misclassiﬁed by Gm(x) have their\n",
      "weights scaled by a factor exp( αm), increasing their relative inﬂuence for\n",
      "inducing the next classiﬁer Gm+1(x) in the sequence.\n",
      "The AdaBoost.M1 algorithm is known as “Discrete AdaBoost” in Fried-\n",
      "man et al. (2000), because the base classiﬁer Gm(x) returns a discrete class\n",
      "label. If the base classiﬁer instead returns a real-valued prediction (e.g.,\n",
      "a probability mapped to the interval [ −1,1]), AdaBoost can be modiﬁed\n",
      "appropriately (see “Real AdaBoost” in Friedman et al. (2000)).\n",
      "The power of AdaBoost to dramatically increase the performance of even\n",
      "a very weak classiﬁer is illustrated in Figure 10.2. The features X1,... ,X 10\n",
      "are standard independent Gaussian, and the deterministic target Yis de-\n",
      "ﬁned by\n",
      "Y={\n",
      "1 if∑10\n",
      "j=1X2\n",
      "j> χ2\n",
      "10(0.5),\n",
      "−1 otherwise .(10.2)\n",
      "Hereχ2\n",
      "10(0.5) = 9 .34 is the median of a chi-squared random variable with\n",
      "10 degrees of freedom (sum of squares of 10 standard Gaussians). There are\n",
      "2000 training cases, with approximately 1000 cases in each class, and 10,0 00\n",
      "test observations. Here the weak classiﬁer is just a “stump”: a two terminal-\n",
      "node classiﬁcation tree. Applying this classiﬁer alone to the training data\n",
      "set yields a very poor test set error rate of 45.8%, compared to 50% for\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "340 10. Boosting and Additive Trees\n",
      "0 100 200 300 4000.0 0.1 0.2 0.3 0.4 0.5\n",
      "Boosting IterationsTest ErrorSingle Stump\n",
      "244 Node Tree\n",
      "FIGURE 10.2. Simulated data (10.2): test error rate for boosting with stump s,\n",
      "as a function of the number of iterations. Also shown are the test error rate for\n",
      "a single stump, and a 244-node classiﬁcation tree.\n",
      "random guessing. However, as boosting iterations proceed the error rate\n",
      "steadily decreases, reaching 5.8% after 400 iterations. Thus, boosting this\n",
      "simple very weak classiﬁer reduces its prediction error rate by almost a\n",
      "factor of four. It also outperforms a single large classiﬁcation tree ( error\n",
      "rate 24 .7%). Since its introduction, much has been written to explain the\n",
      "success of AdaBoost in producing accurate classiﬁers. Most of this work\n",
      "has centered on using classiﬁcation trees as the “base learner” G(x), where\n",
      "improvements are often most dramatic. In fact, Breiman (NIPS Workshop,\n",
      "1996) referred to AdaBoost with trees as the “best oﬀ-the-shelf classiﬁer in\n",
      "the world” (see also Breiman (1998)). This is especially the case for data -\n",
      "mining applications, as discussed more fully in Section 10.7 later in this\n",
      "chapter.\n",
      "10.1.1 Outline of This Chapter\n",
      "Here is an outline of the developments in this chapter:\n",
      "•We show that AdaBoost ﬁts an additive model in a base learner,\n",
      "optimizing a novel exponential loss function. This loss function is\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.2 Boosting Fits an Additive Model 341\n",
      "very similar to the (negative) binomial log-likelihood (Sections 10.2–\n",
      "10.4).\n",
      "•The population minimizer of the exponential loss function is shown\n",
      "to be the log-odds of the class probabilities (Section 10.5).\n",
      "•We describe loss functions for regression and classiﬁcation that are\n",
      "more robust than squared error or exponential loss (Section 10.6).\n",
      "•It is argued that decision trees are an ideal base learner for data\n",
      "mining applications of boosting (Sections 10.7 and 10.9).\n",
      "•We develop a class of gradient boosted models (GBMs), for boosting\n",
      "trees with any loss function (Section 10.10).\n",
      "•The importance of “slow learning” is emphasized, and implemented\n",
      "by shrinkage of each new term that enters the model (Section 10.12),\n",
      "as well as randomization (Section 10.12.2).\n",
      "•Tools for interpretation of the ﬁtted model are described (Section 10.13).\n",
      "10.2 Boosting Fits an Additive Model\n",
      "The success of boosting is really not very mysterious. The key lies in ex-\n",
      "pression (10.1). Boosting is a way of ﬁtting an additive expansion in a set\n",
      "of elementary “basis” functions. Here the basis functions are the individual\n",
      "classiﬁers Gm(x)∈ {−1,1}. More generally, basis function expansions take\n",
      "the form\n",
      "f(x) =M∑\n",
      "m=1βmb(x;γm), (10.3)\n",
      "where βm,m= 1,2,... ,M are the expansion coeﬃcients, and b(x;γ)∈IR\n",
      "are usually simple functions of the multivariate argument x, characterized\n",
      "by a set of parameters γ. We discuss basis expansions in some detail in\n",
      "Chapter 5.\n",
      "Additive expansions like this are at the heart of many of the learning\n",
      "techniques covered in this book:\n",
      "•In single-hidden-layer neural networks (Chapter 11), b(x;γ) =σ(γ0+\n",
      "γT\n",
      "1x), where σ(t) = 1/(1+e−t) is the sigmoid function, and γparam-\n",
      "eterizes a linear combination of the input variables.\n",
      "•In signal processing, wavelets (Section 5.9.1) are a popular choice with\n",
      "γparameterizing the location and scale shifts of a “mother” wavelet.\n",
      "•Multivariate adaptive regression splines (Section 9.4) uses truncated-\n",
      "power spline basis functions where γparameterizes the variables and\n",
      "values for the knots.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "342 10. Boosting and Additive Trees\n",
      "Algorithm 10.2 Forward Stagewise Additive Modeling.\n",
      "1. Initialize f0(x) = 0.\n",
      "2. For m= 1 to M:\n",
      "(a) Compute\n",
      "(βm,γm) = arg min\n",
      "β,γN∑\n",
      "i=1L(yi,fm−1(xi) +βb(xi;γ)).\n",
      "(b) Set fm(x) =fm−1(x) +βmb(x;γm).\n",
      "•For trees, γparameterizes the split variables and split points at the\n",
      "internal nodes, and the predictions at the terminal nodes.\n",
      "Typically these models are ﬁt by minimizing a loss function averaged\n",
      "over the training data, such as the squared-error or a likelihood-based loss\n",
      "function,\n",
      "min\n",
      "{βm,γm}M\n",
      "1N∑\n",
      "i=1L(\n",
      "yi,M∑\n",
      "m=1βmb(xi;γm))\n",
      ". (10.4)\n",
      "For many loss functions L(y,f(x)) and/or basis functions b(x;γ), this re-\n",
      "quires computationally intensive numerical optimization techniques. How-\n",
      "ever, a simple alternative often can be found when it is feasible to rapidly\n",
      "solve the subproblem of ﬁtting just a single basis function,\n",
      "min\n",
      "β,γN∑\n",
      "i=1L(yi,βb(xi;γ)). (10.5)\n",
      "10.3 Forward Stagewise Additive Modeling\n",
      "Forward stagewise modeling approximates the solution to (10.4) by sequen-\n",
      "tially adding new basis functions to the expansion without adjusting the\n",
      "parameters and coeﬃcients of those that have already been added. This is\n",
      "outlined in Algorithm 10.2. At each iteration m, one solves for the optimal\n",
      "basis function b(x;γm) and corresponding coeﬃcient βmto add to the cur-\n",
      "rent expansion fm−1(x). This produces fm(x), and the process is repeated.\n",
      "Previously added terms are not modiﬁed.\n",
      "For squared-error loss\n",
      "L(y,f(x)) = ( y−f(x))2, (10.6)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.4 Exponential Loss and AdaBoost 343\n",
      "one has\n",
      "L(yi,fm−1(xi) +βb(xi;γ)) = ( yi−fm−1(xi)−βb(xi;γ))2\n",
      "= (rim−βb(xi;γ))2, (10.7)\n",
      "where rim=yi−fm−1(xi) is simply the residual of the current model\n",
      "on the ith observation. Thus, for squared-error loss, the term βmb(x;γm)\n",
      "that best ﬁts the current residuals is added to the expansion at each step.\n",
      "This idea is the basis for “least squares” regression boosting discussed in\n",
      "Section 10.10.2. However, as we show near the end of the next section,\n",
      "squared-error loss is generally not a good choice for classiﬁcation; hence\n",
      "the need to consider other loss criteria.\n",
      "10.4 Exponential Loss and AdaBoost\n",
      "We now show that AdaBoost.M1 (Algorithm 10.1) is equivalent to forwar d\n",
      "stagewise additive modeling (Algorithm 10.2) using the loss function\n",
      "L(y,f(x)) = exp( −y f(x)). (10.8)\n",
      "The appropriateness of this criterion is addressed in the next section.\n",
      "For AdaBoost the basis functions are the individual classiﬁers Gm(x)∈\n",
      "{−1,1}. Using the exponential loss function, one must solve\n",
      "(βm,Gm) = arg min\n",
      "β,GN∑\n",
      "i=1exp[−yi(fm−1(xi) +β G(xi))]\n",
      "for the classiﬁer Gmand corresponding coeﬃcient βmto be added at each\n",
      "step. This can be expressed as\n",
      "(βm,Gm) = arg min\n",
      "β,GN∑\n",
      "i=1w(m)\n",
      "iexp(−β yiG(xi)) (10.9)\n",
      "withw(m)\n",
      "i= exp( −yifm−1(xi)). Since each w(m)\n",
      "idepends neither on β\n",
      "norG(x), it can be regarded as a weight that is applied to each observa-\n",
      "tion. This weight depends on fm−1(xi), and so the individual weight values\n",
      "change with each iteration m.\n",
      "The solution to (10.9) can be obtained in two steps. First, for any value\n",
      "ofβ >0, the solution to (10.9) for Gm(x) is\n",
      "Gm= arg min\n",
      "GN∑\n",
      "i=1w(m)\n",
      "iI(yi̸=G(xi)), (10.10)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "344 10. Boosting and Additive Trees\n",
      "which is the classiﬁer that minimizes the weighted error rate in predicting\n",
      "y. This can be easily seen by expressing the criterion in (10.9) as\n",
      "e−β≤∑\n",
      "yi=G(xi)w(m)\n",
      "i+eβ≤∑\n",
      "yi̸=G(xi)w(m)\n",
      "i,\n",
      "which in turn can be written as\n",
      "(\n",
      "eβ−e−β)\n",
      "≤N∑\n",
      "i=1w(m)\n",
      "iI(yi̸=G(xi)) +e−β≤N∑\n",
      "i=1w(m)\n",
      "i. (10.11)\n",
      "Plugging this Gminto (10.9) and solving for βone obtains\n",
      "βm=1\n",
      "2log1−errm\n",
      "errm, (10.12)\n",
      "where err mis the minimized weighted error rate\n",
      "errm=∑N\n",
      "i=1w(m)\n",
      "iI(yi̸=Gm(xi))\n",
      "∑N\n",
      "i=1w(m)\n",
      "i. (10.13)\n",
      "The approximation is then updated\n",
      "fm(x) =fm−1(x) +βmGm(x),\n",
      "which causes the weights for the next iteration to be\n",
      "w(m+1)\n",
      "i =w(m)\n",
      "i≤e−βmyiGm(xi). (10.14)\n",
      "Using the fact that −yiGm(xi) = 2≤I(yi̸=Gm(xi))−1, (10.14) becomes\n",
      "w(m+1)\n",
      "i =w(m)\n",
      "i≤eαmI(yi̸=Gm(xi))≤e−βm, (10.15)\n",
      "where αm= 2βmis the quantity deﬁned at line 2c of AdaBoost.M1 (Al-\n",
      "gorithm 10.1). The factor e−βmin (10.15) multiplies all weights by the\n",
      "same value, so it has no eﬀect. Thus (10.15) is equivalent to line 2(d) of\n",
      "Algorithm 10.1.\n",
      "One can view line 2(a) of the Adaboost.M1 algorithm as a method for\n",
      "approximately solving the minimization in (10.11) and hence (10.10). Hence\n",
      "we conclude that AdaBoost.M1 minimizes the exponential loss criterion\n",
      "(10.8) via a forward-stagewise additive modeling approach.\n",
      "Figure 10.3 shows the training-set misclassiﬁcation error rate and aver-\n",
      "age exponential loss for the simulated data problem (10.2) of Figure 10.2 .\n",
      "The training-set misclassiﬁcation error decreases to zero at around 250 it-\n",
      "erations (and remains there), but the exponential loss keeps decreasing.\n",
      "Notice also in Figure 10.2 that the test-set misclassiﬁcation error conti nues\n",
      "to improve after iteration 250. Clearly Adaboost is not optimizing tra ining-\n",
      "set misclassiﬁcation error; the exponential loss is more sensitive to cha nges\n",
      "in the estimated class probabilities.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.5 Why Exponential Loss? 345\n",
      "0 100 200 300 4000.0 0.2 0.4 0.6 0.8 1.0\n",
      "Boosting IterationsTraining Error\n",
      "Misclassification RateExponential Loss\n",
      "FIGURE 10.3. Simulated data, boosting with stumps: misclassiﬁcation error\n",
      "rate on the training set, and average exponential loss: (1/N)PN\n",
      "i=1exp(−yif(xi)).\n",
      "After about 250iterations, the misclassiﬁcation error is zero, while the expo nential\n",
      "loss continues to decrease.\n",
      "10.5 Why Exponential Loss?\n",
      "The AdaBoost.M1 algorithm was originally motivated from a very diﬀer -\n",
      "ent perspective than presented in the previous section. Its equivalence to\n",
      "forward stagewise additive modeling based on exponential loss was only\n",
      "discovered ﬁve years after its inception. By studying the properties of the\n",
      "exponential loss criterion, one can gain insight into the procedure and dis-\n",
      "cover ways it might be improved.\n",
      "The principal attraction of exponential loss in the context of additive\n",
      "modeling is computational; it leads to the simple modular reweighting Ad-\n",
      "aBoost algorithm. However, it is of interest to inquire about its stat istical\n",
      "properties. What does it estimate and how well is it being estimated? The\n",
      "ﬁrst question is answered by seeking its population minimizer.\n",
      "It is easy to show (Friedman et al., 2000) that\n",
      "f∗(x) = arg min\n",
      "f(x)EY|x(e−Y f(x)) =1\n",
      "2logPr(Y= 1|x)\n",
      "Pr(Y=−1|x), (10.16)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "346 10. Boosting and Additive Trees\n",
      "or equivalently\n",
      "Pr(Y= 1|x) =1\n",
      "1 +e−2f∗(x).\n",
      "Thus, the additive expansion produced by AdaBoost is estimating one-\n",
      "half the log-odds of P(Y= 1|x). This justiﬁes using its sign as the classiﬁ-\n",
      "cation rule in (10.1).\n",
      "Another loss criterion with the same population minimizer is the bi-\n",
      "nomial negative log-likelihood or deviance (also known as cross-entropy),\n",
      "interpreting fas the logit transform. Let\n",
      "p(x) = Pr( Y= 1|x) =ef(x)\n",
      "e−f(x)+ef(x)=1\n",
      "1 +e−2f(x)(10.17)\n",
      "and deﬁne Y′= (Y+ 1)/2∈ {0,1}. Then the binomial log-likelihood loss\n",
      "function is\n",
      "l(Y,p(x)) =Y′logp(x) + (1 −Y′)log(1 −p(x)),\n",
      "or equivalently the deviance is\n",
      "−l(Y,f(x)) = log(\n",
      "1 +e−2Y f(x))\n",
      ". (10.18)\n",
      "Since the population maximizer of log-likelihood is at the true probabilities\n",
      "p(x) = Pr( Y= 1|x), we see from (10.17) that the population minimizers of\n",
      "the deviance E Y|x[−l(Y,f(x))] and E Y|x[e−Y f(x)] are the same. Thus, using\n",
      "either criterion leads to the same solution at the population level. Note that\n",
      "e−Y fitself is not a proper log-likelihood, since it is not the logarithm of\n",
      "any probability mass function for a binary random variable Y∈ {−1,1}.\n",
      "10.6 Loss Functions and Robustness\n",
      "In this section we examine the diﬀerent loss functions for classiﬁcation and\n",
      "regression more closely, and characterize them in terms of their robustness\n",
      "to extreme data.\n",
      "Robust Loss Functions for Classiﬁcation\n",
      "Although both the exponential (10.8) and binomial deviance (10.18) yield\n",
      "the same solution when applied to the population joint distribution, the\n",
      "same is not true for ﬁnite data sets. Both criteria are monotone decreasing\n",
      "functions of the “margin” yf(x). In classiﬁcation (with a −1/1 response)\n",
      "the margin plays a role analogous to the residuals y−f(x) in regression. The\n",
      "classiﬁcation rule G(x) = sign[ f(x)] implies that observations with positive\n",
      "margin yif(xi)>0 are classiﬁed correctly whereas those with negative\n",
      "margin yif(xi)<0 are misclassiﬁed. The decision boundary is deﬁned by\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.6 Loss Functions and Robustness 347\n",
      "−2 −1 0 1 20.0 0.5 1.0 1.5 2.0 2.5 3.0Misclassification\n",
      "Exponential\n",
      "Binomial Deviance\n",
      "Squared Error\n",
      "Support VectorLoss\n",
      "y≤f\n",
      "FIGURE 10.4. Loss functions for two-class classiﬁcation. The response is\n",
      "y=±1; the prediction is f, with class prediction sign(f). The losses are\n",
      "misclassiﬁcation: I(sign( f)̸=y); exponential: exp(−yf); binomial deviance:\n",
      "log(1 + exp( −2yf)); squared error: (y−f)2; and support vector: (1−yf)+(see\n",
      "Section 12.3). Each function has been scaled so that it passes t hrough the point\n",
      "(0,1).\n",
      "f(x) = 0. The goal of the classiﬁcation algorithm is to produce positive\n",
      "margins as frequently as possible. Any loss criterion used for classiﬁcati on\n",
      "should penalize negative margins more heavily than positive ones since\n",
      "positive margin observations are already correctly classiﬁed.\n",
      "Figure 10.4 shows both the exponential (10.8) and binomial deviance\n",
      "criteria as a function of the margin y≤f(x). Also shown is misclassiﬁcation\n",
      "lossL(y,f(x)) =I(y≤f(x)<0), which gives unit penalty for negative mar-\n",
      "gin values, and no penalty at all for positive ones. Both the exponential\n",
      "and deviance loss can be viewed as monotone continuous approximations\n",
      "to misclassiﬁcation loss. They continuously penalize increasingly negative\n",
      "margin values more heavily than they reward increasingly positive ones.\n",
      "The diﬀerence between them is in degree. The penalty associated with bi-\n",
      "nomial deviance increases linearly for large increasingly negative margin,\n",
      "whereas the exponential criterion increases the inﬂuence of such observa-\n",
      "tions exponentially.\n",
      "At any point in the training process the exponential criterion concen-\n",
      "trates much more inﬂuence on observations with large negative margins.\n",
      "Binomial deviance concentrates relatively less inﬂuence on such observa-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "348 10. Boosting and Additive Trees\n",
      "tions, more evenly spreading the inﬂuence among all of the data. It is\n",
      "therefore far more robust in noisy settings where the Bayes error rate is\n",
      "not close to zero, and especially in situations where there is misspeciﬁcation\n",
      "of the class labels in the training data. The performance of AdaBoost has\n",
      "been empirically observed to dramatically degrade in such situations.\n",
      "Also shown in the ﬁgure is squared-error loss. The minimizer of the cor-\n",
      "responding risk on the population is\n",
      "f∗(x) = arg min\n",
      "f(x)EY|x(Y−f(x))2= E(Y|x) = 2≤Pr(Y= 1|x)−1.(10.19)\n",
      "As before the classiﬁcation rule is G(x) = sign[ f(x)]. Squared-error loss\n",
      "is not a good surrogate for misclassiﬁcation error. As seen in Figure 10 .4, it\n",
      "is not a monotone decreasing function of increasing margin yf(x). For mar-\n",
      "gin values yif(xi)>1 it increases quadratically, thereby placing increasing\n",
      "inﬂuence (error) on observations that are correctly classiﬁed with increas-\n",
      "ing certainty, thereby reducing the relative inﬂuence of those incorrectly\n",
      "classiﬁed yif(xi)<0. Thus, if class assignment is the goal, a monotone de-\n",
      "creasing criterion serves as a better surrogate loss function. Figure 12.4 on\n",
      "page 426 in Chapter 12 includes a modiﬁcation of quadratic loss, the “Hu-\n",
      "berized” square hinge loss (Rosset et al., 2004b), which enjoys the favorable\n",
      "properties of the binomial deviance, quadratic loss and the SVM hinge loss.\n",
      "It has the same population minimizer as the quadratic (10.19), is zero for\n",
      "y≤f(x)>1, and becomes linear for y≤f(x)<−1. Since quadratic functions\n",
      "are easier to compute with than exponentials, our experience suggests this\n",
      "to be a useful alternative to the binomial deviance.\n",
      "With K-class classiﬁcation, the response Ytakes values in the unordered\n",
      "setG={G1,... ,Gk}(see Sections 2.4 and 4.4). We now seek a classiﬁer\n",
      "G(x) taking values in G. It is suﬃcient to know the class conditional proba-\n",
      "bilities pk(x) = Pr( Y=Gk|x),k= 1,2,... ,K , for then the Bayes classiﬁer\n",
      "is\n",
      "G(x) =Gkwhere k= arg max\n",
      "ℓpℓ(x). (10.20)\n",
      "In principal, though, we need not learn the pk(x), but simply which one is\n",
      "largest. However, in data mining applications the interest is often more in\n",
      "the class probabilities pℓ(x), ℓ= 1,... ,K themselves, rather than in per-\n",
      "forming a class assignment. As in Section 4.4, the logistic model generalizes\n",
      "naturally to Kclasses,\n",
      "pk(x) =efk(x)\n",
      "∑K\n",
      "l=1efl(x), (10.21)\n",
      "which ensures that 0 ≤pk(x)≤1 and that they sum to one. Note that\n",
      "here we have Kdiﬀerent functions, one per class. There is a redundancy\n",
      "in the functions fk(x), since adding an arbitrary h(x) to each leaves the\n",
      "model unchanged. Traditionally one of them is set to zero: for example,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.6 Loss Functions and Robustness 349\n",
      "fK(x) = 0, as in (4.17). Here we prefer to retain the symmetry, and impose\n",
      "the constraint∑K\n",
      "k=1fk(x) = 0. The binomial deviance extends naturally\n",
      "to the K-class multinomial deviance loss function:\n",
      "L(y,p(x)) = −K∑\n",
      "k=1I(y=Gk)logpk(x)\n",
      "=−K∑\n",
      "k=1I(y=Gk)fk(x) + log(K∑\n",
      "ℓ=1efℓ(x))\n",
      ".(10.22)\n",
      "As in the two-class case, the criterion (10.22) penalizes incorrect predictions\n",
      "only linearly in their degree of incorrectness.\n",
      "Zhu et al. (2005) generalize the exponential loss for K-class problems.\n",
      "See Exercise 10.5 for details.\n",
      "Robust Loss Functions for Regression\n",
      "In the regression setting, analogous to the relationship between exponential\n",
      "loss and binomial log-likelihood is the relationship between squared-error\n",
      "lossL(y,f(x)) = ( y−f(x))2and absolute loss L(y,f(x)) =|y−f(x)|. The\n",
      "population solutions are f(x) = E( Y|x) for squared-error loss, and f(x) =\n",
      "median( Y|x) for absolute loss; for symmetric error distributions these are\n",
      "the same. However, on ﬁnite samples squared-error loss places much more\n",
      "emphasis on observations with large absolute residuals |yi−f(xi)|during\n",
      "the ﬁtting process. It is thus far less robust, and its performance severely\n",
      "degrades for long-tailed error distributions and especially for grossly mis-\n",
      "measured y-values (“outliers”). Other more robust criteria, such as abso-\n",
      "lute loss, perform much better in these situations. In the statistical ro -\n",
      "bustness literature, a variety of regression loss criteria have been proposed\n",
      "that provide strong resistance (if not absolute immunity) to gross outliers\n",
      "while being nearly as eﬃcient as least squares for Gaussian errors. They\n",
      "are often better than either for error distributions with moderately heavy\n",
      "tails. One such criterion is the Huber loss criterion used for M-regression\n",
      "(Huber, 1964)\n",
      "L(y,f(x)) ={\n",
      "[y−f(x)]2for|y−f(x)| ≤δ,\n",
      "2δ|y−f(x)| −δ2otherwise.(10.23)\n",
      "Figure 10.5 compares these three loss functions.\n",
      "These considerations suggest than when robustness is a concern, as is\n",
      "especially the case in data mining applications (see Section 10.7), squared-\n",
      "error loss for regression and exponential loss for classiﬁcation are not the\n",
      "best criteria from a statistical perspective. However, they both lead to the\n",
      "elegant modular boosting algorithms in the context of forward stagewis e\n",
      "additive modeling. For squared-error loss one simply ﬁts the base learner\n",
      "to the residuals from the current model yi−fm−1(xi) at each step. For\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "350 10. Boosting and Additive Trees\n",
      "−3 −2 −1 0 1 2 30 2 4 6 8Squared Error\n",
      "Absolute Error\n",
      "HuberLoss\n",
      "y−f\n",
      "FIGURE 10.5. A comparison of three loss functions for regression, plotted as a\n",
      "function of the margin y−f. The Huber loss function combines the good properties\n",
      "of squared-error loss near zero and absolute error loss when |y−f|is large.\n",
      "exponential loss one performs a weighted ﬁt of the base learner to the\n",
      "output values yi, with weights wi= exp( −yifm−1(xi)). Using other more\n",
      "robust criteria directly in their place does not give rise to such simple\n",
      "feasible boosting algorithms. However, in Section 10.10.2 we show how one\n",
      "can derive simple elegant boosting algorithms based on any diﬀerentiable\n",
      "loss criterion, thereby producing highly robust boosting procedures for data\n",
      "mining.\n",
      "10.7 “Oﬀ-the-Shelf” Procedures for Data Mining\n",
      "Predictive learning is an important aspect of data mining. As can be seen\n",
      "from this book, a wide variety of methods have been developed for predic-\n",
      "tive learning from data. For each particular method there are situations\n",
      "for which it is particularly well suited, and others where it performs badly\n",
      "compared to the best that can be done with that data. We have attempted\n",
      "to characterize appropriate situations in our discussions of each of the re-\n",
      "spective methods. However, it is seldom known in advance which procedure\n",
      "will perform best or even well for any given problem. Table 10.1 summarizes\n",
      "some of the characteristics of a number of learning methods.\n",
      "Industrial and commercial data mining applications tend to be especially\n",
      "challenging in terms of the requirements placed on learning procedures.\n",
      "Data sets are often very large in terms of number of observations and\n",
      "number of variables measured on each of them. Thus, computational con-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.7 “Oﬀ-the-Shelf” Procedures for Data Mining 351\n",
      "TABLE 10.1. Some characteristics of diﬀerent learning methods. Key: ▲= good,\n",
      "◆=fair, and ▼=poor.\n",
      "Characteristic Neural SVM Trees MARS k-NN,\n",
      "Nets Kernels\n",
      "Natural handling of data\n",
      "of “mixed” type▼ ▼ ▲ ▲ ▼\n",
      "Handling of missing values ▼ ▼ ▲ ▲ ▲\n",
      "Robustness to outliers in\n",
      "input space▼ ▼ ▲▼ ▲\n",
      "Insensitive to monotone\n",
      "transformations of inputs▼ ▼ ▲▼ ▼\n",
      "Computational scalability\n",
      "(large N)▼ ▼ ▲ ▲ ▼\n",
      "Ability to deal with irrel-\n",
      "evant inputs▼ ▼ ▲ ▲ ▼\n",
      "Ability to extract linear\n",
      "combinations of features▲ ▲ ▼ ▼ ◆\n",
      "Interpretability ▼ ▼ ◆▲ ▼\n",
      "Predictive power ▲ ▲ ▼◆ ▲\n",
      "siderations play an important role. Also, the data are usually messy : the\n",
      "inputs tend to be mixtures of quantitative, binary, and categorical vari-\n",
      "ables, the latter often with many levels. There are generally many missing\n",
      "values, complete observations being rare. Distributions of numeric predic-\n",
      "tor and response variables are often long-tailed and highly skewed. This\n",
      "is the case for the spam data (Section 9.1.2); when ﬁtting a generalized\n",
      "additive model, we ﬁrst log-transformed each of the predictors in order to\n",
      "get a reasonable ﬁt. In addition they usually contain a substantial fraction\n",
      "of gross mis-measurements (outliers). The predictor variables are generally\n",
      "measured on very diﬀerent scales.\n",
      "In data mining applications, usually only a small fraction of the large\n",
      "number of predictor variables that have been included in the analysis are\n",
      "actually relevant to prediction. Also, unlike many applications such as pat-\n",
      "tern recognition, there is seldom reliable domain knowledge to help create\n",
      "especially relevant features and/or ﬁlter out the irrelevant ones, the inclu-\n",
      "sion of which dramatically degrades the performance of many methods.\n",
      "In addition, data mining applications generally require interpretable mod-\n",
      "els. It is not enough to simply produce predictions. It is also desirable to\n",
      "have information providing qualitative understanding of the relationship\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "352 10. Boosting and Additive Trees\n",
      "between joint values of the input variables and the resulting predicted re-\n",
      "sponse value. Thus, black box methods such as neural networks, which can\n",
      "be quite useful in purely predictive settings such as pattern recognition,\n",
      "are far less useful for data mining.\n",
      "These requirements of speed, interpretability and the messy nature of\n",
      "the data sharply limit the usefulness of most learning procedures as oﬀ-\n",
      "the-shelf methods for data mining. An “oﬀ-the-shelf” method is one that\n",
      "can be directly applied to the data without requiring a great deal of time-\n",
      "consuming data preprocessing or careful tuning of the learning procedure.\n",
      "Of all the well-known learning methods, decision trees come closest to\n",
      "meeting the requirements for serving as an oﬀ-the-shelf procedure for data\n",
      "mining. They are relatively fast to construct and they produce interpretable\n",
      "models (if the trees are small). As discussed in Section 9.2, they naturally\n",
      "incorporate mixtures of numeric and categorical predictor variables and\n",
      "missing values. They are invariant under (strictly monotone) transforma-\n",
      "tions of the individual predictors. As a result, scaling and/or more general\n",
      "transformations are not an issue, and they are immune to the eﬀects of pre-\n",
      "dictor outliers. They perform internal feature selection as an integral part\n",
      "of the procedure. They are thereby resistant, if not completely immune,\n",
      "to the inclusion of many irrelevant predictor variables. These properties of\n",
      "decision trees are largely the reason that they have emerged as the most\n",
      "popular learning method for data mining.\n",
      "Trees have one aspect that prevents them from being the ideal tool for\n",
      "predictive learning, namely inaccuracy. They seldom provide predictive ac-\n",
      "curacy comparable to the best that can be achieved with the data at hand.\n",
      "As seen in Section 10.1, boosting decision trees improves their accuracy,\n",
      "often dramatically. At the same time it maintains most of their desirabl e\n",
      "properties for data mining. Some advantages of trees that are sacriﬁced by\n",
      "boosting are speed, interpretability, and, for AdaBoost, robustness against\n",
      "overlapping class distributions and especially mislabeling of the training\n",
      "data. A gradient boosted model (GBM) is a generalization of tree boosting\n",
      "that attempts to mitigate these problems, so as to produce an accurate and\n",
      "eﬀective oﬀ-the-shelf procedure for data mining.\n",
      "10.8 Example: Spam Data\n",
      "Before we go into the details of gradient boosting, we demonstrate its abi li-\n",
      "ties on a two-class classiﬁcation problem. The spam data are introduced in\n",
      "Chapter 1, and used as an example for many of the procedures in Chapter 9\n",
      "(Sections 9.1.2, 9.2.5, 9.3.1 and 9.4.1).\n",
      "Applying gradient boosting to these data resulted in a test error rate of\n",
      "4.5%, using the same test set as was used in Section 9.1.2. By comparison,\n",
      "an additive logistic regression achieved 5.5%, a CART tree fully grown and\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.9 Boosting Trees 353\n",
      "pruned by cross-validation 8.7%, and MARS 5.5%. The standard error of\n",
      "these estimates is around 0.6%, although gradient boosting is signiﬁcantly\n",
      "better than all of them using the McNemar test (Exercise 10.6).\n",
      "In Section 10.13 below we develop a relative importance measure for\n",
      "each predictor, as well as a partial dependence plot describing a predictor’s\n",
      "contribution to the ﬁtted model. We now illustrate these for the spam data.\n",
      "Figure 10.6 displays the relative importance spectrum for all 57 predictor\n",
      "variables. Clearly some predictors are more important than others in sep-\n",
      "aratingspamfromemail. The frequencies of the character strings !,$,hp,\n",
      "andremove are estimated to be the four most relevant predictor variables.\n",
      "At the other end of the spectrum, the character strings 857,415,table, and\n",
      "3dhave virtually no relevance.\n",
      "The quantity being modeled here is the log-odds of spamversusemail\n",
      "f(x) = logPr(spam|x)\n",
      "Pr(email|x)(10.24)\n",
      "(see Section 10.13 below). Figure 10.7 shows the partial dependence of the\n",
      "log-odds on selected important predictors, two positively associated with\n",
      "spam(!andremove ), and two negatively associated ( eduandhp). These\n",
      "particular dependencies are seen to be essentially monotonic. There is a\n",
      "general agreement with the corresponding functions found by the additive\n",
      "logistic regression model; see Figure 9.1 on page 303.\n",
      "Running a gradient boosted model on these data with J= 2 terminal-\n",
      "node trees produces a purely additive (main eﬀects) model for the log-\n",
      "odds, with a corresponding error rate of 4.7%, as compared to 4.5% for the\n",
      "full gradient boosted model (with J= 5 terminal-node trees). Although\n",
      "not signiﬁcant, this slightly higher error rate suggests that there may be\n",
      "interactions among some of the important predictor variables. This can\n",
      "be diagnosed through two-variable partial dependence plots. Figure 10.8\n",
      "shows one of the several such plots displaying strong interaction eﬀects.\n",
      "One sees that for very low frequencies of hp, the log-odds of spamare\n",
      "greatly increased. For high frequencies of hp, the log-odds of spamtend to\n",
      "be much lower and roughly constant as a function of !. As the frequency\n",
      "ofhpdecreases, the functional relationship with !strengthens.\n",
      "10.9 Boosting Trees\n",
      "Regression and classiﬁcation trees are discussed in detail in Section 9.2.\n",
      "They partition the space of all joint predictor variable values into disjoin t\n",
      "regions Rj,j= 1,2,... ,J , as represented by the terminal nodes of the tree.\n",
      "A constant γjis assigned to each such region and the predictive rule is\n",
      "x∈Rj⇒f(x) =γj.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "354 10. Boosting and Additive Trees\n",
      "!$hpremovefreeCAPAVEyourCAPMAXgeorgeCAPTOTeduyouourmoneywill1999businessre(receiveinternet000emailmeeting;650overmailpmpeopletechnologyhplallorderaddressmakefontprojectdataoriginalreportconferencelab[creditparts#85tablecsdirect415857telnetlabsaddresses3d\n",
      "0 20 40 60 80 100\n",
      "Relative Importance\n",
      "FIGURE 10.6. Predictor variable importance spectrum for the spamdata. The\n",
      "variable names are written on the vertical axis.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.9 Boosting Trees 355\n",
      "!Partial Dependence\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-0.2 0.0 0.2 0.4 0.6 0.8 1.0\n",
      "removePartial Dependence\n",
      "0.0 0.2 0.4 0.6-0.2 0.0 0.2 0.4 0.6 0.8 1.0\n",
      "eduPartial Dependence\n",
      "0.0 0.2 0.4 0.6 0.8 1.0-1.0 -0.6 -0.2 0.0 0.2\n",
      "hpPartial Dependence\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.0-1.0 -0.6 -0.2 0.0 0.2\n",
      "FIGURE 10.7. Partial dependence of log-odds of spamon four important pre-\n",
      "dictors. The red ticks at the base of the plots are deciles of t he input variable.\n",
      "0.51.01.52.02.53.00.20.40.60.81.0-1.0-0.5 0.0 0.5 1.0\n",
      "hp!\n",
      "FIGURE 10.8. Partial dependence of the log-odds of spamvs.email as a func-\n",
      "tion of joint frequencies of hpand the character !.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "356 10. Boosting and Additive Trees\n",
      "Thus a tree can be formally expressed as\n",
      "T(x;Θ) =J∑\n",
      "j=1γjI(x∈Rj), (10.25)\n",
      "with parameters Θ = {Rj,γj}J\n",
      "1.Jis usually treated as a meta-parameter.\n",
      "The parameters are found by minimizing the empirical risk\n",
      "ˆΘ = arg min\n",
      "ΘJ∑\n",
      "j=1∑\n",
      "xi∈RjL(yi,γj). (10.26)\n",
      "This is a formidable combinatorial optimization problem, and we usually\n",
      "settle for approximate suboptimal solutions. It is useful to divide the opti -\n",
      "mization problem into two parts:\n",
      "Finding γjgiven Rj:Given the Rj, estimating the γjis typically trivial,\n",
      "and often ˆ γj= ¯yj, the mean of the yifalling in region Rj. For mis-\n",
      "classiﬁcation loss, ˆ γjis the modal class of the observations falling in\n",
      "region Rj.\n",
      "Finding Rj:This is the diﬃcult part, for which approximate solutions are\n",
      "found. Note also that ﬁnding the Rjentails estimating the γjas well.\n",
      "A typical strategy is to use a greedy, top-down recursive partitioning\n",
      "algorithm to ﬁnd the Rj. In addition, it is sometimes necessary to\n",
      "approximate (10.26) by a smoother and more convenient criterion for\n",
      "optimizing the Rj:\n",
      "˜Θ = arg min\n",
      "ΘN∑\n",
      "i=1˜L(yi,T(xi,Θ)). (10.27)\n",
      "Then given the ˆRj=˜Rj, the γjcan be estimated more precisely\n",
      "using the original criterion.\n",
      "In Section 9.2 we described such a strategy for classiﬁcation trees. The Gini\n",
      "index replaced misclassiﬁcation loss in the growing of the tree (identifying\n",
      "theRj).\n",
      "The boosted tree model is a sum of such trees,\n",
      "fM(x) =M∑\n",
      "m=1T(x;Θm), (10.28)\n",
      "induced in a forward stagewise manner (Algorithm 10.2). At each step in\n",
      "the forward stagewise procedure one must solve\n",
      "ˆΘm= arg min\n",
      "ΘmN∑\n",
      "i=1L(yi,fm−1(xi) +T(xi;Θm)) (10.29)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.9 Boosting Trees 357\n",
      "for the region set and constants Θ m={Rjm,γjm}Jm\n",
      "1of the next tree, given\n",
      "the current model fm−1(x).\n",
      "Given the regions Rjm, ﬁnding the optimal constants γjmin each region\n",
      "is typically straightforward:\n",
      "ˆγjm= arg min\n",
      "γjm∑\n",
      "xi∈RjmL(yi,fm−1(xi) +γjm). (10.30)\n",
      "Finding the regions is diﬃcult, and even more diﬃcult than for a single\n",
      "tree. For a few special cases, the problem simpliﬁes.\n",
      "For squared-error loss, the solution to (10.29) is no harder than for a\n",
      "single tree. It is simply the regression tree that best predicts the current\n",
      "residuals yi−fm−1(xi), and ˆ γjmis the mean of these residuals in each\n",
      "corresponding region.\n",
      "For two-class classiﬁcation and exponential loss, this stagewise approac h\n",
      "gives rise to the AdaBoost method for boosting classiﬁcation trees (Algo -\n",
      "rithm 10.1). In particular, if the trees T(x;Θm) are restricted to be scaled\n",
      "classiﬁcation trees, then we showed in Section 10.4 that the solution to\n",
      "(10.29) is the tree that minimizes the weighted error rate∑N\n",
      "i=1w(m)\n",
      "iI(yi̸=\n",
      "T(xi;Θm)) with weights w(m)\n",
      "i=e−yifm−1(xi). By a scaled classiﬁcation\n",
      "tree, we mean βmT(x;Θm), with the restriction that γjm∈ {−1,1}).\n",
      "Without this restriction, (10.29) still simpliﬁes for exponential loss t o a\n",
      "weighted exponential criterion for the new tree:\n",
      "ˆΘm= arg min\n",
      "ΘmN∑\n",
      "i=1w(m)\n",
      "iexp[−yiT(xi;Θm)]. (10.31)\n",
      "It is straightforward to implement a greedy recursive-partitioning algori thm\n",
      "using this weighted exponential loss as a splitting criterion. Given the Rjm,\n",
      "one can show (Exercise 10.7) that the solution to (10.30) is the weighted\n",
      "log-odds in each corresponding region\n",
      "ˆγjm= log∑\n",
      "xi∈Rjmw(m)\n",
      "iI(yi= 1)\n",
      "∑\n",
      "xi∈Rjmw(m)\n",
      "iI(yi=−1). (10.32)\n",
      "This requires a specialized tree-growing algorithm; in practice, we prefer\n",
      "the approximation presented below that uses a weighted least squares re-\n",
      "gression tree.\n",
      "Using loss criteria such as the absolute error or the Huber loss (10.23) in\n",
      "place of squared-error loss for regression, and the deviance (10.22) in place\n",
      "of exponential loss for classiﬁcation, will serve to robustify boosting trees.\n",
      "Unfortunately, unlike their nonrobust counterparts, these robust criteria\n",
      "do not give rise to simple fast boosting algorithms.\n",
      "For more general loss criteria the solution to (10.30), given the Rjm,\n",
      "is typically straightforward since it is a simple “location” estimat e. For\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "358 10. Boosting and Additive Trees\n",
      "absolute loss it is just the median of the residuals in each respective region.\n",
      "For the other criteria fast iterative algorithms exist for solving (10 .30),\n",
      "and usually their faster “single-step” approximations are adequate. The\n",
      "problem is tree induction. Simple fast algorithms do not exist for solving\n",
      "(10.29) for these more general loss criteria, and approximations like (1 0.27)\n",
      "become essential.\n",
      "10.10 Numerical Optimization via Gradient\n",
      "Boosting\n",
      "Fast approximate algorithms for solving (10.29) with any diﬀerenti able loss\n",
      "criterion can be derived by analogy to numerical optimization. The loss in\n",
      "using f(x) to predict yon the training data is\n",
      "L(f) =N∑\n",
      "i=1L(yi,f(xi)). (10.33)\n",
      "The goal is to minimize L(f) with respect to f, where here f(x) is con-\n",
      "strained to be a sum of trees (10.28). Ignoring this constraint, minimizing\n",
      "(10.33) can be viewed as a numerical optimization\n",
      "ˆf= arg min\n",
      "fL(f), (10.34)\n",
      "where the “parameters” f∈IRNare the values of the approximating func-\n",
      "tionf(xi) at each of the Ndata points xi:\n",
      "f={f(x1),f(x2)),... ,f (xN)}.\n",
      "Numerical optimization procedures solve (10.34) as a sum of component\n",
      "vectors\n",
      "fM=M∑\n",
      "m=0hm,hm∈IRN,\n",
      "where f0=h0is an initial guess, and each successive fmis induced based\n",
      "on the current parameter vector fm−1, which is the sum of the previously\n",
      "induced updates. Numerical optimization methods diﬀer in their prescrip-\n",
      "tions for computing each increment vector hm(“step”).\n",
      "10.10.1 Steepest Descent\n",
      "Steepest descent chooses hm=−ρmgmwhere ρmis a scalar and gm∈IRN\n",
      "is the gradient of L(f) evaluated at f=fm−1. The components of the\n",
      "gradient gmare\n",
      "gim=[∂L(yi,f(xi))\n",
      "∂f(xi)]\n",
      "f(xi)=fm−1(xi)(10.35)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.10 Numerical Optimization via Gradient Boosting 359\n",
      "Thestep length ρmis the solution to\n",
      "ρm= arg min\n",
      "ρL(fm−1−ρgm). (10.36)\n",
      "The current solution is then updated\n",
      "fm=fm−1−ρmgm\n",
      "and the process repeated at the next iteration. Steepest descent can be\n",
      "viewed as a very greedy strategy, since −gmis the local direction in IRN\n",
      "for which L(f) is most rapidly decreasing at f=fm−1.\n",
      "10.10.2 Gradient Boosting\n",
      "Forward stagewise boosting (Algorithm 10.2) is also a very greedy st rategy.\n",
      "At each step the solution tree is the one that maximally reduces (10.29),\n",
      "given the current model fm−1and its ﬁts fm−1(xi). Thus, the tree predic-\n",
      "tionsT(xi;Θm) are analogous to the components of the negative gradient\n",
      "(10.35). The principal diﬀerence between them is that the tree compo-\n",
      "nentstm= (T(x1;Θm),... ,T (xN;Θm) are not independent. They are con-\n",
      "strained to be the predictions of a Jm-terminal node decision tree, whereas\n",
      "the negative gradient is the unconstrained maximal descent direction.\n",
      "The solution to (10.30) in the stagewise approach is analogous to the li ne\n",
      "search (10.36) in steepest descent. The diﬀerence is that (10.30) performs\n",
      "a separate line search for those components of tmthat correspond to each\n",
      "separate terminal region {T(xi;Θm)}xi∈Rjm.\n",
      "If minimizing loss on the training data (10.33) were the only goal, steep-\n",
      "est descent would be the preferred strategy. The gradient (10.35) is trivial\n",
      "to calculate for any diﬀerentiable loss function L(y,f(x)), whereas solving\n",
      "(10.29) is diﬃcult for the robust criteria discussed in Section 10.6. Unfor-\n",
      "tunately the gradient (10.35) is deﬁned only at the training data points xi,\n",
      "whereas the ultimate goal is to generalize fM(x) to new data not repre-\n",
      "sented in the training set.\n",
      "A possible resolution to this dilemma is to induce a tree T(x;Θm) at the\n",
      "mth iteration whose predictions tmare as close as possible to the negative\n",
      "gradient. Using squared error to measure closeness, this leads us to\n",
      "˜Θm= arg min\n",
      "ΘN∑\n",
      "i=1(−gim−T(xi;Θ))2. (10.37)\n",
      "That is, one ﬁts the tree Tto the negative gradient values (10.35) by least\n",
      "squares. As noted in Section 10.9 fast algorithms exist for least squares\n",
      "decision tree induction. Although the solution regions ˜Rjmto (10.37) will\n",
      "not be identical to the regions Rjmthat solve (10.29), it is generally sim-\n",
      "ilar enough to serve the same purpose. In any case, the forward stagewise\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "360 10. Boosting and Additive Trees\n",
      "TABLE 10.2. Gradients for commonly used loss functions.\n",
      "Setting Loss Function −∂L(yi,f(xi))/∂f(xi)\n",
      "Regression1\n",
      "2[yi−f(xi)]2yi−f(xi)\n",
      "Regression |yi−f(xi)| sign[yi−f(xi)]\n",
      "Regression Huber yi−f(xi) for|yi−f(xi)| ≤δm\n",
      "δmsign[yi−f(xi)] for|yi−f(xi)|> δm\n",
      "where δm=αth-quantile {|yi−f(xi)|}\n",
      "Classiﬁcation Deviance kth component: I(yi=Gk)−pk(xi)\n",
      "boosting procedure, and top-down decision tree induction, are themselves\n",
      "approximation procedures. After constructing the tree (10.37), the corre-\n",
      "sponding constants in each region are given by (10.30).\n",
      "Table 10.2 summarizes the gradients for commonly used loss functions.\n",
      "For squared error loss, the negative gradient is just the ordinary residual\n",
      "−gim=yi−fm−1(xi), so that (10.37) on its own is equivalent standard\n",
      "least squares boosting. With absolute error loss, the negative gradient i s\n",
      "thesignof the residual, so at each iteration (10.37) ﬁts the tree to the\n",
      "sign of the current residuals by least squares. For Huber M-regression, the\n",
      "negative gradient is a compromise between these two (see the table).\n",
      "For classiﬁcation the loss function is the multinomial deviance (10.22),\n",
      "andKleast squares trees are constructed at each iteration. Each tree Tkm\n",
      "is ﬁt to its respective negative gradient vector gkm,\n",
      "−gikm=∂L(yi,f1m(xi),... ,f 1m(xi))\n",
      "∂fkm(xi)\n",
      "=I(yi=Gk)−pk(xi), (10.38)\n",
      "withpk(x) given by (10.21). Although Kseparate trees are built at each\n",
      "iteration, they are related through (10.21). For binary classiﬁcation ( K=\n",
      "2), only one tree is needed (exercise 10.10).\n",
      "10.10.3 Implementations of Gradient Boosting\n",
      "Algorithm 10.3 presents the generic gradient tree-boosting algorithm for\n",
      "regression. Speciﬁc algorithms are obtained by inserting diﬀerent loss cri-\n",
      "teriaL(y,f(x)). The ﬁrst line of the algorithm initializes to the optimal\n",
      "constant model, which is just a single terminal node tree. The components\n",
      "of the negative gradient computed at line 2(a) are referred to as general-\n",
      "ized or pseudo residuals, r. Gradients for commonly used loss functions are\n",
      "summarized in Table 10.2.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.11 Right-Sized Trees for Boosting 361\n",
      "Algorithm 10.3 Gradient Tree Boosting Algorithm.\n",
      "1. Initialize f0(x) = arg min γ∑N\n",
      "i=1L(yi,γ).\n",
      "2. For m= 1 to M:\n",
      "(a) For i= 1,2,... ,N compute\n",
      "rim=−[∂L(yi,f(xi))\n",
      "∂f(xi)]\n",
      "f=fm−1.\n",
      "(b) Fit a regression tree to the targets rimgiving terminal regions\n",
      "Rjm, j= 1,2,... ,J m.\n",
      "(c) For j= 1,2,... ,J mcompute\n",
      "γjm= arg min\n",
      "γ∑\n",
      "xi∈RjmL(yi,fm−1(xi) +γ).\n",
      "(d) Update fm(x) =fm−1(x) +∑Jm\n",
      "j=1γjmI(x∈Rjm).\n",
      "3. Output ˆf(x) =fM(x).\n",
      "The algorithm for classiﬁcation is similar. Lines 2(a)–(d) are repeated\n",
      "Ktimes at each iteration m, once for each class using (10.38). The result\n",
      "at line 3 is Kdiﬀerent (coupled) tree expansions fkM(x),k= 1,2,... ,K .\n",
      "These produce probabilities via (10.21) or do classiﬁcation as in (10.20).\n",
      "Details are given in Exercise 10.9. Two basic tuning parameters are the\n",
      "number of iterations Mand the sizes of each of the constituent trees\n",
      "Jm, m= 1,2,... ,M .\n",
      "The original implementation of this algorithm was called MART for\n",
      "“multiple additive regression trees,” and was referred to in the ﬁrst edi-\n",
      "tion of this book. Many of the ﬁgures in this chapter were produced by\n",
      "MART. Gradient boosting as described here is implemented in the R gbm\n",
      "package (Ridgeway, 1999, “Gradient Boosted Models”), and is freely avai l-\n",
      "able. The gbmpackage is used in Section 10.14.2, and extensively in Chap-\n",
      "ters 16 and 15. Another R implementation of boosting is mboost (Hothorn\n",
      "and B¨ uhlmann, 2006). A commercial implementation of gradient boost-\n",
      "ing/MART called TreeNetis available from Salford Systems, Inc.\n",
      "10.11 Right-Sized Trees for Boosting\n",
      "Historically, boosting was considered to be a technique for combining mod-\n",
      "els, here trees. As such, the tree building algorithm was regarded as a\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "362 10. Boosting and Additive Trees\n",
      "primitive that produced models to be combined by the boosting proce-\n",
      "dure. In this scenario, the optimal size of each tree is estimated separately\n",
      "in the usual manner when it is built (Section 9.2). A very large (oversized)\n",
      "tree is ﬁrst induced, and then a bottom-up procedure is employed to prune\n",
      "it to the estimated optimal number of terminal nodes. This approach as-\n",
      "sumes implicitly that each tree is the last one in the expansion (10.28).\n",
      "Except perhaps for the very last tree, this is clearly a very poor assump-\n",
      "tion. The result is that trees tend to be much too large, especially during\n",
      "the early iterations. This substantially degrades performance and increases\n",
      "computation.\n",
      "The simplest strategy for avoiding this problem is to restrict all trees\n",
      "to be the same size, Jm=J∀m. At each iteration a J-terminal node\n",
      "regression tree is induced. Thus Jbecomes a meta-parameter of the entire\n",
      "boosting procedure, to be adjusted to maximize estimated performance for\n",
      "the data at hand.\n",
      "One can get an idea of useful values for Jby considering the properties\n",
      "of the “target” function\n",
      "η= arg min\n",
      "fEXYL(Y,f(X)). (10.39)\n",
      "Here the expected value is over the population joint distribution of ( X,Y).\n",
      "The target function η(x) is the one with minimum prediction risk on future\n",
      "data. This is the function we are trying to approximate.\n",
      "One relevant property of η(X) is the degree to which the coordinate vari-\n",
      "ables XT= (X1,X2,... ,X p) interact with one another. This is captured\n",
      "by its ANOVA (analysis of variance) expansion\n",
      "η(X) =∑\n",
      "jηj(Xj)+∑\n",
      "jkηjk(Xj,Xk)+∑\n",
      "jklηjkl(Xj,Xk,Xl)+≤≤≤.(10.40)\n",
      "The ﬁrst sum in (10.40) is over functions of only a single predictor variable\n",
      "Xj. The particular functions ηj(Xj) are those that jointly best approximate\n",
      "η(X) under the loss criterion being used. Each such ηj(Xj) is called the\n",
      "“main eﬀect” of Xj. The second sum is over those two-variable functions\n",
      "that when added to the main eﬀects best ﬁt η(X). These are called the\n",
      "second-order interactions of each respective variable pair ( Xj,Xk). The\n",
      "third sum represents third-order interactions, and so on. For many problems\n",
      "encountered in practice, low-order interaction eﬀects tend to dominate.\n",
      "When this is the case, models that produce strong higher-order interaction\n",
      "eﬀects, such as large decision trees, suﬀer in accuracy.\n",
      "The interaction level of tree-based approximations is limited by the tree\n",
      "sizeJ. Namely, no interaction eﬀects of level greater that J−1 are pos-\n",
      "sible. Since boosted models are additive in the trees (10.28), this limit\n",
      "extends to them as well. Setting J= 2 (single split “decision stump”)\n",
      "produces boosted models with only main eﬀects; no interactions are per-\n",
      "mitted. With J= 3, two-variable interaction eﬀects are also allowed, and\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.11 Right-Sized Trees for Boosting 363\n",
      "Number of TermsTest Error\n",
      "0 100 200 300 4000.0 0.1 0.2 0.3 0.4Stumps\n",
      "10 Node\n",
      "100 Node\n",
      "Adaboost\n",
      "FIGURE 10.9. Boosting with diﬀerent sized trees, applied to the example (10. 2)\n",
      "used in Figure 10.2. Since the generative model is additive, stu mps perform the\n",
      "best. The boosting algorithm used the binomial deviance loss in Algorithm 10.3;\n",
      "shown for comparison is the AdaBoost Algorithm 10.1.\n",
      "so on. This suggests that the value chosen for Jshould reﬂect the level\n",
      "of dominant interactions of η(x). This is of course generally unknown, but\n",
      "in most situations it will tend to be low. Figure 10.9 illustrates the eﬀ ect\n",
      "of interaction order (choice of J) on the simulation example (10.2). The\n",
      "generative function is additive (sum of quadratic monomials), so boosting\n",
      "models with J >2 incurs unnecessary variance and hence the higher test\n",
      "error. Figure 10.10 compares the coordinate functions found by boosted\n",
      "stumps with the true functions.\n",
      "Although in many applications J= 2 will be insuﬃcient, it is unlikely\n",
      "thatJ >10 will be required. Experience so far indicates that 4 ≤J≤8\n",
      "works well in the context of boosting, with results being fairly insensiti ve\n",
      "to particular choices in this range. One can ﬁne-tune the value for Jby\n",
      "trying several diﬀerent values and choosing the one that produces the low-\n",
      "est risk on a validation sample. However, this seldom provides signiﬁcant\n",
      "improvement over using J≃6.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "364 10. Boosting and Additive Trees\n",
      "Coordinate Functions for Additive Logistic Trees\n",
      "f1(x1) f2(x2) f3(x3) f4(x4) f5(x5)\n",
      "f6(x6) f7(x7) f8(x8) f9(x9) f10(x10)\n",
      "FIGURE 10.10. Coordinate functions estimated by boosting stumps for the sim-\n",
      "ulated example used in Figure 10.9. The true quadratic functio ns are shown for\n",
      "comparison.\n",
      "10.12 Regularization\n",
      "Besides the size of the constituent trees, J, the other meta-parameter of\n",
      "gradient boosting is the number of boosting iterations M. Each iteration\n",
      "usually reduces the training risk L(fM), so that for Mlarge enough this risk\n",
      "can be made arbitrarily small. However, ﬁtting the training data too well\n",
      "can lead to overﬁtting, which degrades the risk on future predictions. Thus,\n",
      "there is an optimal number M∗minimizing future risk that is application\n",
      "dependent. A convenient way to estimate M∗is to monitor prediction risk\n",
      "as a function of Mon a validation sample. The value of Mthat minimizes\n",
      "this risk is taken to be an estimate of M∗. This is analogous to the early\n",
      "stopping strategy often used with neural networks (Section 11.4).\n",
      "10.12.1 Shrinkage\n",
      "Controlling the value of Mis not the only possible regularization strategy.\n",
      "As with ridge regression and neural networks, shrinkage techniques can be\n",
      "employed as well (see Sections 3.4.1 and 11.5). The simplest implementation\n",
      "of shrinkage in the context of boosting is to scale the contribution of each\n",
      "tree by a factor 0 < ν < 1 when it is added to the current approximation.\n",
      "That is, line 2(d) of Algorithm 10.3 is replaced by\n",
      "fm(x) =fm−1(x) +ν≤J∑\n",
      "j=1γjmI(x∈Rjm). (10.41)\n",
      "The parameter νcan be regarded as controlling the learning rate of the\n",
      "boosting procedure. Smaller values of ν(more shrinkage) result in larger\n",
      "training risk for the same number of iterations M. Thus, both νandM\n",
      "control prediction risk on the training data. However, these parameters do\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.12 Regularization 365\n",
      "not operate independently. Smaller values of νlead to larger values of M\n",
      "for the same training risk, so that there is a tradeoﬀ between them.\n",
      "Empirically it has been found (Friedman, 2001) that smaller values of ν\n",
      "favor better test error, and require correspondingly larger values of M. In\n",
      "fact, the best strategy appears to be to set νto be very small ( ν <0.1)\n",
      "and then choose Mby early stopping. This yields dramatic improvements\n",
      "(over no shrinkage ν= 1) for regression and for probability estimation. The\n",
      "corresponding improvements in misclassiﬁcation risk via (10.20) are les s,\n",
      "but still substantial. The price paid for these improvements is computa-\n",
      "tional: smaller values of νgive rise to larger values of M, and computation\n",
      "is proportional to the latter. However, as seen below, many iterations ar e\n",
      "generally computationally feasible even on very large data sets. This is\n",
      "partly due to the fact that small trees are induced at each step with no\n",
      "pruning.\n",
      "Figure 10.11 shows test error curves for the simulated example (10.2) of\n",
      "Figure 10.2. A gradient boosted model (MART) was trained using binomial\n",
      "deviance, using either stumps or six terminal-node trees, and with or with-\n",
      "out shrinkage. The beneﬁts of shrinkage are evident, especially when the\n",
      "binomial deviance is tracked. With shrinkage, each test error curve reaches\n",
      "a lower value, and stays there for many iterations.\n",
      "Section 16.2.1 draws a connection between forward stagewise shrinkage\n",
      "in boosting and the use of an L1penalty for regularizing model parame-\n",
      "ters (the “lasso”). We argue that L1penalties may be superior to the L2\n",
      "penalties used by methods such as the support vector machine.\n",
      "10.12.2 Subsampling\n",
      "We saw in Section 8.7 that bootstrap averaging (bagging) improves the\n",
      "performance of a noisy classiﬁer through averaging. Chapter 15 discusses\n",
      "in some detail the variance-reduction mechanism of this sampling followed\n",
      "by averaging. We can exploit the same device in gradient boosting, both\n",
      "to improve performance and computational eﬃciency.\n",
      "Withstochastic gradient boosting (Friedman, 1999), at each iteration we\n",
      "sample a fraction ηof the training observations (without replacement),\n",
      "and grow the next tree using that subsample. The rest of the algorithm is\n",
      "identical. A typical value for ηcan be1\n",
      "2, although for large N,ηcan be\n",
      "substantially smaller than1\n",
      "2.\n",
      "Not only does the sampling reduce the computing time by the same\n",
      "fraction η, but in many cases it actually produces a more accurate model.\n",
      "Figure 10.12 illustrates the eﬀect of subsampling using the simulated\n",
      "example (10.2), both as a classiﬁcation and as a regression example. We\n",
      "see in both cases that sampling along with shrinkage slightly outperform ed\n",
      "the rest. It appears here that subsampling without shrinkage does poorly.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "366 10. Boosting and Additive Trees\n",
      "Boosting IterationsTest Set Deviance\n",
      "0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage\n",
      "Shrinkage=0.2Stumps\n",
      "Deviance\n",
      "Boosting IterationsTest Set Misclassification Error\n",
      "0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage\n",
      "Shrinkage=0.2Stumps\n",
      "Misclassification Error\n",
      "Boosting IterationsTest Set Deviance\n",
      "0 500 1000 1500 20000.0 0.5 1.0 1.5 2.0No shrinkage\n",
      "Shrinkage=0.66-Node Trees\n",
      "Deviance\n",
      "Boosting IterationsTest Set Misclassification Error\n",
      "0 500 1000 1500 20000.0 0.1 0.2 0.3 0.4 0.5No shrinkage\n",
      "Shrinkage=0.66-Node Trees\n",
      "Misclassification Error\n",
      "FIGURE 10.11. Test error curves for simulated example (10.2) of Figure 10.9 ,\n",
      "using gradient boosting (MART). The models were trained using bino mial de-\n",
      "viance, either stumps or six terminal-node trees, and with or wit hout shrinkage.\n",
      "The left panels report test deviance, while the right panels sho w misclassiﬁcation\n",
      "error. The beneﬁcial eﬀect of shrinkage can be seen in all cases, especially for\n",
      "deviance in the left panels.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.13 Interpretation 367\n",
      "0 200 400 600 800 10000.4 0.6 0.8 1.0 1.2 1.4\n",
      "Boosting IterationsTest Set DevianceDeviance4−Node Trees\n",
      "0 200 400 600 800 10000.30 0.35 0.40 0.45 0.50\n",
      "Boosting IterationsTest Set Absolute ErrorNo shrinkage\n",
      "Shrink=0.1\n",
      "Sample=0.5\n",
      "Shrink=0.1 Sample=0.5Absolute Error\n",
      "FIGURE 10.12. Test-error curves for the simulated example (10.2), showing\n",
      "the eﬀect of stochasticity. For the curves labeled “Sample = 0.5”, a diﬀerent 50%\n",
      "subsample of the training data was used each time a tree was grow n. In the left\n",
      "panel the models were ﬁt by gbmusing a binomial deviance loss function; in the\n",
      "right-hand panel using square-error loss.\n",
      "The downside is that we now have four parameters to set: J,M,νand\n",
      "η. Typically some early explorations determine suitable values for J,νand\n",
      "η, leaving Mas the primary parameter.\n",
      "10.13 Interpretation\n",
      "Single decision trees are highly interpretable. The entire model can be com-\n",
      "pletely represented by a simple two-dimensional graphic (binary tree) that\n",
      "is easily visualized. Linear combinations of trees (10.28) lose this import ant\n",
      "feature, and must therefore be interpreted in a diﬀerent way.\n",
      "10.13.1 Relative Importance of Predictor Variables\n",
      "In data mining applications the input predictor variables are seldom equally\n",
      "relevant. Often only a few of them have substantial inﬂuence on the re-\n",
      "sponse; the vast majority are irrelevant and could just as well have not\n",
      "been included. It is often useful to learn the relative importance or contri-\n",
      "bution of each input variable in predicting the response.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "368 10. Boosting and Additive Trees\n",
      "For a single decision tree T, Breiman et al. (1984) proposed\n",
      "I2\n",
      "ℓ(T) =J−1∑\n",
      "t=1ˆı2\n",
      "tI(v(t) =ℓ) (10.42)\n",
      "as a measure of relevance for each predictor variable Xℓ. The sum is over\n",
      "theJ−1 internal nodes of the tree. At each such node t, one of the input\n",
      "variables Xv(t)is used to partition the region associated with that node into\n",
      "two subregions; within each a separate constant is ﬁt to the response values.\n",
      "The particular variable chosen is the one that gives maximal estimated\n",
      "improvement ˆ ı2\n",
      "tin squared error risk over that for a constant ﬁt over the\n",
      "entire region. The squared relative importance of variable Xℓis the sum of\n",
      "such squared improvements over all internal nodes for which it was chosen\n",
      "as the splitting variable.\n",
      "This importance measure is easily generalized to additive tree expansions\n",
      "(10.28); it is simply averaged over the trees\n",
      "I2\n",
      "ℓ=1\n",
      "MM∑\n",
      "m=1I2\n",
      "ℓ(Tm). (10.43)\n",
      "Due to the stabilizing eﬀect of averaging, this measure turns out to be more\n",
      "reliable than is its counterpart (10.42) for a single tree. Also, because of\n",
      "shrinkage (Section 10.12.1) the masking of important variables by other s\n",
      "with which they are highly correlated is much less of a problem. Note\n",
      "that (10.42) and (10.43) refer to squared relevance; the actual relevances\n",
      "are their respective square roots. Since these measures are relative, it is\n",
      "customary to assign the largest a value of 100 and then scale the others\n",
      "accordingly. Figure 10.6 shows the relevant importance of the 57 inputs in\n",
      "predicting spamversusemail.\n",
      "ForK-class classiﬁcation, Kseparate models fk(x),k= 1,2,... ,K are\n",
      "induced, each consisting of a sum of trees\n",
      "fk(x) =M∑\n",
      "m=1Tkm(x). (10.44)\n",
      "In this case (10.43) generalizes to\n",
      "I2\n",
      "ℓk=1\n",
      "MM∑\n",
      "m=1I2\n",
      "ℓ(Tkm). (10.45)\n",
      "HereIℓkis the relevance of Xℓin separating the class kobservations from\n",
      "the other classes. The overall relevance of Xℓis obtained by averaging over\n",
      "all of the classes\n",
      "I2\n",
      "ℓ=1\n",
      "KK∑\n",
      "k=1I2\n",
      "ℓk. (10.46)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.13 Interpretation 369\n",
      "Figures 10.23 and 10.24 illustrate the use of these averaged and separate\n",
      "relative importances.\n",
      "10.13.2 Partial Dependence Plots\n",
      "After the most relevant variables have been identiﬁed, the next step is to\n",
      "attempt to understand the nature of the dependence of the approximation\n",
      "f(X) on their joint values. Graphical renderings of the f(X) as a function\n",
      "of its arguments provides a comprehensive summary of its dependence on\n",
      "the joint values of the input variables.\n",
      "Unfortunately, such visualization is limited to low-dimensional views.\n",
      "We can easily display functions of one or two arguments, either continuous\n",
      "or discrete (or mixed), in a variety of diﬀerent ways; this book is ﬁlled\n",
      "with such displays. Functions of slightly higher dimensions can be plotted\n",
      "by conditioning on particular sets of values of all but one or two of the\n",
      "arguments, producing a trellis of plots (Becker et al., 1996).1\n",
      "For more than two or three variables, viewing functions of the corre-\n",
      "sponding higher-dimensional arguments is more diﬃcult. A useful alterna-\n",
      "tive can sometimes be to view a collection of plots, each one of which shows\n",
      "the partial dependence of the approximation f(X) on a selected small sub-\n",
      "set of the input variables. Although such a collection can seldom provide a\n",
      "comprehensive depiction of the approximation, it can often produce helpful\n",
      "clues, especially when f(x) is dominated by low-order interactions (10.40).\n",
      "Consider the subvector XSofℓ < pof the input predictor variables XT=\n",
      "(X1,X2,... ,X p), indexed by S ⊂ { 1,2,... ,p }. LetCbe the complement\n",
      "set, with S ∪ C ={1,2,... ,p }. A general function f(X) will in principle\n",
      "depend on all of the input variables: f(X) =f(XS,XC). One way to deﬁne\n",
      "the average or partial dependence of f(X) onXSis\n",
      "fS(XS) = E XCf(XS,XC). (10.47)\n",
      "This is a marginal average of f, and can serve as a useful description of the\n",
      "eﬀect of the chosen subset on f(X) when, for example, the variables in XS\n",
      "do not have strong interactions with those in XC.\n",
      "Partial dependence functions can be used to interpret the results of any\n",
      "“black box” learning method. They can be estimated by\n",
      "¯fS(XS) =1\n",
      "NN∑\n",
      "i=1f(XS,xiC), (10.48)\n",
      "where {x1C,x2C,... ,x NC}are the values of XCoccurring in the training\n",
      "data. This requires a pass over the data for each set of joint values of XSfor\n",
      "which ¯fS(XS) is to be evaluated. This can be computationally intensive,\n",
      "1lattice in R.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "370 10. Boosting and Additive Trees\n",
      "even for moderately sized data sets. Fortunately with decision trees, ¯fS(XS)\n",
      "(10.48) can be rapidly computed from the tree itself without reference to\n",
      "the data (Exercise 10.11).\n",
      "It is important to note that partial dependence functions deﬁned in\n",
      "(10.47) represent the eﬀect of XSonf(X) after accounting for the (av-\n",
      "erage) eﬀects of the other variables XConf(X). They are notthe eﬀect\n",
      "ofXSonf(X)ignoring the eﬀects of XC. The latter is given by the con-\n",
      "ditional expectation\n",
      "˜fS(XS) = E( f(XS,XC)|XS), (10.49)\n",
      "and is the best least squares approximation to f(X) by a function of XS\n",
      "alone. The quantities ˜fS(XS) and ¯fS(XS) will be the same only in the\n",
      "unlikely event that XSandXCare independent. For example, if the eﬀect\n",
      "of the chosen variable subset happens to be purely additive,\n",
      "f(X) =h1(XS) +h2(XC). (10.50)\n",
      "Then (10.47) produces the h1(XS) up to an additive constant. If the eﬀect\n",
      "is purely multiplicative,\n",
      "f(X) =h1(XS)≤h2(XC), (10.51)\n",
      "then (10.47) produces h1(XS) up to a multiplicative constant factor. On\n",
      "the other hand, (10.49) will not produce h1(XS) in either case. In fact,\n",
      "(10.49) can produce strong eﬀects on variable subsets for which f(X) has\n",
      "no dependence at all.\n",
      "Viewing plots of the partial dependence of the boosted-tree approxima-\n",
      "tion (10.28) on selected variables subsets can help to provide a qualitative\n",
      "description of its properties. Illustrations are shown in Sections 10.8 and\n",
      "10.14. Owing to the limitations of computer graphics, and human percep-\n",
      "tion, the size of the subsets XSmust be small ( l≈1,2,3). There are of\n",
      "course a large number of such subsets, but only those chosen from among\n",
      "the usually much smaller set of highly relevant predictors are likely to be\n",
      "informative. Also, those subsets whose eﬀect on f(X) is approximately\n",
      "additive (10.50) or multiplicative (10.51) will be most revealing.\n",
      "ForK-class classiﬁcation, there are Kseparate models (10.44), one for\n",
      "each class. Each one is related to the respective probabilities (10.21) thro ugh\n",
      "fk(X) = log pk(X)−1\n",
      "KK∑\n",
      "l=1logpl(X). (10.52)\n",
      "Thus each fk(X) is a monotone increasing function of its respective prob-\n",
      "ability on a logarithmic scale. Partial dependence plots of each respective\n",
      "fk(X) (10.44) on its most relevant predictors (10.45) can help reveal how\n",
      "the log-odds of realizing that class depend on the respective input variables.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.14 Illustrations 371\n",
      "10.14 Illustrations\n",
      "In this section we illustrate gradient boosting on a number of larger data sets,\n",
      "using diﬀerent loss functions as appropriate.\n",
      "10.14.1 California Housing\n",
      "This data set (Pace and Barry, 1997) is available from the Carnegie-Mellon\n",
      "StatLib repository2. It consists of aggregated data from each of 20,460\n",
      "neighborhoods (1990 census block groups) in California. The response vari-\n",
      "ableYis the median house value in each neighborhood measured in units of\n",
      "$100,000. The predictor variables are demographics such as median income\n",
      "MedInc , housing density as reﬂected by the number of houses House, and the\n",
      "average occupancy in each house AveOccup . Also included as predictors are\n",
      "the location of each neighborhood ( longitude andlatitude ), and several\n",
      "quantities reﬂecting the properties of the houses in the neighborhood: av-\n",
      "erage number of rooms AveRooms and bedrooms AveBedrms . There are thus\n",
      "a total of eight predictors, all numeric.\n",
      "We ﬁt a gradient boosting model using the MART procedure, with J= 6\n",
      "terminal nodes, a learning rate (10.41) of ν= 0.1, and the Huber loss\n",
      "criterion for predicting the numeric response. We randomly divided the\n",
      "dataset into a training set (80%) and a test set (20%).\n",
      "Figure 10.13 shows the average absolute error\n",
      "AAE = E|y−ˆfM(x)| (10.53)\n",
      "as a function for number of iterations Mon both the training data and test\n",
      "data. The test error is seen to decrease monotonically with increasing M,\n",
      "more rapidly during the early stages and then leveling oﬀ to being nearly\n",
      "constant as iterations increase. Thus, the choice of a particular value of M\n",
      "is not critical, as long as it is not too small. This tends to be the case in\n",
      "many applications. The shrinkage strategy (10.41) tends to eliminate the\n",
      "problem of overﬁtting, especially for larger data sets.\n",
      "The value of AAE after 800 iterations is 0.31. This can be compared to\n",
      "that of the optimal constant predictor median {yi}which is 0.89. In terms of\n",
      "more familiar quantities, the squared multiple correlation coeﬃcient of t his\n",
      "model is R2= 0.84. Pace and Barry (1997) use a sophisticated spatial auto-\n",
      "regression procedure, where prediction for each neighborhood is based on\n",
      "median house values in nearby neighborhoods, using the other predictors as\n",
      "covariates. Experimenting with transformations they achieved R2= 0.85,\n",
      "predicting log Y. Using log Yas the response the corresponding value for\n",
      "gradient boosting was R2= 0.86.\n",
      "2http://lib.stat.cmu.edu.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "372 10. Boosting and Additive Trees\n",
      "0 200 400 600 8000.0 0.2 0.4 0.6 0.8\n",
      "Iterations MAbsolute ErrorTraining and Test Absolute Error\n",
      "Train Error\n",
      "Test Error\n",
      "FIGURE 10.13. Average-absolute error as a function of number of iterations\n",
      "for the California housing data.\n",
      "Figure 10.14 displays the relative variable importances for each of the\n",
      "eight predictor variables. Not surprisingly, median income in the neigh-\n",
      "borhood is the most relevant predictor. Longitude, latitude, and average\n",
      "occupancy all have roughly half the relevance of income, whereas the others\n",
      "are somewhat less inﬂuential.\n",
      "Figure 10.15 shows single-variable partial dependence plots on the most\n",
      "relevant nonlocation predictors. Note that the plots are not strictly smoot h.\n",
      "This is a consequence of using tree-based models. Decision trees produce\n",
      "discontinuous piecewise constant models (10.25). This carries over to sums\n",
      "of trees (10.28), with of course many more pieces. Unlike most of the meth-\n",
      "ods discussed in this book, there is no smoothness constraint imposed on\n",
      "the result. Arbitrarily sharp discontinuities can be modeled. The fact that\n",
      "these curves generally exhibit a smooth trend is because that is what is\n",
      "estimated to best predict the response for this problem. This is often the\n",
      "case.\n",
      "The hash marks at the base of each plot delineate the deciles of the\n",
      "data distribution of the corresponding variables. Note that here the data\n",
      "density is lower near the edges, especially for larger values. This causes the\n",
      "curves to be somewhat less well determined in those regions. The vertical\n",
      "scales of the plots are the same, and give a visual comparison of the relativ e\n",
      "importance of the diﬀerent variables.\n",
      "The partial dependence of median house value on median income is\n",
      "monotonic increasing, being nearly linear over the main body of data. House\n",
      "value is generally monotonic decreasing with increasing average occupancy,\n",
      "except perhaps for average occupancy rates less than one. Median house\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.14 Illustrations 373\n",
      "MedIncLongitudeAveOccupLatitudeHouseAgeAveRoomsAveBedrmsPopulation\n",
      "0 20 40 60 80 100\n",
      "Relative importance\n",
      "FIGURE 10.14. Relative importance of the predictors for the California hous ing\n",
      "data.\n",
      "value has a nonmonotonic partial dependence on average number of rooms.\n",
      "It has a minimum at approximately three rooms and is increasing both for\n",
      "smaller and larger values.\n",
      "Median house value is seen to have a very weak partial dependence on\n",
      "house age that is inconsistent with its importance ranking (Figure 10.14) .\n",
      "This suggests that this weak main eﬀect may be masking stronger interac-\n",
      "tion eﬀects with other variables. Figure 10.16 shows the two-variable part ial\n",
      "dependence of housing value on joint values of median age and average oc-\n",
      "cupancy. An interaction between these two variables is apparent. For values\n",
      "of average occupancy greater than two, house value is nearly independent\n",
      "of median age, whereas for values less than two there is a strong dependence\n",
      "on age.\n",
      "Figure 10.17 shows the two-variable partial dependence of the ﬁtted\n",
      "model on joint values of longitude and latitude, displayed as a shaded\n",
      "contour plot. There is clearly a very strong dependence of median house\n",
      "value on the neighborhood location in California. Note that Figure 10. 17 is\n",
      "nota plot of house value versus location ignoring the eﬀects of the other\n",
      "predictors (10.49). Like all partial dependence plots, it represents the eﬀect\n",
      "of location after accounting for the eﬀects of the other neighborhood and\n",
      "house attributes (10.47). It can be viewed as representing an extra premium\n",
      "one pays for location. This premium is seen to be relatively large near the\n",
      "Paciﬁc coast especially in the Bay Area and Los Angeles–San Diego re-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "374 10. Boosting and Additive Trees\n",
      "MedIncPartial Dependence\n",
      "2 4 6 8 10-0.5 0.0 0.5 1.0 1.5 2.0\n",
      "AveOccupPartial Dependence\n",
      "2 3 4 5-1.0 -0.5 0.0 0.5 1.0 1.5\n",
      "HouseAgePartial Dependence\n",
      "10 20 30 40 50-1.0 -0.5 0.0 0.5 1.0\n",
      "AveRoomsPartial Dependence\n",
      "4 6 8 10-1.0 -0.5 0.0 0.5 1.0 1.5\n",
      "FIGURE 10.15. Partial dependence of housing value on the nonlocation vari-\n",
      "ables for the California housing data. The red ticks at the base of the plot are\n",
      "deciles of the input variables.\n",
      "2\n",
      "3\n",
      "4\n",
      "510203040500.00.51.0\n",
      "AveOccupHouseAge\n",
      "FIGURE 10.16. Partial dependence of house value on median age and aver-\n",
      "age occupancy. There appears to be a strong interaction eﬀect be tween these two\n",
      "variables.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.14 Illustrations 375\n",
      "−124 −122 −120 −118 −116 −11434 36 38 40 42\n",
      "LongitudeLatitude\n",
      "−1.0−0.5 0.0 0.5 1.0\n",
      "FIGURE 10.17. Partial dependence of median house value on location in Cal-\n",
      "ifornia. One unit is $100,000, at1990prices, and the values plotted are relative\n",
      "to the overall median of $180,000.\n",
      "gions. In the northern, central valley, and southeastern desert regions of\n",
      "California, location costs considerably less.\n",
      "10.14.2 New Zealand Fish\n",
      "Plant and animal ecologists use regression models to predict species pres-\n",
      "ence, abundance and richness as a function of environmental variables.\n",
      "Although for many years simple linear and parametric models were popu-\n",
      "lar, recent literature shows increasing interest in more sophisticated mod-\n",
      "els such as generalized additive models (Section 9.1, GAM), multivariate\n",
      "adaptive regression splines (Section 9.4, MARS) and boosted regression\n",
      "trees (Leathwick et al., 2005; Leathwick et al., 2006). Here we model the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "376 10. Boosting and Additive Trees\n",
      "presence and abundance of the Black Oreo Dory , a marine ﬁsh found in the\n",
      "oceanic waters around New Zealand.3\n",
      "Figure 10.18 shows the locations of 17,000 trawls (deep-water net ﬁshing,\n",
      "with a maximum depth of 2km), and the red points indicate those 2353\n",
      "trawls for which the Black Oreo was present, one of over a hundred species\n",
      "regularly recorded. The catch size in kg for each species was recorded for\n",
      "each trawl. Along with the species catch, a number of environmental mea-\n",
      "surements are available for each trawl. These include the average depth of\n",
      "the trawl ( AvgDepth ), and the temperature and salinity of the water. Since\n",
      "the latter two are strongly correlated with depth, Leathwick et al. (2006)\n",
      "derived instead TempResid andSalResid , the residuals obtained when these\n",
      "two measures are adjusted for depth (via separate non-parametric regres-\n",
      "sions).SSTGrad is a measure of the gradient of the sea surface temperature,\n",
      "andChlais a broad indicator of ecosytem productivity via satellite-image\n",
      "measurements. SusPartMatter provides a measure of suspended particulate\n",
      "matter, particularly in coastal waters, and is also satellite derived.\n",
      "The goal of this analysis is to estimate the probability of ﬁnding Black\n",
      "Oreo in a trawl, as well as the expected catch size, standardized to take\n",
      "into account the eﬀects of variation in trawl speed and distance, as well\n",
      "as the mesh size of the trawl net. The authors used logistic regression\n",
      "for estimating the probability. For the catch size, it might seem natural\n",
      "to assume a Poisson distribution and model the log of the mean count,\n",
      "but this is often not appropriate because of the excessive number of zeros.\n",
      "Although specialized approaches have been developed, such as the zero-\n",
      "inﬂated Poisson (Lambert, 1992), they chose a simpler approach. If Yis\n",
      "the (non-negative) catch size,\n",
      "E(Y|X) = E( Y|Y >0,X)≤Pr(Y >0|X). (10.54)\n",
      "The second term is estimated by the logistic regression, and the ﬁrst term\n",
      "can be estimated using only the 2353 trawls with a positive catch.\n",
      "For the logistic regression the authors used a gradient boosted model\n",
      "(GBM)4with binomial deviance loss function, depth-10 trees, and a shrink-\n",
      "age factor ν= 0.025. For the positive-catch regression, they modeled\n",
      "log(Y) using a GBM with squared-error loss (also depth-10 trees, but\n",
      "ν= 0.01), and un-logged the predictions. In both cases they used 10-fold\n",
      "cross-validation for selecting the number of terms, as well as the shrinkage\n",
      "factor.\n",
      "3The models, data, and maps shown here were kindly provided by Dr John Leathwick\n",
      "of the National Institute of Water and Atmospheric Research in New Zealand, and Dr\n",
      "Jane Elith, School of Botany, University of Melbourne. The co llection of the research\n",
      "trawl data took place from 1979–2005, and was funded by the Ne w Zealand Ministry of\n",
      "Fisheries.\n",
      "4Version 1.5-7 of package gbmin R, ver. 2.2.0.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.14 Illustrations 377\n",
      "FIGURE 10.18. Map of New Zealand and its surrounding exclusive economic\n",
      "zone, showing the locations of 17,000 trawls (small blue dots) t aken between 1979\n",
      "and 2005. The red points indicate trawls for which the species Black Oreo Dory\n",
      "were present.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "378 10. Boosting and Additive Trees\n",
      "0 500 1000 15000.24 0.26 0.28 0.30 0.32 0.34\n",
      "Number of TreesMean DevianceGBM Test\n",
      "GBM CV\n",
      "GAM Test\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\n",
      "SpecificitySensitivity\n",
      "AUC\n",
      "GAM 0.97\n",
      "GBM 0.98\n",
      "FIGURE 10.19. The left panel shows the mean deviance as a function of the\n",
      "number of trees for the GBM logistic regression model ﬁt to the p resence/absence\n",
      "data. Shown are 10-fold cross-validation on the training data ( and1×s.e. bars),\n",
      "and test deviance on the test data. Also shown for comparison is the test deviance\n",
      "using a GAM model with 8d ffor each term. The right panel shows ROC curves\n",
      "on the test data for the chosen GBM model (vertical line in left plot) and the\n",
      "GAM model.\n",
      "Figure 10.19 (left panel) shows the mean binomial deviance for the se-\n",
      "quence of GBM models, both for 10-fold CV and test data. There is a mod-\n",
      "est improvement over the performance of a GAM model, ﬁt using smoothing\n",
      "splines with 8 degrees-of-freedom (df) per term. The right panel shows the\n",
      "ROC curves (see Section 9.2.5) for both models, which measures predictive\n",
      "performance. From this point of view, the performance looks very simi-\n",
      "lar, with GBM perhaps having a slight edge as summarized by the AUC\n",
      "(area under the curve). At the point of equal sensitivity/speciﬁcity, GBM\n",
      "achieves 91%, and GAM 90%.\n",
      "Figure 10.20 summarizes the contributions of the variables in the logistic\n",
      "GBM ﬁt. We see that there is a well-deﬁned depth range over which Black\n",
      "Oreo are caught, with much more frequent capture in colder waters. We do\n",
      "not give details of the quantitative catch model; the important variabl es\n",
      "were much the same.\n",
      "All the predictors used in these models are available on a ﬁne geographi-\n",
      "cal grid; in fact they were derived from environmental atlases, satellite im -\n",
      "ages and the like—see Leathwick et al. (2006) for details. This also means\n",
      "that predictions can be made on this grid, and imported into GIS mapping\n",
      "systems. Figure 10.21 shows prediction maps for both presence and catch\n",
      "size, with both standardized to a common set of trawl conditions; since the\n",
      "predictors vary in a continuous fashion with geographical location, so do\n",
      "the predictions.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.14 Illustrations 379\n",
      "OrbVelSpeedDistanceDisOrgMatterCodendSizePentadeTidalCurrSlopeChlaCase2SSTGradSalResidSusPartMatterAvgDepthTempResid\n",
      "Relative influence0 10 25 −4 0 2 4 6−7 −5 −3 −1\n",
      "TempResidf(TempResid)\n",
      "0 500 1000 2000−6 −4 −2\n",
      "AvgDepthf(AvgDepth)\n",
      "0 5 10 15−7 −5 −3\n",
      "SusPartMatterf(SusPartMatter)\n",
      "−0.8 −0.4 0.0 0.4−7 −5 −3 −1\n",
      "SalResidf(SalResid)\n",
      "0.00 0.05 0.10 0.15−7 −5 −3 −1\n",
      "SSTGradf(SSTGrad)\n",
      "FIGURE 10.20. The top-left panel shows the relative inﬂuence computed from\n",
      "the GBM logistic regression model. The remaining panels show th e partial de-\n",
      "pendence plots for the leading ﬁve variables, all plotted on the s ame scale for\n",
      "comparison.\n",
      "Because of their ability to model interactions and automatically select\n",
      "variables, as well as robustness to outliers and missing data, GBM models\n",
      "are rapidly gaining popularity in this data-rich and enthusiastic community .\n",
      "10.14.3 Demographics Data\n",
      "In this section we illustrate gradient boosting on a multiclass classiﬁca -\n",
      "tion problem, using MART. The data come from 9243 questionnaires ﬁlled\n",
      "out by shopping mall customers in the San Francisco Bay Area (Impact\n",
      "Resources, Inc., Columbus, OH). Among the questions are 14 concerning\n",
      "demographics. For this illustration the goal is to predict occupation us-\n",
      "ing the other 13 variables as predictors, and hence identify demographic\n",
      "variables that discriminate between diﬀerent occupational categories. We\n",
      "randomly divided the data into a training set (80%) and test set (20%),\n",
      "and used J= 6 node trees with a learning rate ν= 0.1.\n",
      "Figure 10.22 shows the K= 9 occupation class values along with their\n",
      "corresponding error rates. The overall error rate is 42.5%, which can be\n",
      "compared to the null rate of 69% obtained by predicting the most numerous\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "380 10. Boosting and Additive Trees\n",
      "FIGURE 10.21. Geological prediction maps of the presence probability (lef t\n",
      "map) and catch size (right map) obtained from the gradient boost ed models.\n",
      "classProf/Man (Professional/Managerial). The four best predicted classes\n",
      "are seen to be Retired ,Student ,Prof/Man , andHomemaker .\n",
      "Figure 10.23 shows the relative predictor variable importances as aver-\n",
      "aged over all classes (10.46). Figure 10.24 displays the individual relati ve\n",
      "importance distributions (10.45) for each of the four best predicted classes.\n",
      "One sees that the most relevant predictors are generally diﬀerent for each\n",
      "respective class. An exception is agewhich is among the three most relevant\n",
      "for predicting Retired ,Student , andProf/Man .\n",
      "Figure 10.25 shows the partial dependence of the log-odds (10.52) on age\n",
      "for these three classes. The abscissa values are ordered codes for respective\n",
      "equally spaced age intervals. One sees that after accounting for the contri-\n",
      "butions of the other variables, the odds of being retired are higher for older\n",
      "people, whereas the opposite is the case for being a student. The odds of\n",
      "being professional/managerial are highest for middle-aged people. These\n",
      "results are of course not surprising. They illustrate that inspecting partial\n",
      "dependences separately for each class can lead to sensible results.\n",
      "Bibliographic Notes\n",
      "Schapire (1990) developed the ﬁrst simple boosting procedure in the PAC\n",
      "learning framework (Valiant, 1984; Kearns and Vazirani, 1994). Schapire\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.14 Illustrations 381\n",
      "SalesUnemployedMilitaryClericalLaborHomemakerProf/ManRetiredStudent\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "Error RateOverall Error Rate = 0.425\n",
      "FIGURE 10.22. Error rate for each occupation in the demographics data.\n",
      "ageincomeeduhsld-statmar-dlincsexethnicmar-stattyp-homelangnum-hsldchildrenyrs-BA\n",
      "0 20 40 60 80 100\n",
      "Relative Importance\n",
      "FIGURE 10.23. Relative importance of the predictors as averaged over all\n",
      "classes for the demographics data.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "382 10. Boosting and Additive Trees\n",
      "agemar-dlincsexethnicincomehsld-statmar-statlangtyp-homechildrenedunum-hsldyrs-BA\n",
      "0 20 40 60 80 100\n",
      "Relative ImportanceClass =  Retired\n",
      "hsld-statageincomemar-stateduethnicnum-hsldtyp-homesexmar-dlinclangyrs-BAchildren\n",
      "0 20 40 60 80 100\n",
      "Relative ImportanceClass =  Student\n",
      "eduincomeagemar-dlincethnichsld-stattyp-homesexnum-hsldlangmar-statyrs-BAchildren\n",
      "0 20 40 60 80 100\n",
      "Relative ImportanceClass =  Prof/Man\n",
      "sexmar-dlincchildrenethnicnum-hsldedumar-statlangtyp-homeincomeagehsld-statyrs-BA\n",
      "0 20 40 60 80 100\n",
      "Relative ImportanceClass =  Homemaker\n",
      "FIGURE 10.24. Predictor variable importances separately for each of the fo ur\n",
      "classes with lowest error rate for the demographics data.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "10.14 Illustrations 383\n",
      "agePartial Dependence\n",
      "1 2 3 4 5 6 70 1 2 3 4Retired\n",
      "agePartial Dependence\n",
      "1 2 3 4 5 6 7-2 -1 0 1 2Student\n",
      "agePartial Dependence\n",
      "1 2 3 4 5 6 7-2 -1 0 1 2Prof/Man\n",
      "FIGURE 10.25. Partial dependence of the odds of three diﬀerent occupations\n",
      "on age, for the demographics data.\n",
      "showed that a weak learner could always improve its performance by train-\n",
      "ing two additional classiﬁers on ﬁltered versions of the input data stream.\n",
      "A weak learner is an algorithm for producing a two-class classiﬁer with\n",
      "performance guaranteed (with high probability) to be signiﬁcantly better\n",
      "than a coin-ﬂip. After learning an initial classiﬁer G1on the ﬁrst Ntraining\n",
      "points,\n",
      "•G2is learned on a new sample of Npoints, half of which are misclas-\n",
      "siﬁed by G1;\n",
      "•G3is learned on Npoints for which G1andG2disagree;\n",
      "•the boosted classiﬁer is GB=majority vote (G1,G2,G3).\n",
      "Schapire’s “Strength of Weak Learnability” theorem proves that GBhas\n",
      "improved performance over G1.\n",
      "Freund (1995) proposed a “boost by majority” variation which combined\n",
      "many weak learners simultaneously and improved the performance of the\n",
      "simple boosting algorithm of Schapire. The theory supporting both of these\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "384 10. Boosting and Additive Trees\n",
      "algorithms requires the weak learner to produce a classiﬁer with a ﬁxed\n",
      "error rate. This led to the more adaptive and realistic AdaBoost (Freund\n",
      "and Schapire, 1996a) and its oﬀspring, where this assumption was dropped.\n",
      "Freund and Schapire (1996a) and Schapire and Singer (1999) provide\n",
      "some theory to support their algorithms, in the form of upper bounds on\n",
      "generalization error. This theory has evolved in the computational learning\n",
      "community, initially based on the concepts of PAC learning. Other theo-\n",
      "ries attempting to explain boosting come from game theory (Freund and\n",
      "Schapire, 1996b; Breiman, 1999; Breiman, 1998), and VC theory (Schapire\n",
      "et al., 1998). The bounds and the theory associated with the AdaBoost\n",
      "algorithms are interesting, but tend to be too loose to be of practical im-\n",
      "portance. In practice, boosting achieves results far more impressive than\n",
      "the bounds would imply. Schapire (2002) and Meir and R¨ atsch (2003) give\n",
      "useful overviews more recent than the ﬁrst edition of this book.\n",
      "Friedman et al. (2000) and Friedman (2001) form the basis for our expo-\n",
      "sition in this chapter. Friedman et al. (2000) analyze AdaBoost statist ically,\n",
      "derive the exponential criterion, and show that it estimates the log-odds\n",
      "of the class probability. They propose additive tree models, the right-sized\n",
      "trees and ANOVA representation of Section 10.11, and the multiclass logit\n",
      "formulation. Friedman (2001) developed gradient boosting and shrinkage\n",
      "for classiﬁcation and regression, while Friedman (1999) explored stochast ic\n",
      "variants of boosting. Mason et al. (2000) also embraced a gradient appro ach\n",
      "to boosting. As the published discussions of Friedman et al. (2000) shows,\n",
      "there is some controversy about how and why boosting works.\n",
      "Since the publication of the ﬁrst edition of this book, these debates have\n",
      "continued, and spread into the statistical community with a series of papers\n",
      "on consistency of boosting (Jiang, 2004; Lugosi and Vayatis, 2004; Zhang\n",
      "and Yu, 2005; Bartlett and Traskin, 2007). Mease and Wyner (2008),\n",
      "through a series of simulation examples, challenge some of our interpre-\n",
      "tations of boosting; our response (Friedman et al., 2008a) puts most of\n",
      "these objections to rest. A recent survey by B¨ uhlmann and Hothorn (2007)\n",
      "supports our approach to boosting.\n",
      "Exercises\n",
      "Ex. 10.1 Derive expression (10.12) for the update parameter in AdaBoost.\n",
      "Ex. 10.2 Prove result (10.16), that is, the minimizer of the population\n",
      "version of the AdaBoost criterion, is one-half of the log odds.\n",
      "Ex. 10.3 Show that the marginal average (10.47) recovers additive and\n",
      "multiplicative functions (10.50) and (10.51), while the conditional expec-\n",
      "tation (10.49) does not.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 385\n",
      "Ex. 10.4\n",
      "(a) Write a program implementing AdaBoost with trees.\n",
      "(b) Redo the computations for the example of Figure 10.2. Plot the train-\n",
      "ing error as well as test error, and discuss its behavior.\n",
      "(c) Investigate the number of iterations needed to make the test error\n",
      "ﬁnally start to rise.\n",
      "(d) Change the setup of this example as follows: deﬁne two classes, with\n",
      "the features in Class 1 being X1,X2,... ,X 10, standard indepen-\n",
      "dent Gaussian variates. In Class 2, the features X1,X2,... ,X 10are\n",
      "also standard independent Gaussian, but conditioned on the event∑\n",
      "jX2\n",
      "j>12. Now the classes have signiﬁcant overlap in feature space.\n",
      "Repeat the AdaBoost experiments as in Figure 10.2 and discuss the\n",
      "results.\n",
      "Ex. 10.5 Multiclass exponential loss (Zhu et al., 2005). For a K-class clas-\n",
      "siﬁcation problem, consider the coding Y= (Y1,... ,Y K)Twith\n",
      "Yk={1, ifG=Gk\n",
      "−1\n",
      "K−1,otherwise .(10.55)\n",
      "Letf= (f1,... ,f K)Twith∑K\n",
      "k=1fk= 0, and deﬁne\n",
      "L(Y,f) = exp(\n",
      "−1\n",
      "KYTf)\n",
      ". (10.56)\n",
      "(a) Using Lagrange multipliers, derive the population minimizer f∗of\n",
      "E(Y,f), subject to the zero-sum constraint, and relate these to the\n",
      "class probabilities.\n",
      "(b) Show that a multiclass boosting using this loss function leads to a\n",
      "reweighting algorithm similar to Adaboost, as in Section 10.4.\n",
      "Ex. 10.6 McNemar test (Agresti, 1996). We report the test error rates on\n",
      "the spam data to be 5.5% for a generalized additive model (GAM), and\n",
      "4.5% for gradient boosting (GBM), with a test sample of size 1536.\n",
      "(a) Show that the standard error of these estimates is about 0.6%.\n",
      "Since the same test data are used for both methods, the error rates are\n",
      "correlated, and we cannot perform a two-sample t-test. We can compare\n",
      "the methods directly on each test observation, leading to the summary\n",
      "GBM\n",
      "GAM Correct Error\n",
      "Correct 1434 18\n",
      "Error 33 51\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "386 10. Boosting and Additive Trees\n",
      "The McNemar test focuses on the discordant errors, 33 vs. 18.\n",
      "(b) Conduct a test to show that GAM makes signiﬁcantly more errors\n",
      "than gradient boosting, with a two-sided p-value of 0 .036.\n",
      "Ex. 10.7 Derive expression (10.32).\n",
      "Ex. 10.8 Consider a K-class problem where the targets yikare coded as\n",
      "1 if observation iis in class kand zero otherwise. Suppose we have a\n",
      "current model fk(x), k= 1,... ,K , with∑K\n",
      "k=1fk(x) = 0 (see (10.21) in\n",
      "Section 10.6). We wish to update the model for observations in a region R\n",
      "in predictor space, by adding constants fk(x) +γk, with γK= 0.\n",
      "(a) Write down the multinomial log-likelihood for this problem, and its\n",
      "ﬁrst and second derivatives.\n",
      "(b) Using only the diagonal of the Hessian matrix in (1), and starting\n",
      "fromγk= 0∀k, show that a one-step approximate Newton update\n",
      "forγkis\n",
      "γ1\n",
      "k=∑\n",
      "xi∈R(yik−pik)∑\n",
      "xi∈Rpik(1−pik), k= 1,... ,K −1, (10.57)\n",
      "where pik= exp( fk(xi))/(∑K\n",
      "ℓ=1fℓ(xi)).\n",
      "(c) We prefer our update to sum to zero, as the current model does. Using\n",
      "symmetry arguments, show that\n",
      "ˆγk=K−1\n",
      "K(γ1\n",
      "k−1\n",
      "KK∑\n",
      "ℓ=1γ1\n",
      "ℓ), k= 1,... ,K (10.58)\n",
      "is an appropriate update, where γ1\n",
      "kis deﬁned as in (10.57) for all\n",
      "k= 1,... ,K .\n",
      "Ex. 10.9 Consider a K-class problem where the targets yikare coded as\n",
      "1 if observation iis in class kand zero otherwise. Using the multinomial\n",
      "deviance loss function (10.22) and the symmetric logistic transform, use\n",
      "the arguments leading to the gradient boosting Algorithm 10.3 to derive\n",
      "Algorithm 10.4. Hint: See exercise 10.8 for step 2(b)iii.\n",
      "Ex. 10.10 Show that for K= 2 class classiﬁcation, only one tree needs to\n",
      "be grown at each gradient-boosting iteration.\n",
      "Ex. 10.11 Show how to compute the partial dependence function fS(XS)\n",
      "in (10.47) eﬃciently.\n",
      "Ex. 10.12 Referring to (10.49), let S={1}andC={2}, with f(X1,X2) =\n",
      "X1. Assume X1andX2are bivariate Gaussian, each with mean zero, vari-\n",
      "ance one, and E( X1,X2) =ρ. Show that E(f(X1,X2|X2) =ρX2, even\n",
      "though fis not a function of X2.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 387\n",
      "Algorithm 10.4 Gradient Boosting for K-class Classiﬁcation.\n",
      "1. Initialize fk0(x) = 0, k= 1,2,... ,K .\n",
      "2. For m=1 to M:\n",
      "(a) Set\n",
      "pk(x) =efk(x)\n",
      "∑K\n",
      "ℓ=1efℓ(x), k= 1,2,... ,K.\n",
      "(b) For k= 1 to K:\n",
      "i. Compute rikm=yik−pk(xi), i= 1,2,... ,N .\n",
      "ii. Fit a regression tree to the targets rikm, i= 1,2,... ,N ,\n",
      "giving terminal regions Rjkm, j= 1,2,... ,J m.\n",
      "iii. Compute\n",
      "γjkm=K−1\n",
      "K∑\n",
      "xi∈Rjkmrikm∑\n",
      "xi∈Rjkm|rikm|(1− |rikm|), j= 1,2,... ,J m.\n",
      "iv. Update fkm(x) =fk,m−1(x) +∑Jm\n",
      "j=1γjkmI(x∈Rjkm).\n",
      "3. Output ˆfk(x) =fkM(x), k= 1,2,... ,K .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "388 10. Boosting and Additive Trees\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 389\n",
      "Printer: Opaque this\n",
      "11\n",
      "Neural Networks\n",
      "11.1 Introduction\n",
      "In this chapter we describe a class of learning methods that was developed\n",
      "separately in diﬀerent ﬁelds—statistics and artiﬁcial intelligence—based\n",
      "on essentially identical models. The central idea is to extract linear com-\n",
      "binations of the inputs as derived features, and then model the target as\n",
      "a nonlinear function of these features. The result is a powerful learning\n",
      "method, with widespread applications in many ﬁelds. We ﬁrst discuss the\n",
      "projection pursuit model, which evolved in the domain of semiparamet-\n",
      "ric statistics and smoothing. The rest of the chapter is devoted to neural\n",
      "network models.\n",
      "11.2 Projection Pursuit Regression\n",
      "As in our generic supervised learning problem, assume we have an input\n",
      "vector Xwithpcomponents, and a target Y. Letωm, m= 1,2,... ,M, be\n",
      "unitp-vectors of unknown parameters. The projection pursuit regression\n",
      "(PPR) model has the form\n",
      "f(X) =M∑\n",
      "m=1gm(ωT\n",
      "mX). (11.1)\n",
      "This is an additive model, but in the derived features Vm=ωT\n",
      "mXrather\n",
      "than the inputs themselves. The functions gmare unspeciﬁed and are esti-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "390 Neural Networks\n",
      "g(V)\n",
      "X1X2g(V)\n",
      "X1X2\n",
      "FIGURE 11.1. Perspective plots of two ridge functions.\n",
      "(Left:) g(V) = 1/[1 + exp( −5(V−0.5))], where V= (X1+X2)/√\n",
      "2.\n",
      "(Right:) g(V) = (V+ 0.1) sin(1 /(V/3 + 0.1)), where V=X1.\n",
      "mated along with the directions ωmusing some ﬂexible smoothing method\n",
      "(see below).\n",
      "The function gm(ωT\n",
      "mX) is called a ridge function in IRp. It varies only\n",
      "in the direction deﬁned by the vector ωm. The scalar variable Vm=ωT\n",
      "mX\n",
      "is the projection of Xonto the unit vector ωm, and we seek ωmso that\n",
      "the model ﬁts well, hence the name “projection pursuit.” Figure 11.1 shows\n",
      "some examples of ridge functions. In the example on the left ω= (1/√\n",
      "2)(1,1)T,\n",
      "so that the function only varies in the direction X1+X2. In the example\n",
      "on the right, ω= (1,0).\n",
      "The PPR model (11.1) is very general, since the operation of forming\n",
      "nonlinear functions of linear combinations generates a surprisingly large\n",
      "class of models. For example, the product X1≤X2can be written as [( X1+\n",
      "X2)2−(X1−X2)2]/4, and higher-order products can be represented simi-\n",
      "larly.\n",
      "In fact, if Mis taken arbitrarily large, for appropriate choice of gmthe\n",
      "PPR model can approximate any continuous function in IRparbitrarily\n",
      "well. Such a class of models is called a universal approximator . However\n",
      "this generality comes at a price. Interpretation of the ﬁtted model is usually\n",
      "diﬃcult, because each input enters into the model in a complex and multi-\n",
      "faceted way. As a result, the PPR model is most useful for prediction, and\n",
      "not very useful for producing an understandable model for the data. The\n",
      "M= 1 model, known as the single index model in econometrics, is an\n",
      "exception. It is slightly more general than the linear regression model, and\n",
      "oﬀers a similar interpretation.\n",
      "How do we ﬁt a PPR model, given training data ( xi,yi),i= 1,2,... ,N ?\n",
      "We seek the approximate minimizers of the error function\n",
      "N∑\n",
      "i=1[\n",
      "yi−M∑\n",
      "m=1gm(ωT\n",
      "mxi)]2\n",
      "(11.2)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.2 Projection Pursuit Regression 391\n",
      "over functions gmand direction vectors ωm,m= 1,2,... ,M . As in other\n",
      "smoothing problems, we need either explicitly or implicitly to impose com-\n",
      "plexity constraints on the gm, to avoid overﬁt solutions.\n",
      "Consider just one term ( M= 1, and drop the subscript). Given the\n",
      "direction vector ω, we form the derived variables vi=ωTxi. Then we have\n",
      "a one-dimensional smoothing problem, and we can apply any scatterplot\n",
      "smoother, such as a smoothing spline, to obtain an estimate of g.\n",
      "On the other hand, given g, we want to minimize (11.2) over ω. A Gauss–\n",
      "Newton search is convenient for this task. This is a quasi-Newton method,\n",
      "in which the part of the Hessian involving the second derivative of gis\n",
      "discarded. It can be simply derived as follows. Let ωoldbe the current\n",
      "estimate for ω. We write\n",
      "g(ωTxi)≈g(ωT\n",
      "oldxi) +g′(ωT\n",
      "oldxi)(ω−ωold)Txi (11.3)\n",
      "to give\n",
      "N∑\n",
      "i=1[\n",
      "yi−g(ωTxi)]2≈N∑\n",
      "i=1g′(ωT\n",
      "oldxi)2[(\n",
      "ωT\n",
      "oldxi+yi−g(ωT\n",
      "oldxi)\n",
      "g′(ωT\n",
      "oldxi))\n",
      "−ωTxi]2\n",
      ".\n",
      "(11.4)\n",
      "To minimize the right-hand side, we carry out a least squares regression\n",
      "with target ωT\n",
      "oldxi+(yi−g(ωT\n",
      "oldxi))/g′(ωT\n",
      "oldxi) on the input xi, with weights\n",
      "g′(ωT\n",
      "oldxi)2and no intercept (bias) term. This produces the updated coef-\n",
      "ﬁcient vector ωnew.\n",
      "These two steps, estimation of gandω, are iterated until convergence.\n",
      "With more than one term in the PPR model, the model is built in a forward\n",
      "stage-wise manner, adding a pair ( ωm,gm) at each stage.\n",
      "There are a number of implementation details.\n",
      "•Although any smoothing method can in principle be used, it is conve-\n",
      "nient if the method provides derivatives. Local regression and smooth-\n",
      "ing splines are convenient.\n",
      "•After each step the gm’s from previous steps can be readjusted using\n",
      "the backﬁtting procedure described in Chapter 9. While this may\n",
      "lead ultimately to fewer terms, it is not clear whether it improves\n",
      "prediction performance.\n",
      "•Usually the ωmare not readjusted (partly to avoid excessive compu-\n",
      "tation), although in principle they could be as well.\n",
      "•The number of terms Mis usually estimated as part of the forward\n",
      "stage-wise strategy. The model building stops when the next term\n",
      "does not appreciably improve the ﬁt of the model. Cross-validation\n",
      "can also be used to determine M.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "392 Neural Networks\n",
      "There are many other applications, such as density estimation (Friedman\n",
      "et al., 1984; Friedman, 1987), where the projection pursuit idea can be used.\n",
      "In particular, see the discussion of ICA in Section 14.7 and its relationship\n",
      "with exploratory projection pursuit. However the projection pursuit re-\n",
      "gression model has not been widely used in the ﬁeld of statistics, perhaps\n",
      "because at the time of its introduction (1981), its computational demands\n",
      "exceeded the capabilities of most readily available computers. But it does\n",
      "represent an important intellectual advance, one that has blossomed in its\n",
      "reincarnation in the ﬁeld of neural networks, the topic of the rest of this\n",
      "chapter.\n",
      "11.3 Neural Networks\n",
      "The term neural network has evolved to encompass a large class of models\n",
      "and learning methods. Here we describe the most widely used “vanilla” neu-\n",
      "ral net, sometimes called the single hidden layer back-propagation network,\n",
      "or single layer perceptron. There has been a great deal of hypesurrounding\n",
      "neural networks, making them seem magical and mysterious. As we make\n",
      "clear in this section, they are just nonlinear statistical models, much like\n",
      "the projection pursuit regression model discussed above.\n",
      "A neural network is a two-stage regression or classiﬁcation model, typ-\n",
      "ically represented by a network diagram as in Figure 11.2. This network\n",
      "applies both to regression or classiﬁcation. For regression, typically K= 1\n",
      "and there is only one output unit Y1at the top. However, these networks\n",
      "can handle multiple quantitative responses in a seamless fashion, so we will\n",
      "deal with the general case.\n",
      "ForK-class classiﬁcation, there are Kunits at the top, with the kth\n",
      "unit modeling the probability of class k. There are Ktarget measurements\n",
      "Yk, k= 1,... ,K , each being coded as a 0 −1 variable for the kth class.\n",
      "Derived features Zmare created from linear combinations of the inputs,\n",
      "and then the target Ykis modeled as a function of linear combinations of\n",
      "theZm,\n",
      "Zm=σ(α0m+αT\n",
      "mX), m= 1,... ,M,\n",
      "Tk=β0k+βT\n",
      "kZ, k= 1,... ,K,\n",
      "fk(X) =gk(T), k= 1,... ,K,(11.5)\n",
      "where Z= (Z1,Z2,... ,Z M), and T= (T1,T2,... ,T K).\n",
      "The activation function σ(v) is usually chosen to be the sigmoid σ(v) =\n",
      "1/(1 +e−v); see Figure 11.3 for a plot of 1 /(1 +e−v). Sometimes Gaussian\n",
      "radial basis functions (Chapter 6) are used for the σ(v), producing what is\n",
      "known as a radial basis function network .\n",
      "Neural network diagrams like Figure 11.2 are sometimes drawn with an\n",
      "additional biasunit feeding into every unit in the hidden and output layers.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.3 Neural Networks 393\n",
      " Y  Y Y 2 1 K\n",
      " Z  Z  Z1 Z2 3 m\n",
      " X  X Z  Z1 Z2 3\n",
      "1  Xp  X p-1  X2  X3M\n",
      " X p-1 3 X 2 X 1p Z Y  Y Y\n",
      " XK 1 2\n",
      "                                                                                                                                                /0/0/0\n",
      "/1/1/1\n",
      "/0/0/0\n",
      "/1/1/1\n",
      "/0/0/0\n",
      "/1/1/1\n",
      "/0/0/0\n",
      "/1/1/1\n",
      "/0/0/0\n",
      "/1/1/1\n",
      "/0/0/0\n",
      "/1/1/1\n",
      "FIGURE 11.2. Schematic of a single hidden layer, feed-forward neural network .\n",
      "Thinking of the constant “1” as an additional input feature, this bias unit\n",
      "captures the intercepts α0mandβ0kin model (11.5).\n",
      "The output function gk(T) allows a ﬁnal transformation of the vector of\n",
      "outputs T. For regression we typically choose the identity function gk(T) =\n",
      "Tk. Early work in K-class classiﬁcation also used the identity function, but\n",
      "this was later abandoned in favor of the softmax function\n",
      "gk(T) =eTk\n",
      "∑K\n",
      "ℓ=1eTℓ. (11.6)\n",
      "This is of course exactly the transformation used in the multilogit model\n",
      "(Section 4.4), and produces positive estimates that sum to one. In Sec-\n",
      "tion 4.2 we discuss other problems with linear activation functions, in par-\n",
      "ticular potentially severe masking eﬀects.\n",
      "The units in the middle of the network, computing the derived features\n",
      "Zm, are called hidden units because the values Zmare not directly ob-\n",
      "served. In general there can be more than one hidden layer, as illustrated\n",
      "in the example at the end of this chapter. We can think of the Zmas a\n",
      "basis expansion of the original inputs X; the neural network is then a stan-\n",
      "dard linear model, or linear multilogit model, using these transformations\n",
      "as inputs. There is, however, an important enhancement over the basis-\n",
      "expansion techniques discussed in Chapter 5; here the parameters of the\n",
      "basis functions are learned from the data.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "394 Neural Networks\n",
      "-10 -5 0 5 100.0 0.5 1.01/(1 +e−v)\n",
      "v\n",
      "FIGURE 11.3. Plot of the sigmoid function σ(v) = 1/(1+exp( −v))(red curve),\n",
      "commonly used in the hidden layer of a neural network. Included ar eσ(sv)for\n",
      "s=1\n",
      "2(blue curve) and s= 10(purple curve). The scale parameter scontrols\n",
      "the activation rate, and we can see that large samounts to a hard activation at\n",
      "v= 0. Note that σ(s(v−v0))shifts the activation threshold from 0tov0.\n",
      "Notice that if σis the identity function, then the entire model collapses\n",
      "to a linear model in the inputs. Hence a neural network can be thought of\n",
      "as a nonlinear generalization of the linear model, both for regression and\n",
      "classiﬁcation. By introducing the nonlinear transformation σ, it greatly\n",
      "enlarges the class of linear models. In Figure 11.3 we see that the rate of\n",
      "activation of the sigmoid depends on the norm of αm, and if ∥αm∥is very\n",
      "small, the unit will indeed be operating in the linear part of its activation\n",
      "function.\n",
      "Notice also that the neural network model with one hidden layer has\n",
      "exactly the same form as the projection pursuit model described above.\n",
      "The diﬀerence is that the PPR model uses nonparametric functions gm(v),\n",
      "while the neural network uses a far simpler function based on σ(v), with\n",
      "three free parameters in its argument. In detail, viewing the neural network\n",
      "model as a PPR model, we identify\n",
      "gm(ωT\n",
      "mX) = βmσ(α0m+αT\n",
      "mX)\n",
      "=βmσ(α0m+∥αm∥(ωT\n",
      "mX)), (11.7)\n",
      "where ωm=αm/∥αm∥is the mth unit-vector. Since σβ,α0,s(v) =βσ(α0+\n",
      "sv) has lower complexity than a more general nonparametric g(v), it is not\n",
      "surprising that a neural network might use 20 or 100 such functions, while\n",
      "the PPR model typically uses fewer terms ( M= 5 or 10, for example).\n",
      "Finally, we note that the name “neural networks” derives from the fact\n",
      "that they were ﬁrst developed as models for the human brain. Each unit\n",
      "represents a neuron, and the connections (links in Figure 11.2) represent\n",
      "synapses. In early models, the neurons ﬁred when the total signal passed to\n",
      "that unit exceeded a certain threshold. In the model above, this corresponds\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.4 Fitting Neural Networks 395\n",
      "to use of a step function for σ(Z) and gm(T). Later the neural network was\n",
      "recognized as a useful tool for nonlinear statistical modeling, and for this\n",
      "purpose the step function is not smooth enough for optimization. Hence the\n",
      "step function was replaced by a smoother threshold function, the sigmoid\n",
      "in Figure 11.3.\n",
      "11.4 Fitting Neural Networks\n",
      "The neural network model has unknown parameters, often called weights ,\n",
      "and we seek values for them that make the model ﬁt the training data well.\n",
      "We denote the complete set of weights by θ, which consists of\n",
      "{α0m,αm;m= 1,2,... ,M }M(p+ 1) weights ,\n",
      "{β0k,βk;k= 1,2,... ,K }K(M+ 1) weights .(11.8)\n",
      "For regression, we use sum-of-squared errors as our measure of ﬁt (error\n",
      "function)\n",
      "R(θ) =K∑\n",
      "k=1N∑\n",
      "i=1(yik−fk(xi))2. (11.9)\n",
      "For classiﬁcation we use either squared error or cross-entropy (deviance):\n",
      "R(θ) =−N∑\n",
      "i=1K∑\n",
      "k=1yiklogfk(xi), (11.10)\n",
      "and the corresponding classiﬁer is G(x) = argmaxkfk(x). With the softmax\n",
      "activation function and the cross-entropy error function, the neural network\n",
      "model is exactly a linear logistic regression model in the hidden units, and\n",
      "all the parameters are estimated by maximum likelihood.\n",
      "Typically we don’t want the global minimizer of R(θ), as this is likely\n",
      "to be an overﬁt solution. Instead some regularization is needed: this is\n",
      "achieved directly through a penalty term, or indirectly by early stopping.\n",
      "Details are given in the next section.\n",
      "The generic approach to minimizing R(θ) is by gradient descent, called\n",
      "back-propagation in this setting. Because of the compositional form of the\n",
      "model, the gradient can be easily derived using the chain rule for diﬀeren-\n",
      "tiation. This can be computed by a forward and backward sweep over the\n",
      "network, keeping track only of quantities local to each unit.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "396 Neural Networks\n",
      "Here is back-propagation in detail for squared error loss. Let zmi=\n",
      "σ(α0m+αT\n",
      "mxi), from (11.5) and let zi= (z1i,z2i,... ,z Mi). Then we have\n",
      "R(θ)≡N∑\n",
      "i=1Ri\n",
      "=N∑\n",
      "i=1K∑\n",
      "k=1(yik−fk(xi))2, (11.11)\n",
      "with derivatives\n",
      "∂Ri\n",
      "∂βkm=−2(yik−fk(xi))g′\n",
      "k(βT\n",
      "kzi)zmi,\n",
      "∂Ri\n",
      "∂αmℓ=−K∑\n",
      "k=12(yik−fk(xi))g′\n",
      "k(βT\n",
      "kzi)βkmσ′(αT\n",
      "mxi)xiℓ.(11.12)\n",
      "Given these derivatives, a gradient descent update at the ( r+ 1)st iter-\n",
      "ation has the form\n",
      "β(r+1)\n",
      "km=β(r)\n",
      "km−γrN∑\n",
      "i=1∂Ri\n",
      "∂β(r)\n",
      "km,\n",
      "α(r+1)\n",
      "mℓ=α(r)\n",
      "mℓ−γrN∑\n",
      "i=1∂Ri\n",
      "∂α(r)\n",
      "mℓ,(11.13)\n",
      "where γris the learning rate , discussed below.\n",
      "Now write (11.12) as\n",
      "∂Ri\n",
      "∂βkm=δkizmi,\n",
      "∂Ri\n",
      "∂αmℓ=smixiℓ.(11.14)\n",
      "The quantities δkiandsmiare “errors” from the current model at the\n",
      "output and hidden layer units, respectively. From their deﬁnitions, these\n",
      "errors satisfy\n",
      "smi=σ′(αT\n",
      "mxi)K∑\n",
      "k=1βkmδki, (11.15)\n",
      "known as the back-propagation equations . Using this, the updates in (11.13)\n",
      "can be implemented with a two-pass algorithm. In the forward pass , the\n",
      "current weights are ﬁxed and the predicted values ˆfk(xi) are computed\n",
      "from formula (11.5). In the backward pass , the errors δkiare computed,\n",
      "and then back-propagated via (11.15) to give the errors smi. Both sets of\n",
      "errors are then used to compute the gradients for the updates in (11.13),\n",
      "via (11.14).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.5 Some Issues in Training Neural Networks 397\n",
      "This two-pass procedure is what is known as back-propagation. It has\n",
      "also been called the delta rule (Widrow and Hoﬀ, 1960). The computational\n",
      "components for cross-entropy have the same form as those for the sum of\n",
      "squares error function, and are derived in Exercise 11.3.\n",
      "The advantages of back-propagation are its simple, local nature. In the\n",
      "back propagation algorithm, each hidden unit passes and receives infor-\n",
      "mation only to and from units that share a connection. Hence it can be\n",
      "implemented eﬃciently on a parallel architecture computer.\n",
      "The updates in (11.13) are a kind of batch learning , with the parame-\n",
      "ter updates being a sum over all of the training cases. Learning can also\n",
      "be carried out online—processing each observation one at a time, updat-\n",
      "ing the gradient after each training case, and cycling through the training\n",
      "cases many times. In this case, the sums in equations (11.13) are replaced\n",
      "by a single summand. A training epoch refers to one sweep through the\n",
      "entire training set. Online training allows the network to handle very large\n",
      "training sets, and also to update the weights as new observations come in.\n",
      "The learning rate γrfor batch learning is usually taken to be a con-\n",
      "stant, and can also be optimized by a line search that minimizes the error\n",
      "function at each update. With online learning γrshould decrease to zero\n",
      "as the iteration r→ ∞. This learning is a form of stochastic approxima-\n",
      "tion(Robbins and Munro, 1951); results in this ﬁeld ensure convergence if\n",
      "γr→0,∑\n",
      "rγr=∞, and∑\n",
      "rγ2\n",
      "r<∞(satisﬁed, for example, by γr= 1/r).\n",
      "Back-propagation can be very slow, and for that reason is usually not\n",
      "the method of choice. Second-order techniques such as Newton’s method\n",
      "are not attractive here, because the second derivative matrix of R(the\n",
      "Hessian) can be very large. Better approaches to ﬁtting include conjugate\n",
      "gradients and variable metric methods. These avoid explicit computation\n",
      "of the second derivative matrix while still providing faster convergence.\n",
      "11.5 Some Issues in Training Neural Networks\n",
      "There is quite an art in training neural networks. The model is generally\n",
      "overparametrized, and the optimization problem is nonconvex and unstable\n",
      "unless certain guidelines are followed. In this section we summarize some\n",
      "of the important issues.\n",
      "11.5.1 Starting Values\n",
      "Note that if the weights are near zero, then the operative part of the sigmoid\n",
      "(Figure 11.3) is roughly linear, and hence the neural network collapses into\n",
      "an approximately linear model (Exercise 11.2). Usually starting values fo r\n",
      "weights are chosen to be random values near zero. Hence the model starts\n",
      "out nearly linear, and becomes nonlinear as the weights increase. Individual\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "398 Neural Networks\n",
      "units localize to directions and introduce nonlinearities where needed. Use\n",
      "of exact zero weights leads to zero derivatives and perfect symmetry, and\n",
      "the algorithm never moves. Starting instead with large weights often leads\n",
      "to poor solutions.\n",
      "11.5.2 Overﬁtting\n",
      "Often neural networks have too many weights and will overﬁt the data at\n",
      "the global minimum of R. In early developments of neural networks, either\n",
      "by design or by accident, an early stopping rule was used to avoid over-\n",
      "ﬁtting. Here we train the model only for a while, and stop well before we\n",
      "approach the global minimum. Since the weights start at a highly regular-\n",
      "ized (linear) solution, this has the eﬀect of shrinking the ﬁnal model toward\n",
      "a linear model. A validation dataset is useful for determining when to stop,\n",
      "since we expect the validation error to start increasing.\n",
      "A more explicit method for regularization is weight decay , which is anal-\n",
      "ogous to ridge regression used for linear models (Section 3.4.1). We add a\n",
      "penalty to the error function R(θ) +λJ(θ), where\n",
      "J(θ) =∑\n",
      "kmβ2\n",
      "km+∑\n",
      "mℓα2\n",
      "mℓ (11.16)\n",
      "andλ≥0 is a tuning parameter. Larger values of λwill tend to shrink\n",
      "the weights toward zero: typically cross-validation is used to estimate λ.\n",
      "The eﬀect of the penalty is to simply add terms 2 βkmand 2 αmℓto the\n",
      "respective gradient expressions (11.13). Other forms for the penalty have\n",
      "been proposed, for example,\n",
      "J(θ) =∑\n",
      "kmβ2\n",
      "km\n",
      "1 +β2\n",
      "km+∑\n",
      "mℓα2\n",
      "mℓ\n",
      "1 +α2\n",
      "mℓ, (11.17)\n",
      "known as the weight elimination penalty. This has the eﬀect of shrinking\n",
      "smaller weights more than (11.16) does.\n",
      "Figure 11.4 shows the result of training a neural network with ten hidden\n",
      "units, without weight decay (upper panel) and with weight decay (lower\n",
      "panel), to the mixture example of Chapter 2. Weight decay has clearly\n",
      "improved the prediction. Figure 11.5 shows heat maps of the estimated\n",
      "weights from the training (grayscale versions of these are called Hinton\n",
      "diagrams. ) We see that weight decay has dampened the weights in both\n",
      "layers: the resulting weights are spread fairly evenly over the ten hidden\n",
      "units.\n",
      "11.5.3 Scaling of the Inputs\n",
      "Since the scaling of the inputs determines the eﬀective scaling of the weights\n",
      "in the bottom layer, it can have a large eﬀect on the quality of the ﬁnal\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.5 Some Issues in Training Neural Networks 399\n",
      "Neural Network - 10 Units, No Weight Decay\n",
      ". . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . .. . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "Training Error: 0.100\n",
      "Test Error:       0.259\n",
      "Bayes Error:    0.210\n",
      "Neural Network - 10 Units, Weight Decay=0.02\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". .. .. .. .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "Training Error: 0.160\n",
      "Test Error:       0.223\n",
      "Bayes Error:    0.210\n",
      "FIGURE 11.4. A neural network on the mixture example of Chapter 2. The\n",
      "upper panel uses no weight decay, and overﬁts the training data. The lower panel\n",
      "uses weight decay, and achieves close to the Bayes error rate ( broken purple\n",
      "boundary). Both use the softmax activation function and cross- entropy error.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "400 Neural Networks\n",
      "1 1\n",
      "11\n",
      "x1 x1x2 x2y1 y1y2 y2\n",
      "z1z1\n",
      "z1z1\n",
      "z2z2\n",
      "z2z2\n",
      "z3z3\n",
      "z3z3\n",
      "z1z1\n",
      "z1z1\n",
      "z5z5\n",
      "z5z5\n",
      "z6z6\n",
      "z6z6\n",
      "z7z7\n",
      "z7z7\n",
      "z8z8\n",
      "z8z8\n",
      "z9z9\n",
      "z9z9\n",
      "z10z10\n",
      "z10z10No weight decay Weight decay\n",
      "FIGURE 11.5. Heat maps of the estimated weights from the training of neural\n",
      "networks from Figure 11.4. The display ranges from bright gree n (negative) to\n",
      "bright red (positive).\n",
      "solution. At the outset it is best to standardize all inputs to have mean zero\n",
      "and standard deviation one. This ensures all inputs are treated equally in\n",
      "the regularization process, and allows one to choose a meaningful range for\n",
      "the random starting weights. With standardized inputs, it is typical to take\n",
      "random uniform weights over the range [ −0.7,+0.7].\n",
      "11.5.4 Number of Hidden Units and Layers\n",
      "Generally speaking it is better to have too many hidden units than too few.\n",
      "With too few hidden units, the model might not have enough ﬂexibility to\n",
      "capture the nonlinearities in the data; with too many hidden units, the\n",
      "extra weights can be shrunk toward zero if appropriate regularization is\n",
      "used. Typically the number of hidden units is somewhere in the range of\n",
      "5 to 100, with the number increasing with the number of inputs and num-\n",
      "ber of training cases. It is most common to put down a reasonably large\n",
      "number of units and train them with regularization. Some researchers use\n",
      "cross-validation to estimate the optimal number, but this seems unneces-\n",
      "sary if cross-validation is used to estimate the regularization parameter .\n",
      "Choice of the number of hidden layers is guided by background knowledge\n",
      "and experimentation. Each layer extracts features of the input for regres-\n",
      "sion or classiﬁcation. Use of multiple hidden layers allows construction of\n",
      "hierarchical features at diﬀerent levels of resolution. An example of the\n",
      "eﬀective use of multiple layers is given in Section 11.6.\n",
      "11.5.5 Multiple Minima\n",
      "The error function R(θ) is nonconvex, possessing many local minima. As a\n",
      "result, the ﬁnal solution obtained is quite dependent on the choice of start-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.6 Example: Simulated Data 401\n",
      "ing weights. One must at least try a number of random starting conﬁgura-\n",
      "tions, and choose the solution giving lowest (penalized) error. Probably a\n",
      "better approach is to use the average predictions over the collection of net-\n",
      "works as the ﬁnal prediction (Ripley, 1996). This is preferable to averaging\n",
      "the weights, since the nonlinearity of the model implies that this averaged\n",
      "solution could be quite poor. Another approach is via bagging , which aver-\n",
      "ages the predictions of networks training from randomly perturbed versions\n",
      "of the training data. This is described in Section 8.7.\n",
      "11.6 Example: Simulated Data\n",
      "We generated data from two additive error models Y=f(X) +ε:\n",
      "Sum of sigmoids: Y=σ(aT\n",
      "1X) +σ(aT\n",
      "2X) +ε1;\n",
      "Radial: Y=10∏\n",
      "m=1φ(Xm) +ε2.\n",
      "HereXT= (X1,X2,... ,X p), each Xjbeing a standard Gaussian variate,\n",
      "withp= 2 in the ﬁrst model, and p= 10 in the second.\n",
      "For the sigmoid model, a1= (3,3), a2= (3,−3); for the radial model,\n",
      "φ(t) = (1 /2π)1/2exp(−t2/2). Both ε1andε2are Gaussian errors, with\n",
      "variance chosen so that the signal-to-noise ratio\n",
      "Var(E( Y|X))\n",
      "Var(Y−E(Y|X))=Var(f(X))\n",
      "Var(ε)(11.18)\n",
      "is 4 in both models. We took a training sample of size 100 and a test sample\n",
      "of size 10 ,000. We ﬁt neural networks with weight decay and various num-\n",
      "bers of hidden units, and recorded the average test error E Test(Y−ˆf(X))2\n",
      "for each of 10 random starting weights. Only one training set was gen-\n",
      "erated, but the results are typical for an “average” training set. The test\n",
      "errors are shown in Figure 11.6. Note that the zero hidden unit model refers\n",
      "to linear least squares regression. The neural network is perfectly suited to\n",
      "the sum of sigmoids model, and the two-unit model does perform the best,\n",
      "achieving an error close to the Bayes rate. (Recall that the Bayes rate for\n",
      "regression with squared error is the error variance; in the ﬁgures, we report\n",
      "test error relative to the Bayes error). Notice, however, that with more hid-\n",
      "den units, overﬁtting quickly creeps in, and with some starting weights the\n",
      "model does worse than the linear model (zero hidden unit) model. Even\n",
      "with two hidden units, two of the ten starting weight conﬁgurations pro-\n",
      "duced results no better than the linear model, conﬁrming the importance\n",
      "of multiple starting values.\n",
      "A radial function is in a sense the most diﬃcult for the neural net, as it is\n",
      "spherically symmetric and with no preferred directions. We see in the right\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "402 Neural Networks\n",
      "1.0 1.5 2.0 2.5 3.0\n",
      "0 1 2 3 4 5 6 7 8 9 10\n",
      "Number of Hidden UnitsTest ErrorSum of Sigmoids\n",
      "0 5 10 15 20 25 30\n",
      "0 1 2 3 4 5 6 7 8 9 10\n",
      "Number of Hidden UnitsTest ErrorRadial\n",
      "FIGURE 11.6. Boxplots of test error, for simulated data example, relative t o\n",
      "the Bayes error (broken horizontal line). True function is a sum of two sigmoids\n",
      "on the left, and a radial function is on the right. The test error is displayed for\n",
      "10diﬀerent starting weights, for a single hidden layer neural netwo rk with the\n",
      "number of units as indicated.\n",
      "panel of Figure 11.6 that it does poorly in this case, with the test error\n",
      "staying well above the Bayes error (note the diﬀerent vertical scale from\n",
      "the left panel). In fact, since a constant ﬁt (such as the sample average)\n",
      "achieves a relative error of 5 (when the SNR is 4), we see that the neural\n",
      "networks perform increasingly worse than the mean.\n",
      "In this example we used a ﬁxed weight decay parameter of 0 .0005, rep-\n",
      "resenting a mild amount of regularization. The results in the left panel of\n",
      "Figure 11.6 suggest that more regularization is needed with greater num-\n",
      "bers of hidden units.\n",
      "In Figure 11.7 we repeated the experiment for the sum of sigmoids model,\n",
      "with no weight decay in the left panel, and stronger weight decay ( λ= 0.1)\n",
      "in the right panel. With no weight decay, overﬁtting becomes even more\n",
      "severe for larger numbers of hidden units. The weight decay value λ= 0.1\n",
      "produces good results for all numbers of hidden units, and there does not\n",
      "appear to be overﬁtting as the number of units increase. Finally, Figure 11.8\n",
      "shows the test error for a ten hidden unit network, varying the weight decay\n",
      "parameter over a wide range. The value 0 .1 is approximately optimal.\n",
      "In summary, there are two free parameters to select: the weight decay λ\n",
      "and number of hidden units M. As a learning strategy, one could ﬁx either\n",
      "parameter at the value corresponding to the least constrained model, to\n",
      "ensure that the model is rich enough, and use cross-validation to choose\n",
      "the other parameter. Here the least constrained values are zero weight decay\n",
      "and ten hidden units. Comparing the left panel of Figure 11.7 to Figure\n",
      "11.8, we see that the test error is less sensitive to the value of the weight\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.6 Example: Simulated Data 4031.0 1.5 2.0 2.5 3.0\n",
      "0 1 2 3 4 5 6 7 8 9 10\n",
      "Number of Hidden UnitsTest ErrorNo Weight Decay\n",
      "1.0 1.5 2.0 2.5 3.0\n",
      "0 1 2 3 4 5 6 7 8 9 10\n",
      "Number of Hidden UnitsTest ErrorWeight Decay=0.1\n",
      "FIGURE 11.7. Boxplots of test error, for simulated data example, relative t o the\n",
      "Bayes error. True function is a sum of two sigmoids. The test er ror is displayed\n",
      "for ten diﬀerent starting weights, for a single hidden layer neur al network with\n",
      "the number units as indicated. The two panels represent no weight de cay (left)\n",
      "and strong weight decay λ= 0.1(right).1.0 1.2 1.4 1.6 1.8 2.0 2.2\n",
      "0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14\n",
      "Weight Decay ParameterTest ErrorSum of Sigmoids, 10 Hidden Unit Model\n",
      "FIGURE 11.8. Boxplots of test error, for simulated data example. True functi on\n",
      "is a sum of two sigmoids. The test error is displayed for ten di ﬀerent starting\n",
      "weights, for a single hidden layer neural network with ten hidde n units and weight\n",
      "decay parameter value as indicated.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "404 Neural Networks\n",
      "FIGURE 11.9. Examples of training cases from ZIP code data. Each image is\n",
      "a16×16 8-bit grayscale representation of a handwritten digit.\n",
      "decay parameter, and hence cross-validation of this parameter would be\n",
      "preferred.\n",
      "11.7 Example: ZIP Code Data\n",
      "This example is a character recognition task: classiﬁcation of handwritten\n",
      "numerals. This problem captured the attention of the machine learning and\n",
      "neural network community for many years, and has remained a benchmark\n",
      "problem in the ﬁeld. Figure 11.9 shows some examples of normalized hand-\n",
      "written digits, automatically scanned from envelopes by the U.S. Postal\n",
      "Service. The original scanned digits are binary and of diﬀerent sizes and\n",
      "orientations; the images shown here have been deslanted and size normal-\n",
      "ized, resulting in 16 ×16 grayscale images (Le Cun et al., 1990). These 256\n",
      "pixel values are used as inputs to the neural network classiﬁer.\n",
      "Ablack box neural network is not ideally suited to this pattern recogni-\n",
      "tion task, partly because the pixel representation of the images lack certain\n",
      "invariances (such as small rotations of the image). Consequently early at -\n",
      "tempts with neural networks yielded misclassiﬁcation rates around 4 .5%\n",
      "on various examples of the problem. In this section we show some of the\n",
      "pioneering eﬀorts to handcraft the neural network to overcome some these\n",
      "deﬁciencies (Le Cun, 1989), which ultimately led to the state of the art in\n",
      "neural network performance(Le Cun et al., 1998)1.\n",
      "Although current digit datasets have tens of thousands of training and\n",
      "test examples, the sample size here is deliberately modest in order to em-\n",
      "1The ﬁgures and tables in this example were recreated from Le C un (1989).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.7 Example: ZIP Code Data 405\n",
      "16x168x8x2\n",
      "16x1610\n",
      "4x44x4\n",
      "8x8x210\n",
      "Shared WeightsNet-5Net-4Net-1\n",
      "4x4x4Local Connectivity10\n",
      "1010\n",
      "Net-3Net-28x812\n",
      "16x1616x1616x16\n",
      "FIGURE 11.10. Architecture of the ﬁve networks used in the ZIP code example.\n",
      "phasize the eﬀects. The examples were obtained by scanning some actual\n",
      "hand-drawn digits, and then generating additional images by random hor-\n",
      "izontal shifts. Details may be found in Le Cun (1989). There are 320 digi ts\n",
      "in the training set, and 160 in the test set.\n",
      "Five diﬀerent networks were ﬁt to the data:\n",
      "Net-1: No hidden layer, equivalent to multinomial logistic regression.\n",
      "Net-2: One hidden layer, 12 hidden units fully connected.\n",
      "Net-3: Two hidden layers locally connected.\n",
      "Net-4: Two hidden layers, locally connected with weight sharing.\n",
      "Net-5: Two hidden layers, locally connected, two levels of weight sharing.\n",
      "These are depicted in Figure 11.10. Net-1 for example has 256 inputs, one\n",
      "each for the 16 ×16 input pixels, and ten output units for each of the digits\n",
      "0–9. The predicted value ˆfk(x) represents the estimated probability that\n",
      "an image xhas digit class k, fork= 0,1,2,... ,9.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "406 Neural Networks\n",
      "Training Epochs% Correct on Test Data\n",
      "0 5 10 15 20 25 3060708090100\n",
      "Net-1Net-2Net-3Net-4Net-5\n",
      "FIGURE 11.11. Test performance curves, as a function of the number of train-\n",
      "ing epochs, for the ﬁve networks of Table 11.1 applied to the ZIP c ode data.\n",
      "(Le Cun, 1989)\n",
      "The networks all have sigmoidal output units, and were all ﬁt with the\n",
      "sum-of-squares error function. The ﬁrst network has no hidden layer, and\n",
      "hence is nearly equivalent to a linear multinomial regression model (Exer-\n",
      "cise 11.4). Net-2 is a single hidden layer network with 12 hidden units, of\n",
      "the kind described above.\n",
      "The training set error for all of the networks was 0%, since in all cases\n",
      "there are more parameters than training observations. The evolution of the\n",
      "test error during the training epochs is shown in Figure 11.11. The linear\n",
      "network (Net-1) starts to overﬁt fairly quickly, while test performance o f\n",
      "the others level oﬀ at successively superior values.\n",
      "The other three networks have additional features which demonstrate\n",
      "the power and ﬂexibility of the neural network paradigm. They introduce\n",
      "constraints on the network, natural for the problem at hand, which allow\n",
      "for more complex connectivity but fewer parameters.\n",
      "Net-3 uses local connectivity: this means that each hidden unit is con-\n",
      "nected to only a small patch of units in the layer below. In the ﬁrst hidden\n",
      "layer (an 8 ×8 array), each unit takes inputs from a 3 ×3 patch of the input\n",
      "layer; for units in the ﬁrst hidden layer that are one unit apart, their recep-\n",
      "tive ﬁelds overlap by one row or column, and hence are two pixels apart.\n",
      "In the second hidden layer, inputs are from a 5 ×5 patch, and again units\n",
      "that are one unit apart have receptive ﬁelds that are two units apart. The\n",
      "weights for all other connections are set to zero. Local connectivity makes\n",
      "each unit responsible for extracting local features from the layer below, and\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.7 Example: ZIP Code Data 407\n",
      "TABLE 11.1. Test set performance of ﬁve diﬀerent neural networks on a hand-\n",
      "written digit classiﬁcation example (Le Cun, 1989).\n",
      "Network Architecture Links Weights % Correct\n",
      "Net-1: Single layer network 2570 2570 80.0%\n",
      "Net-2: Two layer network 3214 3214 87.0%\n",
      "Net-3: Locally connected 1226 1226 88.5%\n",
      "Net-4: Constrained network 1 2266 1132 94.0%\n",
      "Net-5: Constrained network 2 5194 1060 98.4%\n",
      "reduces considerably the total number of weights. With many more hidden\n",
      "units than Net-2, Net-3 has fewer links and hence weights (1226 vs. 3214),\n",
      "and achieves similar performance.\n",
      "Net-4 and Net-5 have local connectivity with shared weights. All units\n",
      "in a local feature map perform the sameoperation on diﬀerent parts of the\n",
      "image, achieved by sharing the same weights. The ﬁrst hidden layer of Net-\n",
      "4 has two 8 ×8 arrays, and each unit takes input from a 3 ×3 patch just like\n",
      "in Net-3. However, each of the units in a single 8 ×8 feature map share the\n",
      "same set of nine weights (but have their own bias parameter). This forces\n",
      "the extracted features in diﬀerent parts of the image to be computed by\n",
      "the same linear functional, and consequently these networks are sometimes\n",
      "known as convolutional networks . The second hidden layer of Net-4 has\n",
      "no weight sharing, and is the same as in Net-3. The gradient of the error\n",
      "function Rwith respect to a shared weight is the sum of the gradients of\n",
      "Rwith respect to each connection controlled by the weights in question.\n",
      "Table 11.1 gives the number of links, the number of weights and the\n",
      "optimal test performance for each of the networks. We see that Net-4 has\n",
      "more links but fewer weights than Net-3, and superior test performance.\n",
      "Net-5 has four 4 ×4 feature maps in the second hidden layer, each unit\n",
      "connected to a 5 ×5 local patch in the layer below. Weights are shared\n",
      "in each of these feature maps. We see that Net-5 does the best, having\n",
      "errors of only 1.6%, compared to 13% for the “vanilla” network Net-2.\n",
      "The clever design of network Net-5, motivated by the fact that features of\n",
      "handwriting style should appear in more than one part of a digit, was the\n",
      "result of many person years of experimentation. This and similar networks\n",
      "gave better performance on ZIP code problems than any other learning\n",
      "method at that time (early 1990s). This example also shows that neural\n",
      "networks are not a fully automatic tool, as they are sometimes advertised.\n",
      "As with all statistical models, subject matter knowledge can and should be\n",
      "used to improve their performance.\n",
      "This network was later outperformed by the tangent distance approach\n",
      "(Simard et al., 1993) described in Section 13.3.3, which explicitly incorpo-\n",
      "rates natural aﬃne invariances. At this point the digit recognition datasets\n",
      "become test beds for every new learning procedure, and researchers worked\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "408 Neural Networks\n",
      "hard to drive down the error rates. As of this writing, the best error rates o n\n",
      "a large database (60 ,000 training, 10 ,000 test observations), derived from\n",
      "standard NIST2databases, were reported to be the following: (Le Cun et\n",
      "al., 1998):\n",
      "•1.1% for tangent distance with a 1-nearest neighbor classiﬁer (Sec-\n",
      "tion 13.3.3);\n",
      "•0.8% for a degree-9 polynomial SVM (Section 12.3);\n",
      "•0.8% for LeNet-5 , a more complex version of the convolutional net-\n",
      "work described here;\n",
      "•0.7% for boosted LeNet-4 . Boosting is described in Chapter 8. LeNet-\n",
      "4is a predecessor of LeNet-5.\n",
      "Le Cun et al. (1998) report a much larger table of performance results, and\n",
      "it is evident that many groups have been working very hard to bring these\n",
      "test error rates down. They report a standard error of 0 .1% on the error\n",
      "estimates, which is based on a binomial average with N= 10,000 and\n",
      "p≈0.01. This implies that error rates within 0 .1—0.2% of one another\n",
      "are statistically equivalent. Realistically the standard error is even hi gher,\n",
      "since the test data has been implicitly used in the tuning of the various\n",
      "procedures.\n",
      "11.8 Discussion\n",
      "Both projection pursuit regression and neural networks take nonlinear func-\n",
      "tions of linear combinations (“derived features”) of the inputs. This is a\n",
      "powerful and very general approach for regression and classiﬁcation, and\n",
      "has been shown to compete well with the best learning methods on many\n",
      "problems.\n",
      "These tools are especially eﬀective in problems with a high signal-to-noise\n",
      "ratio and settings where prediction without interpretation is the goal. They\n",
      "are less eﬀective for problems where the goal is to describe the physical pro-\n",
      "cess that generated the data and the roles of individual inputs. Each input\n",
      "enters into the model in many places, in a nonlinear fashion. Some authors\n",
      "(Hinton, 1989) plot a diagram of the estimated weights into each hidden\n",
      "unit, to try to understand the feature that each unit is extracting. This\n",
      "is limited however by the lack of identiﬁability of the parameter vectors\n",
      "αm, m= 1,... ,M . Often there are solutions with αmspanning the same\n",
      "linear space as the ones found during training, giving predicted values that\n",
      "2The National Institute of Standards and Technology maintai n large databases, in-\n",
      "cluding handwritten character databases; http://www.nist.gov/srd/ .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 409\n",
      "are roughly the same. Some authors suggest carrying out a principal com-\n",
      "ponent analysis of these weights, to try to ﬁnd an interpretable solution. In\n",
      "general, the diﬃculty of interpreting these models has limited their use in\n",
      "ﬁelds like medicine, where interpretation of the model is very important.\n",
      "There has been a great deal of research on the training of neural net-\n",
      "works. Unlike methods like CART and MARS, neural networks are smooth\n",
      "functions of real-valued parameters. This facilitates the development of\n",
      "Bayesian inference for these models. The next sections discusses a success-\n",
      "ful Bayesian implementation of neural networks.\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003\n",
      "Challenge\n",
      "A classiﬁcation competition was held in 2003, in which ﬁve labeled train-\n",
      "ing datasets were provided to participants. It was organized for a Neural\n",
      "Information Processing Systems (NIPS) workshop. Each of the data sets\n",
      "constituted a two-class classiﬁcation problems, with diﬀerent sizes and from\n",
      "a variety of domains (see Table 11.2). Feature measurements for a valida-\n",
      "tion dataset were also available.\n",
      "Participants developed and applied statistical learning procedures to\n",
      "make predictions on the datasets, and could submit predictions to a web-\n",
      "site on the validation set for a period of 12 weeks. With this feedback,\n",
      "participants were then asked to submit predictions for a separate test set\n",
      "and they received their results. Finally, the class labels for the validation\n",
      "set were released and participants had one week to train their algorithms\n",
      "on the combined training and validation sets, and submit their ﬁnal pre-\n",
      "dictions to the competition website. A total of 75 groups participated, with\n",
      "20 and 16 eventually making submissions on the validation and test sets,\n",
      "respectively.\n",
      "There was an emphasis on feature extraction in the competition. Arti-\n",
      "ﬁcial “probes” were added to the data: these are noise features with dis-\n",
      "tributions resembling the real features but independent of the class labels.\n",
      "The percentage of probes that were added to each dataset, relative to the\n",
      "total set of features, is shown on Table 11.2. Thus each learning algorithm\n",
      "had to ﬁgure out a way of identifying the probes and downweighting or\n",
      "eliminating them.\n",
      "A number of metrics were used to evaluate the entries, including the\n",
      "percentage correct on the test set, the area under the ROC curve, and a\n",
      "combined score that compared each pair of classiﬁers head-to-head. The\n",
      "results of the competition are very interesting and are detailed in Guyon et\n",
      "al. (2006). The most notable result: the entries of Neal and Zhang (2006)\n",
      "were the clear overall winners. In the ﬁnal competition they ﬁnished ﬁrst\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "410 Neural Networks\n",
      "TABLE 11.2. NIPS 2003 challenge data sets. The column labeled pis the number\n",
      "of features. For the Dorothea dataset the features are binary. Ntr,NvalandNte\n",
      "are the number of training, validation and test cases, respectiv ely\n",
      "Dataset Domain Feature p Percent Ntr Nval Nte\n",
      "Type Probes\n",
      "Arcene Mass spectrometry Dense 10,000 30 100 100 700\n",
      "Dexter Text classiﬁcation Sparse 20,000 50 300 300 2000\n",
      "Dorothea Drug discovery Sparse 100,000 50 800 350 800\n",
      "Gisette Digit recognition Dense 5000 30 6000 1000 6500\n",
      "Madelon Artiﬁcial Dense 500 96 2000 600 1800\n",
      "in three of the ﬁve datasets, and were 5th and 7th on the remaining two\n",
      "datasets.\n",
      "In their winning entries, Neal and Zhang (2006) used a series of pre-\n",
      "processing feature-selection steps, followed by Bayesian neural networks,\n",
      "Dirichlet diﬀusion trees, and combinations of these methods. Here we focus\n",
      "only on the Bayesian neural network approach, and try to discern which\n",
      "aspects of their approach were important for its success. We rerun their\n",
      "programs and compare the results to boosted neural networks and boosted\n",
      "trees, and other related methods.\n",
      "11.9.1 Bayes, Boosting and Bagging\n",
      "Let us ﬁrst review brieﬂy the Bayesian approach to inference and its appli-\n",
      "cation to neural networks. Given training data Xtr,ytr, we assume a sam-\n",
      "pling model with parameters θ; Neal and Zhang (2006) use a two-hidden-\n",
      "layer neural network, with output nodes the class probabilities Pr( Y|X,θ)\n",
      "for the binary outcomes. Given a prior distribution Pr( θ), the posterior\n",
      "distribution for the parameters is\n",
      "Pr(θ|Xtr,ytr) =Pr(θ)Pr(ytr|Xtr,θ)∫\n",
      "Pr(θ)Pr(ytr|Xtr,θ)dθ(11.19)\n",
      "For a test case with features Xnew, the predictive distribution for the\n",
      "labelYnewis\n",
      "Pr(Ynew|Xnew,Xtr,ytr) =∫\n",
      "Pr(Ynew|Xnew,θ)Pr(θ|Xtr,ytr)dθ(11.20)\n",
      "(c.f. equation 8.24). Since the integral in (11.20) is intractable, sophis ticated\n",
      "Markov Chain Monte Carlo (MCMC) methods are used to sample from the\n",
      "posterior distribution Pr( Ynew|Xnew,Xtr,ytr). A few hundred values θare\n",
      "generated and then a simple average of these values estimates the integral.\n",
      "Neal and Zhang (2006) use diﬀuse Gaussian priors for all of the parame-\n",
      "ters. The particular MCMC approach that was used is called hybrid Monte\n",
      "Carlo, and may be important for the success of the method. It includes\n",
      "an auxiliary momentum vector and implements Hamiltonian dynamics in\n",
      "which the potential function is the target density. This is done to avoid\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 411\n",
      "random walk behavior; the successive candidates move across the sample\n",
      "space in larger steps. They tend to be less correlated and hence converge\n",
      "to the target distribution more rapidly.\n",
      "Neal and Zhang (2006) also tried diﬀerent forms of pre-processing of the\n",
      "features:\n",
      "1. univariate screening using t-tests, and\n",
      "2. automatic relevance determination.\n",
      "In the latter method (ARD), the weights (coeﬃcients) for the jth feature\n",
      "to each of the ﬁrst hidden layer units all share a common prior variance\n",
      "σ2\n",
      "j, and prior mean zero. The posterior distributions for each variance σ2\n",
      "j\n",
      "are computed, and the features whose posterior variance concentrates on\n",
      "small values are discarded.\n",
      "There are thus three main features of this approach that could be im-\n",
      "portant for its success:\n",
      "(a) the feature selection and pre-processing,\n",
      "(b) the neural network model, and\n",
      "(c) the Bayesian inference for the model using MCMC.\n",
      "According to Neal and Zhang (2006), feature screening in (a) is carried\n",
      "out purely for computational eﬃciency; the MCMC procedure is slow with\n",
      "a large number of features. There is no need to use feature selection to avoid\n",
      "overﬁtting. The posterior average (11.20) takes care of this automatica lly.\n",
      "We would like to understand the reasons for the success of the Bayesian\n",
      "method. In our view, power of modern Bayesian methods does not lie in\n",
      "their use as a formal inference procedure; most people would not believe\n",
      "that the priors in a high-dimensional, complex neural network model are\n",
      "actually correct. Rather the Bayesian/MCMC approach gives an eﬃcient\n",
      "way of sampling the relevant parts of model space, and then averaging the\n",
      "predictions for the high-probability models.\n",
      "Bagging and boosting are non-Bayesian procedures that have some simi-\n",
      "larity to MCMC in a Bayesian model. The Bayesian approach ﬁxes the data\n",
      "and perturbs the parameters, according to current estimate of the poste-\n",
      "rior distribution. Bagging perturbs the data in an i.i.d fashion and then\n",
      "re-estimates the model to give a new set of model parameters. At the end,\n",
      "a simple average of the model predictions from diﬀerent bagged samples is\n",
      "computed. Boosting is similar to bagging, but ﬁts a model that is additive\n",
      "in the models of each individual base learner, which are learned using non\n",
      "i.i.d. samples. We can write all of these models in the form\n",
      "ˆf(xnew) =L∑\n",
      "ℓ=1wℓE(Ynew|xnew,ˆθℓ) (11.21)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "412 Neural Networks\n",
      "In all cases the ˆθℓare a large collection of model parameters. For the\n",
      "Bayesian model the wℓ= 1/L, and the average estimates the posterior\n",
      "mean (11.21) by sampling θℓfrom the posterior distribution. For bagging,\n",
      "wℓ= 1/Las well, and the ˆθℓare the parameters reﬁt to bootstrap re-\n",
      "samples of the training data. For boosting, the weights are all equal to\n",
      "1, but the ˆθℓare typically chosen in a nonrandom sequential fashion to\n",
      "constantly improve the ﬁt.\n",
      "11.9.2 Performance Comparisons\n",
      "Based on the similarities above, we decided to compare Bayesian neural\n",
      "networks to boosted trees, boosted neural networks, random forests and\n",
      "bagged neural networks on the ﬁve datasets in Table 11.2. Bagging and\n",
      "boosting of neural networks are not methods that we have previously used\n",
      "in our work. We decided to try them here, because of the success of Bayesian\n",
      "neural networks in this competition, and the good performance of bagging\n",
      "and boosting with trees. We also felt that by bagging and boosting neural\n",
      "nets, we could assess both the choice of model as well as the model search\n",
      "strategy.\n",
      "Here are the details of the learning methods that were compared:\n",
      "Bayesian neural nets. The results here are taken from Neal and Zhang\n",
      "(2006), using their Bayesian approach to ﬁtting neural networks. The\n",
      "models had two hidden layers of 20 and 8 units. We re-ran some\n",
      "networks for timing purposes only.\n",
      "Boosted trees. We used the gbmpackage (version 1.5-7) in the R language.\n",
      "Tree depth and shrinkage factors varied from dataset to dataset. We\n",
      "consistently bagged 80% of the data at each boosting iteration (the\n",
      "default is 50%). Shrinkage was between 0.001 and 0.1. Tree depth was\n",
      "between 2 and 9.\n",
      "Boosted neural networks. Since boosting is typically most eﬀective with\n",
      "“weak” learners, we boosted a single hidden layer neural network with\n",
      "two or four units, ﬁt with the nnetpackage (version 7.2-36) in R.\n",
      "Random forests. We used the R package randomForest (version 4.5-16)\n",
      "with default settings for the parameters.\n",
      "Bagged neural networks. We used the same architecture as in the Bayesian\n",
      "neural network above (two hidden layers of 20 and 8 units), ﬁt using\n",
      "both Neal’s C language package “Flexible Bayesian Modeling” (2004-\n",
      "11-10 release), and Matlab neural-net toolbox (version 5.1).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "11.9 Bayesian Neural Nets and the NIPS 2003 Challenge 413Test Error (%)\n",
      "Arcene Dexter Dorothea Gisette Madelon5 15 25Univariate Screened Features\n",
      "Bayesian neural nets\n",
      "boosted trees \n",
      "boosted neural nets\n",
      "random forests\n",
      "bagged neural networks \n",
      "Test Error (%)\n",
      "Arcene Dexter Dorothea Gisette Madelon5 15 25ARD Reduced Features\n",
      "FIGURE 11.12. Performance of diﬀerent learning methods on ﬁve problems,\n",
      "using both univariate screening of features (top panel) and a reduc ed feature set\n",
      "from automatic relevance determination. The error bars at the t op of each plot\n",
      "have width equal to one standard error of the diﬀerence between t wo error rates.\n",
      "On most of the problems several competitors are within this e rror bound.\n",
      "This analysis was carried out by Nicholas Johnson, and full details may\n",
      "be found in Johnson (2008)3. The results are shown in Figure 11.12 and\n",
      "Table 11.3.\n",
      "The ﬁgure and table show Bayesian, boosted and bagged neural networks,\n",
      "boosted trees, and random forests, using both the screened and reduced\n",
      "features sets. The error bars at the top of each plot indicate one standard\n",
      "error of the diﬀerence between two error rates. Bayesian neural networks\n",
      "again emerge as the winner, although for some datasets the diﬀerences\n",
      "between the test error rates is not statistically signiﬁcant. Random forests\n",
      "performs the best among the competitors using the selected feature set,\n",
      "while the boosted neural networks perform best with the reduced feature\n",
      "set, and nearly match the Bayesian neural net.\n",
      "The superiority of boosted neural networks over boosted trees suggest\n",
      "that the neural network model is better suited to these particular prob-\n",
      "lems. Speciﬁcally, individual features might not be good predictors here\n",
      "3We also thank Isabelle Guyon for help in preparing the result s of this section.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "414 Neural Networks\n",
      "TABLE 11.3. Performance of diﬀerent methods. Values are average rank of tes t\n",
      "error across the ﬁve problems (low is good), and mean computati on time and\n",
      "standard error of the mean, in minutes.\n",
      "Screened Features ARD Reduced Features\n",
      "Method Average Average Average Average\n",
      "Rank Time Rank Time\n",
      "Bayesian neural networks 1.5 384(138) 1.6 600(186)\n",
      "Boosted trees 3.4 3.03(2.5) 4.0 34.1(32.4)\n",
      "Boosted neural networks 3.8 9.4(8.6) 2.2 35.6(33.5)\n",
      "Random forests 2.7 1.9(1.7) 3.2 11.2(9.3)\n",
      "Bagged neural networks 3.6 3.5(1.1) 4.0 6.4(4.4)\n",
      "and linear combinations of features work better. However the impressive\n",
      "performance of random forests is at odds with this explanation, and came\n",
      "as a surprise to us.\n",
      "Since the reduced feature sets come from the Bayesian neural network\n",
      "approach, only the methods that use the screened features are legitimate,\n",
      "self-contained procedures. However, this does suggest that better methods\n",
      "for internal feature selection might help the overall performance of boosted\n",
      "neural networks.\n",
      "The table also shows the approximate training time required for each\n",
      "method. Here the non-Bayesian methods show a clear advantage.\n",
      "Overall, the superior performance of Bayesian neural networks here may\n",
      "be due to the fact that\n",
      "(a) the neural network model is well suited to these ﬁve problems, and\n",
      "(b) the MCMC approach provides an eﬃcient way of exploring the im-\n",
      "portant part of the parameter space, and then averaging the resulting\n",
      "models according to their quality.\n",
      "The Bayesian approach works well for smoothly parametrized models like\n",
      "neural nets; it is not yet clear that it works as well for non-smooth models\n",
      "like trees.\n",
      "11.10 Computational Considerations\n",
      "WithNobservations, ppredictors, Mhidden units and Ltraining epochs, a\n",
      "neural network ﬁt typically requires O(NpML ) operations. There are many\n",
      "packages available for ﬁtting neural networks, probably many more than\n",
      "exist for mainstream statistical methods. Because the available softwar e\n",
      "varies widely in quality, and the learning problem for neural networks is\n",
      "sensitive to issues such as input scaling, such software should be carefully\n",
      "chosen and tested.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 415\n",
      "Bibliographic Notes\n",
      "Projection pursuit was proposed by Friedman and Tukey (1974), and spe-\n",
      "cialized to regression by Friedman and Stuetzle (1981). Huber (1985) gives\n",
      "a scholarly overview, and Roosen and Hastie (1994) present a formulatio n\n",
      "using smoothing splines. The motivation for neural networks dates back\n",
      "to McCulloch and Pitts (1943), Widrow and Hoﬀ (1960) (reprinted in An-\n",
      "derson and Rosenfeld (1988)) and Rosenblatt (1962). Hebb (1949) heavily\n",
      "inﬂuenced the development of learning algorithms. The resurgence of neural\n",
      "networks in the mid 1980s was due to Werbos (1974), Parker (1985) and\n",
      "Rumelhart et al. (1986), who proposed the back-propagation algorithm.\n",
      "Today there are many books written on the topic, for a broad range of\n",
      "audiences. For readers of this book, Hertz et al. (1991), Bishop (1995) and\n",
      "Ripley (1996) may be the most informative. Bayesian learning for neural\n",
      "networks is described in Neal (1996). The ZIP code example was taken from\n",
      "Le Cun (1989); see also Le Cun et al. (1990) and Le Cun et al. (1998).\n",
      "We do not discuss theoretical topics such as approximation properties of\n",
      "neural networks, such as the work of Barron (1993), Girosi et al. (1995 )\n",
      "and Jones (1992). Some of these results are summarized by Ripley (1996).\n",
      "Exercises\n",
      "Ex. 11.1 Establish the exact correspondence between the projection pur-\n",
      "suit regression model (11.1) and the neural network (11.5). In particular,\n",
      "show that the single-layer regression network is equivalent to a PPR model\n",
      "withgm(ωT\n",
      "mx) =βmσ(α0m+sm(ωT\n",
      "mx)), where ωmis the mth unit vector.\n",
      "Establish a similar equivalence for a classiﬁcation network.\n",
      "Ex. 11.2 Consider a neural network for a quantitative outcome as in (11.5),\n",
      "using squared-error loss and identity output function gk(t) =t. Suppose\n",
      "that the weights αmfrom the input to hidden layer are nearly zero. Show\n",
      "that the resulting model is nearly linear in the inputs.\n",
      "Ex. 11.3 Derive the forward and backward propagation equations for the\n",
      "cross-entropy loss function.\n",
      "Ex. 11.4 Consider a neural network for a Kclass outcome that uses cross-\n",
      "entropy loss. If the network has no hidden layer, show that the model is\n",
      "equivalent to the multinomial logistic model described in Chapter 4.\n",
      "Ex. 11.5\n",
      "(a) Write a program to ﬁt a single hidden layer neural network (ten hidden\n",
      "units) via back-propagation and weight decay.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "416 Neural Networks\n",
      "(b) Apply it to 100 observations from the model\n",
      "Y=σ(aT\n",
      "1X) + (aT\n",
      "2X)2+ 0.30≤Z,\n",
      "where σis the sigmoid function, Zis standard normal, XT= (X1,X2),\n",
      "eachXjbeing independent standard normal, and a1= (3,3),a2=\n",
      "(3,−3). Generate a test sample of size 1000, and plot the training and\n",
      "test error curves as a function of the number of training epochs, for\n",
      "diﬀerent values of the weight decay parameter. Discuss the overﬁtting\n",
      "behavior in each case.\n",
      "(c) Vary the number of hidden units in the network, from 1 up to 10, and\n",
      "determine the minimum number needed to perform well for this task.\n",
      "Ex. 11.6 Write a program to carry out projection pursuit regression, using\n",
      "cubic smoothing splines with ﬁxed degrees of freedom. Fit it to the data\n",
      "from the previous exercise, for various values of the smoothing parameter\n",
      "and number of model terms. Find the minimum number of model terms\n",
      "necessary for the model to perform well and compare this to the number\n",
      "of hidden units from the previous exercise.\n",
      "Ex. 11.7 Fit a neural network to the spamdata of Section 9.1.2, and compare\n",
      "the results to those for the additive model given in that chapter. Compare\n",
      "both the classiﬁcation performance and interpretability of the ﬁnal model.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 417\n",
      "Printer: Opaque this\n",
      "12\n",
      "Support Vector Machines and\n",
      "Flexible Discriminants\n",
      "12.1 Introduction\n",
      "In this chapter we describe generalizations of linear decision boundaries\n",
      "for classiﬁcation. Optimal separating hyperplanes are introduced in Chap-\n",
      "ter 4 for the case when two classes are linearly separable. Here we cover\n",
      "extensions to the nonseparable case, where the classes overlap. These tech-\n",
      "niques are then generalized to what is known as the support vector machine ,\n",
      "which produces nonlinear boundaries by constructing a linear boundary in\n",
      "a large, transformed version of the feature space. The second set of methods\n",
      "generalize Fisher’s linear discriminant analysis (LDA). The generalizations\n",
      "include ﬂexible discriminant analysis which facilitates construction of non-\n",
      "linear boundaries in a manner very similar to the support vector machines,\n",
      "penalized discriminant analysis for problems such as signal and image clas-\n",
      "siﬁcation where the large number of features are highly correlated, and\n",
      "mixture discriminant analysis for irregularly shaped classes.\n",
      "12.2 The Support Vector Classiﬁer\n",
      "In Chapter 4 we discussed a technique for constructing an optimal separat-\n",
      "ing hyperplane between two perfectly separated classes. We review this and\n",
      "generalize to the nonseparable case, where the classes may not be separable\n",
      "by a linear boundary.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "418 12. Flexible Discriminants\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "•\n",
      "marginM=1\n",
      "∥β∥\n",
      "M=1\n",
      "∥β∥xTβ+β0= 0\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "• •\n",
      "marginξ∗\n",
      "1ξ∗\n",
      "1ξ∗\n",
      "1\n",
      "ξ∗\n",
      "2ξ∗\n",
      "2ξ∗\n",
      "2ξ∗\n",
      "3ξ∗\n",
      "3ξ∗\n",
      "4ξ∗\n",
      "4ξ∗\n",
      "4ξ∗\n",
      "5\n",
      "M=1\n",
      "∥β∥\n",
      "M=1\n",
      "∥β∥xTβ+β0= 0\n",
      "FIGURE 12.1. Support vector classiﬁers. The left panel shows the separable\n",
      "case. The decision boundary is the solid line, while broken line s bound the shaded\n",
      "maximal margin of width 2M= 2/∥β∥. The right panel shows the nonseparable\n",
      "(overlap) case. The points labeled ξ∗\n",
      "jare on the wrong side of their margin by\n",
      "an amount ξ∗\n",
      "j=Mξj; points on the correct side have ξ∗\n",
      "j= 0. The margin is\n",
      "maximized subject to a total budgetPξi≤constant. HencePξ∗\n",
      "jis the total\n",
      "distance of points on the wrong side of their margin.\n",
      "Our training data consists of Npairs ( x1,y1),(x2,y2),... ,(xN,yN), with\n",
      "xi∈IRpandyi∈ {− 1,1}. Deﬁne a hyperplane by\n",
      "{x:f(x) =xTβ+β0= 0}, (12.1)\n",
      "where βis a unit vector: ∥β∥= 1. A classiﬁcation rule induced by f(x) is\n",
      "G(x) = sign[ xTβ+β0]. (12.2)\n",
      "The geometry of hyperplanes is reviewed in Section 4.5, where we show that\n",
      "f(x) in (12.1) gives the signed distance from a point xto the hyperplane\n",
      "f(x) =xTβ+β0= 0. Since the classes are separable, we can ﬁnd a function\n",
      "f(x) =xTβ+β0withyif(xi)>0∀i. Hence we are able to ﬁnd the\n",
      "hyperplane that creates the biggest margin between the training points for\n",
      "class 1 and −1 (see Figure 12.1). The optimization problem\n",
      "max\n",
      "β,β0,∥β∥=1M\n",
      "subject to yi(xT\n",
      "iβ+β0)≥M, i= 1,... ,N,(12.3)\n",
      "captures this concept. The band in the ﬁgure is Munits away from the\n",
      "hyperplane on either side, and hence 2 Munits wide. It is called the margin .\n",
      "We showed that this problem can be more conveniently rephrased as\n",
      "min\n",
      "β,β0∥β∥\n",
      "subject to yi(xT\n",
      "iβ+β0)≥1, i= 1,... ,N,(12.4)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.2 The Support Vector Classiﬁer 419\n",
      "where we have dropped the norm constraint on β. Note that M= 1/∥β∥.\n",
      "Expression (12.4) is the usual way of writing the support vector criterion\n",
      "for separated data. This is a convex optimization problem (quadratic cri-\n",
      "terion, linear inequality constraints), and the solution is characterized in\n",
      "Section 4.5.2.\n",
      "Suppose now that the classes overlap in feature space. One way to deal\n",
      "with the overlap is to still maximize M, but allow for some points to be on\n",
      "the wrong side of the margin. Deﬁne the slack variables ξ= (ξ1,ξ2,... ,ξ N).\n",
      "There are two natural ways to modify the constraint in (12.3):\n",
      "yi(xT\n",
      "iβ+β0)≥M−ξi, (12.5)\n",
      "or\n",
      "yi(xT\n",
      "iβ+β0)≥M(1−ξi), (12.6)\n",
      "∀i, ξi≥0,∑N\n",
      "i=1ξi≤constant. The two choices lead to diﬀerent solutions.\n",
      "The ﬁrst choice seems more natural, since it measures overlap in actual\n",
      "distance from the margin; the second choice measures the overlap in relative\n",
      "distance, which changes with the width of the margin M. However, the ﬁrst\n",
      "choice results in a nonconvex optimization problem, while the second is\n",
      "convex; thus (12.6) leads to the “standard” support vector classiﬁer, which\n",
      "we use from here on.\n",
      "Here is the idea of the formulation. The value ξiin the constraint yi(xT\n",
      "iβ+\n",
      "β0)≥M(1−ξi) is the proportional amount by which the prediction\n",
      "f(xi) =xT\n",
      "iβ+β0is on the wrong side of its margin. Hence by bounding the\n",
      "sum∑ξi, we bound the total proportional amount by which predictions\n",
      "fall on the wrong side of their margin. Misclassiﬁcations occur when ξi>1,\n",
      "so bounding∑ξiat a value Ksay, bounds the total number of training\n",
      "misclassiﬁcations at K.\n",
      "As in (4.48) in Section 4.5.2, we can drop the norm constraint on β,\n",
      "deﬁne M= 1/∥β∥, and write (12.4) in the equivalent form\n",
      "min∥β∥subject to{\n",
      "yi(xT\n",
      "iβ+β0)≥1−ξi∀i,\n",
      "ξi≥0,∑ξi≤constant .(12.7)\n",
      "This is the usual way the support vector classiﬁer is deﬁned for the non-\n",
      "separable case. However we ﬁnd confusing the presence of the ﬁxed scale\n",
      "“1” in the constraint yi(xT\n",
      "iβ+β0)≥1−ξi, and prefer to start with (12.6).\n",
      "The right panel of Figure 12.1 illustrates this overlapping case.\n",
      "By the nature of the criterion (12.7), we see that points well inside their\n",
      "class boundary do not play a big role in shaping the boundary. This seems\n",
      "like an attractive property, and one that diﬀerentiates it from linear dis-\n",
      "criminant analysis (Section 4.3). In LDA, the decision boundary is deter-\n",
      "mined by the covariance of the class distributions and the positions of the\n",
      "class centroids. We will see in Section 12.3.3 that logistic regression i s more\n",
      "similar to the support vector classiﬁer in this regard.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "420 12. Flexible Discriminants\n",
      "12.2.1 Computing the Support Vector Classiﬁer\n",
      "The problem (12.7) is quadratic with linear inequality constraints, hence it\n",
      "is a convex optimization problem. We describe a quadratic programming\n",
      "solution using Lagrange multipliers. Computationally it is convenient to\n",
      "re-express (12.7) in the equivalent form\n",
      "min\n",
      "β,β01\n",
      "2∥β∥2+CN∑\n",
      "i=1ξi\n",
      "subject to ξi≥0, yi(xT\n",
      "iβ+β0)≥1−ξi∀i,(12.8)\n",
      "where the “cost” parameter Creplaces the constant in (12.7); the separable\n",
      "case corresponds to C=∞.\n",
      "The Lagrange (primal) function is\n",
      "LP=1\n",
      "2∥β∥2+CN∑\n",
      "i=1ξi−N∑\n",
      "i=1αi[yi(xT\n",
      "iβ+β0)−(1−ξi)]−N∑\n",
      "i=1θiξi,(12.9)\n",
      "which we minimize w.r.t β,β0andξi. Setting the respective derivatives to\n",
      "zero, we get\n",
      "β=N∑\n",
      "i=1αiyixi, (12.10)\n",
      "0 =N∑\n",
      "i=1αiyi, (12.11)\n",
      "αi=C−θi,∀i, (12.12)\n",
      "as well as the positivity constraints αi, θi, ξi≥0∀i. By substituting\n",
      "(12.10)–(12.12) into (12.9), we obtain the Lagrangian (Wolfe) dua l objec-\n",
      "tive function\n",
      "LD=N∑\n",
      "i=1αi−1\n",
      "2N∑\n",
      "i=1N∑\n",
      "i′=1αiαi′yiyi′xT\n",
      "ixi′, (12.13)\n",
      "which gives a lower bound on the objective function (12.8) for any feasible\n",
      "point. We maximize LDsubject to 0 ≤αi≤Cand∑N\n",
      "i=1αiyi= 0. In\n",
      "addition to (12.10)–(12.12), the Karush–Kuhn–Tucker conditions include\n",
      "the constraints\n",
      "αi[yi(xT\n",
      "iβ+β0)−(1−ξi)] = 0 , (12.14)\n",
      "θiξi= 0, (12.15)\n",
      "yi(xT\n",
      "iβ+β0)−(1−ξi)≥0, (12.16)\n",
      "fori= 1,... ,N . Together these equations (12.10)–(12.16) uniquely char-\n",
      "acterize the solution to the primal and dual problem.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.2 The Support Vector Classiﬁer 421\n",
      "From (12.10) we see that the solution for βhas the form\n",
      "ˆβ=N∑\n",
      "i=1ˆαiyixi, (12.17)\n",
      "with nonzero coeﬃcients ˆ αionly for those observations ifor which the\n",
      "constraints in (12.16) are exactly met (due to (12.14)). These observati ons\n",
      "are called the support vectors , since ˆβis represented in terms of them\n",
      "alone. Among these support points, some will lie on the edge of the margin\n",
      "(ˆξi= 0), and hence from (12.15) and (12.12) will be characterized by\n",
      "0<ˆαi< C; the remainder ( ˆξi>0) have ˆ αi=C. From (12.14) we can\n",
      "see that any of these margin points (0 <ˆαi,ˆξi= 0) can be used to solve\n",
      "forβ0, and we typically use an average of all the solutions for numerical\n",
      "stability.\n",
      "Maximizing the dual (12.13) is a simpler convex quadratic programming\n",
      "problem than the primal (12.9), and can be solved with standard techniques\n",
      "(Murray et al., 1981, for example).\n",
      "Given the solutions ˆβ0andˆβ, the decision function can be written as\n",
      "ˆG(x) = sign[ ˆf(x)]\n",
      "= sign[ xTˆβ+ˆβ0]. (12.18)\n",
      "The tuning parameter of this procedure is the cost parameter C.\n",
      "12.2.2 Mixture Example (Continued)\n",
      "Figure 12.2 shows the support vector boundary for the mixture example\n",
      "of Figure 2.5 on page 21, with two overlapping classes, for two diﬀerent\n",
      "values of the cost parameter C. The classiﬁers are rather similar in their\n",
      "performance. Points on the wrong side of the boundary are support vectors.\n",
      "In addition, points on the correct side of the boundary but close to it (in\n",
      "the margin), are also support vectors. The margin is larger for C= 0.01\n",
      "than it is for C= 10,000. Hence larger values of Cfocus attention more\n",
      "on (correctly classiﬁed) points near the decision boundary, while smaller\n",
      "values involve data further away. Either way, misclassiﬁed points are gi ven\n",
      "weight, no matter how far away. In this example the procedure is not very\n",
      "sensitive to choices of C, because of the rigidity of a linear boundary.\n",
      "The optimal value for Ccan be estimated by cross-validation, as dis-\n",
      "cussed in Chapter 7. Interestingly, the leave-one-out cross-validation error\n",
      "can be bounded above by the proportion of support points in the data. The\n",
      "reason is that leaving out an observation that is not a support vector will\n",
      "not change the solution. Hence these observations, being classiﬁed correctly\n",
      "by the original boundary, will be classiﬁed correctly in the cross-validatio n\n",
      "process. However this bound tends to be too high, and not generally useful\n",
      "for choosing C(62% and 85%, respectively, in our examples).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "422 12. Flexible Discriminants\n",
      ".. . . .. . . . . . .. . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . .. . . . ..\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "••\n",
      "•\n",
      "Training Error: 0.270\n",
      "Test Error:       0.288\n",
      "Bayes Error:    0.210\n",
      "C= 10000\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ".. .. . . .. . . . .. . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . .. . . . . . . .. . . . . . .. . . . .. . . .. ..\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o•\n",
      "Training Error: 0.26\n",
      "Test Error:       0.30\n",
      "Bayes Error:    0.21\n",
      "C= 0.01\n",
      "FIGURE 12.2. The linear support vector boundary for the mixture data exam-\n",
      "ple with two overlapping classes, for two diﬀerent values of C. The broken lines\n",
      "indicate the margins, where f(x) =±1. The support points ( αi>0) are all the\n",
      "points on the wrong side of their margin. The black solid dots are those support\n",
      "points falling exactly on the margin ( ξi= 0, αi>0). In the upper panel 62%of\n",
      "the observations are support points, while in the lower panel 85%are. The broken\n",
      "purple curve in the background is the Bayes decision boundary.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.3 Support Vector Machines and Kernels 423\n",
      "12.3 Support Vector Machines and Kernels\n",
      "The support vector classiﬁer described so far ﬁnds linear boundaries in the\n",
      "input feature space. As with other linear methods, we can make the pro-\n",
      "cedure more ﬂexible by enlarging the feature space using basis expansions\n",
      "such as polynomials or splines (Chapter 5). Generally linear boundaries\n",
      "in the enlarged space achieve better training-class separation, and trans-\n",
      "late to nonlinear boundaries in the original space. Once the basis functions\n",
      "hm(x), m= 1,... ,M are selected, the procedure is the same as before. We\n",
      "ﬁt the SV classiﬁer using input features h(xi) = (h1(xi),h2(xi),... ,h M(xi)),\n",
      "i= 1,... ,N , and produce the (nonlinear) function ˆf(x) =h(x)Tˆβ+ˆβ0.\n",
      "The classiﬁer is ˆG(x) = sign( ˆf(x)) as before.\n",
      "Thesupport vector machine classiﬁer is an extension of this idea, where\n",
      "the dimension of the enlarged space is allowed to get very large, inﬁnite\n",
      "in some cases. It might seem that the computations would become pro-\n",
      "hibitive. It would also seem that with suﬃcient basis functions, the data\n",
      "would be separable, and overﬁtting would occur. We ﬁrst show how the\n",
      "SVM technology deals with these issues. We then see that in fact the SVM\n",
      "classiﬁer is solving a function-ﬁtting problem using a particular criterion\n",
      "and form of regularization, and is part of a much bigger class of problems\n",
      "that includes the smoothing splines of Chapter 5. The reader may wish\n",
      "to consult Section 5.8, which provides background material and overlaps\n",
      "somewhat with the next two sections.\n",
      "12.3.1 Computing the SVM for Classiﬁcation\n",
      "We can represent the optimization problem (12.9) and its solution in a\n",
      "special way that only involves the input features via inner products. We do\n",
      "this directly for the transformed feature vectors h(xi). We then see that for\n",
      "particular choices of h, these inner products can be computed very cheaply.\n",
      "The Lagrange dual function (12.13) has the form\n",
      "LD=N∑\n",
      "i=1αi−1\n",
      "2N∑\n",
      "i=1N∑\n",
      "i′=1αiαi′yiyi′⟨h(xi),h(xi′)⟩. (12.19)\n",
      "From (12.10) we see that the solution function f(x) can be written\n",
      "f(x) = h(x)Tβ+β0\n",
      "=N∑\n",
      "i=1αiyi⟨h(x),h(xi)⟩+β0. (12.20)\n",
      "As before, given αi,β0can be determined by solving yif(xi) = 1 in (12.20)\n",
      "for any (or all) xifor which 0 < αi< C.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "424 12. Flexible Discriminants\n",
      "So both (12.19) and (12.20) involve h(x) only through inner products. In\n",
      "fact, we need not specify the transformation h(x) at all, but require only\n",
      "knowledge of the kernel function\n",
      "K(x,x′) =⟨h(x),h(x′)⟩ (12.21)\n",
      "that computes inner products in the transformed space. Kshould be a\n",
      "symmetric positive (semi-) deﬁnite function; see Section 5.8.1.\n",
      "Three popular choices for Kin the SVM literature are\n",
      "dth-Degree polynomial: K(x,x′) = (1 + ⟨x,x′⟩)d,\n",
      "Radial basis: K(x,x′) = exp( −γ∥x−x′∥2),\n",
      "Neural network: K(x,x′) = tanh( κ1⟨x,x′⟩+κ2).(12.22)\n",
      "Consider for example a feature space with two inputs X1andX2, and a\n",
      "polynomial kernel of degree 2. Then\n",
      "K(X,X′) = (1 + ⟨X,X′⟩)2\n",
      "= (1 + X1X′\n",
      "1+X2X′\n",
      "2)2\n",
      "= 1 + 2 X1X′\n",
      "1+ 2X2X′\n",
      "2+ (X1X′\n",
      "1)2+ (X2X′\n",
      "2)2+ 2X1X′\n",
      "1X2X′\n",
      "2.\n",
      "(12.23)\n",
      "Then M= 6, and if we choose h1(X) = 1, h2(X) =√\n",
      "2X1,h3(X) =√\n",
      "2X2,h4(X) =X2\n",
      "1,h5(X) =X2\n",
      "2, andh6(X) =√\n",
      "2X1X2, then K(X,X′) =\n",
      "⟨h(X),h(X′)⟩. From (12.20) we see that the solution can be written\n",
      "ˆf(x) =N∑\n",
      "i=1ˆαiyiK(x,xi) +ˆβ0. (12.24)\n",
      "The role of the parameter Cis clearer in an enlarged feature space,\n",
      "since perfect separation is often achievable there. A large value of Cwill\n",
      "discourage any positive ξi, and lead to an overﬁt wiggly boundary in the\n",
      "original feature space; a small value of Cwill encourage a small value of\n",
      "∥β∥, which in turn causes f(x) and hence the boundary to be smoother.\n",
      "Figure 12.3 show two nonlinear support vector machines applied to the\n",
      "mixture example of Chapter 2. The regularization parameter was chosen\n",
      "in both cases to achieve good test error. The radial basis kernel produces\n",
      "a boundary quite similar to the Bayes optimal boundary for this example;\n",
      "compare Figure 2.5.\n",
      "In the early literature on support vectors, there were claims that the\n",
      "kernel property of the support vector machine is unique to it and allows\n",
      "one to ﬁnesse the curse of dimensionality. Neither of these claims is true,\n",
      "and we go into both of these issues in the next three subsections.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.3 Support Vector Machines and Kernels 425\n",
      "SVM - Degree-4 Polynomial in Feature Space\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . . .. . . . . . .. . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "•••••••\n",
      "•\n",
      "• ••\n",
      "•\n",
      "•••\n",
      "Training Error: 0.180\n",
      "Test Error:       0.245\n",
      "Bayes Error:    0.210\n",
      "SVM - Radial Kernel in Feature Space\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . .. . . . . . .. . . . . .. . . . . .. . . . .. . . . .. . . .. . . .. . . .. . . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . . .. . . .. . . .. . . .. . . . .. . . . .. . . . . .. . . . . .. . . . . . .. . . . . . .. . . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "•\n",
      "•••\n",
      "•\n",
      "••••\n",
      "••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "Training Error: 0.160\n",
      "Test Error:       0.218\n",
      "Bayes Error:    0.210\n",
      "FIGURE 12.3. Two nonlinear SVMs for the mixture data. The upper plot uses\n",
      "a4th degree polynomial kernel, the lower a radial basis kernel (wi thγ= 1). In\n",
      "each case Cwas tuned to approximately achieve the best test error perform ance,\n",
      "andC= 1worked well in both cases. The radial basis kernel performs th e best\n",
      "(close to Bayes optimal), as might be expected given the data a rise from mixtures\n",
      "of Gaussians. The broken purple curve in the background is the B ayes decision\n",
      "boundary.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "426 12. Flexible Discriminants\n",
      "−3 −2 −1 0 1 2 30.0 0.5 1.0 1.5 2.0 2.5 3.0Hinge Loss\n",
      "Binomial Deviance\n",
      "Squared Error\n",
      "Class HuberLoss\n",
      "yf\n",
      "FIGURE 12.4. The support vector loss function (hinge loss), compared to the\n",
      "negative log-likelihood loss (binomial deviance) for logisti c regression, squared-er-\n",
      "ror loss, and a “Huberized” version of the squared hinge loss. A ll are shown as a\n",
      "function of yfrather than f, because of the symmetry between the y= +1 and\n",
      "y=−1case. The deviance and Huber have the same asymptotes as the SVM\n",
      "loss, but are rounded in the interior. All are scaled to have the limiting left-tail\n",
      "slope of −1.\n",
      "12.3.2 The SVM as a Penalization Method\n",
      "With f(x) =h(x)Tβ+β0, consider the optimization problem\n",
      "min\n",
      "β0, βN∑\n",
      "i=1[1−yif(xi)]++λ\n",
      "2∥β∥2(12.25)\n",
      "where the subscript “+” indicates positive part. This has the form loss+\n",
      "penalty , which is a familiar paradigm in function estimation. It is easy to\n",
      "show (Exercise 12.1) that the solution to (12.25), with λ= 1/C, is the\n",
      "same as that for (12.8).\n",
      "Examination of the “hinge” loss function L(y,f) = [1 −yf]+shows that\n",
      "it is reasonable for two-class classiﬁcation, when compared to other more\n",
      "traditional loss functions. Figure 12.4 compares it to the log-likelihood l oss\n",
      "for logistic regression, as well as squared-error loss and a variant thereo f.\n",
      "The (negative) log-likelihood or binomial deviance has similar tails as the\n",
      "SVM loss, giving zero penalty to points well inside their margin, and a\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.3 Support Vector Machines and Kernels 427\n",
      "TABLE 12.1. The population minimizers for the diﬀerent loss functions in Fig -\n",
      "ure 12.4. Logistic regression uses the binomial log-likelih ood or deviance. Linear\n",
      "discriminant analysis (Exercise 4.2) uses squared-error loss. The SVM hinge loss\n",
      "estimates the mode of the posterior class probabilities, wh ereas the others estimate\n",
      "a linear transformation of these probabilities.\n",
      "Loss Function L[y, f(x)] Minimizing Function\n",
      "Binomial\n",
      "Deviance log[1 + e−yf(x)]f(x) = logPr(Y= +1|x)\n",
      "Pr(Y= -1|x)\n",
      "SVM Hinge\n",
      "Loss[1−yf(x)]+ f(x) = sign[Pr( Y= +1|x)−1\n",
      "2]\n",
      "Squared\n",
      "Error[y−f(x)]2= [1−yf(x)]2f(x) = 2Pr( Y= +1|x)−1\n",
      "“Huberised”\n",
      "Square\n",
      "Hinge Loss−4yf(x), yf (x)<-1\n",
      "[1−yf(x)]2\n",
      "+otherwisef(x) = 2Pr( Y= +1|x)−1\n",
      "linear penalty to points on the wrong side and far away. Squared-error, on\n",
      "the other hand gives a quadratic penalty, and points well inside their own\n",
      "margin have a strong inﬂuence on the model as well. The squared hinge\n",
      "lossL(y,f) = [1 −yf]2\n",
      "+is like the quadratic, except it is zero for points\n",
      "inside their margin. It still rises quadratically in the left tail, and wil l be\n",
      "less robust than hinge or deviance to misclassiﬁed observations. Recently\n",
      "Rosset and Zhu (2007) proposed a “Huberized” version of the squared hinge\n",
      "loss, which converts smoothly to a linear loss at yf=−1.\n",
      "We can characterize these loss functions in terms of what they are es-\n",
      "timating at the population level. We consider minimizing E L(Y,f(X)).\n",
      "Table 12.1 summarizes the results. Whereas the hinge loss estimates the\n",
      "classiﬁer G(x) itself, all the others estimate a transformation of the class\n",
      "posterior probabilities. The “Huberized” square hinge loss shares attractive\n",
      "properties of logistic regression (smooth loss function, estimates proba bili-\n",
      "ties), as well as the SVM hinge loss (support points).\n",
      "Formulation (12.25) casts the SVM as a regularized function estimation\n",
      "problem, where the coeﬃcients of the linear expansion f(x) =β0+h(x)Tβ\n",
      "are shrunk toward zero (excluding the constant). If h(x) represents a hierar-\n",
      "chical basis having some ordered structure (such as ordered in roughness),\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "428 12. Flexible Discriminants\n",
      "then the uniform shrinkage makes more sense if the rougher elements hjin\n",
      "the vector hhave smaller norm.\n",
      "All the loss-function in Table 12.1 except squared-error are so called\n",
      "“margin maximizing loss-functions” (Rosset et al., 2004b). This means that\n",
      "if the data are separable, then the limit of ˆβλin (12.25) as λ→0 deﬁnes\n",
      "the optimal separating hyperplane1.\n",
      "12.3.3 Function Estimation and Reproducing Kernels\n",
      "Here we describe SVMs in terms of function estimation in reproducing\n",
      "kernel Hilbert spaces, where the kernel property abounds. This material is\n",
      "discussed in some detail in Section 5.8. This provides another view of the\n",
      "support vector classiﬁer, and helps to clarify how it works.\n",
      "Suppose the basis harises from the (possibly ﬁnite) eigen-expansion of\n",
      "a positive deﬁnite kernel K,\n",
      "K(x,x′) =∞∑\n",
      "m=1φm(x)φm(x′)δm (12.26)\n",
      "andhm(x) =√δmφm(x). Then with θm=√δmβm, we can write (12.25)\n",
      "as\n",
      "min\n",
      "β0, θN∑\n",
      "i=1[\n",
      "1−yi(β0+∞∑\n",
      "m=1θmφm(xi))]\n",
      "++λ\n",
      "2∞∑\n",
      "m=1θ2\n",
      "m\n",
      "δm. (12.27)\n",
      "Now (12.27) is identical in form to (5.49) on page 169 in Section 5.8, a nd\n",
      "the theory of reproducing kernel Hilbert spaces described there guarantees\n",
      "a ﬁnite-dimensional solution of the form\n",
      "f(x) =β0+N∑\n",
      "i=1αiK(x,xi). (12.28)\n",
      "In particular we see there an equivalent version of the optimization crite-\n",
      "rion (12.19) [Equation (5.67) in Section 5.8.2; see also Wahba et al. (2000)],\n",
      "min\n",
      "β0,αN∑\n",
      "i=1(1−yif(xi))++λ\n",
      "2αTKα, (12.29)\n",
      "whereKis the N×Nmatrix of kernel evaluations for all pairs of training\n",
      "features (Exercise 12.2).\n",
      "These models are quite general, and include, for example, the entire fam-\n",
      "ily of smoothing splines, additive and interaction spline models discussed\n",
      "1For logistic regression with separable data, ˆβλdiverges, but ˆβλ/||ˆβλconverges to\n",
      "the optimal separating direction.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.3 Support Vector Machines and Kernels 429\n",
      "in Chapters 5 and 9, and in more detail in Wahba (1990) and Hastie and\n",
      "Tibshirani (1990). They can be expressed more generally as\n",
      "min\n",
      "f∈HN∑\n",
      "i=1[1−yif(xi)]++λJ(f), (12.30)\n",
      "where His the structured space of functions, and J(f) an appropriate reg-\n",
      "ularizer on that space. For example, suppose His the space of additive\n",
      "functions f(x) =∑p\n",
      "j=1fj(xj), and J(f) =∑\n",
      "j∫\n",
      "{f′′\n",
      "j(xj)}2dxj. Then the\n",
      "solution to (12.30) is an additive cubic spline, and has a kernel representa-\n",
      "tion (12.28) with K(x,x′) =∑p\n",
      "j=1Kj(xj,x′\n",
      "j). Each of the Kjis the kernel\n",
      "appropriate for the univariate smoothing spline in xj(Wahba, 1990).\n",
      "Conversely this discussion also shows that, for example, anyof the kernels\n",
      "described in (12.22) above can be used with anyconvex loss function, and\n",
      "will also lead to a ﬁnite-dimensional representation of the form (12.28).\n",
      "Figure 12.5 uses the same kernel functions as in Figure 12.3, except using\n",
      "the binomial log-likelihood as a loss function2. The ﬁtted function is hence\n",
      "an estimate of the log-odds,\n",
      "ˆf(x) = logˆPr(Y= +1|x)\n",
      "ˆPr(Y=−1|x)\n",
      "=ˆβ0+N∑\n",
      "i=1ˆαiK(x,xi), (12.31)\n",
      "or conversely we get an estimate of the class probabilities\n",
      "ˆPr(Y= +1|x) =1\n",
      "1 +e−ˆβ0−PN\n",
      "i=1ˆαiK(x,xi). (12.32)\n",
      "The ﬁtted models are quite similar in shape and performance. Examples\n",
      "and more details are given in Section 5.8.\n",
      "It does happen that for SVMs, a sizable fraction of the Nvalues of αi\n",
      "can be zero (the nonsupport points). In the two examples in Figure 12.3,\n",
      "these fractions are 42% and 45%, respectively. This is a consequence of the\n",
      "piecewise linear nature of the ﬁrst part of the criterion (12.25). The lower\n",
      "the class overlap (on the training data), the greater this fraction will be.\n",
      "Reducing λwill generally reduce the overlap (allowing a more ﬂexible f).\n",
      "A small number of support points means that ˆf(x) can be evaluated more\n",
      "quickly, which is important at lookup time. Of course, reducing the overlap\n",
      "too much can lead to poor generalization.\n",
      "2Ji Zhu assisted in the preparation of these examples.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "430 12. Flexible Discriminants\n",
      "LR - Degree-4 Polynomial in Feature Space\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . .. . . . . . . .. . . . . . .. . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "Training Error: 0.190\n",
      "Test Error:       0.263\n",
      "Bayes Error:    0.210\n",
      "LR - Radial Kernel in Feature Space\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ".. . . .. . . . .. . . . . .. . . . . . .. . . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "Training Error: 0.150\n",
      "Test Error:       0.221\n",
      "Bayes Error:    0.210\n",
      "FIGURE 12.5. The logistic regression versions of the SVM models in Fig-\n",
      "ure 12.3, using the identical kernels and hence penalties, but the l og-likelihood\n",
      "loss instead of the SVM loss function. The two broken contours corr espond to\n",
      "posterior probabilities of 0.75and0.25for the +1 class (or vice versa). The bro-\n",
      "ken purple curve in the background is the Bayes decision bounda ry.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.3 Support Vector Machines and Kernels 431\n",
      "TABLE 12.2. Skin of the orange: Shown are mean (standard error of the mean)\n",
      "of the test error over 50simulations. BRUTO ﬁts an additive spline model adap-\n",
      "tively, while MARS ﬁts a low-order interaction model adaptivel y.\n",
      "Test Error (SE)\n",
      "Method No Noise Features Six Noise Features\n",
      "1 SV Classiﬁer 0.450 (0.003) 0.472 (0.003)\n",
      "2 SVM/poly 2 0.078 (0.003) 0.152 (0.004)\n",
      "3 SVM/poly 5 0.180 (0.004) 0.370 (0.004)\n",
      "4 SVM/poly 10 0.230 (0.003) 0.434 (0.002)\n",
      "5 BRUTO 0.084 (0.003) 0.090 (0.003)\n",
      "6 MARS 0.156 (0.004) 0.173 (0.005)\n",
      "Bayes 0.029 0.029\n",
      "12.3.4 SVMs and the Curse of Dimensionality\n",
      "In this section, we address the question of whether SVMs have some edge\n",
      "on the curse of dimensionality. Notice that in expression (12.23) we are not\n",
      "allowed a fully general inner product in the space of powers and products.\n",
      "For example, all terms of the form 2 XjX′\n",
      "jare given equal weight, and the\n",
      "kernel cannot adapt itself to concentrate on subspaces. If the number of\n",
      "features pwere large, but the class separation occurred only in the linear\n",
      "subspace spanned by say X1andX2, this kernel would not easily ﬁnd the\n",
      "structure and would suﬀer from having many dimensions to search over.\n",
      "One would have to build knowledge about the subspace into the kernel;\n",
      "that is, tell it to ignore all but the ﬁrst two inputs. If such knowledge were\n",
      "available a priori, much of statistical learning would be made much easier .\n",
      "A major goal of adaptive methods is to discover such structure.\n",
      "We support these statements with an illustrative example. We generated\n",
      "100 observations in each of two classes. The ﬁrst class has four standard\n",
      "normal independent features X1,X2,X3,X4. The second class also has four\n",
      "standard normal independent features, but conditioned on 9 ≤∑X2\n",
      "j≤16.\n",
      "This is a relatively easy problem. As a second harder problem, we aug-\n",
      "mented the features with an additional six standard Gaussian noise fea-\n",
      "tures. Hence the second class almost completely surrounds the ﬁrst, like the\n",
      "skin surrounding the orange, in a four-dimensional subspace. The Bayes er-\n",
      "ror rate for this problem is 0 .029 (irrespective of dimension). We generated\n",
      "1000 test observations to compare diﬀerent procedures. The average test\n",
      "errors over 50 simulations, with and without noise features, are shown in\n",
      "Table 12.2.\n",
      "Line 1 uses the support vector classiﬁer in the original feature space.\n",
      "Lines 2–4 refer to the support vector machine with a 2-, 5- and 10-dimension-\n",
      "al polynomial kernel. For all support vector procedures, we chose the cost\n",
      "parameter Cto minimize the test error, to be as fair as possible to the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "432 12. Flexible Discriminants\n",
      "1e−01 1e+01 1e+030.20 0.25 0.30 0.35\n",
      "1e−01 1e+01 1e+03 1e−01 1e+01 1e+03 1e−01 1e+01 1e+03Test Error\n",
      "CTest Error Curves − SVM with Radial Kernel\n",
      "γ= 5 γ= 1 γ= 0.5 γ= 0.1\n",
      "FIGURE 12.6. Test-error curves as a function of the cost parameter Cfor the\n",
      "radial-kernel SVM classiﬁer on the mixture data. At the top of eac h plot is the\n",
      "scale parameter γfor the radial kernel: Kγ(x, y) = exp −γ||x−y||2. The optimal\n",
      "value for Cdepends quite strongly on the scale of the kernel. The Bayes erro r\n",
      "rate is indicated by the broken horizontal lines.\n",
      "method. Line 5 ﬁts an additive spline model to the ( −1,+1) response by\n",
      "least squares, using the BRUTO algorithm for additive models, described\n",
      "in Hastie and Tibshirani (1990). Line 6 uses MARS (multivariate adaptiv e\n",
      "regression splines) allowing interaction of all orders, as described in Chap-\n",
      "ter 9; as such it is comparable with the SVM/poly 10. Both BRUTO and\n",
      "MARS have the ability to ignore redundant variables. Test error was not\n",
      "used to choose the smoothing parameters in either of lines 5 or 6.\n",
      "In the original feature space, a hyperplane cannot separate the classes,\n",
      "and the support vector classiﬁer (line 1) does poorly. The polynomial sup-\n",
      "port vector machine makes a substantial improvement in test error rate,\n",
      "but is adversely aﬀected by the six noise features. It is also very sensitive to\n",
      "the choice of kernel: the second degree polynomial kernel (line 2) does best,\n",
      "since the true decision boundary is a second-degree polynomial. However,\n",
      "higher-degree polynomial kernels (lines 3 and 4) do much worse. BRUTO\n",
      "performs well, since the boundary is additive. BRUTO and MARS adapt\n",
      "well: their performance does not deteriorate much in the presence of noise.\n",
      "12.3.5 A Path Algorithm for the SVM Classiﬁer\n",
      "The regularization parameter for the SVM classiﬁer is the cost parameter\n",
      "C, or its inverse λin (12.25). Common usage is to set Chigh, leading often\n",
      "to somewhat overﬁt classiﬁers.\n",
      "Figure 12.6 shows the test error on the mixture data as a function of\n",
      "C, using diﬀerent radial-kernel parameters γ. When γ= 5 (narrow peaked\n",
      "kernels), the heaviest regularization (small C) is called for. With γ= 1\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.3 Support Vector Machines and Kernels 433\n",
      "−0.5 0.0 0.5 1.0 1.5 2.0−1.0 −0.5 0.0 0.5 1.0 1.5789\n",
      "1011\n",
      "12\n",
      "123\n",
      "45\n",
      "61/||β|| f(x) = 0f(x) = +1\n",
      "f(x) =−1\n",
      "0.0 0.2 0.4 0.6 0.8 1.00 2 4 6 8 101\n",
      "2\n",
      "34\n",
      "56\n",
      "789\n",
      "1011\n",
      "12\n",
      "αi(λ)λ\n",
      "FIGURE 12.7. A simple example illustrates the SVM path algorithm. (left\n",
      "panel:) This plot illustrates the state of the model at λ= 0.05. The ‘ ‘ + 1”\n",
      "points are orange, the “ −1” blue. λ= 1/2, and the width of the soft margin\n",
      "is2/||β||= 2×0.587. Two blue points {3,5}are misclassiﬁed, while the two or-\n",
      "ange points {10,12}are correctly classiﬁed, but on the wrong side of their margin\n",
      "f(x) = +1 ; each of these has yif(xi)<1. The three square shaped points {2,6,7}\n",
      "are exactly on their margins. (right panel:) This plot shows the piecewise linear\n",
      "proﬁles αi(λ). The horizontal broken line at λ= 1/2indicates the state of the αi\n",
      "for the model in the left plot.\n",
      "(the value used in Figure 12.3), an intermediate value of Cis required.\n",
      "Clearly in situations such as these, we need to determine a good choice\n",
      "forC, perhaps by cross-validation. Here we describe a path algorithm (in\n",
      "the spirit of Section 3.8) for eﬃciently ﬁtting the entire sequence of SVM\n",
      "models obtained by varying C.\n",
      "It is convenient to use the loss+penalty formulation (12.25), along with\n",
      "Figure 12.4. This leads to a solution for βat a given value of λ:\n",
      "βλ=1\n",
      "λN∑\n",
      "i=1αiyixi. (12.33)\n",
      "Theαiare again Lagrange multipliers, but in this case they all lie in [0 ,1].\n",
      "Figure 12.7 illustrates the setup. It can be shown that the KKT optimal-\n",
      "ity conditions imply that the labeled points ( xi,yi) fall into three distinct\n",
      "groups:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "434 12. Flexible Discriminants\n",
      "•Observations correctly classiﬁed and outside their margins. They have\n",
      "yif(xi)>1, and Lagrange multipliers αi= 0. Examples are the\n",
      "orange points 8, 9 and 11, and the blue points 1 and 4.\n",
      "•Observations sitting on their margins with yif(xi) = 1, with Lagrange\n",
      "multipliers αi∈[0,1]. Examples are the orange 7 and the blue 2 and\n",
      "8.\n",
      "•Observations inside their margins have yif(xi)<1, with αi= 1.\n",
      "Examples are the blue 3 and 5, and the orange 10 and 12.\n",
      "The idea for the path algorithm is as follows. Initially λis large, the\n",
      "margin 1 /||βλ||is wide, and all points are inside their margin and have\n",
      "αi= 1. As λdecreases, 1 /||βλ||decreases, and the margin gets narrower.\n",
      "Some points will move from inside their margins to outside their margins,\n",
      "and their αiwill change from 1 to 0. By continuity of the αi(λ), these points\n",
      "willlinger on the margin during this transition. From (12.33) we see that\n",
      "the points with αi= 1 make ﬁxed contributions to β(λ), and those with\n",
      "αi= 0 make no contribution. So all that changes as λdecreases are the\n",
      "αi∈[0,1] of those (small number) of points on the margin. Since all these\n",
      "points have yif(xi) = 1, this results in a small set of linear equations that\n",
      "prescribe how αi(λ) and hence βλchanges during these transitions. This\n",
      "results in piecewise linear paths for each of the αi(λ). The breaks occur\n",
      "when points cross the margin. Figure 12.7 (right panel) shows the αi(λ)\n",
      "proﬁles for the small example in the left panel.\n",
      "Although we have described this for linear SVMs, exactly the same idea\n",
      "works for nonlinear models, in which (12.33) is replaced by\n",
      "fλ(x) =1\n",
      "λN∑\n",
      "i=1αiyiK(x,xi). (12.34)\n",
      "Details can be found in Hastie et al. (2004). An Rpackagesvmpath is\n",
      "available on CRAN for ﬁtting these models.\n",
      "12.3.6 Support Vector Machines for Regression\n",
      "In this section we show how SVMs can be adapted for regression with a\n",
      "quantitative response, in ways that inherit some of the properties of the\n",
      "SVM classiﬁer. We ﬁrst discuss the linear regression model\n",
      "f(x) =xTβ+β0, (12.35)\n",
      "and then handle nonlinear generalizations. To estimate β, we consider min-\n",
      "imization of\n",
      "H(β,β0) =N∑\n",
      "i=1V(yi−f(xi)) +λ\n",
      "2∥β∥2, (12.36)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.3 Support Vector Machines and Kernels 435\n",
      "-4 -2 0 2 4-1 0 1 2 3 4\n",
      "-4 -2 0 2 40 2 4 6 8 10 12\n",
      "ǫ −ǫ c −cVH(r)Vǫ(r)\n",
      "r r\n",
      "FIGURE 12.8. The left panel shows the ǫ-insensitive error function used by the\n",
      "support vector regression machine. The right panel shows the e rror function used\n",
      "in Huber’s robust regression (blue curve). Beyond |c|, the function changes from\n",
      "quadratic to linear.\n",
      "where\n",
      "Vǫ(r) ={\n",
      "0 if |r|< ǫ,\n",
      "|r| −ǫ,otherwise.(12.37)\n",
      "This is an “ ǫ-insensitive” error measure, ignoring errors of size less than\n",
      "ǫ(left panel of Figure 12.8). There is a rough analogy with the support\n",
      "vector classiﬁcation setup, where points on the correct side of the deci-\n",
      "sion boundary and far away from it, are ignored in the optimization. In\n",
      "regression, these “low error” points are the ones with small residuals.\n",
      "It is interesting to contrast this with error measures used in robust re-\n",
      "gression in statistics. The most popular, due to Huber (1964), has the for m\n",
      "VH(r) ={\n",
      "r2/2 if |r| ≤c,\n",
      "c|r| −c2/2,|r|> c,(12.38)\n",
      "shown in the right panel of Figure 12.8. This function reduces from quadratic\n",
      "to linear the contributions of observations with absolute residual greater\n",
      "than a prechosen constant c. This makes the ﬁtting less sensitive to out-\n",
      "liers. The support vector error measure (12.37) also has linear tails (beyo nd\n",
      "ǫ), but in addition it ﬂattens the contributions of those cases with small\n",
      "residuals.\n",
      "Ifˆβ,ˆβ0are the minimizers of H, the solution function can be shown to\n",
      "have the form\n",
      "ˆβ=N∑\n",
      "i=1(ˆα∗\n",
      "i−ˆαi)xi, (12.39)\n",
      "ˆf(x) =N∑\n",
      "i=1(ˆα∗\n",
      "i−ˆαi)⟨x,xi⟩+β0, (12.40)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "436 12. Flexible Discriminants\n",
      "where ˆ αi,ˆα∗\n",
      "iare positive and solve the quadratic programming problem\n",
      "min\n",
      "αi,α∗\n",
      "iǫN∑\n",
      "i=1(α∗\n",
      "i+αi)−N∑\n",
      "i=1yi(α∗\n",
      "i−αi) +1\n",
      "2N∑\n",
      "i,i′=1(α∗\n",
      "i−αi)(α∗\n",
      "i′−αi′)⟨xi,xi′⟩\n",
      "subject to the constraints\n",
      "0≤αi, α∗\n",
      "i≤1/λ,\n",
      "N∑\n",
      "i=1(α∗\n",
      "i−αi) = 0, (12.41)\n",
      "αiα∗\n",
      "i= 0.\n",
      "Due to the nature of these constraints, typically only a subset of the solution\n",
      "values (ˆ α∗\n",
      "i−ˆαi) are nonzero, and the associated data values are called the\n",
      "support vectors. As was the case in the classiﬁcation setting, the solution\n",
      "depends on the input values only through the inner products ⟨xi,xi′⟩. Thus\n",
      "we can generalize the methods to richer spaces by deﬁning an appropriate\n",
      "inner product, for example, one of those deﬁned in (12.22).\n",
      "Note that there are parameters, ǫandλ, associated with the criterion\n",
      "(12.36). These seem to play diﬀerent roles. ǫis a parameter of the loss\n",
      "function Vǫ, just like cis for VH. Note that both VǫandVHdepend on the\n",
      "scale of yand hence r. If we scale our response (and hence use VH(r/σ) and\n",
      "Vǫ(r/σ) instead), then we might consider using preset values for candǫ(the\n",
      "value c= 1.345 achieves 95% eﬃciency for the Gaussian). The quantity λ\n",
      "is a more traditional regularization parameter, and can be estimated for\n",
      "example by cross-validation.\n",
      "12.3.7 Regression and Kernels\n",
      "As discussed in Section 12.3.3, this kernel property is not unique to sup-\n",
      "port vector machines. Suppose we consider approximation of the regression\n",
      "function in terms of a set of basis functions {hm(x)},m= 1,2,... ,M :\n",
      "f(x) =M∑\n",
      "m=1βmhm(x) +β0. (12.42)\n",
      "To estimate βandβ0we minimize\n",
      "H(β,β0) =N∑\n",
      "i=1V(yi−f(xi)) +λ\n",
      "2∑\n",
      "β2\n",
      "m (12.43)\n",
      "for some general error measure V(r). For any choice of V(r), the solution\n",
      "ˆf(x) =∑ˆβmhm(x) +ˆβ0has the form\n",
      "ˆf(x) =N∑\n",
      "i=1ˆaiK(x,xi) (12.44)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.3 Support Vector Machines and Kernels 437\n",
      "withK(x,y) =∑M\n",
      "m=1hm(x)hm(y). Notice that this has the same form\n",
      "as both the radial basis function expansion and a regularization estimate,\n",
      "discussed in Chapters 5 and 6.\n",
      "For concreteness, let’s work out the case V(r) =r2. LetHbe the N×M\n",
      "basis matrix with imth element hm(xi), and suppose that M > N is large.\n",
      "For simplicity we assume that β0= 0, or that the constant is absorbed in\n",
      "h; see Exercise 12.3 for an alternative.\n",
      "We estimate βby minimizing the penalized least squares criterion\n",
      "H(β) = (y−Hβ)T(y−Hβ) +λ∥β∥2. (12.45)\n",
      "The solution is\n",
      "ˆy=Hˆβ (12.46)\n",
      "withˆβdetermined by\n",
      "−HT(y−Hˆβ) +λˆβ= 0. (12.47)\n",
      "From this it appears that we need to evaluate the M×Mmatrix of inner\n",
      "products in the transformed space. However, we can premultiply by Hto\n",
      "give\n",
      "Hˆβ= (HHT+λI)−1HHTy. (12.48)\n",
      "TheN×Nmatrix HHTconsists of inner products between pairs of obser-\n",
      "vations i,i′; that is, the evaluation of an inner product kernel {HHT}i,i′=\n",
      "K(xi,xi′). It is easy to show (12.44) directly in this case, that the predicted\n",
      "values at an arbitrary xsatisfy\n",
      "ˆf(x) = h(x)Tˆβ\n",
      "=N∑\n",
      "i=1ˆαiK(x,xi), (12.49)\n",
      "where ˆ α= (HHT+λI)−1y. As in the support vector machine, we need not\n",
      "specify or evaluate the large set of functions h1(x),h2(x),... ,h M(x). Only\n",
      "the inner product kernel K(xi,xi′) need be evaluated, at the Ntraining\n",
      "points for each i,i′and at points xfor predictions there. Careful choice\n",
      "ofhm(such as the eigenfunctions of particular, easy-to-evaluate kernels\n",
      "K) means, for example, that HHTcan be computed at a cost of N2/2\n",
      "evaluations of K, rather than the direct cost N2M.\n",
      "Note, however, that this property depends on the choice of squared norm\n",
      "∥β∥2in the penalty. It does not hold, for example, for the L1norm |β|,\n",
      "which may lead to a superior model.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "438 12. Flexible Discriminants\n",
      "12.3.8 Discussion\n",
      "The support vector machine can be extended to multiclass problems, es-\n",
      "sentially by solving many two-class problems. A classiﬁer is built for each\n",
      "pair of classes, and the ﬁnal classiﬁer is the one that dominates the most\n",
      "(Kressel, 1999; Friedman, 1996; Hastie and Tibshirani, 1998). Alternati vely,\n",
      "one could use the multinomial loss function along with a suitable kernel,\n",
      "as in Section 12.3.3. SVMs have applications in many other supervised\n",
      "and unsupervised learning problems. At the time of this writing, empirical\n",
      "evidence suggests that it performs well in many real learning problems.\n",
      "Finally, we mention the connection of the support vector machine and\n",
      "structural risk minimization (7.9). Suppose the training points (or their\n",
      "basis expansion) are contained in a sphere of radius R, and let G(x) =\n",
      "sign[f(x)] = sign[ βTx+β0] as in (12.2). Then one can show that the class\n",
      "of functions {G(x),∥β∥ ≤A}has VC-dimension hsatisfying\n",
      "h≤R2A2. (12.50)\n",
      "Iff(x) separates the training data, optimally for ∥β∥ ≤A, then with\n",
      "probability at least 1 −ηover training sets (Vapnik, 1996, page 139):\n",
      "Error Test≤4h[log (2 N/h) + 1]−log (η/4)\n",
      "N. (12.51)\n",
      "The support vector classiﬁer was one of the ﬁrst practical learning pro-\n",
      "cedures for which useful bounds on the VC dimension could be obtained,\n",
      "and hence the SRM program could be carried out. However in the deriva-\n",
      "tion, balls are put around the data points—a process that depends on the\n",
      "observed values of the features. Hence in a strict sense, the VC complexity\n",
      "of the class is not ﬁxed a priori, before seeing the features.\n",
      "The regularization parameter Ccontrols an upper bound on the VC\n",
      "dimension of the classiﬁer. Following the SRM paradigm, we could choose C\n",
      "by minimizing the upper bound on the test error, given in (12.51). However,\n",
      "it is not clear that this has any advantage over the use of cross-validation\n",
      "for choice of C.\n",
      "12.4 Generalizing Linear Discriminant Analysis\n",
      "In Section 4.3 we discussed linear discriminant analysis (LDA), a funda-\n",
      "mental tool for classiﬁcation. For the remainder of this chapter we discuss\n",
      "a class of techniques that produce better classiﬁers than LDA by directly\n",
      "generalizing LDA.\n",
      "Some of the virtues of LDA are as follows:\n",
      "•It is a simple prototype classiﬁer. A new observation is classiﬁed to the\n",
      "class with closest centroid. A slight twist is that distance is measured\n",
      "in the Mahalanobis metric, using a pooled covariance estimate.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.4 Generalizing Linear Discriminant Analysis 439\n",
      "•LDA is the estimated Bayes classiﬁer if the observations are multi-\n",
      "variate Gaussian in each class, with a common covariance matrix.\n",
      "Since this assumption is unlikely to be true, this might not seem to\n",
      "be much of a virtue.\n",
      "•The decision boundaries created by LDA are linear, leading to deci-\n",
      "sion rules that are simple to describe and implement.\n",
      "•LDA provides natural low-dimensional views of the data. For exam-\n",
      "ple, Figure 12.12 is an informative two-dimensional view of data in\n",
      "256 dimensions with ten classes.\n",
      "•Often LDA produces the best classiﬁcation results, because of its\n",
      "simplicity and low variance. LDA was among the top three classiﬁers\n",
      "for 11 of the 22 datasets studied in the STATLOG project (Michie et\n",
      "al., 1994)3.\n",
      "Unfortunately the simplicity of LDA causes it to fail in a number of situa-\n",
      "tions as well:\n",
      "•Often linear decision boundaries do not adequately separate the classes.\n",
      "When Nis large, it is possible to estimate more complex decision\n",
      "boundaries. Quadratic discriminant analysis (QDA) is often useful\n",
      "here, and allows for quadratic decision boundaries. More generally\n",
      "we would like to be able to model irregular decision boundaries.\n",
      "•The aforementioned shortcoming of LDA can often be paraphrased\n",
      "by saying that a single prototype per class is insuﬃcient. LDA uses\n",
      "a single prototype (class centroid) plus a common covariance matrix\n",
      "to describe the spread of the data in each class. In many situations,\n",
      "several prototypes are more appropriate.\n",
      "•At the other end of the spectrum, we may have way too many (corre-\n",
      "lated) predictors, for example, in the case of digitized analogue signals\n",
      "and images. In this case LDA uses too many parameters, which are\n",
      "estimated with high variance, and its performance suﬀers. In cases\n",
      "such as this we need to restrict or regularize LDA even further.\n",
      "In the remainder of this chapter we describe a class of techniques that\n",
      "attend to all these issues by generalizing the LDA model. This is achieved\n",
      "largely by three diﬀerent ideas.\n",
      "The ﬁrst idea is to recast the LDA problem as a linear regression problem.\n",
      "Many techniques exist for generalizing linear regression to more ﬂexible,\n",
      "nonparametric forms of regression. This in turn leads to more ﬂexible forms\n",
      "of discriminant analysis, which we call FDA. In most cases of interest, t he\n",
      "3This study predated the emergence of SVMs.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "440 12. Flexible Discriminants\n",
      "regression procedures can be seen to identify an enlarged set of predictors\n",
      "via basis expansions. FDA amounts to LDA in this enlarged space, the\n",
      "same paradigm used in SVMs.\n",
      "In the case of too many predictors, such as the pixels of a digitized image,\n",
      "we do not want to expand the set: it is already too large. The second idea is\n",
      "to ﬁt an LDA model, but penalize its coeﬃcients to be smooth or otherwise\n",
      "coherent in the spatial domain, that is, as an image. We call this procedure\n",
      "penalized discriminant analysis or PDA. With FDA itself, the expanded\n",
      "basis set is often so large that regularization is also required (again a s in\n",
      "SVMs). Both of these can be achieved via a suitably regularized regression\n",
      "in the context of the FDA model.\n",
      "The third idea is to model each class by a mixture of two or more Gaus-\n",
      "sians with diﬀerent centroids, but with every component Gaussian, both\n",
      "within and between classes, sharing the same covariance matrix. This allows\n",
      "for more complex decision boundaries, and allows for subspace reduction\n",
      "as in LDA. We call this extension mixture discriminant analysis or MDA.\n",
      "All three of these generalizations use a common framework by exploiting\n",
      "their connection with LDA.\n",
      "12.5 Flexible Discriminant Analysis\n",
      "In this section we describe a method for performing LDA using linear re-\n",
      "gression on derived responses. This in turn leads to nonparametric and ﬂex-\n",
      "ible alternatives to LDA. As in Chapter 4, we assume we have observations\n",
      "with a quantitative response Gfalling into one of Kclasses G={1,... ,K },\n",
      "each having measured features X. Suppose θ:G ↦→IR1is a function that\n",
      "assigns scores to the classes, such that the transformed class labels are op-\n",
      "timally predicted by linear regression on X: If our training sample has the\n",
      "form ( gi,xi), i= 1,2,... ,N , then we solve\n",
      "min\n",
      "β,θN∑\n",
      "i=1(\n",
      "θ(gi)−xT\n",
      "iβ)2, (12.52)\n",
      "with restrictions on θto avoid a trivial solution (mean zero and unit vari-\n",
      "ance over the training data). This produces a one-dimensional separation\n",
      "between the classes.\n",
      "More generally, we can ﬁnd up to L≤K−1 sets of independent scorings\n",
      "for the class labels, θ1,θ2,... ,θ L, andLcorresponding linear maps ηℓ(X) =\n",
      "XTβℓ, ℓ= 1,... ,L , chosen to be optimal for multiple regression in IRp. The\n",
      "scores θℓ(g) and the maps βℓare chosen to minimize the average squared\n",
      "residual,\n",
      "ASR=1\n",
      "NL∑\n",
      "ℓ=1[N∑\n",
      "i=1(\n",
      "θℓ(gi)−xT\n",
      "iβℓ)2]\n",
      ". (12.53)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.5 Flexible Discriminant Analysis 441\n",
      "The set of scores are assumed to be mutually orthogonal and normalized\n",
      "with respect to an appropriate inner product to prevent trivial zero\n",
      "solutions.\n",
      "Why are we going down this road? It can be shown that the sequence\n",
      "of discriminant (canonical) vectors νℓderived in Section 4.3.3 are identical\n",
      "to the sequence βℓup to a constant (Mardia et al., 1979; Hastie et al.,\n",
      "1995). Moreover, the Mahalanobis distance of a test point xto the kth\n",
      "class centroid ˆ θkis given by\n",
      "δJ(x,ˆθk) =K−1∑\n",
      "ℓ=1wℓ(ˆηℓ(x)−¯ηk\n",
      "ℓ)2+D(x), (12.54)\n",
      "where ¯ ηk\n",
      "ℓis the mean of the ˆ ηℓ(xi) in the kth class, and D(x) does not\n",
      "depend on k. Here wℓare coordinate weights that are deﬁned in terms of\n",
      "the mean squared residual r2\n",
      "ℓof the ℓth optimally scored ﬁt\n",
      "wℓ=1\n",
      "r2\n",
      "ℓ(1−r2\n",
      "ℓ). (12.55)\n",
      "In Section 4.3.2 we saw that these canonical distances are all that is needed\n",
      "for classiﬁcation in the Gaussian setup, with equal covariances in each class.\n",
      "To summarize:\n",
      "LDA can be performed by a sequence of linear regressions, fol-\n",
      "lowed by classiﬁcation to the closest class centroid in the space\n",
      "of ﬁts. The analogy applies both to the reduced rank version,\n",
      "or the full rank case when L=K−1.\n",
      "The real power of this result is in the generalizations that it invites. We\n",
      "can replace the linear regression ﬁts ηℓ(x) =xTβℓby far more ﬂexible,\n",
      "nonparametric ﬁts, and by analogy achieve a more ﬂexible classiﬁer than\n",
      "LDA. We have in mind generalized additive ﬁts, spline functions, MARS\n",
      "models and the like. In this more general form the regression problems are\n",
      "deﬁned via the criterion\n",
      "ASR({θℓ,ηℓ}L\n",
      "ℓ=1) =1\n",
      "NL∑\n",
      "ℓ=1[N∑\n",
      "i=1(θℓ(gi)−ηℓ(xi))2+λJ(ηℓ)]\n",
      ",(12.56)\n",
      "where Jis a regularizer appropriate for some forms of nonparametric regres-\n",
      "sion, such as smoothing splines, additive splines and lower-order ANOVA\n",
      "spline models. Also included are the classes of functions and associated\n",
      "penalties generated by kernels, as in Section 12.3.3.\n",
      "Before we describe the computations involved in this generalization, let\n",
      "us consider a very simple example. Suppose we use degree-2 polynomial\n",
      "regression for each ηℓ. The decision boundaries implied by the (12.54) will\n",
      "be quadratic surfaces, since each of the ﬁtted functions is quadratic, and as\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "442 12. Flexible Discriminants\n",
      "-2 0 2-2 0 2o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o o\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "FIGURE 12.9. The data consist of 50points generated from each of N(0, I)and\n",
      "N(0,9\n",
      "4I). The solid black ellipse is the decision boundary found by FDA u sing\n",
      "degree-two polynomial regression. The dashed purple circle i s the Bayes decision\n",
      "boundary.\n",
      "in LDA their squares cancel out when comparing distances. We could have\n",
      "achieved identical quadratic boundaries in a more conventional way, by\n",
      "augmenting our original predictors with their squares and cross-products.\n",
      "In the enlarged space one performs an LDA, and the linear boundaries in\n",
      "the enlarged space map down to quadratic boundaries in the original space.\n",
      "A classic example is a pair of multivariate Gaussians centered at the origi n,\n",
      "one having covariance matrix I, and the other cIforc >1; Figure 12.9\n",
      "illustrates. The Bayes decision boundary is the sphere ∥x∥=pclogc\n",
      "2(c−1), which\n",
      "is a linear boundary in the enlarged space.\n",
      "Many nonparametric regression procedures operate by generating a basis\n",
      "expansion of derived variables, and then performing a linear regression in\n",
      "the enlarged space. The MARS procedure (Chapter 9) is exactly of this\n",
      "form. Smoothing splines and additive spline models generate an extremely\n",
      "large basis set ( N×pbasis functions for additive splines), but then perform\n",
      "a penalized regression ﬁt in the enlarged space. SVMs do as well; see also\n",
      "the kernel-based regression example in Section 12.3.7. FDA in this case can\n",
      "be shown to perform a penalized linear discriminant analysis in the enlarged\n",
      "space. We elaborate in Section 12.6. Linear boundaries in the enlarged space\n",
      "map down to nonlinear boundaries in the reduced space. This is exactly the\n",
      "same paradigm that is used with support vector machines (Section 12.3).\n",
      "We illustrate FDA on the speech recognition example used in Chapter\n",
      "4.), with K= 11 classes and p= 10 predictors. The classes correspond to\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.5 Flexible Discriminant Analysis 443\n",
      "oooo oo\n",
      "oooooooo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooooooooooo\n",
      "ooo\n",
      "o\n",
      "oooooooo\n",
      "ooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "ooooooooooooo\n",
      "ooooooooooo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooooo\n",
      "oooooo o\n",
      "o\n",
      "oooooooooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooooooooooo\n",
      "oooooooooooooooooooooooooooooo\n",
      "o\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "o\n",
      "ooooooo\n",
      "oooooooooooo\n",
      "o\n",
      "ooooooooooooooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooooooo\n",
      "ooooooooooooooooo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "ooooooooooooo\n",
      "oo\n",
      "oooooo\n",
      "ooooooooo\n",
      "ooo\n",
      "ooooooooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooooooooooooo\n",
      "ooo\n",
      "oooooooo\n",
      "oooo\n",
      "ooooooooooooo\n",
      "o\n",
      "oooo\n",
      "ooooo\n",
      "oooooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oooooo\n",
      "oooo\n",
      "o\n",
      "ooooooooooooo\n",
      "o ooooooooooooooooo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "ooooooooooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "ooooooo\n",
      "oooooo\n",
      "oooooo\n",
      "oooooooooo\n",
      "o\n",
      "ooooooo\n",
      "oooooooooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "Coordinate 1 for Training DataCoordinate 2 for Training DataLinear Discriminant Analysis\n",
      "oo oooooo\n",
      "oooo\n",
      "oooooooooooo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooooo\n",
      "oooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooooo\n",
      "oooooo\n",
      "ooooooooooo\n",
      "ooooooo\n",
      "ooooooo\n",
      "o\n",
      "oooo\n",
      "oooooo\n",
      "ooo ooooooooo\n",
      "oooooooo\n",
      "oooo\n",
      "oooooo\n",
      "o\n",
      "oooo\n",
      "ooooooo\n",
      "oooooo\n",
      "oooooooooooooooooo\n",
      "oooooooooooooooooooooooooooo\n",
      "ooooooooooooo\n",
      "ooo\n",
      "o\n",
      "o oooooooooooooooooooo\n",
      "oooo\n",
      "o\n",
      "ooooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooooo\n",
      "ooo\n",
      "o\n",
      "oooo ooooooooo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "oooooooooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oooooo\n",
      "ooo\n",
      "ooooooo\n",
      "ooo\n",
      "ooooooooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "o\n",
      "ooooooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oooooooooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "o\n",
      "o\n",
      "oooooo\n",
      "o\n",
      "oooooooooooooooooo\n",
      "oooooooo\n",
      "oooo\n",
      "oooooo\n",
      "ooooooooo\n",
      "o\n",
      "oo\n",
      "ooooooo\n",
      "oooooo\n",
      "o\n",
      "oooo\n",
      "ooooooooooo\n",
      "ooooo oo\n",
      "oooo\n",
      "oooo\n",
      "oooo\n",
      "Coordinate 1 for Training DataCoordinate 2 for Training DataFlexible Discriminant Analysis -- Bruto\n",
      "FIGURE 12.10. The left plot shows the ﬁrst two LDA canonical variates for\n",
      "the vowel training data. The right plot shows the corresponding p rojection when\n",
      "FDA/BRUTO is used to ﬁt the model; plotted are the ﬁtted regress ion functions\n",
      "ˆη1(xi)andˆη2(xi). Notice the improved separation. The colors represent the ele ven\n",
      "diﬀerent vowel sounds.\n",
      "11 vowel sounds, each contained in 11 diﬀerent words. Here are the words,\n",
      "preceded by the symbols that represent them:\n",
      "Vowel Word Vowel Word Vowel Word Vowel Word\n",
      "i: heed O hod I hid C: hoard\n",
      "E head U hood A had u: who’d\n",
      "a: hard 3: heard Y hud\n",
      "Each of eight speakers spoke each word six times in the training set, and\n",
      "likewise seven speakers in the test set. The ten predictors are derived from\n",
      "the digitized speech in a rather complicated way, but standard in the speech\n",
      "recognition world. There are thus 528 training observations, and 462 test\n",
      "observations. Figure 12.10 shows two-dimensional projections produced by\n",
      "LDA and FDA. The FDA model used adaptive additive-spline regression\n",
      "functions to model the ηℓ(x), and the points plotted in the right plot have\n",
      "coordinates ˆ η1(xi) and ˆ η2(xi). The routine used in S-PLUS is called bruto ,\n",
      "hence the heading on the plot and in Table 12.3. We see that ﬂexible model-\n",
      "ing has helped to separate the classes in this case. Table 12.3 shows training\n",
      "and test error rates for a number of classiﬁcation techniques. FDA/MARS\n",
      "refers to Friedman’s multivariate adaptive regression splines; degree = 2\n",
      "means pairwise products are permitted. Notice that for FDA/MARS, the\n",
      "best classiﬁcation results are obtained in a reduced-rank subspace.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "444 12. Flexible Discriminants\n",
      "TABLE 12.3. Vowel recognition data performance results. The results for ne ural\n",
      "networks are the best among a much larger set, taken from a neural network\n",
      "archive. The notation FDA/BRUTO refers to the regression met hod used with\n",
      "FDA.\n",
      "Technique Error Rates\n",
      "Training Test\n",
      "(1) LDA 0.32 0.56\n",
      "Softmax 0.48 0.67\n",
      "(2) QDA 0.01 0.53\n",
      "(3) CART 0.05 0.56\n",
      "(4) CART (linear combination splits) 0.05 0.54\n",
      "(5) Single-layer perceptron 0.67\n",
      "(6) Multi-layer perceptron (88 hidden units) 0.49\n",
      "(7) Gaussian node network (528 hidden units) 0.45\n",
      "(8) Nearest neighbor 0.44\n",
      "(9) FDA/BRUTO 0.06 0.44\n",
      "Softmax 0.11 0.50\n",
      "(10) FDA/MARS (degree = 1) 0.09 0.45\n",
      "Best reduced dimension (=2) 0.18 0.42\n",
      "Softmax 0.14 0.48\n",
      "(11) FDA/MARS (degree = 2) 0.02 0.42\n",
      "Best reduced dimension (=6) 0.13 0.39\n",
      "Softmax 0.10 0.50\n",
      "12.5.1 Computing the FDA Estimates\n",
      "The computations for the FDA coordinates can be simpliﬁed in many im-\n",
      "portant cases, in particular when the nonparametric regression procedure\n",
      "can be represented as a linear operator. We will denote this operator by\n",
      "Sλ; that is, ˆy=Sλy, where yis the vector of responses and ˆythe vector\n",
      "of ﬁts. Additive splines have this property, if the smoothing parameters are\n",
      "ﬁxed, as does MARS once the basis functions are selected. The subscript λ\n",
      "denotes the entire set of smoothing parameters. In this case optimal scoring\n",
      "is equivalent to a canonical correlation problem, and the solution can be\n",
      "computed by a single eigen-decomposition. This is pursued in Exercise 12.6,\n",
      "and the resulting algorithm is presented here.\n",
      "We create an N×Kindicator response matrix Yfrom the responses gi,\n",
      "such that yik= 1 if gi=k, otherwise yik= 0. For a ﬁve-class problem Y\n",
      "might look like the following:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.5 Flexible Discriminant Analysis 445\n",
      "0\n",
      "BBBBBBBBB@C1C2C3C4C5\n",
      "g1= 2 0 1 0 0 0\n",
      "g2= 1 1 0 0 0 0\n",
      "g3= 1 1 0 0 0 0\n",
      "g4= 5 0 0 0 0 1\n",
      "g5= 4 0 0 0 1 0\n",
      "......\n",
      "gN= 3 0 0 1 0 01\n",
      "CCCCCCCCCA\n",
      "Here are the computational steps:\n",
      "1.Multivariate nonparametric regression. Fit a multiresponse, adaptive\n",
      "nonparametric regression of YonX, giving ﬁtted values ˆY. LetSλ\n",
      "be the linear operator that ﬁts the ﬁnal chosen model, and η∗(x) be\n",
      "the vector of ﬁtted regression functions.\n",
      "2.Optimal scores. Compute the eigen-decomposition of YTˆY=YTSλY,\n",
      "where the eigenvectors Θare normalized: ΘTDπΘ=I. Here Dπ=\n",
      "YTY/Nis a diagonal matrix of the estimated class prior\n",
      "probabilities.\n",
      "3.Update the model from step 1 using the optimal scores: η(x) =ΘTη∗(x).\n",
      "The ﬁrst of the Kfunctions in η(x) is the constant function— a trivial\n",
      "solution; the remaining K−1 functions are the discriminant functions. The\n",
      "constant function, along with the normalization, causes all the remaining\n",
      "functions to be centered.\n",
      "AgainSλcan correspond to any regression method. When Sλ=HX, the\n",
      "linear regression projection operator, then FDA is linear discriminant anal-\n",
      "ysis. The software that we reference in the Computational Considerations\n",
      "section on page 455 makes good use of this modularity; the fdafunction\n",
      "has amethod= argument that allows one to supply anyregression function,\n",
      "as long as it follows some natural conventions. The regression functions\n",
      "we provide allow for polynomial regression, adaptive additive models and\n",
      "MARS. They all eﬃciently handle multiple responses, so step (1) is a single\n",
      "call to a regression routine. The eigen-decomposition in step (2) simulta-\n",
      "neously computes all the optimal scoring functions.\n",
      "In Section 4.2 we discussed the pitfalls of using linear regression on an\n",
      "indicator response matrix as a method for classiﬁcation. In particular, se-\n",
      "vere masking can occur with three or more classes. FDA uses the ﬁts from\n",
      "such a regression in step (1), but then transforms them further to produce\n",
      "useful discriminant functions that are devoid of these pitfalls. Exercise 12.9\n",
      "takes another view of this phenomenon.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "446 12. Flexible Discriminants\n",
      "12.6 Penalized Discriminant Analysis\n",
      "Although FDA is motivated by generalizing optimal scoring, it can also be\n",
      "viewed directly as a form of regularized discriminant analysis. Suppose the\n",
      "regression procedure used in FDA amounts to a linear regression onto a\n",
      "basis expansion h(X), with a quadratic penalty on the coeﬃcients:\n",
      "ASR({θℓ,βℓ}L\n",
      "ℓ=1) =1\n",
      "NL∑\n",
      "ℓ=1[N∑\n",
      "i=1(θℓ(gi)−hT(xi)βℓ)2+λβT\n",
      "ℓΩβℓ]\n",
      ".(12.57)\n",
      "The choice of Ωdepends on the problem. If ηℓ(x) =h(x)βℓis an expansion\n",
      "on spline basis functions, Ωmight constrain ηℓto be smooth over IRp. In\n",
      "the case of additive splines, there are Nspline basis functions for each\n",
      "coordinate, resulting in a total of Npbasis functions in h(x);Ωin this case\n",
      "isNp×Npand block diagonal.\n",
      "The steps in FDA can then be viewed as a generalized form of LDA,\n",
      "which we call penalized discriminant analysis , or PDA:\n",
      "•Enlarge the set of predictors Xvia a basis expansion h(X).\n",
      "•Use (penalized) LDA in the enlarged space, where the penalized\n",
      "Mahalanobis distance is given by\n",
      "D(x,θ) = (h(x)−h(θ))T(ΣW+λΩ)−1(h(x)−h(θ)),(12.58)\n",
      "where ΣWis the within-class covariance matrix of the derived vari-\n",
      "ables h(xi).\n",
      "•Decompose the classiﬁcation subspace using a penalized metric:\n",
      "maxuTΣBetusubject to uT(ΣW+λΩ)u= 1.\n",
      "Loosely speaking, the penalized Mahalanobis distance tends to give less\n",
      "weight to “rough” coordinates, and more weight to “smooth” ones; since\n",
      "the penalty is not diagonal, the same applies to linear combinations that\n",
      "are rough or smooth.\n",
      "For some classes of problems, the ﬁrst step, involving the basis expansion,\n",
      "is not needed; we already have far too many (correlated) predictors. A\n",
      "leading example is when the objects to be classiﬁed are digitized analog\n",
      "signals:\n",
      "•the log-periodogram of a fragment of spoken speech, sampled at a set\n",
      "of 256 frequencies; see Figure 5.5 on page 149.\n",
      "•the grayscale pixel values in a digitized image of a handwritten digit.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.6 Penalized Discriminant Analysis 447\n",
      "LDA: Coefficient 1 PDA: Coefficient 1 LDA: Coefficient 2 PDA: Coefficient 2 LDA: Coefficient 3 PDA: Coefficient 3\n",
      "LDA: Coefficient 4 PDA: Coefficient 4 LDA: Coefficient 5 PDA: Coefficient 5 LDA: Coefficient 6 PDA: Coefficient 6\n",
      "LDA: Coefficient 7 PDA: Coefficient 7 LDA: Coefficient 8 PDA: Coefficient 8 LDA: Coefficient 9 PDA: Coefficient 9\n",
      "FIGURE 12.11. The images appear in pairs, and represent the nine discrim-\n",
      "inant coeﬃcient functions for the digit recognition problem. The l eft member of\n",
      "each pair is the LDA coeﬃcient, while the right member is the PD A coeﬃcient,\n",
      "regularized to enforce spatial smoothness.\n",
      "It is also intuitively clear in these cases why regularization is needed.\n",
      "Take the digitized image as an example. Neighboring pixel values will tend\n",
      "to be correlated, being often almost the same. This implies that the pair\n",
      "of corresponding LDA coeﬃcients for these pixels can be wildly diﬀerent\n",
      "and opposite in sign, and thus cancel when applied to similar pixel values.\n",
      "Positively correlated predictors lead to noisy, negatively correlated coeﬃ-\n",
      "cient estimates, and this noise results in unwanted sampling variance. A\n",
      "reasonable strategy is to regularize the coeﬃcients to be smooth over the\n",
      "spatial domain, as with images. This is what PDA does. The computations\n",
      "proceed just as for FDA, except that an appropriate penalized regression\n",
      "method is used. Here hT(X)βℓ=Xβℓ, and Ω is chosen so that βT\n",
      "ℓΩβℓ\n",
      "penalizes roughness in βℓwhen viewed as an image. Figure 1.2 on page 4\n",
      "shows some examples of handwritten digits. Figure 12.11 shows the dis-\n",
      "criminant variates using LDA and PDA. Those produced by LDA appear\n",
      "assalt-and-pepper images, while those produced by PDA are smooth im-\n",
      "ages. The ﬁrst smooth image can be seen as the coeﬃcients of a linear\n",
      "contrast functional for separating images with a dark central vertical stri p\n",
      "(ones, possibly sevens) from images that are hollow in the middle (zeros,\n",
      "some fours). Figure 12.12 supports this interpretation, and with more di f-\n",
      "ﬁculty allows an interpretation of the second coordinate. This and other\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "448 12. Flexible Discriminants\n",
      "-5 0 5-6 -4 -2 0 2 4 60\n",
      "0\n",
      "00\n",
      "0000\n",
      "0\n",
      "0000\n",
      "00\n",
      "00\n",
      "00000\n",
      "00\n",
      "00000\n",
      "0\n",
      "0000\n",
      "000\n",
      "00\n",
      "0\n",
      "00\n",
      "00000\n",
      "0\n",
      "0000\n",
      "0\n",
      "0\n",
      "0000\n",
      "0\n",
      "00000\n",
      "0\n",
      "000\n",
      "00\n",
      "00\n",
      "000\n",
      "000\n",
      "0000\n",
      "00\n",
      "00\n",
      "000\n",
      "00\n",
      "000\n",
      "00\n",
      "000\n",
      "0\n",
      "0000\n",
      "0000\n",
      "000\n",
      "0\n",
      "00\n",
      "00\n",
      "00\n",
      "00\n",
      "00\n",
      "00\n",
      "000\n",
      "00000 000\n",
      "0000\n",
      "00\n",
      "0\n",
      "0000\n",
      "0000\n",
      "000\n",
      "000\n",
      "00 0\n",
      "0\n",
      "0000\n",
      "0\n",
      "00\n",
      "0 000\n",
      "0\n",
      "000\n",
      "00\n",
      "0\n",
      "0\n",
      "000\n",
      "00 00\n",
      "000\n",
      "00\n",
      "0\n",
      "000\n",
      "00\n",
      "0\n",
      "00\n",
      "00 0 0\n",
      "000\n",
      "0\n",
      "00\n",
      "0\n",
      "000000\n",
      "0\n",
      "0 0\n",
      "000\n",
      "00\n",
      "0 00\n",
      "000 0\n",
      "00\n",
      "000\n",
      "00\n",
      "000\n",
      "000\n",
      "0\n",
      "000\n",
      "000\n",
      "000\n",
      "000\n",
      "000\n",
      "00\n",
      "000\n",
      "00\n",
      "00\n",
      "00\n",
      "000\n",
      "0\n",
      "000\n",
      "0000\n",
      "0\n",
      "00 00\n",
      "0000\n",
      "00\n",
      "0\n",
      "000\n",
      "00\n",
      "00\n",
      "0\n",
      "00\n",
      "0\n",
      "0000\n",
      "0\n",
      "000000\n",
      "00\n",
      "000\n",
      "0\n",
      "000\n",
      "000\n",
      "00\n",
      "000\n",
      "000\n",
      "00\n",
      "000\n",
      "00000\n",
      "00\n",
      "0\n",
      "0000\n",
      "00\n",
      "0\n",
      "0\n",
      "011\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1111\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "111\n",
      "1\n",
      "111\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "1111\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "1111\n",
      "1\n",
      "111\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "11111\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11111\n",
      "111\n",
      "1\n",
      "11111\n",
      "1\n",
      "111\n",
      "1111\n",
      "11\n",
      "11\n",
      "11\n",
      "11111\n",
      "1111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "1\n",
      "1 1\n",
      "11\n",
      "1\n",
      "111\n",
      "111\n",
      "11\n",
      "1\n",
      "1\n",
      "111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "11111\n",
      "111\n",
      "1\n",
      "11111\n",
      "111111\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "1111111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "1\n",
      "11111111\n",
      "1\n",
      "111\n",
      "111\n",
      "1\n",
      "22\n",
      "22\n",
      "2222\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "22\n",
      "2222\n",
      "222\n",
      "2\n",
      "22\n",
      "2\n",
      "2\n",
      "2\n",
      "2222\n",
      "2222\n",
      "22222\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "22\n",
      "2\n",
      "2\n",
      "22\n",
      "22\n",
      "222222\n",
      "222\n",
      "2\n",
      "2\n",
      "2222222\n",
      "22222\n",
      "222\n",
      "222\n",
      "2\n",
      "22222\n",
      "2\n",
      "2\n",
      "2222\n",
      "222\n",
      "22\n",
      "222\n",
      "222\n",
      "2\n",
      "22\n",
      "2\n",
      "2222\n",
      "2\n",
      "222\n",
      "2222\n",
      "22\n",
      "222\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "222 22\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2222\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "2222\n",
      "333\n",
      "3\n",
      "33\n",
      "3\n",
      "3 3\n",
      "333\n",
      "333\n",
      "333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "33333\n",
      "33\n",
      "33\n",
      "3\n",
      "33333\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "3333\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "3\n",
      "3\n",
      "333\n",
      "33333\n",
      "3\n",
      "3333\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3333\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "33\n",
      "3\n",
      "333333\n",
      "3\n",
      "3\n",
      "33 3\n",
      "333\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "4\n",
      "4444\n",
      "4\n",
      "4 44444\n",
      "44\n",
      "4\n",
      "44\n",
      "444\n",
      "4\n",
      "444\n",
      "44\n",
      "44\n",
      "4\n",
      "44\n",
      "444\n",
      "44\n",
      "4\n",
      "4\n",
      "44\n",
      "44\n",
      "44\n",
      "44\n",
      "444\n",
      "44444\n",
      "44\n",
      "444\n",
      "4\n",
      "44\n",
      "44\n",
      "4\n",
      "44\n",
      "4\n",
      "44444\n",
      "4\n",
      "444\n",
      "44\n",
      "44\n",
      "4 44\n",
      "4\n",
      "4\n",
      "444\n",
      "4\n",
      "444\n",
      "44\n",
      "4444\n",
      "4\n",
      "4444\n",
      "4444\n",
      "4\n",
      "444\n",
      "44\n",
      "44\n",
      "444\n",
      "44\n",
      "444\n",
      "44\n",
      "44\n",
      "4\n",
      "4\n",
      "44444\n",
      "44\n",
      "4\n",
      "4\n",
      "444\n",
      "44 44\n",
      "44\n",
      "444\n",
      "4\n",
      "44\n",
      "4\n",
      "44\n",
      "4\n",
      "444\n",
      "444444\n",
      "444\n",
      "4444\n",
      "44\n",
      "44\n",
      "4\n",
      "44\n",
      "4\n",
      "444\n",
      "444\n",
      "44\n",
      "444\n",
      "44\n",
      "444\n",
      "4455\n",
      "55\n",
      "5\n",
      "55\n",
      "555\n",
      "5555\n",
      "5\n",
      "5 5\n",
      "555\n",
      "5\n",
      "555\n",
      "55\n",
      "55\n",
      "55\n",
      "5\n",
      "5555\n",
      "555\n",
      "55\n",
      "5555\n",
      "5555\n",
      "5\n",
      "55\n",
      "55\n",
      "55\n",
      "5\n",
      "555\n",
      "55\n",
      "55\n",
      "55555\n",
      "555\n",
      "5 55\n",
      "5\n",
      "5555\n",
      "555\n",
      "5\n",
      "555\n",
      "5\n",
      "555\n",
      "555\n",
      "55\n",
      "555\n",
      "55\n",
      "5\n",
      "555\n",
      "55\n",
      "55\n",
      "555\n",
      "555\n",
      "55\n",
      "55\n",
      "555\n",
      "55\n",
      "5 555\n",
      "55\n",
      "5\n",
      "5\n",
      "5\n",
      "55\n",
      "5\n",
      "555555\n",
      "5\n",
      "55\n",
      "55\n",
      "55\n",
      "55\n",
      "55\n",
      "55\n",
      "555\n",
      "5\n",
      "55\n",
      "6\n",
      "6666\n",
      "66\n",
      "66\n",
      "6\n",
      "66\n",
      "66\n",
      "666\n",
      "6\n",
      "66\n",
      "66\n",
      "6\n",
      "666\n",
      "666\n",
      "6\n",
      "6\n",
      "6\n",
      "666\n",
      "6\n",
      "66\n",
      "66666\n",
      "66\n",
      "6\n",
      "66\n",
      "6\n",
      "666\n",
      "66\n",
      "666\n",
      "66\n",
      "66\n",
      "6\n",
      "66\n",
      "6\n",
      "66\n",
      "666\n",
      "6\n",
      "6\n",
      "66\n",
      "666\n",
      "6\n",
      "6666\n",
      "6\n",
      "666\n",
      "66\n",
      "66\n",
      "666\n",
      "66 6\n",
      "6 66\n",
      "66\n",
      "6\n",
      "666\n",
      "666\n",
      "6\n",
      "6\n",
      "66\n",
      "666\n",
      "666\n",
      "6666\n",
      "6\n",
      "6 66\n",
      "6\n",
      "66\n",
      "6\n",
      "66\n",
      "66\n",
      "6666\n",
      "666\n",
      "66\n",
      "6666\n",
      "666\n",
      "6666\n",
      "66\n",
      "666\n",
      "666\n",
      "66\n",
      "666\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "7777\n",
      "77\n",
      "777\n",
      "77\n",
      "777\n",
      "777\n",
      "77777\n",
      "777\n",
      "7\n",
      "777\n",
      "77\n",
      "77\n",
      "7\n",
      "777\n",
      "777777\n",
      "7\n",
      "777\n",
      "77\n",
      "7 77\n",
      "7\n",
      "77\n",
      "77\n",
      "77777\n",
      "7\n",
      "77\n",
      "77 7\n",
      "777\n",
      "77\n",
      "7\n",
      "77\n",
      "777\n",
      "77777\n",
      "77\n",
      "77\n",
      "7\n",
      "77777\n",
      "777\n",
      "77\n",
      "7\n",
      "77\n",
      "77\n",
      "777\n",
      "7\n",
      "7\n",
      "7\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "77\n",
      "7777\n",
      "7\n",
      "77\n",
      "77\n",
      "777\n",
      "77\n",
      "7777\n",
      "77777\n",
      "7\n",
      "7\n",
      "788\n",
      "8\n",
      "8\n",
      "88\n",
      "8888\n",
      "88\n",
      "8\n",
      "8\n",
      "888\n",
      "88\n",
      "8\n",
      "888\n",
      "88\n",
      "88\n",
      "8\n",
      "888\n",
      "8\n",
      "888\n",
      "88\n",
      "88\n",
      "888\n",
      "88\n",
      "888\n",
      "88 888\n",
      "8888\n",
      "88\n",
      "8888\n",
      "88 88\n",
      "88\n",
      "88\n",
      "8888\n",
      "88 8\n",
      "8\n",
      "8 8\n",
      "88\n",
      "888\n",
      "888\n",
      "88\n",
      "8\n",
      "888\n",
      "88\n",
      "888\n",
      "8\n",
      "8\n",
      "88\n",
      "8888\n",
      "888\n",
      "888\n",
      "88\n",
      "8\n",
      "888\n",
      "88\n",
      "88\n",
      "8\n",
      "8\n",
      "88888\n",
      "88\n",
      "8\n",
      "8888888 8\n",
      "8\n",
      "8\n",
      "8\n",
      "88\n",
      "888 888\n",
      "888\n",
      "88\n",
      "8\n",
      "888\n",
      "8\n",
      "8 888\n",
      "9999\n",
      "999\n",
      "999\n",
      "99999\n",
      "999\n",
      "99\n",
      "999\n",
      "999\n",
      "9\n",
      "9\n",
      "99\n",
      "999\n",
      "9\n",
      "9\n",
      "999\n",
      "999\n",
      "999\n",
      "9999\n",
      "9\n",
      "99\n",
      "999\n",
      "99\n",
      "999\n",
      "99\n",
      "9\n",
      "9\n",
      "9999\n",
      "99\n",
      "999\n",
      "99\n",
      "9\n",
      "99\n",
      "9\n",
      "9\n",
      "99\n",
      "99\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "999\n",
      "999\n",
      "9\n",
      "9\n",
      "99\n",
      "9\n",
      "99\n",
      "9\n",
      "99\n",
      "999\n",
      "99\n",
      "9999\n",
      "99999\n",
      "9\n",
      "999\n",
      "99\n",
      "999\n",
      "99\n",
      "9\n",
      "999\n",
      "9\n",
      "99\n",
      "999\n",
      "9\n",
      "99\n",
      "99\n",
      "99\n",
      "9\n",
      "99\n",
      "99\n",
      "99\n",
      "9\n",
      "99\n",
      "9\n",
      "999\n",
      "99\n",
      "999\n",
      "9\n",
      "9999\n",
      "9\n",
      "99\n",
      "999\n",
      "9901\n",
      "2\n",
      "3\n",
      "456\n",
      "78\n",
      "9\n",
      "PDA: Discriminant Coordinate 1PDA: Discriminant Coordinate 2\n",
      "FIGURE 12.12. The ﬁrst two penalized canonical variates, evaluated for the\n",
      "test data. The circles indicate the class centroids. The ﬁrst co ordinate contrasts\n",
      "mainly 0’s and 1’s, while the second contrasts 6’s and 7/9’s.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.7 Mixture Discriminant Analysis 449\n",
      "examples are discussed in more detail in Hastie et al. (1995), who also show\n",
      "that the regularization improves the classiﬁcation performance of LDA on\n",
      "independent test data by a factor of around 25% in the cases they tried.\n",
      "12.7 Mixture Discriminant Analysis\n",
      "Linear discriminant analysis can be viewed as a prototype classiﬁer. Each\n",
      "class is represented by its centroid, and we classify to the closest using an\n",
      "appropriate metric. In many situations a single prototype is not suﬃcient\n",
      "to represent inhomogeneous classes, and mixture models are more appro-\n",
      "priate. In this section we review Gaussian mixture models and show how\n",
      "they can be generalized via the FDA and PDA methods discussed earlier.\n",
      "A Gaussian mixture model for the kth class has density\n",
      "P(X|G=k) =Rk∑\n",
      "r=1πkrφ(X;θkr,Σ), (12.59)\n",
      "where the mixing proportions πkrsum to one. This has Rkprototypes for\n",
      "thekth class, and in our speciﬁcation, the same covariance matrix Σis\n",
      "used as the metric throughout. Given such a model for each class, the class\n",
      "posterior probabilities are given by\n",
      "P(G=k|X=x) =∑Rk\n",
      "r=1πkrφ(X;θkr,Σ)Πk∑K\n",
      "ℓ=1∑Rℓ\n",
      "r=1πℓrφ(X;θℓr,Σ)Πℓ, (12.60)\n",
      "where Π krepresent the class prior probabilities.\n",
      "We saw these calculations for the special case of two components in\n",
      "Chapter 8. As in LDA, we estimate the parameters by maximum likelihood,\n",
      "using the joint log-likelihood based on P(G,X):\n",
      "K∑\n",
      "k=1∑\n",
      "gi=klog[Rk∑\n",
      "r=1πkrφ(xi;θkr,Σ)Πk]\n",
      ". (12.61)\n",
      "The sum within the log makes this a rather messy optimization problem\n",
      "if tackled directly. The classical and natural method for computing the\n",
      "maximum-likelihood estimates (MLEs) for mixture distributions is the EM\n",
      "algorithm (Dempster et al., 1977), which is known to possess good conver -\n",
      "gence properties. EM alternates between the two steps:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "450 12. Flexible Discriminants\n",
      "E-step: Given the current parameters, compute the responsibility of sub-\n",
      "classckrwithin class kfor each of the class- kobservations ( gi=k):\n",
      "W(ckr|xi,gi) =πkrφ(xi;θkr,Σ)∑Rk\n",
      "ℓ=1πkℓφ(xi;θkℓ,Σ). (12.62)\n",
      "M-step: Compute the weighted MLEs for the parameters of each of the\n",
      "component Gaussians within each of the classes, using the weights\n",
      "from the E-step.\n",
      "In the E-step, the algorithm apportions the unit weight of an observation\n",
      "in class kto the various subclasses assigned to that class. If it is close to the\n",
      "centroid of a particular subclass, and far from the others, it will receive a\n",
      "mass close to one for that subclass. On the other hand, observations halfway\n",
      "between two subclasses will get approximately equal weight for both.\n",
      "In the M-step, an observation in class kis used Rktimes, to estimate the\n",
      "parameters in each of the Rkcomponent densities, with a diﬀerent weight\n",
      "for each. The EM algorithm is studied in detail in Chapter 8. The algorithm\n",
      "requires initialization, which can have an impact, since mixture likelihoods\n",
      "are generally multimodal. Our software (referenced in the Computational\n",
      "Considerations on page 455) allows several strategies; here we describe the\n",
      "default. The user supplies the number Rkof subclasses per class. Within\n",
      "classk, ak-means clustering model, with multiple random starts, is ﬁtted\n",
      "to the data. This partitions the observations into Rkdisjoint groups, from\n",
      "which an initial weight matrix, consisting of zeros and ones, is created.\n",
      "Our assumption of an equal component covariance matrix Σthroughout\n",
      "buys an additional simplicity; we can incorporate rank restrictions in the\n",
      "mixture formulation just like in LDA. To understand this, we review a litt le-\n",
      "known fact about LDA. The rank- LLDA ﬁt (Section 4.3.3) is equivalent to\n",
      "the maximum-likelihood ﬁt of a Gaussian model,where the diﬀerent mean\n",
      "vectors in each class are conﬁned to a rank- Lsubspace of IRp(Exercise 4.8).\n",
      "We can inherit this property for the mixture model, and maximize the log-\n",
      "likelihood (12.61) subject to rank constraints on allthe∑\n",
      "kRkcentroids:\n",
      "rank{θkℓ}=L.\n",
      "Again the EM algorithm is available, and the M-step turns out to be\n",
      "a weighted version of LDA, with R=∑K\n",
      "k=1Rk“classes.” Furthermore,\n",
      "we can use optimal scoring as before to solve the weighted LDA problem,\n",
      "which allows us to use a weighted version of FDA or PDA at this stage.\n",
      "One would expect, in addition to an increase in the number of “classes,” a\n",
      "similar increase in the number of “observations” in the kth class by a factor\n",
      "ofRk. It turns out that this is not the case if linear operators are used for\n",
      "the optimal scoring regression. The enlarged indicator Ymatrix collapses\n",
      "in this case to a blurred response matrix Z, which is intuitively pleasing.\n",
      "For example, suppose there are K= 3 classes, and Rk= 3 subclasses per\n",
      "class. Then Zmight be\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.7 Mixture Discriminant Analysis 451\n",
      "\n",
      "c11c12c13c21c22c23c31c32c33\n",
      "g1= 2 0 0 0 0 .3 0.5 0.2 0 0 0\n",
      "g2= 1 0 .9 0.1 0.0 0 0 0 0 0 0\n",
      "g3= 1 0 .1 0.8 0.1 0 0 0 0 0 0\n",
      "g4= 3 0 0 0 0 0 0 0 .5 0.4 0.1\n",
      "g5= 2 0 0 0 0 .7 0.1 0.2 0 0 0\n",
      "......\n",
      "gN= 3 0 0 0 0 0 0 0 .1 0.1 0.8\n",
      ",(12.63)\n",
      "where the entries in a class- krow correspond to W(ckr|x,gi).\n",
      "The remaining steps are the same:\n",
      "ˆZ=SZ\n",
      "ZTˆZ=ΘDΘT\n",
      "Update πs and Πs\n",
      "\n",
      "M-step of MDA .\n",
      "These simple modiﬁcations add considerable ﬂexibility to the mixture\n",
      "model:\n",
      "•The dimension reduction step in LDA, FDA or PDA is limited by\n",
      "the number of classes; in particular, for K= 2 classes no reduction is\n",
      "possible. MDA substitutes subclasses for classes, and then allows us\n",
      "to look at low-dimensional views of the subspace spanned by these\n",
      "subclass centroids. This subspace will often be an important one for\n",
      "discrimination.\n",
      "•By using FDA or PDA in the M-step, we can adapt even more to par-\n",
      "ticular situations. For example, we can ﬁt MDA models to digitized\n",
      "analog signals and images, with smoothness constraints built in.\n",
      "Figure 12.13 compares FDA and MDA on the mixture example.\n",
      "12.7.1 Example: Waveform Data\n",
      "We now illustrate some of these ideas on a popular simulated example,\n",
      "taken from Breiman et al. (1984, pages 49–55), and used in Hastie and\n",
      "Tibshirani (1996b) and elsewhere. It is a three-class problem with 21 vari-\n",
      "ables, and is considered to be a diﬃcult pattern recognition problem. The\n",
      "predictors are deﬁned by\n",
      "Xj=Uh1(j) + (1 −U)h2(j) +ǫjClass 1 ,\n",
      "Xj=Uh1(j) + (1 −U)h3(j) +ǫjClass 2 , (12.64)\n",
      "Xj=Uh2(j) + (1 −U)h3(j) +ǫjClass 3 ,\n",
      "where j= 1,2,... ,21,Uis uniform on (0 ,1),ǫjare standard normal vari-\n",
      "ates, and the hℓare the shifted triangular waveforms: h1(j) = max(6 −\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "452 12. Flexible Discriminants\n",
      "FDA / MARS - Degree 2\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "Training Error: 0.185\n",
      "Test Error:       0.235\n",
      "Bayes Error:    0.210\n",
      "MDA - 5 Subclasses per Class\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ".... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "Training Error: 0.17\n",
      "Test Error:       0.22\n",
      "Bayes Error:    0.21\n",
      "FIGURE 12.13. FDA and MDA on the mixture data. The upper plot uses\n",
      "FDA with MARS as the regression procedure. The lower plot uses MDA with\n",
      "ﬁve mixture centers per class (indicated). The MDA solution is cl ose to Bayes\n",
      "optimal, as might be expected given the data arise from mixture s of Gaussians.\n",
      "The broken purple curve in the background is the Bayes decisio n boundary.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "12.7 Mixture Discriminant Analysis 453\n",
      "1 1 1 1 1 1111111111\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1 2 2 2 2 22222222222\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2 3 3 3 3 3333333\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3 4 4 4 4 4444444\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "44444 5 5 5 5 55555555555\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5Class 1\n",
      "11111111111\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1 1 1 1 1 22222222222\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2 2 2 2 2 33333333333\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3 3 3 3 3 4444444\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4 4 4 4 4 5555555\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5 5 5 5 5Class 2\n",
      "1111111\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "111111111 2222222\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "222222222 3333333\n",
      "3\n",
      "3333333\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3 4444444\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "444444444 5555555\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "555555555Class 3\n",
      "FIGURE 12.14. Some examples of the waveforms generated from model (12.64)\n",
      "before the Gaussian noise is added.\n",
      "|j−11|,0),h2(j) =h1(j−4) and h3(j) =h1(j+ 4). Figure 12.14 shows\n",
      "some example waveforms from each class.\n",
      "Table 12.4 shows the results of MDA applied to the waveform data, as\n",
      "well as several other methods from this and other chapters. Each train-\n",
      "ing sample has 300 observations, and equal priors were used, so there are\n",
      "roughly 100 observations in each class. We used test samples of size 500.\n",
      "The two MDA models are described in the caption.\n",
      "Figure 12.15 shows the leading canonical variates for the penalized MDA\n",
      "model, evaluated at the test data. As we might have guessed, the classes\n",
      "appear to lie on the edges of a triangle. This is because the hj(i) are repre-\n",
      "sented by three points in 21-space, thereby forming vertices of a triangle,\n",
      "and each class is represented as a convex combination of a pair of vertices,\n",
      "and hence lie on an edge. Also it is clear visually that all the information\n",
      "lies in the ﬁrst two dimensions; the percentage of variance explained by the\n",
      "ﬁrst two coordinates is 99 .8%, and we would lose nothing by truncating the\n",
      "solution there. The Bayes risk for this problem has been estimated to be\n",
      "about 0 .14 (Breiman et al., 1984). MDA comes close to the optimal rate,\n",
      "which is not surprising since the structure of the MDA model is similar to\n",
      "the generating model.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "454 12. Flexible Discriminants\n",
      "TABLE 12.4. Results for waveform data. The values are averages over ten s im-\n",
      "ulations, with the standard error of the average in parentheses . The ﬁve entries\n",
      "above the line are taken from Hastie et al. (1994). The ﬁrst mode l below the line\n",
      "is MDA with three subclasses per class. The next line is the same, except that the\n",
      "discriminant coeﬃcients are penalized via a roughness penalty to e ﬀectively 4df.\n",
      "The third is the corresponding penalized LDA or PDA model.\n",
      "Technique Error Rates\n",
      "Training Test\n",
      "LDA 0.121(0.006) 0.191(0.006)\n",
      "QDA 0.039(0.004) 0.205(0.006)\n",
      "CART 0.072(0.003) 0.289(0.004)\n",
      "FDA/MARS (degree = 1) 0.100(0.006) 0.191(0.006)\n",
      "FDA/MARS (degree = 2) 0.068(0.004) 0.215(0.002)\n",
      "MDA (3 subclasses) 0.087(0.005) 0.169(0.006)\n",
      "MDA (3 subclasses, penalized 4 df) 0.137(0.006) 0.157(0.005)\n",
      "PDA (penalized 4 df) 0.150(0.005) 0.171(0.005)\n",
      "Bayes 0.140\n",
      "Discriminant Var 1Discriminant Var 2\n",
      "-6 -4 -2 0 2 4 6-6 -4 -2 0 2 411\n",
      "111\n",
      "11\n",
      "111\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "1\n",
      "1\n",
      "11\n",
      "1\n",
      "111\n",
      "1111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "1\n",
      "1\n",
      "11\n",
      "1\n",
      "111\n",
      "111111\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "1111\n",
      "1\n",
      "11\n",
      "111\n",
      "111\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "11\n",
      "1\n",
      "11111\n",
      "11\n",
      "1\n",
      "11111\n",
      "1\n",
      "11\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "111\n",
      "111\n",
      "1\n",
      "1\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "11\n",
      "11111\n",
      "11\n",
      "22\n",
      "22\n",
      "2\n",
      "22222\n",
      "2 22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "22\n",
      "22222\n",
      "2\n",
      "2222\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "222\n",
      "2\n",
      "2\n",
      "2\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "22\n",
      "2\n",
      "2\n",
      "222\n",
      "222\n",
      "2\n",
      "2\n",
      "22\n",
      "2\n",
      "222\n",
      "22\n",
      "22\n",
      "222\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "222222\n",
      "22\n",
      "2\n",
      "22\n",
      "2\n",
      "2\n",
      "22\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "2\n",
      "2\n",
      "22\n",
      "2\n",
      "22\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "33\n",
      "3 333\n",
      "3333 33\n",
      "33\n",
      "3\n",
      "3 3333\n",
      "33\n",
      "333\n",
      "333\n",
      "333\n",
      "3\n",
      "33\n",
      "33\n",
      "333\n",
      "33\n",
      "33\n",
      "3333333\n",
      "3\n",
      "3\n",
      "333\n",
      "3333\n",
      "333\n",
      "33\n",
      "333333\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "333 3\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "3\n",
      "333 3\n",
      "3 3\n",
      "333\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "333\n",
      "333\n",
      "3\n",
      "33\n",
      "333\n",
      "333\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "333\n",
      "33 333\n",
      "33\n",
      "3\n",
      "3 33\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "33333\n",
      "333 Subclasses, Penalized 4 df\n",
      "Discriminant Var 3Discriminant Var 4\n",
      "-2 -1 0 1 2-1.0 0.0 0.5 1.01\n",
      "1\n",
      "11\n",
      "111\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11 111\n",
      "111\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "111\n",
      "11\n",
      "11111\n",
      "11111\n",
      "1\n",
      "1111\n",
      "1\n",
      "11\n",
      "1 1\n",
      "11\n",
      "1 111\n",
      "11\n",
      "1\n",
      "11\n",
      "1111\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "111\n",
      "1\n",
      "11\n",
      "11\n",
      "1\n",
      "11\n",
      "11\n",
      "11\n",
      "1\n",
      "11111\n",
      "1\n",
      "111\n",
      "11\n",
      "111\n",
      "1\n",
      "1\n",
      "1\n",
      "111\n",
      "11\n",
      "11\n",
      "11\n",
      "111\n",
      "1\n",
      "1\n",
      "111\n",
      "1111\n",
      "11\n",
      "111\n",
      "11\n",
      "1111\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "22\n",
      "222\n",
      "22 2\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "222\n",
      "22\n",
      "2\n",
      "22222\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22222\n",
      "22\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "2 22\n",
      "222\n",
      "2\n",
      "222\n",
      "22\n",
      "2\n",
      "22222\n",
      "222\n",
      "22\n",
      "222\n",
      "222\n",
      "2222\n",
      "222\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "2\n",
      "2\n",
      "22\n",
      "2\n",
      "222\n",
      "2\n",
      "222\n",
      "2\n",
      "22\n",
      "22\n",
      "22\n",
      "2\n",
      "22\n",
      "22 2\n",
      "22\n",
      "22\n",
      "2\n",
      "2\n",
      "2\n",
      "22\n",
      "2222\n",
      "2\n",
      "22\n",
      "222\n",
      "222\n",
      "3\n",
      "333\n",
      "3\n",
      "33\n",
      "3 33\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "33\n",
      "3333\n",
      "3\n",
      "333\n",
      "3\n",
      "33\n",
      "333\n",
      "33\n",
      "3\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "333\n",
      "3\n",
      "33333\n",
      "3\n",
      "3\n",
      "33\n",
      "33\n",
      "33\n",
      "3\n",
      "3\n",
      "3333\n",
      "33\n",
      "3333\n",
      "3 3\n",
      "33\n",
      "3333\n",
      "3\n",
      "3333\n",
      "33\n",
      "33\n",
      "3333\n",
      "33\n",
      "33\n",
      "333\n",
      "3\n",
      "33 3\n",
      "33\n",
      "3333\n",
      "333\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "333\n",
      "33\n",
      "33\n",
      "333\n",
      "3333333\n",
      "33\n",
      "3333\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "333\n",
      "3\n",
      "33\n",
      "3\n",
      "33\n",
      "3\n",
      "333\n",
      "33333 Subclasses, Penalized 4 df\n",
      "FIGURE 12.15. Some two-dimensional views of the MDA model ﬁtted to a\n",
      "sample of the waveform model. The points are independent test dat a, projected\n",
      "on to the leading two canonical coordinates (left panel), and the th ird and fourth\n",
      "(right panel). The subclass centers are indicated.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 455\n",
      "Computational Considerations\n",
      "With Ntraining cases, ppredictors, and msupport vectors, the support\n",
      "vector machine requires m3+mN+mpN operations, assuming m≈N.\n",
      "They do not scale well with N, although computational shortcuts are avail-\n",
      "able (Platt, 1999). Since these are evolving rapidly, the reader is urged to\n",
      "search the web for the latest technology.\n",
      "LDA requires Np2+p3operations, as does PDA. The complexity of\n",
      "FDA depends on the regression method used. Many techniques are linear\n",
      "inN, such as additive models and MARS. General splines and kernel-based\n",
      "regression methods will typically require N3operations.\n",
      "Software is available for ﬁtting FDA, PDA and MDA models in the R\n",
      "packagemda, which is also available in S-PLUS.\n",
      "Bibliographic Notes\n",
      "The theory behind support vector machines is due to Vapnik and is de-\n",
      "scribed in Vapnik (1996). There is a burgeoning literature on SVMs; an\n",
      "online bibliography, created and maintained by Alex Smola and Bernhard\n",
      "Sch¨ olkopf, can be found at:\n",
      "http://www.kernel-machines.org .\n",
      "Our treatment is based on Wahba et al. (2000) and Evgeniou et al. (2000),\n",
      "and the tutorial by Burges (Burges, 1998).\n",
      "Linear discriminant analysis is due to Fisher (1936) and Rao (1973). The\n",
      "connection with optimal scoring dates back at least to Breiman and Ihaka\n",
      "(1984), and in a simple form to Fisher (1936). There are strong connections\n",
      "with correspondence analysis (Greenacre, 1984). The description of ﬂexible,\n",
      "penalized and mixture discriminant analysis is taken from Hastie et al.\n",
      "(1994), Hastie et al. (1995) and Hastie and Tibshirani (1996b), and al l\n",
      "three are summarized in Hastie et al. (1998); see also Ripley (1996).\n",
      "Exercises\n",
      "Ex. 12.1 Show that the criteria (12.25) and (12.8) are equivalent.\n",
      "Ex. 12.2 Show that the solution to (12.29) is the same as the solution to\n",
      "(12.25) for a particular kernel.\n",
      "Ex. 12.3 Consider a modiﬁcation to (12.43) where you do not penalize the\n",
      "constant. Formulate the problem, and characterize its solution.\n",
      "Ex. 12.4 Suppose you perform a reduced-subspace linear discriminant anal-\n",
      "ysis for a K-group problem. You compute the canonical variables of di-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "456 12. Flexible Discriminants\n",
      "mension L≤K−1 given by z=UTx, where Uis the p×Lmatrix of\n",
      "discriminant coeﬃcients, and p > K is the dimension of x.\n",
      "(a) If L=K−1 show that\n",
      "∥z−¯zk∥2− ∥z−¯zk′∥2=∥x−¯xk∥2\n",
      "W− ∥x−¯xk′∥2\n",
      "W,\n",
      "where ∥≤∥Wdenotes Mahalanobis distance with respect to the covari-\n",
      "anceW.\n",
      "(b) If L < K −1, show that the same expression on the left measures\n",
      "the diﬀerence in Mahalanobis squared distances for the distributions\n",
      "projected onto the subspace spanned by U.\n",
      "Ex. 12.5 The data in phoneme.subset , available from this book’s website\n",
      "http://www-stat.stanford.edu/ElemStatLearn\n",
      "consists of digitized log-periodograms for phonemes uttered by 60 speakers,\n",
      "each speaker having produced phonemes from each of ﬁve classes. It is\n",
      "appropriate to plot each vector of 256 “features” against the frequencies\n",
      "0–255.\n",
      "(a) Produce a separate plot of all the phoneme curves against frequency\n",
      "for each class.\n",
      "(b) You plan to use a nearest prototype classiﬁcation scheme to classify\n",
      "the curves into phoneme classes. In particular, you will use a K-means\n",
      "clustering algorithm in each class ( kmeans() inR), and then classify\n",
      "observations to the class of the closest cluster center. The curves are\n",
      "high-dimensional and you have a rather small sample-size-to-variables\n",
      "ratio. You decide to restrict all the prototypes to be smooth functions\n",
      "of frequency. In particular, you decide to represent each prototype m\n",
      "asm=Bθwhere Bis a 256 ×Jmatrix of natural spline basis\n",
      "functions with Jknots uniformly chosen in (0 ,255) and boundary\n",
      "knots at 0 and 255. Describe how to proceed analytically, and in\n",
      "particular, how to avoid costly high-dimensional ﬁtting procedures.\n",
      "(Hint: It may help to restrict Bto be orthogonal.)\n",
      "(c) Implement your procedure on the phoneme data, and try it out. Divide\n",
      "the data into a training set and a test set (50-50), making sure that\n",
      "speakers are not split across sets (why?). Use K= 1,3,5,7 centers\n",
      "per class, and for each use J= 5,10,15 knots (taking care to start\n",
      "theK-means procedure at the same starting values for each value of\n",
      "J), and compare the results.\n",
      "Ex. 12.6 Suppose that the regression procedure used in FDA (Section 12.5.1)\n",
      "is a linear expansion of basis functions hm(x), m= 1,... ,M . LetDπ=\n",
      "YTY/Nbe the diagonal matrix of class proportions.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 457\n",
      "(a) Show that the optimal scoring problem (12.52) can be written in vector\n",
      "notation as\n",
      "min\n",
      "θ,β∥Yθ−Hβ∥2, (12.65)\n",
      "where θis a vector of Kreal numbers, and His the N×Mmatrix\n",
      "of evaluations hj(xi).\n",
      "(b) Suppose that the normalization on θisθTDπ1 = 0 and θTDπθ= 1.\n",
      "Interpret these normalizations in terms of the original scored θ(gi).\n",
      "(c) Show that, with this normalization, (12.65) can be partially optimized\n",
      "w.r.t. β, and leads to\n",
      "max\n",
      "θθTSθ, (12.66)\n",
      "subject to the normalization constraints, where Sis the projection\n",
      "operator corresponding to the basis matrix H.\n",
      "(d) Suppose that the hjinclude the constant function. Show that the\n",
      "largest eigenvalue of Sis 1.\n",
      "(e) Let Θbe aK×Kmatrix of scores (in columns), and suppose the\n",
      "normalization is ΘTDπΘ=I. Show that the solution to (12.53) is\n",
      "given by the complete set of eigenvectors of S; the ﬁrst eigenvector is\n",
      "trivial, and takes care of the centering of the scores. The remainder\n",
      "characterize the optimal scoring solution.\n",
      "Ex. 12.7 Derive the solution to the penalized optimal scoring problem\n",
      "(12.57).\n",
      "Ex. 12.8 Show that coeﬃcients βℓfound by optimal scoring are proportional\n",
      "to the discriminant directions νℓfound by linear discriminant analysis.\n",
      "Ex. 12.9 LetˆY=XˆBbe the ﬁtted N×Kindicator response matrix after\n",
      "linear regression on the N×pmatrix X, where p > K . Consider the reduced\n",
      "features x∗\n",
      "i=ˆBTxi. Show that LDA using x∗\n",
      "iis equivalent to LDA in the\n",
      "original space.\n",
      "Ex. 12.10 Kernels and linear discriminant analysis . Suppose you wish to\n",
      "carry out a linear discriminant analysis (two classes) using a vector of\n",
      "transformations of the input variables h(x). Since h(x) is high-dimensional,\n",
      "you will use a regularized within-class covariance matrix Wh+γI. Show\n",
      "that the model can be estimated using only the inner products K(xi,xi′) =\n",
      "⟨h(xi),h(xi′)⟩. Hence the kernel property of support vector machines is also\n",
      "shared by regularized linear discriminant analysis.\n",
      "Ex. 12.11 The MDA procedure models each class as a mixture of Gaussians.\n",
      "Hence each mixture center belongs to one and only one class. A more\n",
      "general model allows each mixture center to be shared by all classes. We\n",
      "take the joint density of labels and features to be\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "458 12. Flexible Discriminants\n",
      "P(G,X) =R∑\n",
      "r=1πrPr(G,X), (12.67)\n",
      "a mixture of joint densities. Furthermore we assume\n",
      "Pr(G,X) =Pr(G)φ(X;θr,Σ). (12.68)\n",
      "This model consists of regions centered at θr, and for each there is a class\n",
      "proﬁle Pr(G). The posterior class distribution is given by\n",
      "P(G=k|X=x) =∑R\n",
      "r=1πrPr(G=k)φ(x;θr,Σ)\n",
      "∑R\n",
      "r=1πrφ(x;θr,Σ), (12.69)\n",
      "where the denominator is the marginal distribution P(X).\n",
      "(a) Show that this model (called MDA2) can be viewed as a generalization\n",
      "of MDA since\n",
      "P(X|G=k) =∑R\n",
      "r=1πrPr(G=k)φ(x;θr,Σ)\n",
      "∑R\n",
      "r=1πrPr(G=k), (12.70)\n",
      "where πrk=πrPr(G=k)/∑R\n",
      "r=1πrPr(G=k) corresponds to the\n",
      "mixing proportions for the kth class.\n",
      "(b) Derive the EM algorithm for MDA2.\n",
      "(c) Show that if the initial weight matrix is constructed as in MDA, in-\n",
      "volving separate k-means clustering in each class, then the algorithm\n",
      "for MDA2 is identical to the original MDA procedure.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 459\n",
      "Printer: Opaque this\n",
      "13\n",
      "Prototype Methods and\n",
      "Nearest-Neighbors\n",
      "13.1 Introduction\n",
      "In this chapter we discuss some simple and essentially model-free methods\n",
      "for classiﬁcation and pattern recognition. Because they are highly unstruc-\n",
      "tured, they typically are not useful for understanding the nature of the\n",
      "relationship between the features and class outcome. However, as black box\n",
      "prediction engines, they can be very eﬀective, and are often among the best\n",
      "performers in real data problems. The nearest-neighbor technique can also\n",
      "be used in regression; this was touched on in Chapter 2 and works reason-\n",
      "ably well for low-dimensional problems. However, with high-dimensional\n",
      "features, the bias–variance tradeoﬀ does not work as favorably for nearest-\n",
      "neighbor regression as it does for classiﬁcation.\n",
      "13.2 Prototype Methods\n",
      "Throughout this chapter, our training data consists of the Npairs ( x1,g1),\n",
      "... ,(xn,gN) where giis a class label taking values in {1,2,... ,K }. Pro-\n",
      "totype methods represent the training data by a set of points in feature\n",
      "space. These prototypes are typically not examples from the training sam-\n",
      "ple, except in the case of 1-nearest-neighbor classiﬁcation discussed later.\n",
      "Each prototype has an associated class label, and classiﬁcation of a query\n",
      "point xis made to the class of the closest prototype. “Closest” is usually\n",
      "deﬁned by Euclidean distance in the feature space, after each feature has\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "460 13. Prototypes and Nearest-Neighbors\n",
      "been standardized to have overall mean 0 and variance 1 in the training\n",
      "sample. Euclidean distance is appropriate for quantitative features. We\n",
      "discuss distance measures between qualitative and other kinds of feature\n",
      "values in Chapter 14.\n",
      "These methods can be very eﬀective if the prototypes are well positioned\n",
      "to capture the distribution of each class. Irregular class boundaries can be\n",
      "represented, with enough prototypes in the right places in feature space.\n",
      "The main challenge is to ﬁgure out how many prototypes to use and where\n",
      "to put them. Methods diﬀer according to the number and way in which\n",
      "prototypes are selected.\n",
      "13.2.1 K-means Clustering\n",
      "K-means clustering is a method for ﬁnding clusters and cluster centers in a\n",
      "set of unlabeled data. One chooses the desired number of cluster centers, say\n",
      "R, and the K-means procedure iteratively moves the centers to minimize\n",
      "the total within cluster variance.1Given an initial set of centers, the K-\n",
      "means algorithm alternates the two steps:\n",
      "•for each center we identify the subset of training points (its cluster)\n",
      "that is closer to it than any other center;\n",
      "•the means of each feature for the data points in each cluster are\n",
      "computed, and this mean vector becomes the new center for that\n",
      "cluster.\n",
      "These two steps are iterated until convergence. Typically the initial centers\n",
      "areRrandomly chosen observations from the training data. Details of the\n",
      "K-means procedure, as well as generalizations allowing for diﬀerent variable\n",
      "types and more general distance measures, are given in Chapter 14.\n",
      "To use K-means clustering for classiﬁcation of labeled data, the steps\n",
      "are:\n",
      "•apply K-means clustering to the training data in each class sepa-\n",
      "rately, using Rprototypes per class;\n",
      "•assign a class label to each of the K×Rprototypes;\n",
      "•classify a new feature xto the class of the closest prototype.\n",
      "Figure 13.1 (upper panel) shows a simulated example with three classes\n",
      "and two features. We used R= 5 prototypes per class, and show the clas-\n",
      "siﬁcation regions and the decision boundary. Notice that a number of the\n",
      "1The “ K” inK-means refers to the number of cluster centers. Since we have already\n",
      "reserved Kto denote the number of classes, we denote the number of clust ers by R.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "13.2 Prototype Methods 461\n",
      "K-means - 5 Prototypes  per Class\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................. ........................ ................................ ..................................... ............................................................................................................................................................................. .................................. ....................................... ......................................... ............................................ .............................................. .................................................. ......................... .............................. ........................ ................................... ......................... ..................................... .......................... ...................................... ............................. .................................. .................................. ....................... ....................................... ............. ............................................... ...................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ........... ........................................ ............................................................. ........ ........................................... ........................................................ ..... ................................................................................................. .. .............................................................................................. . .......................................................................................... .... .................................................................................... ....... .............................................................................. .......... ............................................................................ ......... ............................................................................. ......... ............................................................................. ......... ............................................................................. ......... ............................................................................... ....... .................................................................................... ..... ........................................................................................ ... ............................................................................................ . ............................................................................................... . ................................................................................................ ... ................................................................................................ ..... ................................................................................................ ....... ............................................ ................................ .................... .... ....................................................................... ..................... .... ........................................................... ................. ............. .......................................... ............. ...................... ........................ .............. ....\n",
      "................................................................................ ......................... ......................................... .................... .................................................... ................ ........................................................... ............... ............................................................. ............... ............................................................... ............... ............................................................... ............... ........................................................... ................ ....................................................... ................ .................................................... ................ ................................................... ............... ................................................. .............. .............................................. .............. ......................................... .................................................... ............................................... ............................................... ........... .......................................... .......... ................................................ .......... ...................................................... ......... .................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ .............................................................................................. ............................................................................................... ................................................................................................ ................................................................................................. ................................................\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o o o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•• •\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "LVQ - 5 Prototypes per Class\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................... ................................... ........................................ .............................................. .................................................. .................... .................................. ..................... ...................................... ..................... ......................................... ....................... ..................................... .............................. ............................ ..................................... .............. ............................................ .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... .... ............................................... ............................................................ .... ............................................... ........................................................... .... ............................................... .......................................................... ... ................................................ ........................................................ ... ................................................ ...................................................... ... ................................................ ..................................................... .. .................................................................................................... .. .................................................................................................. . ................................................................................................. . ................................................................................................ . ............................................................................................... ............................................................................................. ........................................................................................... . ......................................................................................... . ............................................................ .................... ......... ........................................... ........................ ..............\n",
      ".......................... ................................. ............................... ....................... ................................................. ................ .......................................................... ............... ............................................................ ............... ............................................................. ............... .............................................................. ............... .............................................................. ............... ................................................................ .............. .................................................................. .............. ................................................................... .............. ................................................................ .............. ........................................................... ............... ..................................................... ................ ................................................ ............... ............................................. .............. ......................................... ................................................... ................................................ .................................................. ........... .............................................. ........... ...................................................... .......... ....................................................................... .......................................................................... ........................................................................... ............................................................................ ............................................................................. .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................ ..........................................\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o o o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "••••\n",
      "••\n",
      "•••\n",
      "•\n",
      "• •\n",
      "••\n",
      "•• •\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "FIGURE 13.1. Simulated example with three classes and ﬁve prototypes per\n",
      "class. The data in each class are generated from a mixture of Gau ssians. In the\n",
      "upper panel, the prototypes were found by applying the K-means clustering algo-\n",
      "rithm separately in each class. In the lower panel, the LVQ alg orithm (starting\n",
      "from the K-means solution) moves the prototypes away from the decision b ound-\n",
      "ary. The broken purple curve in the background is the Bayes dec ision boundary.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "462 13. Prototypes and Nearest-Neighbors\n",
      "Algorithm 13.1 Learning Vector Quantization—LVQ.\n",
      "1. Choose Rinitial prototypes for each class: m1(k),m2(k),... ,m R(k),\n",
      "k= 1,2,... ,K , for example, by sampling Rtraining points at random\n",
      "from each class.\n",
      "2. Sample a training point xirandomly (with replacement), and let ( j,k)\n",
      "index the closest prototype mj(k) toxi.\n",
      "(a) If gi=k(i.e., they are in the same class), move the prototype\n",
      "towards the training point:\n",
      "mj(k)←mj(k) +ǫ(xi−mj(k)),\n",
      "where ǫis the learning rate .\n",
      "(b) If gi̸=k(i.e., they are in diﬀerent classes), move the prototype\n",
      "away from the training point:\n",
      "mj(k)←mj(k)−ǫ(xi−mj(k)).\n",
      "3. Repeat step 2, decreasing the learning rate ǫwith each iteration to-\n",
      "wards zero.\n",
      "prototypes are near the class boundaries, leading to potential misclassiﬁca-\n",
      "tion errors for points near these boundaries. This results from an obvious\n",
      "shortcoming with this method: for each class, the other classes do not have\n",
      "a say in the positioning of the prototypes for that class. A better approach,\n",
      "discussed next, uses all of the data to position all prototypes.\n",
      "13.2.2 Learning Vector Quantization\n",
      "In this technique due to Kohonen (1989), prototypes are placed strategically\n",
      "with respect to the decision boundaries in an ad-hoc way. LVQ is an online\n",
      "algorithm—observations are processed one at a time.\n",
      "The idea is that the training points attract prototypes of the correct class,\n",
      "and repel other prototypes. When the iterations settle down, prototypes\n",
      "should be close to the training points in their class. The learning rate ǫis\n",
      "decreased to zero with each iteration, following the guidelines for stochastic\n",
      "approximation learning rates (Section 11.4.)\n",
      "Figure 13.1 (lower panel) shows the result of LVQ, using the K-means\n",
      "solution as starting values. The prototypes have tended to move away from\n",
      "the decision boundaries, and away from prototypes of competing classes.\n",
      "The procedure just described is actually called LVQ1. Modiﬁcations\n",
      "(LVQ2, LVQ3, etc.) have been proposed, that can sometimes improve per-\n",
      "formance. A drawback of learning vector quantization methods is the fact\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "13.3k-Nearest-Neighbor Classiﬁers 463\n",
      "that they are deﬁned by algorithms, rather than optimization of some ﬁxed\n",
      "criteria; this makes it diﬃcult to understand their properties.\n",
      "13.2.3 Gaussian Mixtures\n",
      "The Gaussian mixture model can also be thought of as a prototype method,\n",
      "similar in spirit to K-means and LVQ. We discuss Gaussian mixtures in\n",
      "some detail in Sections 6.8, 8.5 and 12.7. Each cluster is described in terms\n",
      "of a Gaussian density, which has a centroid (as in K-means), and a covari-\n",
      "ance matrix. The comparison becomes crisper if we restrict the component\n",
      "Gaussians to have a scalar covariance matrix (Exercise 13.1). The two st eps\n",
      "of the alternating EM algorithm are very similar to the two steps in K-\n",
      "means:\n",
      "•In the E-step, each observation is assigned a responsibility or weight\n",
      "for each cluster, based on the likelihood of each of the correspond-\n",
      "ing Gaussians. Observations close to the center of a cluster will most\n",
      "likely get weight 1 for that cluster, and weight 0 for every other clus-\n",
      "ter. Observations half-way between two clusters divide their weight\n",
      "accordingly.\n",
      "•In the M-step, each observation contributes to the weighted means\n",
      "(and covariances) for every cluster.\n",
      "As a consequence, the Gaussian mixture model is often referred to as a soft\n",
      "clustering method, while K-means is hard.\n",
      "Similarly, when Gaussian mixture models are used to represent the fea-\n",
      "ture density in each class, it produces smooth posterior probabilities ˆ p(x) =\n",
      "{ˆp1(x),... ,ˆpK(x)}for classifying x(see (12.60) on page 449.) Often this\n",
      "is interpreted as a soft classiﬁcation, while in fact the classiﬁcation rule i s\n",
      "ˆG(x) = arg max kˆpk(x). Figure 13.2 compares the results of K-means and\n",
      "Gaussian mixtures on the simulated mixture problem of Chapter 2. We\n",
      "see that although the decision boundaries are roughly similar, those for the\n",
      "mixture model are smoother (although the prototypes are in approximately\n",
      "the same positions.) We also see that while both procedures devote a blue\n",
      "prototype (incorrectly) to a region in the northwest, the Gaussian mixtur e\n",
      "classiﬁer can ultimately ignore this region, while K-means cannot. LVQ\n",
      "gave very similar results to K-means on this example, and is not shown.\n",
      "13.3 k-Nearest-Neighbor Classiﬁers\n",
      "These classiﬁers are memory-based , and require no model to be ﬁt. Given\n",
      "a query point x0, we ﬁnd the ktraining points x(r),r= 1,... ,k closest in\n",
      "distance to x0, and then classify using majority vote among the kneighbors.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "464 13. Prototypes and Nearest-Neighbors\n",
      "K-means - 5 Prototypes per Class\n",
      "... .. . . .. . . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "Training Error: 0.170\n",
      "Test Error:       0.243\n",
      "Bayes Error:    0.210\n",
      "Gaussian Mixtures - 5 Subclasses per Class\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ".... .. .. . .. . .. . .. . . . .. . . . .. . . . . .. . . . . . . .. . . . . . . .. . . . . . . . . .. . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "Training Error: 0.17\n",
      "Test Error:       0.22\n",
      "Bayes Error:    0.21\n",
      "FIGURE 13.2. The upper panel shows the K-means classiﬁer applied to the\n",
      "mixture data example. The decision boundary is piecewise linear . The lower panel\n",
      "shows a Gaussian mixture model with a common covariance for all component\n",
      "Gaussians. The EM algorithm for the mixture model was started a t theK-means\n",
      "solution. The broken purple curve in the background is the Baye s decision\n",
      "boundary.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "13.3k-Nearest-Neighbor Classiﬁers 465\n",
      "Ties are broken at random. For simplicity we will assume that the features\n",
      "are real-valued, and we use Euclidean distance in feature space:\n",
      "d(i)=||x(i)−x0||. (13.1)\n",
      "Typically we ﬁrst standardize each of the features to have mean zero and\n",
      "variance 1, since it is possible that they are measured in diﬀerent units. In\n",
      "Chapter 14 we discuss distance measures appropriate for qualitative and\n",
      "ordinal features, and how to combine them for mixed data. Adaptively\n",
      "chosen distance metrics are discussed later in this chapter.\n",
      "Despite its simplicity, k-nearest-neighbors has been successful in a large\n",
      "number of classiﬁcation problems, including handwritten digits, satellite\n",
      "image scenes and EKG patterns. It is often successful where each class\n",
      "has many possible prototypes, and the decision boundary is very irregular.\n",
      "Figure 13.3 (upper panel) shows the decision boundary of a 15-nearest-\n",
      "neighbor classiﬁer applied to the three-class simulated data. The decision\n",
      "boundary is fairly smooth compared to the lower panel, where a 1-nearest-\n",
      "neighbor classiﬁer was used. There is a close relationship between nearest-\n",
      "neighbor and prototype methods: in 1-nearest-neighbor classiﬁcation, each\n",
      "training point is a prototype.\n",
      "Figure 13.4 shows the training, test and tenfold cross-validation errors\n",
      "as a function of the neighborhood size, for the two-class mixture problem.\n",
      "Since the tenfold CV errors are averages of ten numbers, we can estimate\n",
      "a standard error.\n",
      "Because it uses only the training point closest to the query point, the bias\n",
      "of the 1-nearest-neighbor estimate is often low, but the variance is high.\n",
      "A famous result of Cover and Hart (1967) shows that asymptotically the\n",
      "error rate of the 1-nearest-neighbor classiﬁer is never more than twice the\n",
      "Bayes rate. The rough idea of the proof is as follows (using squared-error\n",
      "loss). We assume that the query point coincides with one of the training\n",
      "points, so that the bias is zero. This is true asymptotically if the dimensio n\n",
      "of the feature space is ﬁxed and the training data ﬁlls up the space in a\n",
      "dense fashion. Then the error of the Bayes rule is just the variance of a\n",
      "Bernoulli random variate (the target at the query point), while the error of\n",
      "1-nearest-neighbor rule is twicethe variance of a Bernoulli random variate,\n",
      "one contribution each for the training and query targets.\n",
      "We now give more detail for misclassiﬁcation loss. At xletk∗be the\n",
      "dominant class, and pk(x) the true conditional probability for class k. Then\n",
      "Bayes error = 1 −pk∗(x), (13.2)\n",
      "1-nearest-neighbor error =K∑\n",
      "k=1pk(x)(1−pk(x)), (13.3)\n",
      "≥1−pk∗(x). (13.4)\n",
      "The asymptotic 1-nearest-neighbor error rate is that of a random rule; we\n",
      "pick both the classiﬁcation and the test point at random with probabili-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "466 13. Prototypes and Nearest-Neighbors\n",
      "15-Nearest Neighbors\n",
      ". ........ ......... ..... ...... ......... ......................................................................................... ............................................................................................................................................................................................................................................................................ .............................................................................................................. .. .. . ...................................... .. . .... .... ......................................... ........... ....................................................... ......... ............................................................ ...... .......................................................................... ............................................................................... ...................... .................................................. ................................................................... ........................... ...................................... ........ .. .................................................................................... . .....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................. . . ................................................... ............................................................. . .................................................. ............................................................ ................................................... ............................................................... .. ................................................. .................................................... ...... ... ................................................... ............................................... ..... ... .. ................................................. .............................................. .... .... ................................................... ............................................... ...... ................................................... ..................................................... .... ............................................... ........................................................ .... ............................................... ........................ ............................... ... .............................................................. .... . . ........................ ....... ...................................................... ........................ ................. ................................... ................. .............................. ............... ....... ... ........................................ ................................................................ .............................. . ...................... ......\n",
      ".... .. . ..................... .............................. .................................................................................................. .......................................... .. ......... ............................................... ........... ..................................................... .............. ............................................................. .............. .................................................................. ........... .................................................................. . ............. .............................................................. ................ .......................................................... .. . . ............ .................... .................................... . . ............... .. ......... .. ................................... .................................................... ............................................... ................................................................................................................................ .................................................................................................... . ....................................................... . ........ ............................................................. ......................................................................... ................ ... ..................................................... ........................................................................... ............................................................................ .............................................................................. ............................................................................... ................................................................................ ................................................................................. .................................................................................. ................................................................................... ................................................................................... .................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ....................................................................................... ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ............................................................................................. ...........................................\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o o o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "1-Nearest Neighbor\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "................. ................. ............ .............. .................. ....................... ... ................ ..... ............... ..... ............. .... ................ ......... ..... ..... ...... ................... ..... ...... ...... .... ....... ... .......... ... .... .. .... . ........ ............ ............ ...... ............ .......... ........ .............. ........ ........ ............. .......... ...... ............... ....... ............. ............. ......... .......................... ............. ......................... . ..... .. .. ............................. ....... .. . ...... .......................... ....... ............ .............. ................ ............... .............................. ............. ........... ............ ....................... ............. ........... ....................... .......................................... ...... ..................... ..................... ..... ......................... .......... ............... ........ ....... ... .... ....................... .... ... ................ ...................... . ........................ . ......... ..... .... .............. ............... .............. ...... ... .................................. ......... .......... ... .............................. ........... ...... .......... ............................. ......... .......... ...................................... ... ................................................... ...................................... ..................................... .......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................................................... ............................................................... ................ ................................... .............................................................. ....... ............................................ .............................................. ...... .... ......................................................................................... ........ ... ................................................ ............................................ ........ ............... .............................................................................. ........ .......... ............................................................................... ....... ........ ......................................................... .............. ......... ... .. .......................................................... ............ ....... ... ........ ...................................................... .......... ...... .... .......... .................................................. ...... .. ....... ................ .................................................... . .. .............. .............. ..................................................... ... ............ .... ... ........... ............................................. .. ............. ... .................. ....................................... .............. ..... ... ..... ..... ....... .......................................... ............. ...... . ....... ... ......... ....................................... ............... .. .......... ...... ..... ... .............................................. .................. .......... ........ . ............................................ ..... .......... ......... ............. .................................. ... ..... ......... ..................... ........................... . ... ... ........ ........................ ....................... ..... ... ....... ............ ............. ................ .. .... ..... ............. ... .... .................... .... ..... . ........... ..... ...... ...........\n",
      "................................................. ............ ................. ................................................................. .................. ..................... ...................... ............................ ......................................... ................................................ .. ... .......... .......................... ...... . ...... ...................................... ...... . ..... ............ .................................... .. .............. .................................... .. . ................ ................................................ ... ....... .................................................. ...... .......... ........................................... ..... ........ ......... ........................................... ... . .. .. ... ................................... .. .. .. .. .................................. ........... .... . ...... . .... ............................................... ..... ..... ................................................... ...... . .................................................. .... .. . ......................................................... ..... ..... .................................................... ...... ............................................................ .................. ........................................................ ................... ......................................................... ................... .......................................................... ............................................................................... .............................................................................. .............................................................................. ................................................................................ .................................................................................. .................................................................................. ................................................................................... .................................................................................... ..................................................................................... ..................................................................................... ...................................................................................... ....................................................................................... ........................................................................................ ........................................................................................ ......................................................................................... .......................................................................................... ........................................................................................... ........................................................................................... .........................................\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oooooo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "o o o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "ooooo\n",
      "o\n",
      "FIGURE 13.3. k-nearest-neighbor classiﬁers applied to the simulation data o f\n",
      "Figure 13.1. The broken purple curve in the background is the B ayes decision\n",
      "boundary.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "13.3k-Nearest-Neighbor Classiﬁers 467\n",
      "Number of NeighborsMisclassification Errors\n",
      "0 5 10 15 20 25 300.0 0.05 0.10 0.15 0.20 0.25 0.30•\n",
      "••••••• • ••• •••\n",
      "•• ••••\n",
      "••••• •• • ••\n",
      "•••••• •••• ••••\n",
      "Test Error\n",
      "10-fold CV\n",
      "Training Error\n",
      "Bayes Error\n",
      "7-Nearest Neighbors\n",
      ".. .. . .. . . . .. . . . . . .. . . . . . .. . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .. . . . . . . . . . . . . . .\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "o\n",
      "oo\n",
      "o oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "ooooooo\n",
      "oo o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "Training Error: 0.145\n",
      "Test Error:       0.225\n",
      "Bayes Error:    0.210\n",
      "FIGURE 13.4. k-nearest-neighbors on the two-class mixture data. The upper\n",
      "panel shows the misclassiﬁcation errors as a function of neighbo rhood size. Stan-\n",
      "dard error bars are included for 10-fold cross validation. The lower panel shows\n",
      "the decision boundary for 7-nearest-neighbors, which appears to be optimal for\n",
      "minimizing test error. The broken purple curve in the backgrou nd is the Bayes\n",
      "decision boundary.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "468 13. Prototypes and Nearest-Neighbors\n",
      "tiespk(x), k= 1,... ,K . For K= 2 the 1-nearest-neighbor error rate is\n",
      "2pk∗(x)(1−pk∗(x))≤2(1−pk∗(x)) (twice the Bayes error rate). More\n",
      "generally, one can show (Exercise 13.3)\n",
      "K∑\n",
      "k=1pk(x)(1−pk(x))≤2(1−pk∗(x))−K\n",
      "K−1(1−pk∗(x))2.(13.5)\n",
      "Many additional results of this kind have been derived; Ripley (1996) sum-\n",
      "marizes a number of them.\n",
      "This result can provide a rough idea about the best performance that\n",
      "is possible in a given problem. For example, if the 1-nearest-neighbor rule\n",
      "has a 10% error rate, then asymptotically the Bayes error rate is at least\n",
      "5%. The kicker here is the asymptotic part, which assumes the bias of the\n",
      "nearest-neighbor rule is zero. In real problems the bias can be substantial.\n",
      "The adaptive nearest-neighbor rules, described later in this chapter, are an\n",
      "attempt to alleviate this bias. For simple nearest-neighbors, the bias and\n",
      "variance characteristics can dictate the optimal number of near neighbors\n",
      "for a given problem. This is illustrated in the next example.\n",
      "13.3.1 Example: A Comparative Study\n",
      "We tested the nearest-neighbors, K-means and LVQ classiﬁers on two sim-\n",
      "ulated problems. There are 10 independent features Xj, each uniformly\n",
      "distributed on [0 ,1]. The two-class 0-1 target variable is deﬁned as follows:\n",
      "Y=I(\n",
      "X1>1\n",
      "2)\n",
      "; problem 1: “easy”,\n",
      "Y=I\n",
      "sign\n",
      "\n",
      "3∏\n",
      "j=1(\n",
      "Xj−1\n",
      "2)\n",
      "\n",
      ">0\n",
      "; problem 2: “diﬃcult.”(13.6)\n",
      "Hence in the ﬁrst problem the two classes are separated by the hyperplane\n",
      "X1= 1/2; in the second problem, the two classes form a checkerboard\n",
      "pattern in the hypercube deﬁned by the ﬁrst three features. The Bayes\n",
      "error rate is zero in both problems. There were 100 training and 1000 test\n",
      "observations.\n",
      "Figure 13.5 shows the mean and standard error of the misclassiﬁcation\n",
      "error for nearest-neighbors, K-means and LVQ over ten realizations, as\n",
      "the tuning parameters are varied. We see that K-means and LVQ give\n",
      "nearly identical results. For the best choices of their tuning parameters,\n",
      "K-means and LVQ outperform nearest-neighbors for the ﬁrst problem, and\n",
      "they perform similarly for the second problem. Notice that the best value\n",
      "of each tuning parameter is clearly situation dependent. For example 25-\n",
      "nearest-neighbors outperforms 1-nearest-neighbor by a factor of 70% in the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "13.3k-Nearest-Neighbor Classiﬁers 469\n",
      "Number of NeighborsMisclassification Error\n",
      "0 20 40 600.1 0.2 0.3 0.4 0.5Nearest Neighbors / Easy\n",
      "Number of Prototypes per ClassMisclassification Error\n",
      "0 5 10 15 20 25 300.1 0.2 0.3 0.4 0.5K-means & LVQ / Easy\n",
      "Number of NeighborsMisclassification Error\n",
      "0 20 40 600.40 0.45 0.50 0.55 0.60Nearest Neighbors / Difficult\n",
      "Number of Prototypes per ClassMisclassification Error\n",
      "0 5 10 15 20 25 300.40 0.45 0.50 0.55 0.60K-means & LVQ / Difficult\n",
      "FIGURE 13.5. Mean ±one standard error of misclassiﬁcation error for near-\n",
      "est-neighbors, K-means (blue) and LVQ (red) over ten realizations for two sim-\n",
      "ulated problems: “easy” and “diﬃcult,” described in the text.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "470 13. Prototypes and Nearest-Neighbors\n",
      "Spectral Band 1 Spectral Band 2 Spectral Band 3\n",
      "Spectral Band 4 Land Usage Predicted Land Usage\n",
      "FIGURE 13.6. The ﬁrst four panels are LANDSAT images for an agricultural\n",
      "area in four spectral bands, depicted by heatmap shading. The r emaining two\n",
      "panels give the actual land usage (color coded) and the predicte d land usage using\n",
      "a ﬁve-nearest-neighbor rule described in the text.\n",
      "ﬁrst problem, while 1-nearest-neighbor is best in the second problem by a\n",
      "factor of 18%. These results underline the importance of using an objective,\n",
      "data-based method like cross-validation to estimate the best value of a\n",
      "tuning parameter (see Figure 13.4 and Chapter 7).\n",
      "13.3.2 Example: k-Nearest-Neighbors and Image Scene\n",
      "Classiﬁcation\n",
      "The STATLOG project (Michie et al., 1994) used part of a LANDSAT\n",
      "image as a benchmark for classiﬁcation (82 ×100 pixels). Figure 13.6 shows\n",
      "four heat-map images, two in the visible spectrum and two in the infrared,\n",
      "for an area of agricultural land in Australia. Each pixel has a class label\n",
      "from the 7-element set G={red soil, cotton, vegetation stubble, mixture,\n",
      "gray soil, damp gray soil, very damp gray soil }, determined manually by\n",
      "research assistants surveying the area. The lower middle panel shows the\n",
      "actual land usage, shaded by diﬀerent colors to indicate the classes. The\n",
      "objective is to classify the land usage at a pixel, based on the information\n",
      "in the four spectral bands.\n",
      "Five-nearest-neighbors produced the predicted map shown in the bot-\n",
      "tom right panel, and was computed as follows. For each pixel we extracted\n",
      "an 8-neighbor feature map—the pixel itself and its 8 immediate neighbors\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "13.3k-Nearest-Neighbor Classiﬁers 471\n",
      "N\n",
      "N\n",
      "N\n",
      " N\n",
      "X\n",
      "N\n",
      "N\n",
      "N\n",
      "N\n",
      "FIGURE 13.7. A pixel and its 8-neighbor feature map.\n",
      "(see Figure 13.7). This is done separately in the four spectral bands, giving\n",
      "(1+8) ×4 = 36 input features per pixel. Then ﬁve-nearest-neighbors classi-\n",
      "ﬁcation was carried out in this 36-dimensional feature space. The resulting\n",
      "test error rate was about 9 .5% (see Figure 13.8). Of all the methods used\n",
      "in the STATLOG project, including LVQ, CART, neural networks, linear\n",
      "discriminant analysis and many others, k-nearest-neighbors performed best\n",
      "on this task. Hence it is likely that the decision boundaries in IR36are quite\n",
      "irregular.\n",
      "13.3.3 Invariant Metrics and Tangent Distance\n",
      "In some problems, the training features are invariant under certain natural\n",
      "transformations. The nearest-neighbor classiﬁer can exploit such invari-\n",
      "ances by incorporating them into the metric used to measure the distances\n",
      "between objects. Here we give an example where this idea was used with\n",
      "great success, and the resulting classiﬁer outperformed all others at the\n",
      "time of its development (Simard et al., 1993).\n",
      "The problem is handwritten digit recognition, as discussed is Chapter 1\n",
      "and Section 11.7. The inputs are grayscale images with 16 ×16 = 256\n",
      "pixels; some examples are shown in Figure 13.9. At the top of Figure 13.1 0,\n",
      "a “3” is shown, in its actual orientation (middle) and rotated 7 .5◦and 15◦\n",
      "in either direction. Such rotations can often occur in real handwriting, and\n",
      "it is obvious to our eye that this “3” is still a “3” after small rotati ons.\n",
      "Hence we want our nearest-neighbor classiﬁer to consider these two “3”s\n",
      "to be close together (similar). However the 256 grayscale pixel values for a\n",
      "rotated “3” will look quite diﬀerent from those in the original image, a nd\n",
      "hence the two objects can be far apart in Euclidean distance in IR256.\n",
      "We wish to remove the eﬀect of rotation in measuring distances between\n",
      "two digits of the same class. Consider the set of pixel values consisting of\n",
      "the original “3” and its rotated versions. This is a one-dimensional curve in\n",
      "IR256, depicted by the green curve passing through the “3” in Figure 13.10.\n",
      "Figure 13.11 shows a stylized version of IR256, with two images indicated by\n",
      "xiandxi′. These might be two diﬀerent “3”s, for example. Through each\n",
      "image we have drawn the curve of rotated versions of that image, called\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "472 13. Prototypes and Nearest-Neighbors\n",
      "STATLOG results\n",
      "MethodTest Misclassification Error\n",
      "2 4 6 8 10 12 140.0 0.05 0.10 0.15LVQRBFALLOC80CART NeuralNewID C4.5QDASMARTLogisticLDA\n",
      "DANNK-NN\n",
      "FIGURE 13.8. Test-error performance for a number of classiﬁers, as reported\n",
      "by the STATLOG project. The entry DANN is a variant of k-nearest neighbors,\n",
      "using an adaptive metric (Section 13.4.2).\n",
      "FIGURE 13.9. Examples of grayscale images of handwritten digits.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "13.3k-Nearest-Neighbor Classiﬁers 473\n",
      "Tangent+ α.Transformations of 30 7.5 −15 −7.5\n",
      "3\n",
      "α=0 α=0.1 α=− 0.2 α=− 0.1 α=0.2\n",
      "Linear equation for \n",
      "images above15\n",
      "Pixel space\n",
      "FIGURE 13.10. The top row shows a “ 3” in its original orientation (middle)\n",
      "and rotated versions of it. The green curve in the middle of the ﬁg ure depicts\n",
      "this set of rotated “ 3” in256-dimensional space. The red line is the tangent line\n",
      "to the curve at the original image, with some “ 3”s on this tangent line, and its\n",
      "equation shown at the bottom of the ﬁgure.\n",
      "invariance manifolds in this context. Now, rather than using the usual\n",
      "Euclidean distance between the two images, we use the shortest distance\n",
      "between the two curves. In other words, the distance between the two\n",
      "images is taken to be the shortest Euclidean distance between any rotated\n",
      "version of ﬁrst image, and any rotated version of the second image. This\n",
      "distance is called an invariant metric .\n",
      "In principle one could carry out 1-nearest-neighbor classiﬁcation using\n",
      "this invariant metric. However there are two problems with it. First, it is\n",
      "very diﬃcult to calculate for real images. Second, it allows large trans-\n",
      "formations that can lead to poor performance. For example a “6” would\n",
      "be considered close to a “9” after a rotation of 180◦. We need to restrict\n",
      "attention to small rotations.\n",
      "The use of tangent distance solves both of these problems. As shown in\n",
      "Figure 13.10, we can approximate the invariance manifold of the image\n",
      "“3” by its tangent at the original image. This tangent can be computed\n",
      "by estimating the direction vector from small rotations of the image, or b y\n",
      "more sophisticated spatial smoothing methods (Exercise 13.4.) For large\n",
      "rotations, the tangent image no longer looks like a “3,” so the problem\n",
      "with large transformations is alleviated.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "474 13. Prototypes and Nearest-Neighbors\n",
      "TransformationsTransformations\n",
      "xixi′ofxi\n",
      "ofxi′Tangent distance\n",
      "Euclidean distance\n",
      "between xiandxi′Distance between\n",
      "transformed\n",
      "xiandxi′\n",
      "FIGURE 13.11. Tangent distance computation for two images xiandxi′.\n",
      "Rather than using the Euclidean distance between xiandxi′, or the shortest\n",
      "distance between the two curves, we use the shortest distance b etween the two\n",
      "tangent lines.\n",
      "The idea then is to compute the invariant tangent line for each training\n",
      "image. For a query image to be classiﬁed, we compute its invariant tangent\n",
      "line, and ﬁnd the closest line to it among the lines in the training set. The\n",
      "class (digit) corresponding to this closest line is our predicted class for the\n",
      "query image. In Figure 13.11 the two tangent lines intersect, but this is only\n",
      "because we have been forced to draw a two-dimensional representation of\n",
      "the actual 256-dimensional situation. In IR256the probability of two such\n",
      "lines intersecting is eﬀectively zero.\n",
      "Now a simpler way to achieve this invariance would be to add into the\n",
      "training set a number of rotated versions of each training image, and then\n",
      "just use a standard nearest-neighbor classiﬁer. This idea is called “hints” in\n",
      "Abu-Mostafa (1995), and works well when the space of invariances is small.\n",
      "So far we have presented a simpliﬁed version of the problem. In addition to\n",
      "rotation, there are six other types of transformations under which we would\n",
      "like our classiﬁer to be invariant. There are translation (two directio ns),\n",
      "scaling (two directions), sheer, and character thickness. Hence the curves\n",
      "and tangent lines in Figures 13.10 and 13.11 are actually 7-dimensional\n",
      "manifolds and hyperplanes. It is infeasible to add transformed versions\n",
      "of each training image to capture all of these possibilities. The tangent\n",
      "manifolds provide an elegant way of capturing the invariances.\n",
      "Table 13.1 shows the test misclassiﬁcation error for a problem with 7291\n",
      "training images and 2007 test digits (the U.S. Postal Services database), for\n",
      "a carefully constructed neural network, and simple 1-nearest-neighbor and\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "13.4 Adaptive Nearest-Neighbor Methods 475\n",
      "TABLE 13.1. Test error rates for the handwritten ZIP code problem.\n",
      "Method Error rate\n",
      "Neural-net 0.049\n",
      "1-nearest-neighbor/Euclidean distance 0.055\n",
      "1-nearest-neighbor/tangent distance 0.026\n",
      "tangent distance 1-nearest-neighbor rules. The tangent distance nearest-\n",
      "neighbor classiﬁer works remarkably well, with test error rates near those\n",
      "for the human eye (this is a notoriously diﬃcult test set). In practice,\n",
      "it turned out that nearest-neighbors are too slow for online classiﬁcation\n",
      "in this application (see Section 13.5), and neural network classiﬁers were\n",
      "subsequently developed to mimic it.\n",
      "13.4 Adaptive Nearest-Neighbor Methods\n",
      "When nearest-neighbor classiﬁcation is carried out in a high-dimensional\n",
      "feature space, the nearest neighbors of a point can be very far away, causing\n",
      "bias and degrading the performance of the rule.\n",
      "To quantify this, consider Ndata points uniformly distributed in the unit\n",
      "cube [−1\n",
      "2,1\n",
      "2]p. LetRbe the radius of a 1-nearest-neighborhood centered at\n",
      "the origin. Then\n",
      "median( R) =v−1/p\n",
      "p(\n",
      "1−1\n",
      "21/N)1/p\n",
      ", (13.7)\n",
      "where vprpis the volume of the sphere of radius rinpdimensions. Fig-\n",
      "ure 13.12 shows the median radius for various training sample sizes and\n",
      "dimensions. We see that median radius quickly approaches 0 .5, the dis-\n",
      "tance to the edge of the cube.\n",
      "What can be done about this problem? Consider the two-class situation\n",
      "in Figure 13.13. There are two features, and a nearest-neighborhood at\n",
      "a query point is depicted by the circular region. Implicit in near-neighbor\n",
      "classiﬁcation is the assumption that the class probabilities are roughly con-\n",
      "stant in the neighborhood, and hence simple averages give good estimates.\n",
      "However, in this example the class probabilities vary only in the horizontal\n",
      "direction. If we knew this, we would stretch the neighborhood in the verti-\n",
      "cal direction, as shown by the tall rectangular region. This will reduce the\n",
      "bias of our estimate and leave the variance the same.\n",
      "In general, this calls for adapting the metric used in nearest-neighbor\n",
      "classiﬁcation, so that the resulting neighborhoods stretch out in directions\n",
      "for which the class probabilities don’t change much. In high-dimensional\n",
      "feature space, the class probabilities might change only a low-dimensional\n",
      "subspace and hence there can be considerable advantage to adapting the\n",
      "metric.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "476 13. Prototypes and Nearest-Neighbors\n",
      "DimensionMedian Radius\n",
      "0 5 10 150.0 0.1 0.2 0.3 0.4 0.5 0.6N=100N=1,000\n",
      "N=10,000\n",
      "FIGURE 13.12. Median radius of a 1-nearest-neighborhood, for uniform data\n",
      "withNobservations in pdimensions.\n",
      "o\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "•5-Nearest Neighborhoods\n",
      "FIGURE 13.13. The points are uniform in the cube, with the vertical line sepa-\n",
      "rating class red and green. The vertical strip denotes the 5-nearest-neighbor region\n",
      "using only the horizontal coordinate to ﬁnd the nearest-neighbors fo r the target\n",
      "point (solid dot). The sphere shows the 5-nearest-neighbor region using both co-\n",
      "ordinates, and we see in this case it has extended into the class-re d region (and\n",
      "is dominated by the wrong class in this instance).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "13.4 Adaptive Nearest-Neighbor Methods 477\n",
      "Friedman (1994a) proposed a method in which rectangular neighbor-\n",
      "hoods are found adaptively by successively carving away edges of a box\n",
      "containing the training data. Here we describe the discriminant adaptive\n",
      "nearest-neighbor (DANN) rule of Hastie and Tibshirani (1996a). Earlier,\n",
      "related proposals appear in Short and Fukunaga (1981) and Myles and\n",
      "Hand (1990).\n",
      "At each query point a neighborhood of say 50 points is formed, and the\n",
      "class distribution among the points is used to decide how to deform the\n",
      "neighborhood—that is, to adapt the metric. The adapted metric is then\n",
      "used in a nearest-neighbor rule at the query point. Thus at each query\n",
      "point a potentially diﬀerent metric is used.\n",
      "In Figure 13.13 it is clear that the neighborhood should be stretched in\n",
      "the direction orthogonal to line joining the class centroids. This direction\n",
      "also coincides with the linear discriminant boundary, and is the direction\n",
      "in which the class probabilities change the least. In general this direction\n",
      "of maximum change will not be orthogonal to the line joining the class cen-\n",
      "troids (see Figure 4.9 on page 116.) Assuming a local discriminant model,\n",
      "the information contained in the local within- and between-class covari-\n",
      "ance matrices is all that is needed to determine the optimal shape of the\n",
      "neighborhood.\n",
      "Thediscriminant adaptive nearest-neighbor (DANN) metric at a query\n",
      "point x0is deﬁned by\n",
      "D(x,x0) = (x−x0)TΣ(x−x0), (13.8)\n",
      "where\n",
      "Σ=W−1/2[W−1/2BW−1/2+ǫI]W−1/2\n",
      "=W−1/2[B∗+ǫI]W−1/2. (13.9)\n",
      "HereWis the pooled within-class covariance matrix∑K\n",
      "k=1πkWkandB\n",
      "is the between class covariance matrix∑K\n",
      "k=1πk(¯xk−¯x)(¯xk−¯x)T, with\n",
      "WandBcomputed using only the 50 nearest neighbors around x0. After\n",
      "computation of the metric, it is used in a nearest-neighbor rule at x0.\n",
      "This complicated formula is actually quite simple in its operation. It ﬁrst\n",
      "spheres the data with respect to W, and then stretches the neighborhood\n",
      "in the zero-eigenvalue directions of B∗(the between-matrix for the sphered\n",
      "data ). This makes sense, since locally the observed class means do not dif-\n",
      "fer in these directions. The ǫparameter rounds the neighborhood, from an\n",
      "inﬁnite strip to an ellipsoid, to avoid using points far away from the quer y\n",
      "point. The value of ǫ= 1 seems to work well in general. Figure 13.14 shows\n",
      "the resulting neighborhoods for a problem where the classes form two con-\n",
      "centric circles. Notice how the neighborhoods stretch out orthogonally to\n",
      "the decision boundaries when both classes are present in the neighborhood.\n",
      "In the pure regions with only one class, the neighborhoods remain circular;\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "478 13. Prototypes and Nearest-Neighbors\n",
      "o\n",
      "oo ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o oo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oooooo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "oooo\n",
      "ooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooooo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "o o\n",
      "ooo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "ooo\n",
      "oo\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "oo\n",
      "o o\n",
      "oooo\n",
      "oo\n",
      "oo\n",
      "oo\n",
      "oooo\n",
      "o\n",
      "ooo\n",
      "ooo\n",
      "oo\n",
      "oo oo\n",
      "o\n",
      "oo\n",
      "oooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oooo\n",
      "o\n",
      "oo\n",
      "ooo o\n",
      "o ooo\n",
      "ooooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "ooo\n",
      "oo\n",
      "oooo ooo\n",
      "oo\n",
      "oo\n",
      "o\n",
      "o\n",
      "oo\n",
      "ooo\n",
      "FIGURE 13.14. Neighborhoods found by the DANN procedure, at various query\n",
      "points (centers of the crosses). There are two classes in the da ta, with one class\n",
      "surrounding the other. 50nearest-neighbors were used to estimate the local met-\n",
      "rics. Shown are the resulting metrics used to form 15-nearest-neighborhoods.\n",
      "in these cases the between matrix B= 0, and the Σin (13.8) is the identity\n",
      "matrix.\n",
      "13.4.1 Example\n",
      "Here we generate two-class data in ten dimensions, analogous to the two-\n",
      "dimensional example of Figure 13.14. All ten predictors in class 1 are in-\n",
      "dependent standard normal, conditioned on the radius being greater than\n",
      "22.4 and less than 40, while the predictors in class 2 are independent stan-\n",
      "dard normal without the restriction. There are 250 observations in each\n",
      "class. Hence the ﬁrst class almost completely surrounds the second class in\n",
      "the full ten-dimensional space.\n",
      "In this example there are no pure noise variables, the kind that a nearest-\n",
      "neighbor subset selection rule might be able to weed out. At any given\n",
      "point in the feature space, the class discrimination occurs along only one\n",
      "direction. However, this direction changes as we move across the feature\n",
      "space and all variables are important somewhere in the space.\n",
      "Figure 13.15 shows boxplots of the test error rates over ten realiza-\n",
      "tions, for standard 5-nearest-neighbors, LVQ, and discriminant adaptive\n",
      "5-nearest-neighbors. We used 50 prototypes per class for LVQ, to make\n",
      "it comparable to 5 nearest-neighbors (since 250 /5 = 50). The adaptive\n",
      "metric signiﬁcantly reduces the error rate, compared to LVQ or standard\n",
      "nearest-neighbors.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "13.4 Adaptive Nearest-Neighbor Methods 4790.0 0.1 0.2 0.3 0.4\n",
      "5NN LVQ DANNTest Error\n",
      "FIGURE 13.15. Ten-dimensional simulated example: boxplots of the test error\n",
      "rates over ten realizations, for standard 5-nearest-neighbors, LVQ with 50centers,\n",
      "and discriminant-adaptive 5-nearest-neighbors\n",
      "13.4.2 Global Dimension Reduction for Nearest-Neighbors\n",
      "The discriminant-adaptive nearest-neighbor method carries out local di-\n",
      "mension reduction—that is, dimension reduction separately at each query\n",
      "point. In many problems we can also beneﬁt from global dimension re-\n",
      "duction, that is, apply a nearest-neighbor rule in some optimally chosen\n",
      "subspace of the original feature space. For example, suppose that the two\n",
      "classes form two nested spheres in four dimensions of feature space, and\n",
      "there are an additional six noise features whose distribution is independent\n",
      "of class. Then we would like to discover the important four-dimensional\n",
      "subspace, and carry out nearest-neighbor classiﬁcation in that reduced sub-\n",
      "space. Hastie and Tibshirani (1996a) discuss a variation of the discriminan t-\n",
      "adaptive nearest-neighbor method for this purpose. At each training point\n",
      "xi, the between-centroids sum of squares matrix Biis computed, and then\n",
      "these matrices are averaged over all training points:\n",
      "¯B=1\n",
      "NN∑\n",
      "i=1Bi. (13.10)\n",
      "Lete1,e2,... ,e pbe the eigenvectors of the matrix ¯B, ordered from largest\n",
      "to smallest eigenvalue θk. Then these eigenvectors span the optimal sub-\n",
      "spaces for global subspace reduction. The derivation is based on the fact\n",
      "that the best rank- Lapproximation to ¯B,¯B[L]=∑L\n",
      "ℓ=1θℓeℓeT\n",
      "ℓ, solves the\n",
      "least squares problem\n",
      "min\n",
      "rank(M)=LN∑\n",
      "i=1trace[(Bi−M)2]. (13.11)\n",
      "Since each Bicontains information on (a) the local discriminant subspace,\n",
      "and (b) the strength of discrimination in that subspace, (13.11) can be seen\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "480 13. Prototypes and Nearest-Neighbors\n",
      "as a way of ﬁnding the best approximating subspace of dimension Lto a\n",
      "series of Nsubspaces by weighted least squares (Exercise 13.5.)\n",
      "In the four-dimensional sphere example mentioned above and examined\n",
      "in Hastie and Tibshirani (1996a), four of the eigenvalues θℓturn out to be\n",
      "large (having eigenvectors nearly spanning the interesting subspace), and\n",
      "the remaining six are near zero. Operationally, we project the data into\n",
      "the leading four-dimensional subspace, and then carry out nearest neighbor\n",
      "classiﬁcation. In the satellite image classiﬁcation example in Section 13. 3.2,\n",
      "the technique labeled DANNin Figure 13.8 used 5-nearest-neighbors in a\n",
      "globally reduced subspace. There are also connections of this technique\n",
      "with the sliced inverse regression proposal of Duan and Li (1991). These\n",
      "authors use similar ideas in the regression setting, but do global rather\n",
      "than local computations. They assume and exploit spherical symmetry of\n",
      "the feature distribution to estimate interesting subspaces.\n",
      "13.5 Computational Considerations\n",
      "One drawback of nearest-neighbor rules in general is the computational\n",
      "load, both in ﬁnding the neighbors and storing the entire training set. With\n",
      "Nobservations and ppredictors, nearest-neighbor classiﬁcation requires Np\n",
      "operations to ﬁnd the neighbors per query point. There are fast algorithms\n",
      "for ﬁnding nearest-neighbors (Friedman et al., 1975; Friedman et al., 1977)\n",
      "which can reduce this load somewhat. Hastie and Simard (1998) reduce\n",
      "the computations for tangent distance by developing analogs of K-means\n",
      "clustering in the context of this invariant metric.\n",
      "Reducing the storage requirements is more diﬃcult, and various editing\n",
      "andcondensing procedures have been proposed. The idea is to isolate a\n",
      "subset of the training set that suﬃces for nearest-neighbor predictions, and\n",
      "throw away the remaining training data. Intuitively, it seems important t o\n",
      "keep the training points that are near the decision boundaries and on the\n",
      "correct side of those boundaries, while some points far from the boundaries\n",
      "could be discarded.\n",
      "Themulti-edit algorithm of Devijver and Kittler (1982) divides the data\n",
      "cyclically into training and test sets, computing a nearest neighbor rule on\n",
      "the training set and deleting test points that are misclassiﬁed. The idea is\n",
      "to keep homogeneous clusters of training observations.\n",
      "Thecondensing procedure of Hart (1968) goes further, trying to keep\n",
      "only important exterior points of these clusters. Starting with a single ran-\n",
      "domly chosen observation as the training set, each additional data item is\n",
      "processed one at a time, adding it to the training set only if it is misclas-\n",
      "siﬁed by a nearest-neighbor rule computed on the current training set.\n",
      "These procedures are surveyed in Dasarathy (1991) and Ripley (1996).\n",
      "They can also be applied to other learning procedures besides nearest-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 481\n",
      "neighbors. While such methods are sometimes useful, we have not had\n",
      "much practical experience with them, nor have we found any systematic\n",
      "comparison of their performance in the literature.\n",
      "Bibliographic Notes\n",
      "The nearest-neighbor method goes back at least to Fix and Hodges (1951).\n",
      "The extensive literature on the topic is reviewed by Dasarathy (1991);\n",
      "Chapter 6 of Ripley (1996) contains a good summary. K-means cluster-\n",
      "ing is due to Lloyd (1957) and MacQueen (1967). Kohonen (1989) intro-\n",
      "duced learning vector quantization. The tangent distance method is due to\n",
      "Simard et al. (1993). Hastie and Tibshirani (1996a) proposed the discrim -\n",
      "inant adaptive nearest-neighbor technique.\n",
      "Exercises\n",
      "Ex. 13.1 Consider a Gaussian mixture model where the covariance matrices\n",
      "are assumed to be scalar: Σr=σI∀r= 1,... ,R , and σis a ﬁxed param-\n",
      "eter. Discuss the analogy between the K-means clustering algorithm and\n",
      "the EM algorithm for ﬁtting this mixture model in detail. Show that in the\n",
      "limitσ→0 the two methods coincide.\n",
      "Ex. 13.2 Derive formula (13.7) for the median radius of the 1-nearest-\n",
      "neighborhood.\n",
      "Ex. 13.3 LetE∗be the error rate of the Bayes rule in a K-class problem,\n",
      "where the true class probabilities are given by pk(x), k= 1,... ,K . As-\n",
      "suming the test point and training point have identical features x, prove\n",
      "(13.5)\n",
      "K∑\n",
      "k=1pk(x)(1−pk(x))≤2(1−pk∗(x))−K\n",
      "K−1(1−pk∗(x))2.\n",
      "where k∗= arg max kpk(x). Hence argue that the error rate of the 1-\n",
      "nearest-neighbor rule converges in L1, as the size of the training set in-\n",
      "creases, to a value E1, bounded above by\n",
      "E∗(\n",
      "2−E∗K\n",
      "K−1)\n",
      ". (13.12)\n",
      "[This statement of the theorem of Cover and Hart (1967) is taken from\n",
      "Chapter 6 of Ripley (1996), where a short proof is also given].\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "482 13. Prototypes and Nearest-Neighbors\n",
      "Ex. 13.4 Consider an image to be a function F(x) : IR2↦→IR1over the two-\n",
      "dimensional spatial domain (paper coordinates). Then F(c+x0+A(x−x0))\n",
      "represents an aﬃne transformation of the image F, where Ais a 2 ×2\n",
      "matrix.\n",
      "1. Decompose A(via Q-R) in such a way that parameters identifying\n",
      "the four aﬃne transformations (two scale, shear and rotation) are\n",
      "clearly identiﬁed.\n",
      "2. Using the chain rule, show that the derivative of F(c+x0+A(x−x0))\n",
      "w.r.t. each of these parameters can be represented in terms of the two\n",
      "spatial derivatives of F.\n",
      "3. Using a two-dimensional kernel smoother (Chapter 6), describe how\n",
      "to implement this procedure when the images are quantized to 16 ×16\n",
      "pixels.\n",
      "Ex. 13.5 LetBi,i= 1,2,... ,N be square p×ppositive semi-deﬁnite ma-\n",
      "trices and let ¯B= (1/N)∑Bi. Write the eigen-decomposition of ¯Bas∑p\n",
      "ℓ=1θℓeℓeT\n",
      "ℓwithθℓ≥θℓ−1≥ ≤≤≤ ≥ θ1. Show that the best rank- Lapprox-\n",
      "imation for the Bi,\n",
      "min\n",
      "rank(M)=LN∑\n",
      "i=1trace[(Bi−M)2],\n",
      "is given by ¯B[L]=∑L\n",
      "ℓ=1θℓeℓeT\n",
      "ℓ. (Hint: Write∑N\n",
      "i=1trace[(Bi−M)2] as\n",
      "N∑\n",
      "i=1trace[(Bi−¯B)2] +N∑\n",
      "i=1trace[(M−¯B)2]).\n",
      "Ex. 13.6 Here we consider the problem of shape averaging . In particular,\n",
      "Li, i= 1,... ,M are each N×2 matrices of points in IR2, each sampled\n",
      "from corresponding positions of handwritten (cursive) letters. We seek an\n",
      "aﬃne invariant average V, also N×2,VTV=I, of the Mletters Liwith\n",
      "the following property: Vminimizes\n",
      "M∑\n",
      "j=1min\n",
      "Aj∥Lj−VAj∥2.\n",
      "Characterize the solution.\n",
      "This solution can suﬀer if some of the letters are bigand dominate the\n",
      "average. An alternative approach is to minimize instead:\n",
      "M∑\n",
      "j=1min\n",
      "AjLjA∗\n",
      "j−V2.\n",
      "Derive the solution to this problem. How do the criteria diﬀer? Use the\n",
      "SVD of the Ljto simplify the comparison of the two approaches.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "Exercises 483\n",
      "Ex. 13.7 Consider the application of nearest-neighbors to the “easy” and\n",
      "“hard” problems in the left panel of Figure 13.5.\n",
      "1. Replicate the results in the left panel of Figure 13.5.\n",
      "2. Estimate the misclassiﬁcation errors using ﬁvefold cross-validation,\n",
      "and compare the error rate curves to those in 1.\n",
      "3. Consider an “AIC-like” penalization of the training set misclassiﬁca-\n",
      "tion error. Speciﬁcally, add 2 t/Nto the training set misclassiﬁcation\n",
      "error, where tis the approximate number of parameters N/r,rbe-\n",
      "ing the number of nearest-neighbors. Compare plots of the resulting\n",
      "penalized misclassiﬁcation error to those in 1 and 2. Which method\n",
      "gives a better estimate of the optimal number of nearest-neighbors:\n",
      "cross-validation or AIC?\n",
      "Ex. 13.8 Generate data in two classes, with two features. These features\n",
      "are all independent Gaussian variates with standard deviation 1. Their\n",
      "mean vectors are ( −1,−1) in class 1 and (1 ,1) in class 2. To each feature\n",
      "vector apply a random rotation of angle θ,θchosen uniformly from 0 to\n",
      "2π. Generate 50 observations from each class to form the training set, and\n",
      "500 in each class as the test set. Apply four diﬀerent classiﬁers:\n",
      "1. Nearest-neighbors.\n",
      "2. Nearest-neighbors with hints: ten randomly rotated versions of each\n",
      "data point are added to the training set before applying nearest-\n",
      "neighbors.\n",
      "3. Invariant metric nearest-neighbors, using Euclidean distance invari-\n",
      "ant to rotations about the origin.\n",
      "4. Tangent distance nearest-neighbors.\n",
      "In each case choose the number of neighbors by tenfold cross-validation.\n",
      "Compare the results.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "484 13. Prototypes and Nearest-Neighbors\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "This is page 485\n",
      "Printer: Opaque this\n",
      "14\n",
      "Unsupervised Learning\n",
      "14.1 Introduction\n",
      "The previous chapters have been concerned with predicting the values\n",
      "of one or more outputs or response variables Y= (Y1,... ,Y m) for a\n",
      "given set of input or predictor variables XT= (X1,... ,X p). Denote by\n",
      "xT\n",
      "i= (xi1,... ,x ip) the inputs for the ith training case, and let yibe a\n",
      "response measurement. The predictions are based on the training sample\n",
      "(x1,y1),... ,(xN,yN) of previously solved cases, where the joint values of\n",
      "all of the variables are known. This is called supervised learning or “learn-\n",
      "ing with a teacher.” Under this metaphor the “student” presents an an-\n",
      "swer ˆyifor each xiin the training sample, and the supervisor or “teacher”\n",
      "provides either the correct answer and/or an error associated with the stu-\n",
      "dent’s answer. This is usually characterized by some loss function L(y,ˆy),\n",
      "for example, L(y,ˆy) = (y−ˆy)2.\n",
      "If one supposes that ( X,Y) are random variables represented by some\n",
      "joint probability density Pr( X,Y), then supervised learning can be formally\n",
      "characterized as a density estimation problem where one is concerned with\n",
      "determining properties of the conditional density Pr( Y|X). Usually the\n",
      "properties of interest are the “location” parameters θthat minimize the\n",
      "expected error at each x,\n",
      "θ(x) = argmin\n",
      "θEY|XL(Y,θ). (14.1)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "486 14. Unsupervised Learning\n",
      "Conditioning one has\n",
      "Pr(X,Y) = Pr( Y|X)≤Pr(X),\n",
      "where Pr( X) is the joint marginal density of the Xvalues alone. In su-\n",
      "pervised learning Pr( X) is typically of no direct concern. One is interested\n",
      "mainly in the properties of the conditional density Pr( Y|X). Since Yis of-\n",
      "ten of low dimension (usually one), and only its location θ(x) is of interest,\n",
      "the problem is greatly simpliﬁed. As discussed in the previous chapters,\n",
      "there are many approaches for successfully addressing supervised learning\n",
      "in a variety of contexts.\n",
      "In this chapter we address unsupervised learning or “learning without a\n",
      "teacher.” In this case one has a set of Nobservations ( x1,x2,... ,x N) of a\n",
      "random p-vector Xhaving joint density Pr( X). The goal is to directly infer\n",
      "the properties of this probability density without the help of a supervisor or\n",
      "teacher providing correct answers or degree-of-error for each observation.\n",
      "The dimension of Xis sometimes much higher than in supervised learn-\n",
      "ing, and the properties of interest are often more complicated than simple\n",
      "location estimates. These factors are somewhat mitigated by the fact that\n",
      "Xrepresents all of the variables under consideration; one is not required\n",
      "to infer how the properties of Pr( X) change, conditioned on the changing\n",
      "values of another set of variables.\n",
      "In low-dimensional problems (say p≤3), there are a variety of eﬀective\n",
      "nonparametric methods for directly estimating the density Pr( X) itself at\n",
      "allX-values, and representing it graphically (Silverman, 1986, e.g.). Owing\n",
      "to the curse of dimensionality, these methods fail in high dimensions. One\n",
      "must settle for estimating rather crude global models, such as Gaussian\n",
      "mixtures or various simple descriptive statistics that characterize Pr( X).\n",
      "Generally, these descriptive statistics attempt to characterize X-values,\n",
      "or collections of such values, where Pr( X) is relatively large. Principal\n",
      "components, multidimensional scaling, self-organizing maps, and principal\n",
      "curves, for example, attempt to identify low-dimensional manifolds within\n",
      "theX-space that represent high data density. This provides information\n",
      "about the associations among the variables and whether or not they can be\n",
      "considered as functions of a smaller set of “latent” variables. Cluster anal-\n",
      "ysis attempts to ﬁnd multiple convex regions of the X-space that contain\n",
      "modes of Pr( X). This can tell whether or not Pr( X) can be represented by\n",
      "a mixture of simpler densities representing distinct types or classes of ob-\n",
      "servations. Mixture modeling has a similar goal. Association rules att empt\n",
      "to construct simple descriptions (conjunctive rules) that describe regions\n",
      "of high density in the special case of very high dimensional binary-valued\n",
      "data.\n",
      "With supervised learning there is a clear measure of success, or lack\n",
      "thereof, that can be used to judge adequacy in particular situations and\n",
      "to compare the eﬀectiveness of diﬀerent methods over various situations.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.2 Association Rules 487\n",
      "Lack of success is directly measured by expected loss over the joint dis-\n",
      "tribution Pr( X,Y). This can be estimated in a variety of ways including\n",
      "cross-validation. In the context of unsupervised learning, there is no such\n",
      "direct measure of success. It is diﬃcult to ascertain the validity of inferences\n",
      "drawn from the output of most unsupervised learning algorithms. One must\n",
      "resort to heuristic arguments not only for motivating the algorithms, as is\n",
      "often the case in supervised learning as well, but also for judgments as to\n",
      "the quality of the results. This uncomfortable situation has led to heavy\n",
      "proliferation of proposed methods, since eﬀectiveness is a matter of opinion\n",
      "and cannot be veriﬁed directly.\n",
      "In this chapter we present those unsupervised learning techniques that\n",
      "are among the most commonly used in practice, and additionally, a few\n",
      "others that are favored by the authors.\n",
      "14.2 Association Rules\n",
      "Association rule analysis has emerged as a popular tool for mining com-\n",
      "mercial data bases. The goal is to ﬁnd joint values of the variables X=\n",
      "(X1,X2,... ,X p) that appear most frequently in the data base. It is most\n",
      "often applied to binary-valued data Xj∈ {0,1}, where it is referred to as\n",
      "“market basket” analysis. In this context the observations are sales trans -\n",
      "actions, such as those occurring at the checkout counter of a store. The\n",
      "variables represent all of the items sold in the store. For observation i, each\n",
      "variable Xjis assigned one of two values; xij= 1 if the jth item is pur-\n",
      "chased as part of the transaction, whereas xij= 0 if it was not purchased.\n",
      "Those variables that frequently have joint values of one represent items that\n",
      "are frequently purchased together. This information can be quite useful for\n",
      "stocking shelves, cross-marketing in sales promotions, catalog design, and\n",
      "consumer segmentation based on buying patterns.\n",
      "More generally, the basic goal of association rule analysis is to ﬁnd a\n",
      "collection of prototype X-values v1,... ,v Lfor the feature vector X, such\n",
      "that the probability density Pr( vl) evaluated at each of those values is rela-\n",
      "tively large. In this general framework, the problem can be viewed as “mode\n",
      "ﬁnding” or “bump hunting.” As formulated, this problem is impossibly dif-\n",
      "ﬁcult. A natural estimator for each Pr( vl) is the fraction of observations\n",
      "for which X=vl. For problems that involve more than a small number\n",
      "of variables, each of which can assume more than a small number of val-\n",
      "ues, the number of observations for which X=vlwill nearly always be too\n",
      "small for reliable estimation. In order to have a tractable problem, both t he\n",
      "goals of the analysis and the generality of the data to which it is applied\n",
      "must be greatly simpliﬁed.\n",
      "The ﬁrst simpliﬁcation modiﬁes the goal. Instead of seeking values x\n",
      "where Pr( x) is large, one seeks regions of the X-space with high probability\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "488 14. Unsupervised Learning\n",
      "content relative to their size or support. Let Sjrepresent the set of all\n",
      "possible values of the jth variable (its support ), and let sj⊆ Sjbe a subset\n",
      "of these values. The modiﬁed goal can be stated as attempting to ﬁnd\n",
      "subsets of variable values s1,... ,s psuch that the probability of each of the\n",
      "variables simultaneously assuming a value within its respective subset,\n",
      "Pr\n",
      "p⋂\n",
      "j=1(Xj∈sj)\n",
      ", (14.2)\n",
      "is relatively large. The intersection of subsets ∩p\n",
      "j=1(Xj∈sj) is called a\n",
      "conjunctive rule . For quantitative variables the subsets sjare contiguous\n",
      "intervals; for categorical variables the subsets are delineated explicitly. No te\n",
      "that if the subset sjis in fact the entire set of values sj=Sj, as is often\n",
      "the case, the variable Xjis said notto appear in the rule (14.2).\n",
      "14.2.1 Market Basket Analysis\n",
      "General approaches to solving (14.2) are discussed in Section 14.2.5. These\n",
      "can be quite useful in many applications. However, they are not feasible\n",
      "for the very large ( p≈104,N≈108) commercial data bases to which\n",
      "market basket analysis is often applied. Several further simpliﬁcations of\n",
      "(14.2) are required. First, only two types of subsets are considered; either\n",
      "sjconsists of a single value of Xj,sj=v0j, or it consists of the entire set\n",
      "of values that Xjcan assume, sj=Sj. This simpliﬁes the problem (14.2)\n",
      "to ﬁnding subsets of the integers J ⊂ { 1,... ,p }, and corresponding values\n",
      "v0j, j∈ J, such that\n",
      "Pr\n",
      "⋂\n",
      "j∈J(Xj=v0j)\n",
      " (14.3)\n",
      "is large. Figure 14.1 illustrates this assumption.\n",
      "One can apply the technique of dummy variables to turn (14.3) into\n",
      "a problem involving only binary-valued variables. Here we assume that\n",
      "the support Sjis ﬁnite for each variable Xj. Speciﬁcally, a new set of\n",
      "variables Z1,... ,Z Kis created, one such variable for each of the values\n",
      "vljattainable by each of the original variables X1,... ,X p. The number of\n",
      "dummy variables Kis\n",
      "K=p∑\n",
      "j=1|Sj|,\n",
      "where |Sj|is the number of distinct values attainable by Xj. Each dummy\n",
      "variable is assigned the value Zk= 1 if the variable with which it is as-\n",
      "sociated takes on the corresponding value to which Zkis assigned, and\n",
      "Zk= 0 otherwise. This transforms (14.3) to ﬁnding a subset of the integers\n",
      "K ⊂ { 1,... ,K }such that\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.2 Association Rules 489\n",
      "X1 X1 X1\n",
      "X2X2X2\n",
      "FIGURE 14.1. Simpliﬁcations for association rules. Here there are two inputs\n",
      "X1andX2, taking four and six distinct values, respectively. The red squ ares\n",
      "indicate areas of high density. To simplify the computations, w e assume that the\n",
      "derived subset corresponds to either a single value of an input o r all values. With\n",
      "this assumption we could ﬁnd either the middle or right pattern, but not the left\n",
      "one.\n",
      "Pr[⋂\n",
      "k∈K(Zk= 1)]\n",
      "= Pr[∏\n",
      "k∈KZk= 1]\n",
      "(14.4)\n",
      "is large. This is the standard formulation of the market basket problem.\n",
      "The set Kis called an “item set.” The number of variables Zkin the item\n",
      "set is called its “size” (note that the size is no bigger than p). The estimated\n",
      "value of (14.4) is taken to be the fraction of observations in the data bas e\n",
      "for which the conjunction in (14.4) is true:\n",
      "ˆPr[∏\n",
      "k∈K(Zk= 1)]\n",
      "=1\n",
      "NN∑\n",
      "i=1∏\n",
      "k∈Kzik. (14.5)\n",
      "Herezikis the value of Zkfor this ith case. This is called the “support” or\n",
      "“prevalence” T(K) of the item set K. An observation ifor which∏\n",
      "k∈Kzik=\n",
      "1 is said to “contain” the item set K.\n",
      "In association rule mining a lower support bound tis speciﬁed, and one\n",
      "seeksallitem sets Klthat can be formed from the variables Z1,... ,Z K\n",
      "with support in the data base greater than this lower bound t\n",
      "{Kl|T(Kl)> t}. (14.6)\n",
      "14.2.2 The Apriori Algorithm\n",
      "The solution to this problem (14.6) can be obtained with feasible compu-\n",
      "tation for very large data bases provided the threshold tis adjusted so that\n",
      "(14.6) consists of only a small fraction of all 2Kpossible item sets. The\n",
      "“Apriori” algorithm (Agrawal et al., 1995) exploits several aspects o f the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "490 14. Unsupervised Learning\n",
      "curse of dimensionality to solve (14.6) with a small number of passes over\n",
      "the data. Speciﬁcally, for a given support threshold t:\n",
      "•The cardinality |{K|T(K)> t}|is relatively small.\n",
      "•Any item set Lconsisting of a subset of the items in Kmust have\n",
      "support greater than or equal to that of K,L ⊆ K ⇒ T(L)≥T(K).\n",
      "The ﬁrst pass over the data computes the support of all single-item sets.\n",
      "Those whose support is less than the threshold are discarded. The second\n",
      "pass computes the support of all item sets of size two that can be formed\n",
      "from pairs of the single items surviving the ﬁrst pass. In other words, to\n",
      "generate all frequent itemsets with |K|=m, we need to consider only\n",
      "candidates such that allof their mancestral item sets of size m−1 are\n",
      "frequent. Those size-two item sets with support less than the threshold are\n",
      "discarded. Each successive pass over the data considers only those item\n",
      "sets that can be formed by combining those that survived the previous\n",
      "pass with those retained from the ﬁrst pass. Passes over the data continue\n",
      "until all candidate rules from the previous pass have support less than the\n",
      "speciﬁed threshold. The Apriori algorithm requires only one pass over the\n",
      "data for each value of |K|, which is crucial since we assume the data cannot\n",
      "be ﬁtted into a computer’s main memory. If the data are suﬃciently sparse\n",
      "(or if the threshold tis high enough), then the process will terminate in\n",
      "reasonable time even for huge data sets.\n",
      "There are many additional tricks that can be used as part of this strat-\n",
      "egy to increase speed and convergence (Agrawal et al., 1995). The Apriori\n",
      "algorithm represents one of the major advances in data mining technology.\n",
      "Each high support item set K(14.6) returned by the Apriori algorithm is\n",
      "cast into a set of “association rules.” The items Zk,k∈ K, are partitioned\n",
      "into two disjoint subsets, A∪B=K, and written\n",
      "A⇒B. (14.7)\n",
      "The ﬁrst item subset Ais called the “antecedent” and the second Bthe\n",
      "“consequent.” Association rules are deﬁned to have several properties based\n",
      "on the prevalence of the antecedent and consequent item sets in the data\n",
      "base. The “support” of the rule T(A⇒B) is the fraction of observations\n",
      "in the union of the antecedent and consequent, which is just the support\n",
      "of the item set Kfrom which they were derived. It can be viewed as an\n",
      "estimate (14.5) of the probability of simultaneously observing both item\n",
      "sets Pr( AandB) in a randomly selected market basket. The “conﬁdence”\n",
      "or “predictability” C(A⇒B) of the rule is its support divided by the\n",
      "support of the antecedent\n",
      "C(A⇒B) =T(A⇒B)\n",
      "T(A), (14.8)\n",
      "which can be viewed as an estimate of Pr( B|A). The notation Pr( A), the\n",
      "probability of an item set Aoccurring in a basket, is an abbreviation for\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.2 Association Rules 491\n",
      "Pr(∏\n",
      "k∈AZk= 1). The “expected conﬁdence” is deﬁned as the support of\n",
      "the consequent T(B), which is an estimate of the unconditional probability\n",
      "Pr(B). Finally, the “lift” of the rule is deﬁned as the conﬁdence divided by\n",
      "the expected conﬁdence\n",
      "L(A⇒B) =C(A⇒B)\n",
      "T(B).\n",
      "This is an estimate of the association measure Pr( AandB)/Pr(A)Pr(B).\n",
      "As an example, suppose the item set K={peanut butter, jelly, bread }\n",
      "and consider the rule {peanut butter, jelly } ⇒ {bread}. A support value\n",
      "of 0.03 for this rule means that peanut butter ,jelly, andbread appeared\n",
      "together in 3% of the market baskets. A conﬁdence of 0.82 for this rule im-\n",
      "plies that when peanut butter andjelly were purchased, 82% of the time\n",
      "bread was also purchased. If bread appeared in 43% of all market baskets\n",
      "then the rule {peanut butter, jelly } ⇒ {bread}would have a lift of 1 .95.\n",
      "The goal of this analysis is to produce association rules (14.7) with bot h\n",
      "high values of support and conﬁdence (14.8). The Apriori algorithm returns\n",
      "all item sets with high support as deﬁned by the support threshold t(14.6).\n",
      "A conﬁdence threshold cis set, and all rules that can be formed from those\n",
      "item sets (14.6) with conﬁdence greater than this value\n",
      "{A⇒B|C(A⇒B)> c} (14.9)\n",
      "are reported. For each item set Kof size |K|there are 2|K|−1−1 rules of\n",
      "the form A⇒(K −A),A⊂ K. Agrawal et al. (1995) present a variant of\n",
      "the Apriori algorithm that can rapidly determine which rules survive the\n",
      "conﬁdence threshold (14.9) from all possible rules that can be formed from\n",
      "the solution item sets (14.6).\n",
      "The output of the entire analysis is a collection of association rules (14.7 )\n",
      "that satisfy the constraints\n",
      "T(A⇒B)> t and C(A⇒B)> c.\n",
      "These are generally stored in a data base that can be queried by the user.\n",
      "Typical requests might be to display the rules in sorted order of conﬁdence,\n",
      "lift or support. More speciﬁcally, one might request such a list conditioned\n",
      "on particular items in the antecedent or especially the consequent. For\n",
      "example, a request might be the following:\n",
      "Display all transactions in which ice skates are the consequ ent\n",
      "that have conﬁdence over 80%and support of more than 2%.\n",
      "This could provide information on those items (antecedent) that predicate\n",
      "sales of ice skates. Focusing on a particular consequent casts the problem\n",
      "into the framework of supervised learning.\n",
      "Association rules have become a popular tool for analyzing very large\n",
      "commercial data bases in settings where market basket is relevant. That is\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "492 14. Unsupervised Learning\n",
      "when the data can be cast in the form of a multidimensional contingency\n",
      "table. The output is in the form of conjunctive rules (14.4) that are easily\n",
      "understood and interpreted. The Apriori algorithm allows this analysis to\n",
      "be applied to huge data bases, much larger that are amenable to other types\n",
      "of analyses. Association rules are among data mining’s biggest successes.\n",
      "Besides the restrictive form of the data to which they can be applied, as-\n",
      "sociation rules have other limitations. Critical to computational feasibi lity\n",
      "is the support threshold (14.6). The number of solution item sets, their size,\n",
      "and the number of passes required over the data can grow exponentially\n",
      "with decreasing size of this lower bound. Thus, rules with high conﬁdence\n",
      "or lift, but low support, will not be discovered. For example, a high conﬁ-\n",
      "dence rule such as vodka ⇒caviar will not be uncovered owing to the low\n",
      "sales volume of the consequent caviar .\n",
      "14.2.3 Example: Market Basket Analysis\n",
      "We illustrate the use of Apriori on a moderately sized demographics data\n",
      "base. This data set consists of N= 9409 questionnaires ﬁlled out by shop-\n",
      "ping mall customers in the San Francisco Bay Area (Impact Resources, Inc.,\n",
      "Columbus OH, 1987). Here we use answers to the ﬁrst 14 questions, relat-\n",
      "ing to demographics, for illustration. These questions are listed in Table\n",
      "14.1. The data are seen to consist of a mixture of ordinal and (unordered)\n",
      "categorical variables, many of the latter having more than a few values.\n",
      "There are many missing values.\n",
      "We used a freeware implementation of the Apriori algorithm due to Chris-\n",
      "tian Borgelt1. After removing observations with missing values, each ordinal\n",
      "predictor was cut at its median and coded by two dummy variables; each\n",
      "categorical predictor with kcategories was coded by kdummy variables.\n",
      "This resulted in a 6876 ×50 matrix of 6876 observations on 50 dummy\n",
      "variables.\n",
      "The algorithm found a total of 6288 association rules, involving ≤5\n",
      "predictors, with support of at least 10%. Understanding this large set of\n",
      "rules is itself a challenging data analysis task. We will not attempt this here,\n",
      "but only illustrate in Figure 14.2 the relative frequency of each dummy\n",
      "variable in the data (top) and the association rules (bottom). Prevalent\n",
      "categories tend to appear more often in the rules, for example, the ﬁrst\n",
      "category in language (English). However, others such as occupation are\n",
      "under-represented, with the exception of the ﬁrst and ﬁfth level.\n",
      "Here are three examples of association rules found by the Apriori algo-\n",
      "rithm:\n",
      "Association rule 1: Support 25%, conﬁdence 99.7% and lift 1.03.\n",
      "1Seehttp://fuzzy.cs.uni-magdeburg.de/ ∼borgelt.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.2 Association Rules 493\n",
      "0 10 20 30 40 500.0 0.02 0.04 0.06\n",
      "AttributeRelative Frequency in Data\n",
      "incomesexmarstatageeduc occup yrs−baydualincperhousperyounghousetypehome ethnic language\n",
      "0 10 20 30 40 500.0 0.04 0.08 0.12\n",
      "AttributeRelative Frequency in Association Rules\n",
      "incomesexmarstatageeduc occup yrs−baydualincperhousperyounghousetypehome ethnic languageFIGURE 14.2. Market basket analysis: relative frequency of each dummy vari -\n",
      "able (coding an input category) in the data (top), and the associ ation rules found\n",
      "by the Apriori algorithm (bottom).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "494 14. Unsupervised Learning\n",
      "TABLE 14.1. Inputs for the demographic data.\n",
      "Feature Demographic # Values Type\n",
      "1 Sex 2 Categorical\n",
      "2 Marital status 5 Categorical\n",
      "3 Age 7 Ordinal\n",
      "4 Education 6 Ordinal\n",
      "5 Occupation 9 Categorical\n",
      "6 Income 9 Ordinal\n",
      "7 Years in Bay Area 5 Ordinal\n",
      "8 Dual incomes 3 Categorical\n",
      "9 Number in household 9 Ordinal\n",
      "10 Number of children 9 Ordinal\n",
      "11 Householder status 3 Categorical\n",
      "12 Type of home 5 Categorical\n",
      "13 Ethnic classiﬁcation 8 Categorical\n",
      "14 Language in home 3 Categorical\n",
      "[number in household = 1\n",
      "number of children = 0]\n",
      "⇓\n",
      "language in home = English\n",
      "Association rule 2: Support 13.4%, conﬁdence 80.8%, and lift 2.13.\n",
      "\n",
      "language in home = English\n",
      "householder status = own\n",
      "occupation = {professional/managerial }\n",
      "\n",
      "⇓\n",
      "income ≥$40,000\n",
      "Association rule 3: Support 26.5%, conﬁdence 82.8% and lift 2.15.\n",
      "\n",
      "language in home = English\n",
      "income <$40,000\n",
      "marital status = not married\n",
      "number of children = 0\n",
      "\n",
      "⇓\n",
      "education /∈ {college graduate, graduate study }\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.2 Association Rules 495\n",
      "We chose the ﬁrst and third rules based on their high support. The second\n",
      "rule is an association rule with a high-income consequent, and could be\n",
      "used to try to target high-income individuals.\n",
      "As stated above, we created dummy variables for each category of the\n",
      "input predictors, for example, Z1=I(income <$40,000) and Z2=\n",
      "I(income ≥$40,000) for below and above the median income. If we were\n",
      "interested only in ﬁnding associations with the high-income category, we\n",
      "would include Z2but not Z1. This is often the case in actual market basket\n",
      "problems, where we are interested in ﬁnding associations with the presence\n",
      "of a relatively rare item, but not associations with its absence.\n",
      "14.2.4 Unsupervised as Supervised Learning\n",
      "Here we discuss a technique for transforming the density estimation prob-\n",
      "lem into one of supervised function approximation. This forms the basis\n",
      "for the generalized association rules described in the next section.\n",
      "Letg(x) be the unknown data probability density to be estimated, and\n",
      "g0(x) be a speciﬁed probability density function used for reference. For ex-\n",
      "ample, g0(x) might be the uniform density over the range of the variables.\n",
      "Other possibilities are discussed below. The data set x1,x2,... ,x Nis pre-\n",
      "sumed to be an i.i.d.random sample drawn from g(x). A sample of size N0\n",
      "can be drawn from g0(x) using Monte Carlo methods. Pooling these two\n",
      "data sets, and assigning mass w=N0/(N+N0) to those drawn from g(x),\n",
      "andw0=N/(N+N0) to those drawn from g0(x), results in a random\n",
      "sample drawn from the mixture density ( g(x) +g0(x))/2. If one assigns\n",
      "the value Y= 1 to each sample point drawn from g(x) and Y= 0 those\n",
      "drawn from g0(x), then\n",
      "θ(x) =E(Y|x) =g(x)\n",
      "g(x) +g0(x)\n",
      "=g(x)/g0(x)\n",
      "1 +g(x)/g0(x)(14.10)\n",
      "can be estimated by supervised learning using the combined sample\n",
      "(y1,x1),(y2,x2),... ,(yN+N0,xN+N0) (14.11)\n",
      "as training data. The resulting estimate ˆ θ(x) can be inverted to provide an\n",
      "estimate for g(x)\n",
      "ˆg(x) =g0(x)ˆθ(x)\n",
      "1−ˆθ(x). (14.12)\n",
      "Generalized versions of logistic regression (Section 4.4) are especially wel l\n",
      "suited for this application since the log-odds,\n",
      "f(x) = logg(x)\n",
      "g0(x), (14.13)\n",
      "are estimated directly. In this case one has\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "496 14. Unsupervised Learning\n",
      "-1 0 1 2-2 0 2 4 6••\n",
      "••••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••••••\n",
      "••••\n",
      "• •\n",
      "••\n",
      "••••\n",
      "••••\n",
      "•\n",
      "•••\n",
      "•••••\n",
      "•\n",
      "• • •\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•\n",
      "••••\n",
      "••••\n",
      "•••••\n",
      "•••\n",
      "••••••••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "• •\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••••\n",
      "•\n",
      "-1 0 1 2-2 0 2 4 6••\n",
      "••••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••••••\n",
      "••••\n",
      "• •\n",
      "••\n",
      "••••\n",
      "••••\n",
      "•\n",
      "•••\n",
      "•••••\n",
      "•\n",
      "• • •\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•\n",
      "••••\n",
      "••••\n",
      "•••••\n",
      "•••\n",
      "••••••••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "• •\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••••\n",
      "••\n",
      "• ••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "••••\n",
      "••\n",
      "••••\n",
      "•\n",
      "••\n",
      "••••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••••• •\n",
      "••••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••••\n",
      "•\n",
      "•••••\n",
      "•\n",
      "•• ••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "•• •••\n",
      "•••••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "• •\n",
      "••\n",
      "••••\n",
      "••\n",
      "• •\n",
      "X1 X1\n",
      "X2X2\n",
      "FIGURE 14.3. Density estimation via classiﬁcation. (Left panel:) Training set\n",
      "of200data points. (Right panel:) Training set plus 200reference data points,\n",
      "generated uniformly over the rectangle containing the training data . The training\n",
      "sample was labeled as class 1, and the reference sample class 0, and a semipara-\n",
      "metric logistic regression model was ﬁt to the data. Some contou rs for ˆg(x)are\n",
      "shown.\n",
      "ˆg(x) =g0(x)eˆf(x). (14.14)\n",
      "An example is shown in Figure 14.3. We generated a training set of size\n",
      "200 shown in the left panel. The right panel shows the reference data (blue)\n",
      "generated uniformly over the rectangle containing the training data. The\n",
      "training sample was labeled as class 1, and the reference sample class 0,\n",
      "and a logistic regression model, using a tensor product of natural splines\n",
      "(Section 5.2.1), was ﬁt to the data. Some probability contours of ˆ θ(x) are\n",
      "shown in the right panel; these are also the contours of the density estimate\n",
      "ˆg(x), since ˆ g(x) = ˆθ(x)/(1−ˆθ(x)), is a monotone function. The contours\n",
      "roughly capture the data density.\n",
      "In principle any reference density can be used for g0(x) in (14.14). In\n",
      "practice the accuracy of the estimate ˆ g(x) can depend greatly on partic-\n",
      "ular choices. Good choices will depend on the data density g(x) and the\n",
      "procedure used to estimate (14.10) or (14.13). If accuracy is the goal, g0(x)\n",
      "should be chosen so that the resulting functions θ(x) orf(x) are approx-\n",
      "imated easily by the method being used. However, accuracy is not always\n",
      "the primary goal. Both θ(x) and f(x) are monotonic functions of the den-\n",
      "sity ratio g(x)/g0(x). They can thus be viewed as “contrast” statistics that\n",
      "provide information concerning departures of the data density g(x) from\n",
      "the chosen reference density g0(x). Therefore, in data analytic settings, a\n",
      "choice for g0(x) is dictated by types of departures that are deemed most\n",
      "interesting in the context of the speciﬁc problem at hand. For example, if\n",
      "departures from uniformity are of interest, g0(x) might be the a uniform\n",
      "density over the range of the variables. If departures from joint normality\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.2 Association Rules 497\n",
      "are of interest, a good choice for g0(x) would be a Gaussian distribution\n",
      "with the same mean vector and covariance matrix as the data. Departures\n",
      "from independence could be investigated by using\n",
      "g0(x) =p∏\n",
      "j=1gj(xj), (14.15)\n",
      "where gj(xj) is the marginal data density of Xj, thejth coordinate of X.\n",
      "A sample from this independent density (14.15) is easily generated from the\n",
      "data itself by applying a diﬀerent random permutation to the data values\n",
      "of each of the variables.\n",
      "As discussed above, unsupervised learning is concerned with revealing\n",
      "properties of the data density g(x). Each technique focuses on a particu-\n",
      "lar property or set of properties. Although this approach of transforming\n",
      "the problem to one of supervised learning (14.10)–(14.14) seems to have\n",
      "been part of the statistics folklore for some time, it does not appear to\n",
      "have had much impact despite its potential to bring well-developed su-\n",
      "pervised learning methodology to bear on unsupervised learning problems.\n",
      "One reason may be that the problem must be enlarged with a simulated\n",
      "data set generated by Monte Carlo techniques. Since the size of this data\n",
      "set should be at least as large as the data sample N0≥N, the compu-\n",
      "tation and memory requirements of the estimation procedure are at least\n",
      "doubled. Also, substantial computation may be required to generate the\n",
      "Monte Carlo sample itself. Although perhaps a deterrent in the past, these\n",
      "increased computational requirements are becoming much less of a burden\n",
      "as increased resources become routinely available. We illustrate the use of\n",
      "supervising learning methods for unsupervised learning in the next section.\n",
      "14.2.5 Generalized Association Rules\n",
      "The more general problem (14.2) of ﬁnding high-density regions in the data\n",
      "space can be addressed using the supervised learning approach described\n",
      "above. Although not applicable to the huge data bases for which market\n",
      "basket analysis is feasible, useful information can be obtained from mod-\n",
      "erately sized data sets. The problem (14.2) can be formulated as ﬁnding\n",
      "subsets of the integers J ⊂ { 1,2,... ,p }and corresponding value subsets\n",
      "sj, j∈ Jfor the corresponding variables Xj, such that\n",
      "ˆPr\n",
      "⋂\n",
      "j∈J(Xj∈sj)\n",
      "=1\n",
      "NN∑\n",
      "i=1I\n",
      "⋂\n",
      "j∈J(xij∈sj)\n",
      " (14.16)\n",
      "is large. Following the nomenclature of association rule analysis, {(Xj∈\n",
      "sj)}j∈Jwill be called a “generalized” item set. The subsets sjcorrespond-\n",
      "ing to quantitative variables are taken to be contiguous intervals wit hin\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "498 14. Unsupervised Learning\n",
      "their range of values, and subsets for categorical variables can involve more\n",
      "than a single value. The ambitious nature of this formulation precludes a\n",
      "thorough search for all generalized item sets with support (14.16) greater\n",
      "than a speciﬁed minimum threshold, as was possible in the more restric-\n",
      "tive setting of market basket analysis. Heuristic search methods must be\n",
      "employed, and the most one can hope for is to ﬁnd a useful collection of\n",
      "such generalized item sets.\n",
      "Both market basket analysis (14.5) and the generalized formulation (14.1 6)\n",
      "implicitly reference the uniform probability distribution. One seeks item\n",
      "sets that are more frequent than would be expected if all joint data values\n",
      "(x1,x2,... ,x N) were uniformly distributed. This favors the discovery of\n",
      "item sets whose marginal constituents ( Xj∈sj) areindividually frequent,\n",
      "that is, the quantity\n",
      "1\n",
      "NN∑\n",
      "i=1I(xij∈sj) (14.17)\n",
      "is large. Conjunctions of frequent subsets (14.17) will tend to appear more\n",
      "often among item sets of high support (14.16) than conjunctions of margin-\n",
      "ally less frequent subsets. This is why the rule vodka ⇒caviar is not likely\n",
      "to be discovered in spite of a high association (lift); neither item has high\n",
      "marginal support, so that their joint support is especially small. Reference\n",
      "to the uniform distribution can cause highly frequent item sets with low\n",
      "associations among their constituents to dominate the collection of highest\n",
      "support item sets.\n",
      "Highly frequent subsets sjare formed as disjunctions of the most fre-\n",
      "quent Xj-values. Using the product of the variable marginal data densities\n",
      "(14.15) as a reference distribution removes the preference for highly fre-\n",
      "quent values of the individual variables in the discovered item sets. This is\n",
      "because the density ratio g(x)/g0(x) is uniform if there are no associations\n",
      "among the variables (complete independence), regardless of the frequency\n",
      "distribution of the individual variable values. Rules like vodka ⇒caviar\n",
      "would have a chance to emerge. It is not clear however, how to incorporate\n",
      "reference distributions other than the uniform into the Apriori algorithm.\n",
      "As explained in Section 14.2.4, it is straightforward to generate a sampl e\n",
      "from the product density (14.15), given the original data set.\n",
      "After choosing a reference distribution, and drawing a sample from it\n",
      "as in (14.11), one has a supervised learning problem with a binary-valued\n",
      "output variable Y∈ {0,1}. The goal is to use this training data to ﬁnd\n",
      "regions\n",
      "R=⋂\n",
      "j∈J(Xj∈sj) (14.18)\n",
      "for which the target function θ(x) =E(Y|x) is relatively large. In addition,\n",
      "one might wish to require that the datasupport of these regions\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.2 Association Rules 499\n",
      "T(R) =∫\n",
      "x∈Rg(x)dx (14.19)\n",
      "not be too small.\n",
      "14.2.6 Choice of Supervised Learning Method\n",
      "The regions (14.18) are deﬁned by conjunctive rules. Hence supervised\n",
      "methods that learn such rules would be most appropriate in this context.\n",
      "The terminal nodes of a CART decision tree are deﬁned by rules precisely\n",
      "of the form (14.18). Applying CART to the pooled data (14.11) will pro-\n",
      "duce a decision tree that attempts to model the target (14.10) over the\n",
      "entire data space by a disjoint set of regions (terminal nodes). Each region\n",
      "is deﬁned by a rule of the form (14.18). Those terminal nodes twith high\n",
      "average y-values\n",
      "¯yt= ave( yi|xi∈t)\n",
      "are candidates for high-support generalized item sets (14.16). The actual\n",
      "(data) support is given by\n",
      "T(R) = ¯yt≤Nt\n",
      "N+N0,\n",
      "where Ntis the number of (pooled) observations within the region repre-\n",
      "sented by the terminal node. By examining the resulting decision tree, one\n",
      "might discover interesting generalized item sets of relatively high-support.\n",
      "These can then be partitioned into antecedents and consequents in a search\n",
      "for generalized association rules of high conﬁdence and/or lift.\n",
      "Another natural learning method for this purpose is the patient rule\n",
      "induction method PRIM described in Section 9.3. PRIM also produces\n",
      "rules precisely of the form (14.18), but it is especially designed for ﬁnding\n",
      "high-support regions that maximize the average target (14.10) value within\n",
      "them, rather than trying to model the target function over the entire data\n",
      "space. It also provides more control over the support/average-target-value\n",
      "tradeoﬀ.\n",
      "Exercise 14.3 addresses an issue that arises with either of these methods\n",
      "when we generate random data from the product of the marginal distribu-\n",
      "tions.\n",
      "14.2.7 Example: Market Basket Analysis (Continued)\n",
      "We illustrate the use of PRIM on the demographics data of Table 14.1.\n",
      "Three of the high-support generalized item sets emerging from the PRIM\n",
      "analysis were the following:\n",
      "Item set 1: Support= 24%.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "500 14. Unsupervised Learning\n",
      "\n",
      "marital status = married\n",
      "householder status = own\n",
      "type of home ̸=apartment\n",
      "\n",
      "Item set 2: Support= 24%.\n",
      "\n",
      "age≤24\n",
      "marital status ∈ {living together-not married, single }\n",
      "occupation /∈ {professional, homemaker, retired }\n",
      "householder status ∈ {rent, live with family }\n",
      "\n",
      "Item set 3: Support= 15%.\n",
      "\n",
      "householder status = rent\n",
      "type of home ̸=house\n",
      "number in household ≤2\n",
      "number of children = 0\n",
      "occupation /∈ {homemaker, student, unemployed }\n",
      "income ∈[$20,000 ,$150,000]\n",
      "\n",
      "Generalized association rules derived from these item sets with conﬁdence\n",
      "(14.8) greater than 95% are the following:\n",
      "Association rule 1: Support 25%, conﬁdence 99.7% and lift 1.35.\n",
      "[\n",
      "marital status = married\n",
      "householder status = own]\n",
      "⇓\n",
      "type of home ̸=apartment\n",
      "Association rule 2: Support 25%, conﬁdence 98.7% and lift 1.97.\n",
      "\n",
      "age≤24\n",
      "occupation /∈ {professional, homemaker, retired }\n",
      "householder status ∈ {rent, live with family }\n",
      "\n",
      "⇓\n",
      "marital status ∈ {single, living together-not married }\n",
      "Association rule 3: Support 25%, conﬁdence 95.9% and lift 2.61.\n",
      "[householder status = own\n",
      "type of home ̸=apartment]\n",
      "⇓\n",
      "marital status = married\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 501\n",
      "Association rule 4: Support 15%, conﬁdence 95.4% and lift 1.50.\n",
      "\n",
      "householder status = rent\n",
      "type of home ̸=house\n",
      "number in household ≤2\n",
      "occupation /∈ {homemaker, student, unemployed }\n",
      "income ∈[$20,000 ,$150,000]\n",
      "\n",
      "⇓\n",
      "number of children = 0\n",
      "There are no great surprises among these particular rules. For the most\n",
      "part they verify intuition. In other contexts where there is less prior in-\n",
      "formation available, unexpected results have a greater chance to emerge.\n",
      "These results do illustrate the type of information generalized associatio n\n",
      "rules can provide, and that the supervised learning approach, coupled with\n",
      "a ruled induction method such as CART or PRIM, can uncover item sets\n",
      "exhibiting high associations among their constituents.\n",
      "How do these generalized association rules compare to those found earlier\n",
      "by the Apriori algorithm? Since the Apriori procedure gives thousands of\n",
      "rules, it is diﬃcult to compare them. However some general points can be\n",
      "made. The Apriori algorithm is exhaustive—it ﬁnds allrules with support\n",
      "greater than a speciﬁed amount. In contrast, PRIM is a greedy algorithm\n",
      "and is not guaranteed to give an “optimal” set of rules. On the other hand,\n",
      "the Apriori algorithm can deal only with dummy variables and hence could\n",
      "not ﬁnd some of the above rules. For example, since type of home is a\n",
      "categorical input, with a dummy variable for each level, Apriori could not\n",
      "ﬁnd a rule involving the set\n",
      "type of home ̸=apartment .\n",
      "To ﬁnd this set, we would have to code a dummy variable for apartment\n",
      "versus the other categories of type of home. It will not generally be feasible\n",
      "to precode all such potentially interesting comparisons.\n",
      "14.3 Cluster Analysis\n",
      "Cluster analysis, also called data segmentation, has a variety of goals. All\n",
      "relate to grouping or segmenting a collection of objects into subsets or\n",
      "“clusters,” such that those within each cluster are more closely related to\n",
      "one another than objects assigned to diﬀerent clusters. An object can be\n",
      "described by a set of measurements, or by its relation to other objects.\n",
      "In addition, the goal is sometimes to arrange the clusters into a natural\n",
      "hierarchy. This involves successively grouping the clusters themselves so\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "502 14. Unsupervised Learning\n",
      "• •••\n",
      "••••\n",
      "• •••\n",
      "••\n",
      "•••••• • ••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••• ••\n",
      "•\n",
      "•••\n",
      "•••••\n",
      "••••••\n",
      "•\n",
      "••••\n",
      "•••••\n",
      "•••••\n",
      "•••••\n",
      "••••\n",
      "•• •\n",
      "•••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "••• ••\n",
      "X1X2\n",
      "FIGURE 14.4. Simulated data in the plane, clustered into three classes (repr e-\n",
      "sented by orange, blue and green) by the K-means clustering algorithm\n",
      "that at each level of the hierarchy, clusters within the same group are more\n",
      "similar to each other than those in diﬀerent groups.\n",
      "Cluster analysis is also used to form descriptive statistics to ascertain\n",
      "whether or not the data consists of a set distinct subgroups, each group\n",
      "representing objects with substantially diﬀerent properties. This latter goa l\n",
      "requires an assessment of the degree of diﬀerence between the objects as-\n",
      "signed to the respective clusters.\n",
      "Central to all of the goals of cluster analysis is the notion of the degree of\n",
      "similarity (or dissimilarity) between the individual objects being clustered.\n",
      "A clustering method attempts to group the objects based on the deﬁnition\n",
      "of similarity supplied to it. This can only come from subject matter consid-\n",
      "erations. The situation is somewhat similar to the speciﬁcation of a loss or\n",
      "cost function in prediction problems (supervised learning). There the cost\n",
      "associated with an inaccurate prediction depends on considerations outside\n",
      "the data.\n",
      "Figure 14.4 shows some simulated data clustered into three groups via\n",
      "the popular K-means algorithm. In this case two of the clusters are not\n",
      "well separated, so that “segmentation” more accurately describes the part\n",
      "of this process than “clustering.” K-means clustering starts with guesses\n",
      "for the three cluster centers. Then it alternates the following steps until\n",
      "convergence:\n",
      "•for each data point, the closest cluster center (in Euclidean distance)\n",
      "is identiﬁed;\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 503\n",
      "•each cluster center is replaced by the coordinate-wise average of all\n",
      "data points that are closest to it.\n",
      "We describe K-means clustering in more detail later, including the prob-\n",
      "lem of how to choose the number of clusters (three in this example). K-\n",
      "means clustering is a top-down procedure, while other cluster approaches\n",
      "that we discuss are bottom-up . Fundamental to all clustering techniques is\n",
      "the choice of distance or dissimilarity measure between two objects. We\n",
      "ﬁrst discuss distance measures before describing a variety of algorithms for\n",
      "clustering.\n",
      "14.3.1 Proximity Matrices\n",
      "Sometimes the data is represented directly in terms of the proximity (alike-\n",
      "ness or aﬃnity) between pairs of objects. These can be either similarities or\n",
      "dissimilarities (diﬀerence or lack of aﬃnity). For example, in social science\n",
      "experiments, participants are asked to judge by how much certain objects\n",
      "diﬀer from one another. Dissimilarities can then be computed by averaging\n",
      "over the collection of such judgments. This type of data can be represented\n",
      "by an N×Nmatrix D, where Nis the number of objects, and each element\n",
      "dii′records the proximity between the ith and i′th objects. This matrix is\n",
      "then provided as input to the clustering algorithm.\n",
      "Most algorithms presume a matrix of dissimilarities with nonnegative\n",
      "entries and zero diagonal elements: dii= 0, i= 1,2,... ,N. If the original\n",
      "data were collected as similarities, a suitable monotone-decreasing function\n",
      "can be used to convert them to dissimilarities. Also, most algorithms as -\n",
      "sume symmetric dissimilarity matrices, so if the original matrix Dis not\n",
      "symmetric it must be replaced by ( D+DT)/2. Subjectively judged dissimi-\n",
      "larities are seldom distances in the strict sense, since the triangle inequality\n",
      "dii′≤dik+di′k, for all k∈ {1,... ,N }does not hold. Thus, some algorithms\n",
      "that assume distances cannot be used with such data.\n",
      "14.3.2 Dissimilarities Based on Attributes\n",
      "Most often we have measurements xijfori= 1,2,... ,N , on variables\n",
      "j= 1,2,... ,p (also called attributes ). Since most of the popular clustering\n",
      "algorithms take a dissimilarity matrix as their input, we must ﬁrst const ruct\n",
      "pairwise dissimilarities between the observations. In the most common cas e,\n",
      "we deﬁne a dissimilarity dj(xij,xi′j) between values of the jth attribute,\n",
      "and then deﬁne\n",
      "D(xi,xi′) =p∑\n",
      "j=1dj(xij,xi′j) (14.20)\n",
      "as the dissimilarity between objects iandi′. By far the most common\n",
      "choice is squared distance\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "504 14. Unsupervised Learning\n",
      "dj(xij,xi′j) = (xij−xi′j)2. (14.21)\n",
      "However, other choices are possible, and can lead to potentially diﬀerent\n",
      "results. For nonquantitative attributes (e.g., categorical data), squared dis-\n",
      "tance may not be appropriate. In addition, it is sometimes desirable to\n",
      "weigh attributes diﬀerently rather than giving them equal weight as in\n",
      "(14.20).\n",
      "We ﬁrst discuss alternatives in terms of the attribute type:\n",
      "Quantitative variables. Measurements of this type of variable or attribute\n",
      "are represented by continuous real-valued numbers. It is natural to\n",
      "deﬁne the “error” between them as a monotone-increasing function\n",
      "of their absolute diﬀerence\n",
      "d(xi,xi′) =l(|xi−xi′|).\n",
      "Besides squared-error loss ( xi−xi′)2, a common choice is the identity\n",
      "(absolute error). The former places more emphasis on larger diﬀer-\n",
      "ences than smaller ones. Alternatively, clustering can be based on the\n",
      "correlation\n",
      "ρ(xi,xi′) =∑\n",
      "j(xij−¯xi)(xi′j−¯xi′)√∑\n",
      "j(xij−¯xi)2∑\n",
      "j(xi′j−¯xi′)2, (14.22)\n",
      "with ¯xi=∑\n",
      "jxij/p. Note that this is averaged over variables , not ob-\n",
      "servations. If the observations are ﬁrst standardized, then∑\n",
      "j(xij−\n",
      "xi′j)2∝2(1−ρ(xi,xi′)). Hence clustering based on correlation (simi-\n",
      "larity) is equivalent to that based on squared distance (dissimilarity).\n",
      "Ordinal variables. The values of this type of variable are often represented\n",
      "as contiguous integers, and the realizable values are considered to be\n",
      "an ordered set. Examples are academic grades (A, B, C, D, F), degree\n",
      "of preference (can’t stand, dislike, OK, like, terriﬁc). Rank data are a\n",
      "special kind of ordinal data. Error measures for ordinal variables are\n",
      "generally deﬁned by replacing their Moriginal values with\n",
      "i−1/2\n",
      "M, i= 1,... ,M (14.23)\n",
      "in the prescribed order of their original values. They are then treated\n",
      "as quantitative variables on this scale.\n",
      "Categorical variables. With unordered categorical (also called nominal)\n",
      "variables, the degree-of-diﬀerence between pairs of values must be\n",
      "delineated explicitly. If the variable assumes Mdistinct values, these\n",
      "can be arranged in a symmetric M×Mmatrix with elements Lrr′=\n",
      "Lr′r,Lrr= 0,Lrr′≥0. The most common choice is Lrr′= 1 for all\n",
      "r̸=r′, while unequal losses can be used to emphasize some errors\n",
      "more than others.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 505\n",
      "14.3.3 Object Dissimilarity\n",
      "Next we deﬁne a procedure for combining the p-individual attribute dissim-\n",
      "ilarities dj(xij,xi′j), j= 1,2,... ,p into a single overall measure of dissim-\n",
      "ilarity D(xi,xi′) between two objects or observations ( xi,xi′) possessing\n",
      "the respective attribute values. This is nearly always done by means of a\n",
      "weighted average (convex combination)\n",
      "D(xi,xi′) =p∑\n",
      "j=1wj≤dj(xij,xi′j);p∑\n",
      "j=1wj= 1. (14.24)\n",
      "Herewjis a weight assigned to the jth attribute regulating the relative\n",
      "inﬂuence of that variable in determining the overall dissimilarity between\n",
      "objects. This choice should be based on subject matter considerations.\n",
      "It is important to realize that setting the weight wjto the same value\n",
      "for each variable (say, wj= 1∀j) does notnecessarily give all attributes\n",
      "equal inﬂuence. The inﬂuence of the jth attribute Xjon object dissimilarity\n",
      "D(xi,xi′) (14.24) depends upon its relative contribution to the average\n",
      "object dissimilarity measure over all pairs of observations in the data set\n",
      "¯D=1\n",
      "N2N∑\n",
      "i=1N∑\n",
      "i′=1D(xi,xi′) =p∑\n",
      "j=1wj≤¯dj,\n",
      "with\n",
      "¯dj=1\n",
      "N2N∑\n",
      "i=1N∑\n",
      "i′=1dj(xij,xi′j) (14.25)\n",
      "being the average dissimilarity on the jth attribute. Thus, the relative in-\n",
      "ﬂuence of the jth variable is wj≤¯dj, and setting wj∼1/¯djwould give all\n",
      "attributes equal inﬂuence in characterizing overall dissimilarity between ob-\n",
      "jects. For example, with pquantitative variables and squared-error distance\n",
      "used for each coordinate, then (14.24) becomes the (weighted) squared Eu-\n",
      "clidean distance\n",
      "DI(xi,xi′) =p∑\n",
      "j=1wj≤(xij−xi′j)2(14.26)\n",
      "between pairs of points in an IRp, with the quantitative variables as axes.\n",
      "In this case (14.25) becomes\n",
      "¯dj=1\n",
      "N2N∑\n",
      "i=1N∑\n",
      "i′=1(xij−xi′j)2= 2≤varj, (14.27)\n",
      "where var jis the sample estimate of Var( Xj). Thus, the relative impor-\n",
      "tance of each such variable is proportional to its variance over the data\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "506 14. Unsupervised Learning\n",
      "-6 -4 -2 0 2 4-6 -4 -2 0 2 4•••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "••••••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••••••••\n",
      "••••••••\n",
      "••\n",
      "•••• •••\n",
      "••••\n",
      "•••\n",
      "•••• •••\n",
      "••\n",
      "••••\n",
      "••••\n",
      "••\n",
      "•••••\n",
      "•••\n",
      "•••\n",
      "•••••\n",
      "•••\n",
      "••\n",
      "-2 -1 0 1 2-2 -1 0 1 2••\n",
      "••••\n",
      "•\n",
      "••••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•••••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "•••••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••••••••\n",
      "••\n",
      "••\n",
      "••\n",
      "X1 X1\n",
      "X2X2\n",
      "FIGURE 14.5. Simulated data: on the left, K-means clustering (with K=2) has\n",
      "been applied to the raw data. The two colors indicate the clust er memberships. On\n",
      "the right, the features were ﬁrst standardized before cluster ing. This is equivalent\n",
      "to using feature weights 1/[2≤var(Xj)]. The standardization has obscured the two\n",
      "well-separated groups. Note that each plot uses the same unit s in the horizontal\n",
      "and vertical axes.\n",
      "set. In general, setting wj= 1/¯djfor all attributes, irrespective of type,\n",
      "will cause each one of them to equally inﬂuence the overall dissimilarity\n",
      "between pairs of objects ( xi,xi′). Although this may seem reasonable, and\n",
      "is often recommended, it can be highly counterproductive. If the goal is to\n",
      "segment the data into groups of similar objects, all attributes may not con-\n",
      "tribute equally to the (problem-dependent) notion of dissimilarity between\n",
      "objects. Some attribute value diﬀerences may reﬂect greater actual object\n",
      "dissimilarity in the context of the problem domain.\n",
      "If the goal is to discover natural groupings in the data, some attributes\n",
      "may exhibit more of a grouping tendency than others. Variables that are\n",
      "more relevant in separating the groups should be assigned a higher inﬂu-\n",
      "ence in deﬁning object dissimilarity. Giving all attributes equal inﬂuence\n",
      "in this case will tend to obscure the groups to the point where a clustering\n",
      "algorithm cannot uncover them. Figure 14.5 shows an example.\n",
      "Although simple generic prescriptions for choosing the individual at-\n",
      "tribute dissimilarities dj(xij,xi′j) and their weights wjcan be comforting,\n",
      "there is no substitute for careful thought in the context of each individ-\n",
      "ual problem. Specifying an appropriate dissimilarity measure is far more\n",
      "important in obtaining success with clustering than choice of clustering\n",
      "algorithm. This aspect of the problem is emphasized less in the cluster-\n",
      "ing literature than the algorithms themselves, since it depends on domain\n",
      "knowledge speciﬁcs and is less amenable to general research.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 507\n",
      "Finally, often observations have missing values in one or more of the\n",
      "attributes. The most common method of incorporating missing values in\n",
      "dissimilarity calculations (14.24) is to omit each observation pair xij,xi′j\n",
      "having at least one value missing, when computing the dissimilarity be-\n",
      "tween observations xiandx′\n",
      "i. This method can fail in the circumstance\n",
      "when both observations have no measured values in common. In this case\n",
      "both observations could be deleted from the analysis. Alternatively, the\n",
      "missing values could be imputed using the mean or median of each attribute\n",
      "over the nonmissing data. For categorical variables, one could consider the\n",
      "value “missing” as just another categorical value, if it were reasonable to\n",
      "consider two objects as being similar if they both have missing values on\n",
      "the same variables.\n",
      "14.3.4 Clustering Algorithms\n",
      "The goal of cluster analysis is to partition the observations into groups\n",
      "(“clusters”) so that the pairwise dissimilarities between those assigned t o\n",
      "the same cluster tend to be smaller than those in diﬀerent clusters. Clus-\n",
      "tering algorithms fall into three distinct types: combinatorial algorit hms,\n",
      "mixture modeling, and mode seeking.\n",
      "Combinatorial algorithms work directly on the observed data with no\n",
      "direct reference to an underlying probability model. Mixture modeling sup-\n",
      "poses that the data is an i.i.dsample from some population described by a\n",
      "probability density function. This density function is characterized by a pa-\n",
      "rameterized model taken to be a mixture of component density functions;\n",
      "each component density describes one of the clusters. This model is then ﬁt\n",
      "to the data by maximum likelihood or corresponding Bayesian approaches.\n",
      "Mode seekers (“bump hunters”) take a nonparametric perspective, attempt-\n",
      "ing to directly estimate distinct modes of the probability density function.\n",
      "Observations “closest” to each respective mode then deﬁne the individual\n",
      "clusters.\n",
      "Mixture modeling is described in Section 6.8. The PRIM algorithm, dis-\n",
      "cussed in Sections 9.3 and 14.2.5, is an example of mode seeking or “bump\n",
      "hunting.” We discuss combinatorial algorithms next.\n",
      "14.3.5 Combinatorial Algorithms\n",
      "The most popular clustering algorithms directly assign each observation\n",
      "to a group or cluster without regard to a probability model describing the\n",
      "data. Each observation is uniquely labeled by an integer i∈ {1,≤ ≤ ≤,N}.\n",
      "A prespeciﬁed number of clusters K < N is postulated, and each one is\n",
      "labeled by an integer k∈ {1,... ,K }. Each observation is assigned to one\n",
      "and only one cluster. These assignments can be characterized by a many-\n",
      "to-one mapping, or encoder k=C(i), that assigns the ith observation to\n",
      "thekth cluster. One seeks the particular encoder C∗(i) that achieves the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "508 14. Unsupervised Learning\n",
      "required goal (details below), based on the dissimilarities d(xi,xi′) between\n",
      "every pair of observations. These are speciﬁed by the user as described\n",
      "above. Generally, the encoder C(i) is explicitly delineated by giving its\n",
      "value (cluster assignment) for each observation i. Thus, the “parameters”\n",
      "of the procedure are the individual cluster assignments for each of the N\n",
      "observations. These are adjusted so as to minimize a “loss” function that\n",
      "characterizes the degree to which the clustering goal is notmet.\n",
      "One approach is to directly specify a mathematical loss function and\n",
      "attempt to minimize it through some combinatorial optimization algorit hm.\n",
      "Since the goal is to assign close points to the same cluster, a natural loss\n",
      "(or “energy”) function would be\n",
      "W(C) =1\n",
      "2K∑\n",
      "k=1∑\n",
      "C(i)=k∑\n",
      "C(i′)=kd(xi,xi′). (14.28)\n",
      "This criterion characterizes the extent to which observations assigned to\n",
      "the same cluster tend to be close to one another. It is sometimes referred\n",
      "to as the “within cluster” point scatter since\n",
      "T=1\n",
      "2N∑\n",
      "i=1N∑\n",
      "i′=1dii′=1\n",
      "2K∑\n",
      "k=1∑\n",
      "C(i)=k\n",
      "∑\n",
      "C(i′)=kdii′+∑\n",
      "C(i′)̸=kdii′\n",
      ",\n",
      "or\n",
      "T=W(C) +B(C),\n",
      "where dii′=d(xi,xi′). Here Tis thetotalpoint scatter, which is a constant\n",
      "given the data, independent of cluster assignment. The quantity\n",
      "B(C) =1\n",
      "2K∑\n",
      "k=1∑\n",
      "C(i)=k∑\n",
      "C(i′)̸=kdii′ (14.29)\n",
      "is the between-cluster point scatter. This will tend to be large when obser-\n",
      "vations assigned to diﬀerent clusters are far apart. Thus one has\n",
      "W(C) =T−B(C)\n",
      "and minimizing W(C) is equivalent to maximizing B(C).\n",
      "Cluster analysis by combinatorial optimization is straightforward in prin-\n",
      "ciple. One simply minimizes Wor equivalently maximizes Bover all pos-\n",
      "sible assignments of the Ndata points to Kclusters. Unfortunately, such\n",
      "optimization by complete enumeration is feasible only for very small data\n",
      "sets. The number of distinct assignments is (Jain and Dubes, 1988)\n",
      "S(N,K) =1\n",
      "K!K∑\n",
      "k=1(−1)K−k(K\n",
      "k)\n",
      "kN. (14.30)\n",
      "For example, S(10,4) = 34 ,105 which is quite feasible. But, S(N,K) grows\n",
      "very rapidly with increasing values of its arguments. Already S(19,4)≃\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 509\n",
      "1010, and most clustering problems involve much larger data sets than\n",
      "N= 19. For this reason, practical clustering algorithms are able to examine\n",
      "only a very small fraction of all possible encoders k=C(i). The goal is to\n",
      "identify a small subset that is likely to contain the optimal one, or at least\n",
      "a good suboptimal partition.\n",
      "Such feasible strategies are based on iterative greedy descent. An initial\n",
      "partition is speciﬁed. At each iterative step, the cluster assignments are\n",
      "changed in such a way that the value of the criterion is improved from\n",
      "its previous value. Clustering algorithms of this type diﬀer in their pre-\n",
      "scriptions for modifying the cluster assignments at each iteration. When\n",
      "the prescription is unable to provide an improvement, the algorithm ter-\n",
      "minates with the current assignments as its solution. Since the assignment\n",
      "of observations to clusters at any iteration is a perturbation of that for the\n",
      "previous iteration, only a very small fraction of all possible assignmen ts\n",
      "(14.30) are examined. However, these algorithms converge to localoptima\n",
      "which may be highly suboptimal when compared to the global optimum.\n",
      "14.3.6 K-means\n",
      "TheK-means algorithm is one of the most popular iterative descent clus-\n",
      "tering methods. It is intended for situations in which all variables are of\n",
      "the quantitative type, and squared Euclidean distance\n",
      "d(xi,xi′) =p∑\n",
      "j=1(xij−xi′j)2=||xi−xi′||2\n",
      "is chosen as the dissimilarity measure. Note that weighted Euclidean dis-\n",
      "tance can be used by redeﬁning the xijvalues (Exercise 14.1).\n",
      "The within-point scatter (14.28) can be written as\n",
      "W(C) =1\n",
      "2K∑\n",
      "k=1∑\n",
      "C(i)=k∑\n",
      "C(i′)=k||xi−xi′||2\n",
      "=K∑\n",
      "k=1Nk∑\n",
      "C(i)=k||xi−¯xk||2, (14.31)\n",
      "where ¯ xk= (¯x1k,... ,¯xpk) is the mean vector associated with the kth clus-\n",
      "ter, and Nk=∑N\n",
      "i=1I(C(i) =k). Thus, the criterion is minimized by\n",
      "assigning the Nobservations to the Kclusters in such a way that within\n",
      "each cluster the average dissimilarity of the observations from the cluster\n",
      "mean, as deﬁned by the points in that cluster, is minimized.\n",
      "An iterative descent algorithm for solving\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "510 14. Unsupervised Learning\n",
      "Algorithm 14.1 K-means Clustering.\n",
      "1. For a given cluster assignment C, the total cluster variance (14.33) is\n",
      "minimized with respect to {m1,... ,m K}yielding the means of the\n",
      "currently assigned clusters (14.32).\n",
      "2. Given a current set of means {m1,... ,m K}, (14.33) is minimized by\n",
      "assigning each observation to the closest (current) cluster mean. That\n",
      "is,\n",
      "C(i) = argmin\n",
      "1≤k≤K||xi−mk||2. (14.34)\n",
      "3. Steps 1 and 2 are iterated until the assignments do not change.\n",
      "C∗= min\n",
      "CK∑\n",
      "k=1Nk∑\n",
      "C(i)=k||xi−¯xk||2\n",
      "can be obtained by noting that for any set of observations S\n",
      "¯xS= argmin\n",
      "m∑\n",
      "i∈S||xi−m||2. (14.32)\n",
      "Hence we can obtain C∗by solving the enlarged optimization problem\n",
      "min\n",
      "C,{mk}K\n",
      "1K∑\n",
      "k=1Nk∑\n",
      "C(i)=k||xi−mk||2. (14.33)\n",
      "This can be minimized by an alternating optimization procedure given in\n",
      "Algorithm 14.1.\n",
      "Each of steps 1 and 2 reduces the value of the criterion (14.33), so that\n",
      "convergence is assured. However, the result may represent a suboptimal\n",
      "local minimum. The algorithm of Hartigan and Wong (1979) goes further,\n",
      "and ensures that there is no single switch of an observation from one group\n",
      "to another group that will decrease the objective. In addition, one should\n",
      "start the algorithm with many diﬀerent random choices for the starting\n",
      "means, and choose the solution having smallest value of the objective func-\n",
      "tion.\n",
      "Figure 14.6 shows some of the K-means iterations for the simulated data\n",
      "of Figure 14.4. The centroids are depicted by “O”s. The straight lines show\n",
      "the partitioning of points, each sector being the set of points closest to\n",
      "each centroid. This partitioning is called the Voronoi tessellation . After 20\n",
      "iterations the procedure has converged.\n",
      "14.3.7 Gaussian Mixtures as Soft K-means Clustering\n",
      "TheK-means clustering procedure is closely related to the EM algorithm\n",
      "for estimating a certain Gaussian mixture model. (Sections 6.8 and 8.5.1).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 511\n",
      "-4 -2 0 2 4 6-2 0 2 4 6Initial Centroids\n",
      "••••\n",
      "••••••••\n",
      "••\n",
      "•••••••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••• ••\n",
      "•\n",
      "••••••••\n",
      "••••••\n",
      "•\n",
      "••••\n",
      "••••••••\n",
      "•••••\n",
      "••••••••\n",
      "•••\n",
      "••••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••••\n",
      "•••\n",
      "•••\n",
      "••••\n",
      "••••••••\n",
      "••\n",
      "•••••••••\n",
      "••••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••• ••\n",
      "•\n",
      "••••••••\n",
      "••••••\n",
      "•\n",
      "••••\n",
      "••••••••\n",
      "•••••\n",
      "••••••••\n",
      "•••\n",
      "••••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••••\n",
      "•••\n",
      "•••Initial Partition\n",
      "••••\n",
      "••••••••\n",
      "••\n",
      "•••••••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••• ••\n",
      "•\n",
      "••••••••\n",
      "••••••\n",
      "•\n",
      "••••\n",
      "••••••\n",
      "••\n",
      "••••••\n",
      "•••\n",
      "••••\n",
      "••\n",
      "••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•••••Iteration Number  2\n",
      "•••\n",
      "•••\n",
      "••••\n",
      "••••••••\n",
      "••\n",
      "•••••••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••• ••\n",
      "•\n",
      "••••••••\n",
      "••••••\n",
      "•\n",
      "••••\n",
      "•••••\n",
      "•••••\n",
      "•••••••••\n",
      "•••\n",
      "•••••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "•••••Iteration Number  20\n",
      "•••\n",
      "•••\n",
      "FIGURE 14.6. Successive iterations of the K-means clustering algorithm for\n",
      "the simulated data of Figure 14.4.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "512 14. Unsupervised Learning\n",
      "• •Responsibilities\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "• •Responsibilities\n",
      "0.0 0.2 0.4 0.6 0.8 1.0σ= 1.0 σ= 1.0\n",
      "σ= 0.2 σ= 0.2\n",
      "FIGURE 14.7. (Left panels:) two Gaussian densities g0(x) and g1(x)(blue and\n",
      "orange) on the real line, and a single data point (green dot) at x= 0.5. The colored\n",
      "squares are plotted at x=−1.0andx= 1.0, the means of each density. (Right\n",
      "panels:) the relative densities g0(x)/(g0(x) +g1(x))andg1(x)/(g0(x) +g1(x)),\n",
      "called the “responsibilities” of each cluster, for this data point. In the top panels,\n",
      "the Gaussian standard deviation σ= 1.0; in the bottom panels σ= 0.2. The\n",
      "EM algorithm uses these responsibilities to make a “soft” ass ignment of each\n",
      "data point to each of the two clusters. When σis fairly large, the responsibilities\n",
      "can be near 0.5(they are 0.36and0.64 in the top right panel). As σ→0, the\n",
      "responsibilities →1, for the cluster center closest to the target point, and 0for\n",
      "all other clusters. This “hard” assignment is seen in the botto m right panel.\n",
      "The E-step of the EM algorithm assigns “responsibilities” for each data\n",
      "point based in its relative density under each mixture component, while\n",
      "the M-step recomputes the component density parameters based on the\n",
      "current responsibilities. Suppose we specify Kmixture components, each\n",
      "with a Gaussian density having scalar covariance matrix σ2I. Then the\n",
      "relative density under each mixture component is a monotone function of\n",
      "the Euclidean distance between the data point and the mixture center.\n",
      "Hence in this setup EM is a “soft” version of K-means clustering, making\n",
      "probabilistic (rather than deterministic) assignments of points to cluster\n",
      "centers. As the variance σ2→0, these probabilities become 0 and 1, and\n",
      "the two methods coincide. Details are given in Exercise 14.2. Figure 14.7\n",
      "illustrates this result for two clusters on the real line.\n",
      "14.3.8 Example: Human Tumor Microarray Data\n",
      "We apply K-means clustering to the human tumor microarray data de-\n",
      "scribed in Chapter 1. This is an example of high-dimensional clustering.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 513\n",
      "Number of Clusters KSum of Squares\n",
      "2 4 6 8 10160000 200000 240000•\n",
      "•\n",
      "•\n",
      "•••••••\n",
      "FIGURE 14.8. Total within-cluster sum of squares for K-means clustering ap-\n",
      "plied to the human tumor microarray data.\n",
      "TABLE 14.2. Human tumor data: number of cancer cases of each type, in each\n",
      "of the three clusters from K-means clustering.\n",
      "Cluster Breast CNS Colon K562 Leukemia MCF7\n",
      "1 3 5 0 0 0 0\n",
      "2 2 0 0 2 6 2\n",
      "3 2 0 7 0 0 0\n",
      "Cluster Melanoma NSCLC Ovarian Prostate Renal Unknown\n",
      "1 1 7 6 2 9 1\n",
      "2 7 2 0 0 0 0\n",
      "3 0 0 0 0 0 0\n",
      "The data are a 6830 ×64 matrix of real numbers, each representing an\n",
      "expression measurement for a gene (row) and sample (column). Here we\n",
      "cluster the samples, each of which is a vector of length 6830, correspond-\n",
      "ing to expression values for the 6830 genes. Each sample has a label such\n",
      "asbreast (for breast cancer), melanoma , and so on; we don’t use these la-\n",
      "bels in the clustering, but will examine posthoc which labels fall into which\n",
      "clusters.\n",
      "We applied K-means clustering with Krunning from 1 to 10, and com-\n",
      "puted the total within-sum of squares for each clustering, shown in Fig-\n",
      "ure 14.8. Typically one looks for a kink in the sum of squares curve (or its\n",
      "logarithm) to locate the optimal number of clusters (see Section 14.3.11).\n",
      "Here there is no clear indication: for illustration we chose K= 3 giving the\n",
      "three clusters shown in Table 14.2.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "514 14. Unsupervised Learning\n",
      "FIGURE 14.9. Sir Ronald A. Fisher ( 1890−1962) was one of the founders\n",
      "of modern day statistics, to whom we owe maximum-likelihood, suﬃciency, and\n",
      "many other fundamental concepts. The image on the left is a 1024×1024grayscale\n",
      "image at 8bits per pixel. The center image is the result of 2×2block VQ, using\n",
      "200code vectors, with a compression rate of 1.9bits/pixel. The right image uses\n",
      "only four code vectors, with a compression rate of 0.50bits/pixel\n",
      "We see that the procedure is successful at grouping together samples of\n",
      "the same cancer. In fact, the two breast cancers in the second cluster were\n",
      "later found to be misdiagnosed and were melanomas that had metastasized.\n",
      "However, K-means clustering has shortcomings in this application. For one,\n",
      "it does not give a linear ordering of objects within a cluster: we have simply\n",
      "listed them in alphabetic order above. Secondly, as the number of clusters\n",
      "Kis changed, the cluster memberships can change in arbitrary ways. That\n",
      "is, with say four clusters, the clusters need not be nested within the three\n",
      "clusters above. For these reasons, hierarchical clustering (described later),\n",
      "is probably preferable for this application.\n",
      "14.3.9 Vector Quantization\n",
      "TheK-means clustering algorithm represents a key tool in the apparently\n",
      "unrelated area of image and signal compression, particularly in vector quan-\n",
      "tization or VQ (Gersho and Gray, 1992). The left image in Figure 14.92is a\n",
      "digitized photograph of a famous statistician, Sir Ronald Fisher. It consist s\n",
      "of 1024 ×1024 pixels, where each pixel is a grayscale value ranging from 0\n",
      "to 255, and hence requires 8 bits of storage per pixel. The entire image oc-\n",
      "cupies 1 megabyte of storage. The center image is a VQ-compressed version\n",
      "of the left panel, and requires 0 .239 of the storage (at some loss in quality).\n",
      "The right image is compressed even more, and requires only 0 .0625 of the\n",
      "storage (at a considerable loss in quality).\n",
      "The version of VQ implemented here ﬁrst breaks the image into small\n",
      "blocks, in this case 2 ×2 blocks of pixels. Each of the 512 ×512 blocks of four\n",
      "2This example was prepared by Maya Gupta.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 515\n",
      "numbers is regarded as a vector in IR4. AK-means clustering algorithm\n",
      "(also known as Lloyd’s algorithm in this context) is run in this space.\n",
      "The center image uses K= 200, while the right image K= 4. Each of\n",
      "the 512 ×512 pixel blocks (or points) is approximated by its closest cluster\n",
      "centroid, known as a codeword. The clustering process is called the encoding\n",
      "step, and the collection of centroids is called the codebook .\n",
      "To represent the approximated image, we need to supply for each block\n",
      "the identity of the codebook entry that approximates it. This will require\n",
      "log2(K) bits per block. We also need to supply the codebook itself, which\n",
      "isK×4 real numbers (typically negligible). Overall, the storage for the\n",
      "compressed image amounts to log2(K)/(4≤8) of the original (0 .239 for\n",
      "K= 200, 0 .063 for K= 4). This is typically expressed as a ratein bits\n",
      "per pixel: log2(K)/4, which are 1 .91 and 0 .50, respectively. The process\n",
      "of constructing the approximate image from the centroids is called the\n",
      "decoding step.\n",
      "Why do we expect VQ to work at all? The reason is that for typical\n",
      "everyday images like photographs, many of the blocks look the same. In\n",
      "this case there are many almost pure white blocks, and similarly pure gray\n",
      "blocks of various shades. These require only one block each to represent\n",
      "them, and then multiple pointers to that block.\n",
      "What we have described is known as lossycompression, since our im-\n",
      "ages are degraded versions of the original. The degradation or distortion is\n",
      "usually measured in terms of mean squared error. In this case D= 0.89\n",
      "forK= 200 and D= 16.95 for K= 4. More generally a rate/distortion\n",
      "curve would be used to assess the tradeoﬀ. One can also perform lossless\n",
      "compression using block clustering, and still capitalize on the repeated pat-\n",
      "terns. If you took the original image and losslessly compressed it, the bes t\n",
      "you would do is 4.48 bits per pixel.\n",
      "We claimed above that log2(K) bits were needed to identify each of the K\n",
      "codewords in the codebook. This uses a ﬁxed-length code, and is ineﬃcient\n",
      "if some codewords occur many more times than others in the image. Using\n",
      "Shannon coding theory, we know that in general a variable length code\n",
      "will do better, and the rate then becomes −∑K\n",
      "ℓ=1pℓlog2(pℓ)/4. The term\n",
      "in the numerator is the entropy of the distribution pℓof the codewords\n",
      "in the image. Using variable length coding our rates come down to 1 .42\n",
      "and 0.39, respectively. Finally, there are many generalizations of VQ that\n",
      "have been developed: for example, tree-structured VQ ﬁnds the centroids\n",
      "with a top-down, 2-means style algorithm, as alluded to in Section 14.3.12.\n",
      "This allows successive reﬁnement of the compression. Further details may\n",
      "be found in Gersho and Gray (1992).\n",
      "14.3.10 K-medoids\n",
      "As discussed above, the K-means algorithm is appropriate when the dis-\n",
      "similarity measure is taken to be squared Euclidean distance D(xi,xi′)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "516 14. Unsupervised Learning\n",
      "Algorithm 14.2 K-medoids Clustering.\n",
      "1. For a given cluster assignment Cﬁnd the observation in the cluster\n",
      "minimizing total distance to other points in that cluster:\n",
      "i∗\n",
      "k= argmin\n",
      "{i:C(i)=k}∑\n",
      "C(i′)=kD(xi,xi′). (14.35)\n",
      "Then mk=xi∗\n",
      "k, k= 1,2,... ,K are the current estimates of the\n",
      "cluster centers.\n",
      "2. Given a current set of cluster centers {m1,... ,m K}, minimize the to-\n",
      "tal error by assigning each observation to the closest (current) cluster\n",
      "center:\n",
      "C(i) = argmin\n",
      "1≤k≤KD(xi,mk). (14.36)\n",
      "3. Iterate steps 1 and 2 until the assignments do not change.\n",
      "(14.112). This requires all of the variables to be of the quantitative t ype. In\n",
      "addition, using squared Euclidean distance places the highest inﬂuence on\n",
      "the largest distances. This causes the procedure to lack robustness against\n",
      "outliers that produce very large distances. These restrictions can be re-\n",
      "moved at the expense of computation.\n",
      "The only part of the K-means algorithm that assumes squared Eu-\n",
      "clidean distance is the minimization step (14.32); the cluster representatives\n",
      "{m1,... ,m K}in (14.33) are taken to be the means of the currently assigned\n",
      "clusters. The algorithm can be generalized for use with arbitrarily deﬁned\n",
      "dissimilarities D(xi,xi′) by replacing this step by an explicit optimization\n",
      "with respect to {m1,... ,m K}in (14.33). In the most common form, cen-\n",
      "ters for each cluster are restricted to be one of the observations assigned\n",
      "to the cluster, as summarized in Algorithm 14.2. This algorithm assumes\n",
      "attribute data, but the approach can also be applied to data described\n",
      "onlyby proximity matrices (Section 14.3.1). There is no need to explicitly\n",
      "compute cluster centers; rather we just keep track of the indices i∗\n",
      "k.\n",
      "Solving (14.32) for each provisional cluster krequires an amount of com-\n",
      "putation proportional to the number of observations assigned to it, whereas\n",
      "for solving (14.35) the computation increases to O(N2\n",
      "k). Given a set of clus-\n",
      "ter “centers,” {i1,... ,i K}, obtaining the new assignments\n",
      "C(i) = argmin\n",
      "1≤k≤Kdii∗\n",
      "k(14.37)\n",
      "requires computation proportional to K≤Nas before. Thus, K-medoids is\n",
      "far more computationally intensive than K-means.\n",
      "Alternating between (14.35) and (14.37) represents a particular heuristic\n",
      "search strategy for trying to solve\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 517\n",
      "TABLE 14.3. Data from a political science survey: values are average pair wise\n",
      "dissimilarities of countries from a questionnaire given to pol itical science students.\n",
      "BEL BRA CHI CUB EGY FRA IND ISR USA USS YUG\n",
      "BRA 5.58\n",
      "CHI 7.00 6.50\n",
      "CUB 7.08 7.00 3.83\n",
      "EGY 4.83 5.08 8.17 5.83\n",
      "FRA 2.17 5.75 6.67 6.92 4.92\n",
      "IND 6.42 5.00 5.58 6.00 4.67 6.42\n",
      "ISR 3.42 5.50 6.42 6.42 5.00 3.92 6.17\n",
      "USA 2.50 4.92 6.25 7.33 4.50 2.25 6.33 2.75\n",
      "USS 6.08 6.67 4.25 2.67 6.00 6.17 6.17 6.92 6.17\n",
      "YUG 5.25 6.83 4.50 3.75 5.75 5.42 6.08 5.83 6.67 3.67\n",
      "ZAI 4.75 3.00 6.08 6.67 5.00 5.58 4.83 6.17 5.67 6.50 6.92\n",
      "min\n",
      "C,{ik}K\n",
      "1K∑\n",
      "k=1∑\n",
      "C(i)=kdiik. (14.38)\n",
      "Kaufman and Rousseeuw (1990) propose an alternative strategy for directly\n",
      "solving (14.38) that provisionally exchanges each center ikwith an obser-\n",
      "vation that is not currently a center, selecting the exchange that produces\n",
      "the greatest reduction in the value of the criterion (14.38). This is repeated\n",
      "until no advantageous exchanges can be found. Massart et al. (1983) derive\n",
      "a branch-and-bound combinatorial method that ﬁnds the global minimum\n",
      "of (14.38) that is practical only for very small data sets.\n",
      "Example: Country Dissimilarities\n",
      "This example, taken from Kaufman and Rousseeuw (1990), comes from a\n",
      "study in which political science students were asked to provide pairwise dis-\n",
      "similarity measures for 12 countries: Belgium, Brazil, Chile, Cuba, Egypt,\n",
      "France, India, Israel, United States, Union of Soviet Socialist Republics,\n",
      "Yugoslavia and Zaire. The average dissimilarity scores are given in Ta -\n",
      "ble 14.3. We applied 3-medoid clustering to these dissimilarities. Note that\n",
      "K-means clustering could not be applied because we have only distances\n",
      "rather than raw observations. The left panel of Figure 14.10 shows the\n",
      "dissimilarities reordered and blocked according to the 3-medoid clustering.\n",
      "The right panel is a two-dimensional multidimensional scaling plot, with\n",
      "the 3-medoid clusters assignments indicated by colors (multidimensional\n",
      "scaling is discussed in Section 14.8.) Both plots show three well-separated\n",
      "clusters, but the MDS display indicates that “Egypt” falls about halfway\n",
      "between two clusters.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "518 14. Unsupervised Learning\n",
      "CHICUBUSSYUGBRAINDZAIBELEGYFRAISR\n",
      "CUBUSSYUGBRAINDZAIBELEGYFRAISRUSA\n",
      "Reordered Dissimilarity Matrix First MDS CoordinateSecond MDS Coordinate\n",
      "-2 0 2 4-2 -1 0 1 2 3CHI\n",
      "CUB\n",
      "USS\n",
      "YUGBRAIND ZAI\n",
      "BELEGY\n",
      "FRAISRUSA\n",
      "FIGURE 14.10. Survey of country dissimilarities. (Left panel:) dissimilari ties\n",
      "reordered and blocked according to 3-medoid clustering. Heat map is coded from\n",
      "most similar (dark red) to least similar (bright red). (Righ t panel:) two-dimen-\n",
      "sional multidimensional scaling plot, with 3-medoid clusters indicated by diﬀerent\n",
      "colors.\n",
      "14.3.11 Practical Issues\n",
      "In order to apply K-means or K-medoids one must select the number of\n",
      "clusters K∗and an initialization. The latter can be deﬁned by specifying\n",
      "an initial set of centers {m1,... ,m K}or{i1,... ,i K}or an initial encoder\n",
      "C(i). Usually specifying the centers is more convenient. Suggestions range\n",
      "from simple random selection to a deliberate strategy based on forward\n",
      "stepwise assignment. At each step a new center ikis chosen to minimize\n",
      "the criterion (14.33) or (14.38), given the centers i1,... ,i k−1chosen at the\n",
      "previous steps. This continues for Ksteps, thereby producing Kinitial\n",
      "centers with which to begin the optimization algorithm.\n",
      "A choice for the number of clusters Kdepends on the goal. For data\n",
      "segmentation Kis usually deﬁned as part of the problem. For example,\n",
      "a company may employ Ksales people, and the goal is to partition a\n",
      "customer database into Ksegments, one for each sales person, such that the\n",
      "customers assigned to each one are as similar as possible. Often, however,\n",
      "cluster analysis is used to provide a descriptive statistic for ascertaining t he\n",
      "extent to which the observations comprising the data base fall into natural\n",
      "distinct groupings. Here the number of such groups K∗is unknown and\n",
      "one requires that it, as well as the groupings themselves, be estimated from\n",
      "the data.\n",
      "Data-based methods for estimating K∗typically examine the within-\n",
      "cluster dissimilarity WKas a function of the number of clusters K. Separate\n",
      "solutions are obtained for K∈ {1,2,... ,K max}. The corresponding values\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 519\n",
      "{W1,W2,... ,W Kmax}generally decrease with increasing K. This will be\n",
      "the case even when the criterion is evaluated on an independent test set,\n",
      "since a large number of cluster centers will tend to ﬁll the feature space\n",
      "densely and thus will be close to all data points. Thus cross-validation\n",
      "techniques, so useful for model selection in supervised learning, cannot be\n",
      "utilized in this context.\n",
      "The intuition underlying the approach is that if there are actually K∗\n",
      "distinct groupings of the observations (as deﬁned by the dissimilarity mea-\n",
      "sure), then for K < K∗the clusters returned by the algorithm will each\n",
      "contain a subset of the true underlying groups. That is, the solution will\n",
      "not assign observations in the same naturally occurring group to diﬀerent\n",
      "estimated clusters. To the extent that this is the case, the solution criterion\n",
      "value will tend to decrease substantially with each successive increase in the\n",
      "number of speciﬁed clusters, WK+1≪WK, as the natural groups are suc-\n",
      "cessively assigned to separate clusters. For K > K∗, one of the estimated\n",
      "clusters must partition at least one of the natural groups into two sub-\n",
      "groups. This will tend to provide a smaller decrease in the criterion as Kis\n",
      "further increased. Splitting a natural group, within which the observations\n",
      "are all quite close to each other, reduces the criterion less than partitioning\n",
      "the union of two well-separated groups into their proper constituents.\n",
      "To the extent this scenario is realized, there will be a sharp decrease in\n",
      "successive diﬀerences in criterion value, WK−WK+1, atK=K∗. That\n",
      "is,{WK−WK+1|K < K∗} ≫ { WK−WK+1|K≥K∗}. An estimate\n",
      "ˆK∗forK∗is then obtained by identifying a “kink” in the plot of WKas a\n",
      "function of K. As with other aspects of clustering procedures, this approach\n",
      "is somewhat heuristic.\n",
      "The recently proposed Gap statistic (Tibshirani et al., 2001b) compares\n",
      "the curve log WKto the curve obtained from data uniformly distributed\n",
      "over a rectangle containing the data. It estimates the optimal number of\n",
      "clusters to be the place where the gap between the two curves is largest.\n",
      "Essentially this is an automatic way of locating the aforementioned “ki nk.”\n",
      "It also works reasonably well when the data fall into a single cluster, and\n",
      "in that case will tend to estimate the optimal number of clusters to be one.\n",
      "This is the scenario where most other competing methods fail.\n",
      "Figure 14.11 shows the result of the Gap statistic applied to simulated\n",
      "data of Figure 14.4. The left panel shows log WKfork= 1,2,... ,8 clusters\n",
      "(green curve) and the expected value of log WKover 20 simulations from\n",
      "uniform data (blue curve). The right panel shows the gap curve, which is the\n",
      "expected curve minus the observed curve. Shown also are error bars of half-\n",
      "width s′\n",
      "K=sK√\n",
      "1 + 1/20, where sKis the standard deviation of log WK\n",
      "over the 20 simulations. The Gap curve is maximized at K= 2 clusters. If\n",
      "G(K) is the Gap curve at Kclusters, the formal rule for estimating K∗is\n",
      "K∗= argmin\n",
      "K{K|G(K)≥G(K+ 1)−s′\n",
      "K+1}. (14.39)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "520 14. Unsupervised Learning\n",
      "Number of Clusters2 4 6 8-3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0•\n",
      "••\n",
      "••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•\n",
      "•\n",
      "•••\n",
      "Number of ClustersGap\n",
      "2 4 6 8-0.5 0.0 0.5 1.0••\n",
      "•••••\n",
      "•logWK\n",
      "FIGURE 14.11. (Left panel): observed (green) and expected (blue) values of\n",
      "logWKfor the simulated data of Figure 14.4. Both curves have been t ranslated\n",
      "to equal zero at one cluster. (Right panel): Gap curve, equal to the diﬀerence\n",
      "between the observed and expected values of logWK. The Gap estimate K∗is the\n",
      "smallest Kproducing a gap within one standard deviation of the gap at K+ 1;\n",
      "hereK∗= 2.\n",
      "This gives K∗= 2, which looks reasonable from Figure 14.4.\n",
      "14.3.12 Hierarchical Clustering\n",
      "The results of applying K-means or K-medoids clustering algorithms de-\n",
      "pend on the choice for the number of clusters to be searched and a starting\n",
      "conﬁguration assignment. In contrast, hierarchical clustering methods do\n",
      "not require such speciﬁcations. Instead, they require the user to specify a\n",
      "measure of dissimilarity between (disjoint) groups of observations, based\n",
      "on the pairwise dissimilarities among the observations in the two groups.\n",
      "As the name suggests, they produce hierarchical representations in which\n",
      "the clusters at each level of the hierarchy are created by merging clusters\n",
      "at the next lower level. At the lowest level, each cluster contains a single\n",
      "observation. At the highest level there is only one cluster containing all of\n",
      "the data.\n",
      "Strategies for hierarchical clustering divide into two basic paradigms: ag-\n",
      "glomerative (bottom-up) and divisive (top-down). Agglomerative strategies\n",
      "start at the bottom and at each level recursively merge a selected pair of\n",
      "clusters into a single cluster. This produces a grouping at the next higher\n",
      "level with one less cluster. The pair chosen for merging consist of the two\n",
      "groups with the smallest intergroup dissimilarity. Divisive methods s tart\n",
      "at the top and at each level recursively split one of the existing clusters at\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 521\n",
      "that level into two new clusters. The split is chosen to produce two new\n",
      "groups with the largest between-group dissimilarity. With both paradigms\n",
      "there are N−1 levels in the hierarchy.\n",
      "Each level of the hierarchy represents a particular grouping of the data\n",
      "into disjoint clusters of observations. The entire hierarchy represents an\n",
      "ordered sequence of such groupings. It is up to the user to decide which\n",
      "level (if any) actually represents a “natural” clustering in the sense that\n",
      "observations within each of its groups are suﬃciently more similar to eac h\n",
      "other than to observations assigned to diﬀerent groups at that level. The\n",
      "Gap statistic described earlier can be used for this purpose.\n",
      "Recursive binary splitting/agglomeration can be represented by a rooted\n",
      "binary tree. The nodes of the trees represent groups. The root node repre-\n",
      "sents the entire data set. The Nterminal nodes each represent one of the\n",
      "individual observations (singleton clusters). Each nonterminal node (“par-\n",
      "ent”) has two daughter nodes. For divisive clustering the two daughters\n",
      "represent the two groups resulting from the split of the parent; for agglom-\n",
      "erative clustering the daughters represent the two groups that were merged\n",
      "to form the parent.\n",
      "All agglomerative and some divisive methods (when viewed bottom-up)\n",
      "possess a monotonicity property. That is, the dissimilarity between merged\n",
      "clusters is monotone increasing with the level of the merger. Thus the\n",
      "binary tree can be plotted so that the height of each node is proportional\n",
      "to the value of the intergroup dissimilarity between its two daughters. The\n",
      "terminal nodes representing individual observations are all plotted at zero\n",
      "height. This type of graphical display is called a dendrogram .\n",
      "A dendrogram provides a highly interpretable complete description of\n",
      "the hierarchical clustering in a graphical format. This is one of the main\n",
      "reasons for the popularity of hierarchical clustering methods.\n",
      "For the microarray data, Figure 14.12 shows the dendrogram resulting\n",
      "from agglomerative clustering with average linkage; agglomerative cl uster-\n",
      "ing and this example are discussed in more detail later in this chapter.\n",
      "Cutting the dendrogram horizontally at a particular height partitions the\n",
      "data into disjoint clusters represented by the vertical lines that intersect\n",
      "it. These are the clusters that would be produced by terminating the pro-\n",
      "cedure when the optimal intergroup dissimilarity exceeds that threshold\n",
      "cut value. Groups that merge at high values, relative to the merger values\n",
      "of the subgroups contained within them lower in the tree, are candidates\n",
      "for natural clusters. Note that this may occur at several diﬀerent levels,\n",
      "indicating a clustering hierarchy: that is, clusters nested within clusters.\n",
      "Such a dendrogram is often viewed as a graphical summary of the data\n",
      "itself, rather than a description of the results of the algorithm. However,\n",
      "such interpretations should be treated with caution. First, diﬀerent hierar-\n",
      "chical methods (see below), as well as small changes in the data, can lead\n",
      "to quite diﬀerent dendrograms. Also, such a summary will be valid only to\n",
      "the extent that the pairwise observation dissimilarities possess the hierar-\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "522 14. Unsupervised Learning\n",
      "CNSCNSCNSRENAL\n",
      "BREASTCNSCNS\n",
      "BREASTNSCLC\n",
      "NSCLCRENAL\n",
      "RENALRENALRENAL\n",
      "RENALRENALRENALBREAST\n",
      "NSCLCRENAL\n",
      "UNKNOWN\n",
      "OVARIAN\n",
      "MELANOMA\n",
      "PROSTATEOVARIANOVARIAN\n",
      "OVARIANOVARIAN\n",
      "OVARIAN\n",
      "PROSTATENSCLCNSCLCNSCLCLEUKEMIAK562B-reproK562A-reproLEUKEMIA\n",
      "LEUKEMIALEUKEMIALEUKEMIALEUKEMIA\n",
      "COLONCOLON\n",
      "COLON\n",
      "COLONCOLONCOLON\n",
      "COLONMCF7A-repro\n",
      "BREAST\n",
      "MCF7D-reproBREASTNSCLC\n",
      "NSCLCNSCLCMELANOMA\n",
      "BREASTBREAST\n",
      "MELANOMA\n",
      "MELANOMA\n",
      "MELANOMAMELANOMAMELANOMA\n",
      "MELANOMA\n",
      "FIGURE 14.12. Dendrogram from agglomerative hierarchical clustering with\n",
      "average linkage to the human tumor microarray data.\n",
      "chical structure produced by the algorithm. Hierarchical methods impose\n",
      "hierarchical structure whether or not such structure actually exists in the\n",
      "data.\n",
      "The extent to which the hierarchical structure produced by a dendro-\n",
      "gram actually represents the data itself can be judged by the cophenetic\n",
      "correlation coeﬃcient . This is the correlation between the N(N−1)/2 pair-\n",
      "wise observation dissimilarities dii′input to the algorithm and their corre-\n",
      "sponding cophenetic dissimilarities Cii′derived from the dendrogram. The\n",
      "cophenetic dissimilarity Cii′between two observations ( i,i′) is the inter-\n",
      "group dissimilarity at which observations iandi′are ﬁrst joined together\n",
      "in the same cluster.\n",
      "The cophenetic dissimilarity is a very restrictive dissimilarity measure.\n",
      "First, the Cii′over the observations must contain many ties, since only N−1\n",
      "of the total N(N−1)/2 values can be distinct. Also these dissimilarities\n",
      "obey the ultrametric inequality\n",
      "Cii′≤max{Cik,Ci′k} (14.40)\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 523\n",
      "for any three observations ( i,i′,k). As a geometric example, suppose the\n",
      "data were represented as points in a Euclidean coordinate system. In order\n",
      "for the set of interpoint distances over the data to conform to (14.40), the\n",
      "triangles formed by all triples of points must be isosceles triangles wit h the\n",
      "unequal length no longer than the length of the two equal sides (Jain and\n",
      "Dubes, 1988). Therefore it is unrealistic to expect general dissimilarities\n",
      "over arbitrary data sets to closely resemble their corresponding cophenetic\n",
      "dissimilarities as calculated from a dendrogram, especially if there are not\n",
      "many tied values. Thus the dendrogram should be viewed mainly as a de-\n",
      "scription of the clustering structure of the data as imposed by the particular\n",
      "algorithm employed.\n",
      "Agglomerative Clustering\n",
      "Agglomerative clustering algorithms begin with every observation repre-\n",
      "senting a singleton cluster. At each of the N−1 steps the closest two (least\n",
      "dissimilar) clusters are merged into a single cluster, producing one less clus-\n",
      "ter at the next higher level. Therefore, a measure of dissimilarity between\n",
      "two clusters (groups of observations) must be deﬁned.\n",
      "LetGandHrepresent two such groups. The dissimilarity d(G,H) be-\n",
      "tween GandHis computed from the set of pairwise observation dissim-\n",
      "ilarities dii′where one member of the pair iis inGand the other i′is\n",
      "inH.Single linkage (SL) agglomerative clustering takes the intergroup\n",
      "dissimilarity to be that of the closest (least dissimilar) pair\n",
      "dSL(G,H) = min\n",
      "i∈G\n",
      "i′∈Hdii′. (14.41)\n",
      "This is also often called the nearest-neighbor technique. Complete linkage\n",
      "(CL) agglomerative clustering ( furthest-neighbor technique) takes the in-\n",
      "tergroup dissimilarity to be that of the furthest (most dissimilar) pai r\n",
      "dCL(G,H) = max\n",
      "i∈G\n",
      "i′∈Hdii′. (14.42)\n",
      "Group average (GA) clustering uses the average dissimilarity between the\n",
      "groups\n",
      "dGA(G,H) =1\n",
      "NGNH∑\n",
      "i∈G∑\n",
      "i′∈Hdii′ (14.43)\n",
      "where NGandNHare the respective number of observations in each group.\n",
      "Although there have been many other proposals for deﬁning intergroup\n",
      "dissimilarity in the context of agglomerative clustering, the above thr ee are\n",
      "the ones most commonly used. Figure 14.13 shows examples of all three.\n",
      "If the data dissimilarities {dii′}exhibit a strong clustering tendency, with\n",
      "each of the clusters being compact and well separated from others, then all\n",
      "three methods produce similar results. Clusters are compact if all of the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "524 14. Unsupervised Learning\n",
      "Average Linkage Complete Linkage Single Linkage\n",
      "FIGURE 14.13. Dendrograms from agglomerative hierarchical clustering of h u-\n",
      "man tumor microarray data.\n",
      "observations within them are relatively close together (small dissimilar ities)\n",
      "as compared with observations in diﬀerent clusters. To the extent this is\n",
      "not the case, results will diﬀer.\n",
      "Single linkage (14.41) only requires that a single dissimilarity dii′,i∈G\n",
      "andi′∈H, be small for two groups GandHto be considered close\n",
      "together, irrespective of the other observation dissimilarities between the\n",
      "groups. It will therefore have a tendency to combine, at relatively low\n",
      "thresholds, observations linked by a series of close intermediate observa-\n",
      "tions. This phenomenon, referred to as chaining , is often considered a de-\n",
      "fect of the method. The clusters produced by single linkage can violate the\n",
      "“compactness” property that all observations within each cluster tend to\n",
      "be similar to one another, based on the supplied observation dissimilari-\n",
      "ties{dii′}. If we deﬁne the diameter DGof a group of observations as the\n",
      "largest dissimilarity among its members\n",
      "DG= max\n",
      "i∈G\n",
      "i′∈Gdii′, (14.44)\n",
      "then single linkage can produce clusters with very large diameters.\n",
      "Complete linkage (14.42) represents the opposite extreme. Two groups\n",
      "GandHare considered close only if all of the observations in their union\n",
      "are relatively similar. It will tend to produce compact clusters with small\n",
      "diameters (14.44). However, it can produce clusters that violate the “close-\n",
      "ness” property. That is, observations assigned to a cluster can be much\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 525\n",
      "closer to members of other clusters than they are to some members of their\n",
      "own cluster.\n",
      "Group average clustering (14.43) represents a compromise between the\n",
      "two extremes of single and complete linkage. It attempts to produce rel-\n",
      "atively compact clusters that are relatively far apart. However, its results\n",
      "depend on the numerical scale on which the observation dissimilarities dii′\n",
      "are measured. Applying a monotone strictly increasing transformation h(≤)\n",
      "to the dii′,hii′=h(dii′), can change the result produced by (14.43). In\n",
      "contrast, (14.41) and (14.42) depend only on the ordering of the dii′and\n",
      "are thus invariant to such monotone transformations. This invariance is\n",
      "often used as an argument in favor of single or complete linkage over group\n",
      "average methods.\n",
      "One can argue that group average clustering has a statistical consis-\n",
      "tency property violated by single and complete linkage. Assume we have\n",
      "attribute-value data XT= (X1,... ,X p) and that each cluster kis a ran-\n",
      "dom sample from some population joint density pk(x). The complete data\n",
      "set is a random sample from a mixture of Ksuch densities. The group\n",
      "average dissimilarity dGA(G,H) (14.43) is an estimate of\n",
      "∫ ∫\n",
      "d(x,x′)pG(x)pH(x′)dx dx′, (14.45)\n",
      "where d(x,x′) is the dissimilarity between points xandx′in the space\n",
      "of attribute values. As the sample size Napproaches inﬁnity dGA(G,H)\n",
      "(14.43) approaches (14.45), which is a characteristic of the relationshi p\n",
      "between the two densities pG(x) and pH(x). For single linkage, dSL(G,H)\n",
      "(14.41) approaches zero as N→ ∞ independent of pG(x) and pH(x). For\n",
      "complete linkage, dCL(G,H) (14.42) becomes inﬁnite as N→ ∞, again\n",
      "independent of the two densities. Thus, it is not clear what aspects of the\n",
      "population distribution are being estimated by dSL(G,H) and dCL(G,H).\n",
      "Example: Human Cancer Microarray Data (Continued)\n",
      "The left panel of Figure 14.13 shows the dendrogram resulting from average\n",
      "linkage agglomerative clustering of the samples (columns) of the microarra y\n",
      "data. The middle and right panels show the result using complete and single\n",
      "linkage. Average and complete linkage gave similar results, while single\n",
      "linkage produced unbalanced groups with long thin clusters. We focus on\n",
      "the average linkage clustering.\n",
      "LikeK-means clustering, hierarchical clustering is successful at clustering\n",
      "simple cancers together. However it has other nice features. By cutting oﬀ\n",
      "the dendrogram at various heights, diﬀerent numbers of clusters emerge,\n",
      "and the sets of clusters are nested within one another. Secondly, it gives\n",
      "some partial ordering information about the samples. In Figure 14.14, w e\n",
      "have arranged the genes (rows) and samples (columns) of the expression\n",
      "matrix in orderings derived from hierarchical clustering.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "526 14. Unsupervised Learning\n",
      "Note that if we ﬂip the orientation of the branches of a dendrogram at any\n",
      "merge, the resulting dendrogram is still consistent with the series of hierar-\n",
      "chical clustering operations. Hence to determine an ordering of the leaves,\n",
      "we must add a constraint. To produce the row ordering of Figure 14.14,\n",
      "we have used the default rule in S-PLUS: at each merge, the subtree with\n",
      "the tighter cluster is placed to the left (toward the bottom in the rotated\n",
      "dendrogram in the ﬁgure.) Individual genes are the tightest clusters possi-\n",
      "ble, and merges involving two individual genes place them in order by their\n",
      "observation number. The same rule was used for the columns. Many other\n",
      "rules are possible—for example, ordering by a multidimensional scaling of\n",
      "the genes; see Section 14.8.\n",
      "The two-way rearrangement of Figure14.14 produces an informative pic-\n",
      "ture of the genes and samples. This picture is more informative than the\n",
      "randomly ordered rows and columns of Figure 1.3 of Chapter 1. Further-\n",
      "more, the dendrograms themselves are useful, as biologists can, for example,\n",
      "interpret the gene clusters in terms of biological processes.\n",
      "Divisive Clustering\n",
      "Divisive clustering algorithms begin with the entire data set as a single\n",
      "cluster, and recursively divide one of the existing clusters into two daugh-\n",
      "ter clusters at each iteration in a top-down fashion. This approach has not\n",
      "been studied nearly as extensively as agglomerative methods in the cluster-\n",
      "ing literature. It has been explored somewhat in the engineering literature\n",
      "(Gersho and Gray, 1992) in the context of compression. In the clustering\n",
      "setting, a potential advantage of divisive over agglomerative methods can\n",
      "occur when interest is focused on partitioning the data into a relatively\n",
      "small number of clusters.\n",
      "The divisive paradigm can be employed by recursively applying any of\n",
      "the combinatorial methods such as K-means (Section 14.3.6) or K-medoids\n",
      "(Section 14.3.10), with K= 2, to perform the splits at each iteration. How-\n",
      "ever, such an approach would depend on the starting conﬁguration speciﬁed\n",
      "at each step. In addition, it would not necessarily produce a splitting se-\n",
      "quence that possesses the monotonicity property required for dendrogram\n",
      "representation.\n",
      "A divisive algorithm that avoids these problems was proposed by Mac-\n",
      "naughton Smith et al. (1965). It begins by placing all observations in a\n",
      "single cluster G. It then chooses that observation whose average dissimi-\n",
      "larity from all the other observations is largest. This observation for ms the\n",
      "ﬁrst member of a second cluster H. At each successive step that observation\n",
      "inGwhose average distance from those in H, minus that for the remaining\n",
      "observations in Gis largest, is transferred to H. This continues until the\n",
      "corresponding diﬀerence in averages becomes negative. That is, there are\n",
      "no longer any observations in Gthat are, on average, closer to those in\n",
      "H. The result is a split of the original cluster into two daughter clusters,\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.3 Cluster Analysis 527\n",
      "FIGURE 14.14. DNA microarray data: average linkage hierarchical clusteri ng\n",
      "has been applied independently to the rows (genes) and columns (sam ples), de-\n",
      "termining the ordering of the rows and columns (see text). The color s range from\n",
      "bright green (negative, under-expressed) to bright red (posit ive, over-expressed).\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "528 14. Unsupervised Learning\n",
      "the observations transferred to H, and those remaining in G. These two\n",
      "clusters represent the second level of the hierarchy. Each successive level\n",
      "is produced by applying this splitting procedure to one of the clusters at\n",
      "the previous level. Kaufman and Rousseeuw (1990) suggest choosing the\n",
      "cluster at each level with the largest diameter (14.44) for splitting. An al -\n",
      "ternative would be to choose the one with the largest average dissimilar ity\n",
      "among its members\n",
      "¯dG=1\n",
      "NG∑\n",
      "i∈G∑\n",
      "i′∈Gdii′.\n",
      "The recursive splitting continues until all clusters either become singletons\n",
      "or all members of each one have zero dissimilarity from one another.\n",
      "14.4 Self-Organizing Maps\n",
      "This method can be viewed as a constrained version of K-means clustering,\n",
      "in which the prototypes are encouraged to lie in a one- or two-dimensional\n",
      "manifold in the feature space. The resulting manifold is also referred to\n",
      "as aconstrained topological map , since the original high-dimensional obser-\n",
      "vations can be mapped down onto the two-dimensional coordinate system.\n",
      "The original SOM algorithm was online—observations are processed one at\n",
      "a time—and later a batch version was proposed. The technique also bears\n",
      "a close relationship to principal curves and surfaces , which are discussed in\n",
      "the next section.\n",
      "We consider a SOM with a two-dimensional rectangular grid of Kproto-\n",
      "types mj∈IRp(other choices, such as hexagonal grids, can also be used).\n",
      "Each of the Kprototypes are parametrized with respect to an integer\n",
      "coordinate pair ℓj∈ Q1× Q2. Here Q1={1,2,... ,q 1}, similarly Q2, and\n",
      "K=q1≤q2. Themjare initialized, for example, to lie in the two-dimensional\n",
      "principal component plane of the data (next section). We can think of the\n",
      "prototypes as “buttons,” “sewn” on the principal component plane in a\n",
      "regular pattern. The SOM procedure tries to bend the plane so that the\n",
      "buttons approximate the data points as well as possible. Once the model is\n",
      "ﬁt, the observations can be mapped down onto the two-dimensional grid.\n",
      "The observations xiare processed one at a time. We ﬁnd the closest\n",
      "prototype mjtoxiin Euclidean distance in IRp, and then for all neighbors\n",
      "mkofmj, move mktoward xivia the update\n",
      "mk←mk+α(xi−mk). (14.46)\n",
      "The “neighbors” of mjare deﬁned to be all mksuch that the distance\n",
      "between ℓjandℓkis small. The simplest approach uses Euclidean distance,\n",
      "and “small” is determined by a threshold r. This neighborhood always\n",
      "includes the closest prototype mjitself.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.4 Self-Organizing Maps 529\n",
      "Notice that distance is deﬁned in the space Q1×Q2of integer topological\n",
      "coordinates of the prototypes, rather than in the feature space IRp. The\n",
      "eﬀect of the update (14.46) is to move the prototypes closer to the data,\n",
      "but also to maintain a smooth two-dimensional spatial relationship between\n",
      "the prototypes.\n",
      "The performance of the SOM algorithm depends on the learning rate\n",
      "αand the distance threshold r. Typically αis decreased from say 1 .0 to\n",
      "0.0 over a few thousand iterations (one per observation). Similarly ris\n",
      "decreased linearly from starting value Rto 1 over a few thousand iterations.\n",
      "We illustrate a method for choosing Rin the example below.\n",
      "We have described the simplest version of the SOM. More sophisticated\n",
      "versions modify the update step according to distance:\n",
      "mk←mk+αh(∥ℓj−ℓk∥)(xi−mk), (14.47)\n",
      "where the neighborhood function hgives more weight to prototypes mkwith\n",
      "indices ℓkcloser to ℓjthan to those further away.\n",
      "If we take the distance rsmall enough so that each neighborhood contains\n",
      "only one point, then the spatial connection between prototypes is lost. In\n",
      "that case one can show that the SOM algorithm is an online version of\n",
      "K-means clustering, and eventually stabilizes at one of the local minima\n",
      "found by K-means. Since the SOM is a constrained version of K-means\n",
      "clustering, it is important to check whether the constraint is reasonable\n",
      "in any given problem. One can do this by computing the reconstruction\n",
      "error∥x−mj∥2, summed over observations, for both methods. This will\n",
      "necessarily be smaller for K-means, but should not be much smaller if the\n",
      "SOM is a reasonable approximation.\n",
      "As an illustrative example, we generated 90 data points in three dimen-\n",
      "sions, near the surface of a half sphere of radius 1. The points were in each\n",
      "of three clusters—red, green, and blue—located near (0 ,1,0), (0,0,1) and\n",
      "(1,0,0). The data are shown in Figure 14.15\n",
      "By design, the red cluster was much tighter than the green or blue ones.\n",
      "(Full details of the data generation are given in Exercise 14.5.) A 5 ×5 grid\n",
      "of prototypes was used, with initial grid size R= 2; this meant that about\n",
      "a third of the prototypes were initially in each neighborhood. We did a\n",
      "total of 40 passes through the dataset of 90 observations, and let randα\n",
      "decrease linearly over the 3600 iterations.\n",
      "In Figure 14.16 the prototypes are indicated by circles, and the points\n",
      "that project to each prototype are plotted randomly within the correspond-\n",
      "ing circle. The left panel shows the initial conﬁguration, while the right\n",
      "panel shows the ﬁnal one. The algorithm has succeeded in separating the\n",
      "clusters; however, the separation of the red cluster indicates that the man-\n",
      "ifold has folded back on itself (see Figure 14.17). Since the distances in the\n",
      "two-dimensional display are not used, there is little indication in the SOM\n",
      "projection that the red cluster is tighter than the others.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "530 14. Unsupervised Learning\n",
      "−1−0.500.511.5\n",
      "−1−0.500.511.5−1−0.500.511.5\n",
      "FIGURE 14.15. Simulated data in three classes, near the surface of a half–\n",
      "sphere.\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•••••\n",
      "••••••••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••••\n",
      "••••\n",
      "•• ••• ••\n",
      "••\n",
      "••• •\n",
      "••••\n",
      "•••••\n",
      "••\n",
      "••••\n",
      "••• •\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•••\n",
      "••\n",
      "1 2 3 4 512345\n",
      "••••\n",
      "••\n",
      "•\n",
      "••\n",
      "••••••\n",
      "•\n",
      "••\n",
      "••••\n",
      "•\n",
      "•••••\n",
      "••\n",
      "• • • •••\n",
      "••\n",
      "•\n",
      "••••\n",
      "•••••\n",
      "•\n",
      "•••\n",
      "•• •• ••\n",
      "• •\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•• •\n",
      "•••\n",
      "• • ••\n",
      "•••\n",
      "1 2 3 4 512345\n",
      "FIGURE 14.16. Self-organizing map applied to half-sphere data example. Left\n",
      "panel is the initial conﬁguration, right panel the ﬁnal one. The 5×5grid of\n",
      "prototypes are indicated by circles, and the points that projec t to each prototype\n",
      "are plotted randomly within the corresponding circle.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.4 Self-Organizing Maps 531\n",
      "FIGURE 14.17. Wiremesh representation of the ﬁtted SOM model in I R3. The\n",
      "lines represent the horizontal and vertical edges of the topolog ical lattice. The\n",
      "double lines indicate that the surface was folded diagonally ba ck on itself in order\n",
      "to model the red points. The cluster members have been jittere d to indicate their\n",
      "color, and the purple points are the node centers.\n",
      "Figure 14.18 shows the reconstruction error, equal to the total sum of\n",
      "squares of each data point around its prototype. For comparison we carried\n",
      "out aK-means clustering with 25 centroids, and indicate its reconstruction\n",
      "error by the horizontal line on the graph. We see that the SOM signiﬁcantly\n",
      "decreases the error, nearly to the level of the K-means solution. This pro-\n",
      "vides evidence that the two-dimensional constraint used by the SOM is\n",
      "reasonable for this particular dataset.\n",
      "In the batch version of the SOM, we update each mjvia\n",
      "mj=∑wkxk∑wk. (14.48)\n",
      "The sum is over points xkthat mapped (i.e., were closest to) neighbors mk\n",
      "ofmj. The weight function may be rectangular, that is, equal to 1 for the\n",
      "neighbors of mk, or may decrease smoothly with distance ∥ℓk−ℓj∥as before.\n",
      "If the neighborhood size is chosen small enough so that it consists only\n",
      "ofmk, with rectangular weights, this reduces to the K-means clustering\n",
      "procedure described earlier. It can also be thought of as a discrete version\n",
      "of principal curves and surfaces, described in Section 14.5.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "532 14. Unsupervised Learning\n",
      "IterationReconstruction Error\n",
      "0 500 1000 1500 2000 25000 10 20 30 40 50••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••••\n",
      "•••••••••••••••••••••••••••••••••••••\n",
      "FIGURE 14.18. Half-sphere data: reconstruction error for the SOM as a func-\n",
      "tion of iteration. Error for k-means clustering is indicated by the horizontal line.\n",
      "Example: Document Organization and Retrieval\n",
      "Document retrieval has gained importance with the rapid development of\n",
      "the Internet and the Web, and SOMs have proved to be useful for organiz-\n",
      "ing and indexing large corpora. This example is taken from the WEBSOM\n",
      "homepage http://websom.hut.fi/ (Kohonen et al., 2000). Figure 14.19 rep-\n",
      "resents a SOM ﬁt to 12,088 newsgroup comp.ai.neural-nets articles. The\n",
      "labels are generated automatically by the WEBSOM software and provide\n",
      "a guide as to the typical content of a node.\n",
      "In applications such as this, the documents have to be reprocessed in\n",
      "order to create a feature vector. A term-document matrix is created, where\n",
      "each row represents a single document. The entries in each row are the\n",
      "relative frequency of each of a predeﬁned set of terms. These terms could\n",
      "be a large set of dictionary entries (50,000 words), or an even larger set\n",
      "of bigrams (word pairs), or subsets of these. These matrices are typically\n",
      "very sparse, and so often some preprocessing is done to reduce the number\n",
      "of features (columns). Sometimes the SVD (next section) is used to reduce\n",
      "the matrix; Kohonen et al. (2000) use a randomized variant thereof. These\n",
      "reduced vectors are then the input to the SOM.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.4 Self-Organizing Maps 533\n",
      "FIGURE 14.19. Heatmap representation of the SOM model ﬁt to a corpus\n",
      "of12,088 newsgroup comp.ai.neural-nets contributions (courtesy WEBSOM\n",
      "homepage). The lighter areas indicate higher-density areas. Populated nodes are\n",
      "automatically labeled according to typical content.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "534 14. Unsupervised Learning\n",
      "•••\n",
      "•\n",
      "••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "• ••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "• ••\n",
      "•v1v1v1v1v1v1v1v1\n",
      "ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1ui1d1\n",
      "xixixixixixixixi\n",
      "FIGURE 14.20. The ﬁrst linear principal component of a set of data. The line\n",
      "minimizes the total squared distance from each point to its orth ogonal projection\n",
      "onto the line.\n",
      "In this application the authors have developed a “zoom” feature, which\n",
      "allows one to interact with the map in order to get more detail. The ﬁnal\n",
      "level of zooming retrieves the actual news articles, which can then be read.\n",
      "14.5 Principal Components, Curves and Surfaces\n",
      "Principal components are discussed in Sections 3.4.1, where they shed light\n",
      "on the shrinkage mechanism of ridge regression. Principal components are\n",
      "a sequence of projections of the data, mutually uncorrelated and ordered\n",
      "in variance. In the next section we present principal components as linear\n",
      "manifolds approximating a set of Npoints xi∈IRp. We then present\n",
      "some nonlinear generalizations in Section 14.5.2. Other recent proposals\n",
      "for nonlinear approximating manifolds are discussed in Section 14.9.\n",
      "14.5.1 Principal Components\n",
      "The principal components of a set of data in IRpprovide a sequence of best\n",
      "linear approximations to that data, of all ranks q≤p.\n",
      "Denote the observations by x1,x2,... ,x N, and consider the rank- qlinear\n",
      "model for representing them\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.5 Principal Components, Curves and Surfaces 535\n",
      "f(λ) =θ+Vqλ, (14.49)\n",
      "where θis a location vector in IRp,Vqis ap×qmatrix with qorthogonal\n",
      "unit vectors as columns, and λis aqvector of parameters. This is the\n",
      "parametric representation of an aﬃne hyperplane of rank q. Figures 14.20\n",
      "and 14.21 illustrate for q= 1 and q= 2, respectively. Fitting such a model\n",
      "to the data by least squares amounts to minimizing the reconstruction error\n",
      "min\n",
      "θ,{λi},VqN∑\n",
      "i=1∥xi−θ−Vqλi∥2. (14.50)\n",
      "We can partially optimize for θand the λi(Exercise 14.7) to obtain\n",
      "ˆθ= ¯x, (14.51)\n",
      "ˆλi=VT\n",
      "q(xi−¯x). (14.52)\n",
      "This leaves us to ﬁnd the orthogonal matrix Vq:\n",
      "min\n",
      "VqN∑\n",
      "i=1||(xi−¯x)−VqVT\n",
      "q(xi−¯x)||2. (14.53)\n",
      "For convenience we assume that ¯ x= 0 (otherwise we simply replace the\n",
      "observations by their centered versions ˜ xi=xi−¯x). The p×pmatrix\n",
      "Hq=VqVT\n",
      "qis aprojection matrix , and maps each point xionto its rank-\n",
      "qreconstruction Hqxi, the orthogonal projection of xionto the subspace\n",
      "spanned by the columns of Vq. The solution can be expressed as follows.\n",
      "Stack the (centered) observations into the rows of an N×pmatrix X. We\n",
      "construct the singular value decomposition ofX:\n",
      "X=UDVT. (14.54)\n",
      "This is a standard decomposition in numerical analysis, and many algo-\n",
      "rithms exist for its computation (Golub and Van Loan, 1983, for example).\n",
      "HereUis anN×porthogonal matrix ( UTU=Ip) whose columns ujare\n",
      "called the left singular vectors ;Vis ap×porthogonal matrix ( VTV=Ip)\n",
      "with columns vjcalled the right singular vectors , andDis ap×pdiagonal\n",
      "matrix, with diagonal elements d1≥d2≥ ≤≤≤ ≥ dp≥0 known as the sin-\n",
      "gular values . For each rank q, the solution Vqto (14.53) consists of the ﬁrst\n",
      "qcolumns of V. The columns of UDare called the principal components\n",
      "ofX(see Section 3.5.1). The Noptimal ˆλiin (14.52) are given by the ﬁrst\n",
      "qprincipal components (the Nrows of the N×qmatrix UqDq).\n",
      "The one-dimensional principal component line in IR2is illustrated in Fig-\n",
      "ure 14.20. For each data point xi, there is a closest point on the line, given\n",
      "byui1d1v1. Here v1is the direction of the line and ˆλi=ui1d1measures\n",
      "distance along the line from the origin. Similarly Figure 14.21 shows the\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "536 14. Unsupervised Learning\n",
      "First principal componentSecond principal component\n",
      "−1.0 −0.5 0.0 0.5 1.0−1.0 −0.5 0.0 0.5 1.0•\n",
      "•••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•\n",
      "•••\n",
      "••••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "••••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "FIGURE 14.21. The best rank-two linear approximation to the half-sphere data .\n",
      "The right panel shows the projected points with coordinates giv en byU2D2, the\n",
      "ﬁrst two principal components of the data.\n",
      "two-dimensional principal component surface ﬁt to the half-sphere data\n",
      "(left panel). The right panel shows the projection of the data onto the\n",
      "ﬁrst two principal components. This projection was the basis for the initial\n",
      "conﬁguration for the SOM method shown earlier. The procedure is quite\n",
      "successful at separating the clusters. Since the half-sphere is nonlinear, a\n",
      "nonlinear projection will do a better job, and this is the topic of the next\n",
      "section.\n",
      "Principal components have many other nice properties, for example, the\n",
      "linear combination Xv1has the highest variance among all linear com-\n",
      "binations of the features; Xv2has the highest variance among all linear\n",
      "combinations satisfying v2orthogonal to v1, and so on.\n",
      "Example: Handwritten Digits\n",
      "Principal components are a useful tool for dimension reduction and com-\n",
      "pression. We illustrate this feature on the handwritten digits data described\n",
      "in Chapter 1. Figure 14.22 shows a sample of 130 handwritten 3’s, each a\n",
      "digitized 16 ×16 grayscale image, from a total of 658 such 3’s. We see\n",
      "considerable variation in writing styles, character thickness and orienta-\n",
      "tion. We consider these images as points xiin IR256, and compute their\n",
      "principal components via the SVD (14.54).\n",
      "Figure 14.23 shows the ﬁrst two principal components of these data. For\n",
      "each of these ﬁrst two principal components ui1andui2, we computed the\n",
      "5%, 25%, 50%, 75% and 95% quantile points, and used them to deﬁne\n",
      "the rectangular grid superimposed on the plot. The circled points indicate\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "14.5 Principal Components, Curves and Surfaces 537\n",
      "FIGURE 14.22. A sample of 130handwritten 3’s shows a variety of writing\n",
      "styles.\n",
      "those images close to the vertices of the grid, where the distance measure\n",
      "focuses mainly on these projected coordinates, but gives some weight to the\n",
      "components in the orthogonal subspace. The right plot shows the images\n",
      "corresponding to these circled points. This allows us to visualize the nature\n",
      "of the ﬁrst two principal components. We see that the v1(horizontal move-\n",
      "ment) mainly accounts for the lengthening of the lower tail of the three,\n",
      "while v2(vertical movement) accounts for character thickness. In terms of\n",
      "the parametrized model (14.49), this two-component model has the form\n",
      "ˆf(λ) = ¯ x+λ1v1+λ2v2\n",
      "= +λ1≤ +λ2≤ . (14.55)\n",
      "Here we have displayed the ﬁrst two principal component directions, v1\n",
      "andv2, as images. Although there are a possible 256 principal components,\n",
      "approximately 50 account for 90% of the variation in the threes, 12 ac-\n",
      "count for 63%. Figure 14.24 compares the singular values to those obtained\n",
      "for equivalent uncorrelated data, obtained by randomly scrambling each\n",
      "column of X. The pixels in a digitized image are inherently correlated,\n",
      "and since these are all the same digit the correlations are even stronger.\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Generate a numbered list of 3 hypothetical questions that the below document could be used to answer:\n",
      "\n",
      "538 14. Unsupervised Learning\n",
      "First Principal ComponentSecond Principal Component\n",
      "-6 -4 -2 0 2 4 6 8-5 0 5••\n",
      "••\n",
      "••\n",
      "••\n",
      "• ••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "• •\n",
      "••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "•••\n",
      "•••••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•\n",
      "••••••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "•\n",
      "•••••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "•\n",
      "•• •\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•••\n",
      "•\n",
      "•••••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•••••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "•• ••\n",
      "••••\n",
      "••\n",
      "••\n",
      "•••\n",
      "••\n",
      "•\n",
      "•••\n",
      "•••\n",
      "•\n",
      "•\n",
      "•\n",
      "•• •\n",
      "•\n",
      "•••\n",
      "• •• ••\n",
      "••••\n",
      "•\n",
      "•\n",
      "••\n",
      "•• •\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "• •••\n",
      "•\n",
      "•\n",
      "•••\n",
      "••••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "••\n",
      "•\n",
      "• ••\n",
      "•••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "••••\n",
      "••\n",
      "• ••\n",
      "••\n",
      "•••••\n",
      "•\n",
      "•\n",
      "••••\n",
      "•\n",
      "••••\n",
      "•\n",
      "••\n",
      "••\n",
      "•\n",
      "• •••\n",
      "••\n",
      "••••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•\n",
      "••\n",
      "••••\n",
      "••••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•• •\n",
      "•\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•\n",
      "•••\n",
      "•••••\n",
      "••••\n",
      "•• •\n",
      "•\n",
      "•••\n",
      "•\n",
      "••\n",
      "••\n",
      "•••\n",
      "•\n",
      "••\n",
      "•• ••\n",
      "••\n",
      "••\n",
      "••••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "•\n",
      "•\n",
      "•••\n",
      "•\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••\n",
      "•\n",
      "•\n",
      "••••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•••• ••\n",
      "••\n",
      "••••\n",
      "••\n",
      "•••\n",
      "••••\n",
      "•\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "•••\n",
      "••\n",
      "••\n",
      "••••\n",
      "•\n",
      "•\n",
      "•••••\n",
      "•••\n",
      "••\n",
      "••••\n",
      "•••••\n",
      "••\n",
      "•••\n",
      "••\n",
      "••\n",
      "••••\n",
      "•\n",
      "••\n",
      "•• •••\n",
      "•••\n",
      "••\n",
      "•\n",
      "••••\n",
      "••• ••\n",
      "•\n",
      "••\n",
      "•••\n",
      "•••\n",
      "•••\n",
      "•\n",
      "•\n",
      "•••\n",
      "•••••\n",
      "••\n",
      "•\n",
      "••\n",
      "••\n",
      "••\n",
      "•\n",
      "••••••\n",
      "••\n",
      "•\n",
      "••\n",
      "•\n",
      "••\n",
      "O O O OOO O OO\n",
      "OOO\n",
      "OO OO O O OOO O OOO\n",
      "FIGURE 14.23. (Left panel:) the ﬁrst two principal components of the hand-\n",
      "written threes. The circled points are the closest projected images to the vertices\n",
      "of a grid, deﬁned by the marginal quantiles of the principal compone nts. (Right\n",
      "panel:) The images corresponding to the circled points. These sh ow the nature of\n",
      "the ﬁrst two principal components.\n",
      "DimensionSingular Values\n",
      "0 50 100 150 200 2500 20 40 60 80•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••\n",
      "•••\n",
      "•\n",
      "••\n",
      "••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••Real Trace\n",
      "•Randomized Trace\n",
      "FIGURE 14.24. The256singular values for the digitized threes, compared to\n",
      "those for a randomized version of the data (each column of Xwas scrambled).\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Sorry! We've encountered an issue with repetitive patterns in your prompt. Please try again with a different prompt.\", 'type': 'invalid_request_error', 'param': 'prompt', 'code': 'invalid_prompt'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m question_docs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sl_data):\n\u001b[1;32m----> 5\u001b[0m     result \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39mrun(doc\u001b[38;5;241m.\u001b[39mpage_content)\n\u001b[0;32m      6\u001b[0m     question_docs\u001b[38;5;241m.\u001b[39mextend([\n\u001b[0;32m      7\u001b[0m         Document(\n\u001b[0;32m      8\u001b[0m             page_content\u001b[38;5;241m=\u001b[39ms,\n\u001b[0;32m      9\u001b[0m             metadata\u001b[38;5;241m=\u001b[39m{id_key: doc_ids[i]}\n\u001b[0;32m     10\u001b[0m         ) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m result\n\u001b[0;32m     11\u001b[0m     ])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     emit_warning()\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\chains\\base.py:598\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    597\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    599\u001b[0m         _output_key\n\u001b[0;32m    600\u001b[0m     ]\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    604\u001b[0m         _output_key\n\u001b[0;32m    605\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:170\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     emit_warning()\n\u001b[1;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\chains\\base.py:381\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    379\u001b[0m }\n\u001b[1;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m    382\u001b[0m     inputs,\n\u001b[0;32m    383\u001b[0m     cast(RunnableConfig, {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}),\n\u001b[0;32m    384\u001b[0m     return_only_outputs\u001b[38;5;241m=\u001b[39mreturn_only_outputs,\n\u001b[0;32m    385\u001b[0m     include_run_info\u001b[38;5;241m=\u001b[39minclude_run_info,\n\u001b[0;32m    386\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\chains\\base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\chains\\llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain\\chains\\llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    139\u001b[0m         prompts,\n\u001b[0;32m    140\u001b[0m         stop,\n\u001b[0;32m    141\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    147\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:776\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    770\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    774\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    775\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    632\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 633\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    634\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    635\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    636\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    637\u001b[0m ]\n\u001b[0;32m    638\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:623\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    622\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 623\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    624\u001b[0m                 m,\n\u001b[0;32m    625\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    626\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    627\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    628\u001b[0m             )\n\u001b[0;32m    629\u001b[0m         )\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:845\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 845\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    846\u001b[0m             messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    847\u001b[0m         )\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    849\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:441\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    435\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    436\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    440\u001b[0m }\n\u001b[1;32m--> 441\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[0;32m    442\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    443\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:356\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    358\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    360\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\resources\\chat\\completions.py:668\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    667\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    669\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    670\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    671\u001b[0m             {\n\u001b[0;32m    672\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[0;32m    673\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    674\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    675\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[0;32m    676\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[0;32m    677\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    678\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    679\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    680\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    681\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[0;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[0;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[0;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    687\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    688\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[0;32m    689\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    690\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[0;32m    691\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[0;32m    692\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[0;32m    693\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    694\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    695\u001b[0m             },\n\u001b[0;32m    696\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    697\u001b[0m         ),\n\u001b[0;32m    698\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    699\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    700\u001b[0m         ),\n\u001b[0;32m    701\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[0;32m    702\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    703\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[0;32m    704\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1259\u001b[0m     )\n\u001b[1;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    938\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    939\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    940\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    941\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    942\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    943\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\langchain\\Lib\\site-packages\\openai\\_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1038\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1040\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1044\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1045\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1049\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[0;32m   1050\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Sorry! We've encountered an issue with repetitive patterns in your prompt. Please try again with a different prompt.\", 'type': 'invalid_request_error', 'param': 'prompt', 'code': 'invalid_prompt'}}"
     ]
    }
   ],
   "source": [
    "from langchain.schema.document import Document\n",
    "\n",
    "question_docs = []\n",
    "for i, doc in enumerate(sl_data):\n",
    "    result = chain.run(doc.page_content)\n",
    "    question_docs.extend([\n",
    "        Document(\n",
    "            page_content=s,\n",
    "            metadata={id_key: doc_ids[i]}\n",
    "        ) for s in result\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "596ca415-4323-46e2-974e-fd8a83d2bb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '3310723c-02cc-4ba2-a042-6090282c3d54'}, page_content='What are the important ideas and concepts in the field of statistics, data mining, and machine learning?'),\n",
       " Document(metadata={'doc_id': '3310723c-02cc-4ba2-a042-6090282c3d54'}, page_content='What are some of the new topics and methods covered in the second edition of \"The Elements of Statistical Learning\"?'),\n",
       " Document(metadata={'doc_id': '3310723c-02cc-4ba2-a042-6090282c3d54'}, page_content='Who are the authors of \"The Elements of Statistical Learning\" and what are their contributions to the field of statistics and data mining?'),\n",
       " Document(metadata={'doc_id': '7bd1af61-cf27-4221-984f-20cedcab43e1'}, page_content='Who are the parents of Valerie and Patrick Hastie?'),\n",
       " Document(metadata={'doc_id': '7bd1af61-cf27-4221-984f-20cedcab43e1'}, page_content='Who are the parents of Vera and Sami Tibshirani?')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50fd7f21-e45b-449e-80fe-930744554c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.vectorstore.add_documents(question_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, sl_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84e6051f-92d5-4d44-8998-6e5c0bd96d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': 'f671344c-8a29-4c01-ae4f-2fad6d2001e2'}, page_content='How can linear methods be applied to regression problems?'),\n",
       " Document(metadata={'doc_id': 'e9bc5669-0511-4ffe-ae8b-e72dc3fd3f83'}, page_content='What is the formula for the linear regression model?'),\n",
       " Document(metadata={'doc_id': 'f671344c-8a29-4c01-ae4f-2fad6d2001e2'}, page_content='What are some linear methods that can be used for regression analysis?'),\n",
       " Document(metadata={'doc_id': 'f744d282-364e-46fd-a9e0-aace721f6a66'}, page_content='What is the linear model used for prediction?')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.vectorstore.similarity_search(\"What is linear regression?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6363601c-1d59-4609-94d9-30f11397e0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 118}, page_content='100 3. Linear Methods for Regression'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 62}, page_content='44 3. Linear Methods for Regression\\n3.2 Linear Regression Models and Least Squares\\nAs introduced in Chapter 2, we have an input vector XT= (X1,X2,... ,X p),\\nand want to predict a real-valued output Y. The linear regression model\\nhas the form\\nf(X) =β0+p∑\\nj=1Xjβj. (3.1)\\nThe linear model either assumes that the regression function E( Y|X) is\\nlinear, or that the linear model is a reasonable approximation. Here the\\nβj’s are unknown parameters or coeﬃcients, and the variables Xjcan come\\nfrom diﬀerent sources:\\n•quantitative inputs;\\n•transformations of quantitative inputs, such as log, square-root or\\nsquare;\\n•basis expansions, such as X2=X2\\n1,X3=X3\\n1, leading to a polynomial\\nrepresentation;\\n•numeric or “dummy” coding of the levels of qualitative inputs. For\\nexample, if Gis a ﬁve-level factor input, we might create Xj, j=\\n1,... ,5,such that Xj=I(G=j). Together this group of Xjrepre-\\nsents the eﬀect of Gby a set of level-dependent constants, since in∑5\\nj=1Xjβj, one of the Xjs is one, and the others are zero.\\n•interactions between variables, for example, X3=X1≤X2.\\nNo matter the source of the Xj, the model is linear in the parameters.\\nTypically we have a set of training data ( x1,y1)...(xN,yN) from which\\nto estimate the parameters β. Each xi= (xi1,xi2,... ,x ip)Tis a vector\\nof feature measurements for the ith case. The most popular estimation\\nmethod is least squares , in which we pick the coeﬃcients β= (β0,β1,... ,β p)T\\nto minimize the residual sum of squares\\nRSS(β) =N∑\\ni=1(yi−f(xi))2\\n=N∑\\ni=1(\\nyi−β0−p∑\\nj=1xijβj)2\\n. (3.2)\\nFrom a statistical point of view, this criterion is reasonable if the tr aining\\nobservations ( xi,yi) represent independent random draws from their popu-\\nlation. Even if the xi’s were not drawn randomly, the criterion is still valid\\nif the yi’s are conditionally independent given the inputs xi. Figure 3.1\\nillustrates the geometry of least-squares ﬁtting in the IRp+1-dimensional'),\n",
       " Document(metadata={'source': 'mixed_data/element_of_SL.pdf', 'page': 29}, page_content='2.3 Least Squares and Nearest Neighbors 11\\nith observation from the N-vector xjconsisting of all the observations on\\nvariable Xj. Since all vectors are assumed to be column vectors, the ith\\nrow of XisxT\\ni, the vector transpose of xi.\\nFor the moment we can loosely state the learning task as follows: given\\nthe value of an input vector X, make a good prediction of the output Y,\\ndenoted by ˆY(pronounced “y-hat”). If Ytakes values in IR then so should\\nˆY; likewise for categorical outputs, ˆGshould take values in the same set G\\nassociated with G.\\nFor a two-class G, one approach is to denote the binary coded target\\nasY, and then treat it as a quantitative output. The predictions ˆYwill\\ntypically lie in [0 ,1], and we can assign to ˆGthe class label according to\\nwhether ˆ y >0.5. This approach generalizes to K-level qualitative outputs\\nas well.\\nWe need data to construct prediction rules, often a lot of it. We thus\\nsuppose we have available a set of measurements ( xi,yi) or ( xi,gi), i=\\n1,... ,N , known as the training data , with which to construct our prediction\\nrule.\\n2.3 Two Simple Approaches to Prediction: Least\\nSquares and Nearest Neighbors\\nIn this section we develop two simple but powerful prediction methods: the\\nlinear model ﬁt by least squares and the k-nearest-neighbor prediction rule.\\nThe linear model makes huge assumptions about structure and yields stable\\nbut possibly inaccurate predictions. The method of k-nearest neighbors\\nmakes very mild structural assumptions: its predictions are often accurate\\nbut can be unstable.\\n2.3.1 Linear Models and Least Squares\\nThe linear model has been a mainstay of statistics for the past 30 years\\nand remains one of our most important tools. Given a vector of inputs\\nXT= (X1,X2,... ,X p), we predict the output Yvia the model\\nˆY=ˆβ0+p∑\\nj=1Xjˆβj. (2.1)\\nThe term ˆβ0is the intercept, also known as the biasin machine learning.\\nOften it is convenient to include the constant variable 1 in X, include ˆβ0in\\nthe vector of coeﬃcients ˆβ, and then write the linear model in vector form\\nas an inner product\\nˆY=XTˆβ, (2.2)')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.get_relevant_documents(\"What is linear regression?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "110a6bb6-897b-44cf-aa35-4fbee8ad7f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Linear regression is a statistical method used to model the relationship between a dependent variable (Y) and one or more independent variables (X). In linear regression, the relationship is assumed to be linear, and the goal is to find the best-fitting line that describes how the dependent variable changes as the independent variable(s) change. The coefficients in the linear regression equation represent the impact of the independent variables on the dependent variable. The most common method used to estimate these coefficients is the least squares method, which minimizes the sum of the squared differences between the observed values and the values predicted by the linear model.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "chain.run(\"What is linear regression?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eba2d1d2-fb0a-4104-bd57-5032fcb106fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pillow_heif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b5877d-a7ab-48c7-af63-fdce144f1eb0",
   "metadata": {},
   "source": [
    "## Parsing a multimodal document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de5814-8267-4532-8137-ccd37b986c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "path = \"multimodal/\"\n",
    "\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=path + \"LLAVA.pdf\",\n",
    "    extract_images_in_pdf=True,\n",
    "    infer_table_structure=True,\n",
    "    chunking_strategy=\"by_title\",\n",
    "    max_characters=4000, \n",
    "    new_after_n_chars=3800, \n",
    "    combine_text_under_n_chars=2000,\n",
    "    image_output_dir_path=path + 'images/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a6a7c53-a609-448c-8917-7ade15a36925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_pdf_elements[20].text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8d72539-c7dc-4902-b072-c4a56c36f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_elements = []\n",
    "# text_elements = []\n",
    "# for element in raw_pdf_elements:\n",
    "#     if element.category == \"Table\":\n",
    "#         table_elements.append(element.text)\n",
    "#     elif element.category == \"CompositeElement\":\n",
    "#         text_elements.append(element.text)\n",
    "\n",
    "# print(len(table_elements))\n",
    "# print(len(text_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7fa33-3a78-4085-8044-5e641ebe520c",
   "metadata": {},
   "source": [
    "## Summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2be12266-cef4-42f1-b222-961a2167291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"\"\"You are an assistant tasked with summarizing tables and text. \n",
    "# Give a concise summary of the table or text.\n",
    "\n",
    "# Table or text chunk: {element}\n",
    "# \"\"\" \n",
    "\n",
    "# model = ChatOpenAI(temperature=0, model_name='gpt-4')\n",
    "# summarize_chain = LLMChain.from_string(\n",
    "#     llm=model,\n",
    "#     template=prompt\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0b10f-5855-47e4-a7fa-051d25b2c41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_summaries = summarize_chain.batch(table_elements)\n",
    "# text_summaries = summarize_chain.batch(text_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5bd33-3fba-46f8-b184-92f32befe303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5ab5f-2fc4-4a81-b5d3-9be71165b0e2",
   "metadata": {},
   "source": [
    "## Images with LLAVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58357c9-d2fa-4154-96c7-8bfa44937aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# # Define the directory containing the images\n",
    "# IMG_DIR=~/Projects/Teaching/Introduction_Langchain/projects/live/multimodal/images/\n",
    "# TEXT_DIR=~/Projects/Teaching/Introduction_Langchain/projects/live/multimodal/text/\n",
    "\n",
    "# # Loop through each image in the directory\n",
    "# for img in \"${IMG_DIR}\"*.jpg; do\n",
    "#     # Extract the base name of the image without extension\n",
    "#     base_name=$(basename \"$img\" .jpg)\n",
    "\n",
    "#     # Define the output file name based on the image name\n",
    "#     output_file=\"${TEXT_DIR}${base_name}.txt\"\n",
    "\n",
    "#     # Execute the command and save the output to the defined output file\n",
    "#     ~/Projects/Teaching/Introduction_Langchain/projects/live/multimodal/llama.cpp/build/bin/llava \\\n",
    "#     -m ~/Projects/Teaching/Introduction_Langchain/projects/live/multimodal/llama.cpp/models/ggml-model-q5_k.gguf \\\n",
    "#     --mmproj ~/Projects/Teaching/Introduction_Langchain/projects/live/multimodal/llama.cpp/models/mmproj-model-f16.gguf \\\n",
    "#     --temp 0.1 \\\n",
    "#     -p \"Describe the image in detail. Be specific about graphs, such as bar plots.\" \\\n",
    "#     --image \"$img\" > \"$output_file\"\n",
    "\n",
    "# done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3d2e8-1dbf-4b25-ad66-2bdcae2ec62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# from PIL import Image\n",
    "\n",
    "# text_path = \"multimodal/text/\"\n",
    "# images_path = \"multimodal/images/\"\n",
    "\n",
    "# text_list = sorted(glob.glob(text_path + \"*.txt\"))\n",
    "# img_list = sorted(glob.glob(images_path + \"*.jpg\"))\n",
    "\n",
    "# logging_header=\"clip_model_load: total allocated memory: 201.27 MB\\n\\n\"\n",
    "# appendix='main: image encoded in'\n",
    "\n",
    "# # Read each file and store its content in a list\n",
    "# img_summaries = []\n",
    "# for i, text_path in enumerate(text_list):\n",
    "#     with open(text_path, 'r') as file:\n",
    "#         summary = file.read()\n",
    "    \n",
    "#     summary = summary.split(logging_header, 1)[1].strip()\n",
    "#     summary = summary.split(appendix, 1)[0].strip()\n",
    "    \n",
    "#     img_path = img_list[i]\n",
    "#     img = Image.open(img_path)\n",
    "    \n",
    "#     img_summaries.append({\n",
    "#         'summary': summary,\n",
    "#         'image': img\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2caf5a-fc88-46a7-809e-3b1edeec21a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "\n",
    "# for img_dict in img_summaries:\n",
    "#     display(img_dict['image'])\n",
    "#     print(img_dict['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0442b4-dd82-47d9-a9c0-57c6f82f4cad",
   "metadata": {},
   "source": [
    "## Index data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "393ff591-277c-41d4-94c5-1ebdc1a43d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_docs(text_list, ids):\n",
    "#     return [\n",
    "#         Document(\n",
    "#             page_content=s, \n",
    "#             metadata={id_key: ids[i]}\n",
    "#         ) for i, s in enumerate(text_list)\n",
    "#     ]\n",
    "\n",
    "# doc_ids = [str(uuid.uuid4()) for _ in text_summaries]\n",
    "# text_docs = get_docs(\n",
    "#     [t['element'] for t in text_summaries], \n",
    "#     doc_ids\n",
    "# )\n",
    "# summary_text_docs = get_docs(\n",
    "#     [t['text'] for t in text_summaries], \n",
    "#     doc_ids\n",
    "# )\n",
    "\n",
    "# table_ids = [str(uuid.uuid4()) for _ in table_summaries]\n",
    "# table_docs = get_docs(\n",
    "#     [t['element'] for t in table_summaries], \n",
    "#     table_ids\n",
    "# )\n",
    "# summary_table_docs = get_docs(\n",
    "#     [t['text'] for t in table_summaries], \n",
    "#     table_ids\n",
    "# )\n",
    "\n",
    "# img_ids = [str(uuid.uuid4()) for _ in img_summaries]\n",
    "# img_summary_docs = get_docs(\n",
    "#     [i['summary'] for i in img_summaries], \n",
    "#     img_ids\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7abc60c3-8981-4fad-b489-09326cf7ca70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = Chroma(\n",
    "#     collection_name=\"llava_pdf\",\n",
    "#     embedding_function=OpenAIEmbeddings()\n",
    "# )\n",
    "\n",
    "# store = InMemoryStore()\n",
    "\n",
    "# retriever = MultiVectorRetriever(\n",
    "#     vectorstore=vectorstore, \n",
    "#     docstore=store, \n",
    "#     id_key=id_key,\n",
    "# )\n",
    "\n",
    "# retriever.vectorstore.add_documents(summary_text_docs)\n",
    "# retriever.docstore.mset(list(zip(doc_ids, text_docs)))\n",
    "\n",
    "# retriever.vectorstore.add_documents(summary_table_docs)\n",
    "# retriever.docstore.mset(list(zip(table_ids, table_docs)))\n",
    "\n",
    "# retriever.vectorstore.add_documents(img_summary_docs)\n",
    "# retriever.docstore.mset(list(zip(img_ids, img_summary_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3adece-c436-4c0d-8455-d6d553a39dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.vectorstore.similarity_search(\"What is specific about LLava?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2af510cd-2521-485d-825c-978e46df4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.get_relevant_documents(\"What is specific about LLava?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a107c7-b596-4256-be9f-163aaa777752",
   "metadata": {},
   "source": [
    "## Multimodal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20237f49-a5fc-487d-97e7-757d25c1460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(temperature=0, model_name='gpt-4')\n",
    "\n",
    "# chain = RetrievalQA.from_chain_type(\n",
    "#     llm=llm, \n",
    "#     retriever=retriever,\n",
    "#     verbose=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2909e330-7c2b-406a-bc29-b142fc6fd2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.run('What makes LLava different GPT-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f685a0c8-ead9-42dc-9b54-2486979cedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain.run('What is the architecture of the LLava model?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547c3e0-9f26-428e-bfc6-9e217752d38d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
